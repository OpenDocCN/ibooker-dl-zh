- en: 3 Selecting and evaluating AI tools
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 选择和评估人工智能工具
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Distinctions among different types of AI, or ways of using AI, and how to select
    the most appropriate one
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型人工智能或使用人工智能的方式之间的区别，以及如何选择最合适的一种
- en: How to assess AI’s performance and select models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何评估人工智能的性能并选择模型
- en: Common ways to measure AI’s performance at a task
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量人工智能在任务中性能的常见方法
- en: 'This chapter provides guidance on selecting an AI model or tool and assessing
    its performance at a given task. We kick off by discussing three common distinctions
    between different types of AI: proprietary versus open source AI, off-the-shelf
    versus fine-tuned AI, and AI apps versus foundation models. We explain what these
    mean and how to pick the most suitable type. Afterward, we discuss a common process
    to assess AI’s performance, which uses different datasets for validation and testing.
    We also discuss some common performance measures such as accuracy. The appendix
    includes a catalog of popular generative AI tools.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了关于选择人工智能模型或工具以及评估其在特定任务中性能的指导。我们首先讨论了不同类型人工智能的三个常见区别：专有与开源人工智能、现成与微调人工智能，以及人工智能应用与基础模型。我们解释了这些含义以及如何选择最合适类型。之后，我们讨论了评估人工智能性能的常见流程，该流程使用不同的数据集进行验证和测试。我们还讨论了一些常见的性能指标，如准确率。附录包括流行生成式人工智能工具的目录。
- en: Proprietary vs. open source
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专有与开源
- en: In proprietary AI, the user isn’t allowed to modify or even see the code that
    powers the underlying ML models. The inner workings of the technology are kept
    secret to prevent others from copying it. One common way of using proprietary
    AI is through customer-facing apps such as ChatGPT. These tend to charge users
    a monthly subscription to access the service, although some provide a free tier
    that grants access to a reduced number of features.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在专有人工智能中，用户不允许修改甚至查看驱动底层机器学习模型的代码。该技术的内部运作被保密，以防止他人复制。使用专有人工智能的一种常见方式是通过面向客户的软件，如ChatGPT。这些通常向用户收取月度订阅费以访问服务，尽管一些提供免费层，允许访问较少的功能。
- en: Another common way of using proprietary AI is via APIs. These let users interact
    with AI programmatically to build apps that utilize it. The AI software runs on
    a remote server behind closed doors, so the user can’t see the code. APIs are
    typically billed based on usage (e.g., the number of input and output tokens).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种使用专有人工智能的常见方式是通过API。这些API允许用户以编程方式与人工智能交互，以构建利用它的应用程序。人工智能软件在远程服务器上运行，因此用户看不到代码。API通常根据使用情况（例如，输入和输出令牌的数量）计费。
- en: In contrast, in open source AI, the provider publicly discloses the internal
    details of the ML model, including the code to use it and the values of all the
    model’s parameters. The user is often authorized to modify or customize the model
    if needed. In addition, users can self-host these models using their own infrastructure;
    for example, you can download a copy of the model to your local computer or your
    own cloud computing instance and run the code yourself. This doesn’t mean you
    *must* self-host the model as it may also be available through APIs, but you have
    the option to self-host it. An example of open source AI is the family of Llama
    models produced by Meta, which are openly available for download on multiple websites.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在开源人工智能中，提供商公开披露了机器学习模型的内部细节，包括使用它的代码和所有模型参数的值。如果需要，用户通常被授权修改或定制模型。此外，用户可以使用自己的基础设施自行托管这些模型；例如，您可以将模型的副本下载到您的本地计算机或自己的云计算实例上并自行运行代码。这并不意味着您*必须*自行托管模型，因为它也可能通过API提供，但您有选择自行托管它的选项。开源人工智能的一个例子是Meta生产的Llama模型系列，这些模型在多个网站上公开提供下载。
- en: 'Open source AI is sometimes not quite as open as it may sound. For starters,
    their manufacturers don’t disclose the data used to train these models. So, while
    you can see the parameters of the final model, you’d be unable to train that exact
    model yourself as you wouldn’t know which data to use. Mistral AI, a company that
    provides open source AI, explains ([https://mng.bz/rKQy](https://mng.bz/rKQy)):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 开源人工智能有时并不像听起来那么开放。首先，它们的制造商不会披露用于训练这些模型的数据。因此，虽然您可以查看最终模型的参数，但您无法自己训练那个精确的模型，因为您不知道要使用哪些数据。提供开源人工智能的Mistral
    AI公司解释说（[https://mng.bz/rKQy](https://mng.bz/rKQy)）：
- en: We do not communicate on our training datasets. We keep proprietary some intermediary
    assets (code and resources) required to produce both the Open-Source models and
    the Optimized models. Among others, this involves the training logic for models,
    and the datasets used in training.
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们不公开我们的训练数据集。我们保留一些中间资产（代码和资源）的专有性，这些资产是生产开源模型和优化模型所必需的。其中还包括模型的训练逻辑和用于训练的数据集。
- en: Note that, just like proprietary models, open source models are improved (or
    aligned) by using reinforcement learning with human feedback (see chapter 1).
    This is performed using data created manually by human labelers, which remains
    undisclosed in most cases.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，就像专有模型一样，开源模型是通过使用带有人类反馈的强化学习（见第1章）来改进（或对齐）的。这通常是通过由人工标注员手动创建的数据来完成的，在大多数情况下这些数据保持未公开。
- en: The licenses to use open source AI often come with restrictions. For example,
    you are not allowed to use a Llama model—even your own copy—for an app with more
    than 700 million monthly users (see [https://mng.bz/VVoG](https://mng.bz/VVoG)).
    In that case, you would have to discuss licensing options with Meta, and you may
    be asked to pay. Moreover, you’re not allowed to use a Llama model or its outputs
    to improve other LLMs; in other words, you can’t use Llama to build products that
    compete with it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源AI的许可证通常带有限制。例如，你不允许在一个每月有超过7亿用户的APP中使用Llama模型——即使是你的副本（见[https://mng.bz/VVoG](https://mng.bz/VVoG)）。在这种情况下，你将不得不与Meta讨论许可选项，并且可能需要付费。此外，你不允许使用Llama模型或其输出以提高其他LLMs；换句话说，你不能使用Llama来构建与其竞争的产品。
- en: Building large ML models is expensive, so the most powerful open source AI is
    built by for-profit companies that charge or intend to charge for services. These
    services often include consulting or access to premium, proprietary models.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 构建大型ML模型成本高昂，因此最强大的开源AI是由盈利公司构建的，这些公司收取或打算收取服务费用。这些服务通常包括咨询或访问高级专有模型。
- en: How to decide
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何决定
- en: Proprietary AI is most suitable when you need a done-for-you solution. Using
    proprietary AI doesn’t usually require specialized knowledge, such as machine
    learning, coding, and DevOps.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 专有AI最适合当你需要一个现成的解决方案时。使用专有AI通常不需要专业知识，例如机器学习、编码和DevOps。
- en: One of the main reasons to use open source AI is to be able to self-host it
    (run it on your own servers), which can provide better transparency and governance,
    as you have full visibility over the code and full control over which data exits
    the organization. Your company may not want to send any sensitive data to a third
    party, such as OpenAI, or it may want to audit the code to ensure it doesn’t do
    anything it’s not supposed to.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源AI的一个主要原因是能够自行托管它（在自己的服务器上运行），这可以提供更好的透明度和治理，因为你可以完全了解代码，并完全控制哪些数据离开组织。你的公司可能不想将任何敏感数据发送给第三方，例如OpenAI，或者它可能想审计代码以确保它不会做它不应该做的事情。
- en: The cost of self-hosting AI, however, tends to be higher than paying for APIs,
    as you need to maintain the required infrastructure, so it is usually not cost-effective
    unless done at a very large scale. You also need to be very careful—malicious
    open source models have been published in the past that executed unintended code
    in the user’s machine (see [https://mng.bz/xKeX](https://mng.bz/xKeX)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自托管AI的成本往往高于支付API的费用，因为你需要维护所需的基础设施，所以除非在非常大的规模上操作，否则通常不具有成本效益。你还需要非常小心——过去已经发布了恶意开源模型，这些模型在用户的机器上执行了意外的代码（见[https://mng.bz/xKeX](https://mng.bz/xKeX)）。
- en: Another reason to use open source AI is customization. If you want to modify
    a model (e.g., by fine-tuning it, which is covered in the next section), then
    open source AI lets you do so most freely. Table 3.1 summarizes the best uses
    of proprietary and open source AI.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用开源AI的另一个原因是定制。如果你想修改一个模型（例如，通过微调它，这在下一节中介绍），那么开源AI让你可以最自由地这样做。表3.1总结了专有AI和开源AI的最佳用途。
- en: Table 3.1  Proprietary vs. open source AI
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1  专有AI与开源AI的比较
- en: '| Proprietary AIBest for . . . | Open source AIBest for . . . |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 专有AI最佳用途 | 开源AI最佳用途 |'
- en: '| --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| • Done-for-you solution• Easy start• No specialized knowledge required• Small-scale
    use, in which pay-as-you-go AI is cheaper than maintaining your own infrastructure
    | • Self-hosting so that you enjoy better governance and transparency• Large-scale
    use, in which maintaining your own infrastructure is cheaper than pay-as-you-go
    AI• Model customization (e.g., fine-tuning) |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| • 一站式解决方案• 易于开始• 无需专业知识• 小规模使用，其中按需付费的AI比维护自己的基础设施更便宜 | • 自托管以享受更好的治理和透明度•
    大规模使用，其中维护自己的基础设施比按需付费的AI更便宜• 模型定制（例如，微调） |'
- en: In terms of the quality of outputs, proprietary AI used to hold an edge over
    open source AI. However, the gap has been narrowing, and many people claim that
    open source AI is already or will soon be as capable as its proprietary counterparts.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出质量方面，专有AI曾经比开源AI有优势。然而，差距正在缩小，许多人声称开源AI已经或很快将与其专有对应物一样强大。
- en: Off-the-shelf vs. fine-tuning
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 现成的模型与微调
- en: 'When it comes to improving the performance of generative AI at a certain task,
    there are two main schools of thought. One of them is using off-the-shelf models—without
    any alterations—and it relies on prompt engineering techniques to make them more
    performant and customized to your intended task. For example, it has become popular
    to include a few demonstrations of how to perform a task inside the prompt, which
    is known as *few-shot prompting* (as opposed to *zero-shot prompting* in which
    you don’t provide any examples). This helps disambiguate the request. Researchers
    from OpenAI argued ([https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165)):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当提到提高生成式AI在特定任务上的性能时，存在两种主要的思想流派。其中之一是使用现成的模型——没有任何修改——并且它依赖于提示工程技术来提高其性能并使其更适合你的目标任务。例如，在提示中包含一些如何执行任务的演示已经变得很流行，这被称为*少样本提示*（与不提供任何示例的*零样本提示*相对）。这有助于消除歧义。来自OpenAI的研究人员争论([https://arxiv.org/pdf/2005.14165](https://arxiv.org/pdf/2005.14165))：
- en: If someone is asked to “make a table of world records for the 200m dash”, this
    request can be ambiguous, as it may not be clear exactly what format the table
    should have or what should be included (and even with careful clarification, understanding
    precisely what is desired can be difficult).
  id: totrans-27
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果有人被要求“制作200米短跑的世界纪录表”，这个请求可能是模糊的，因为它可能不清楚表格应该有什么格式或应该包含什么内容（即使经过仔细的澄清，精确理解所需的内容也可能很困难）。
- en: The researchers went on to show that including a few examples of how to perform
    the task within the prompt steered the LLM in the right direction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员继续表明，在提示中包含一些如何执行任务的示例可以引导LLM走向正确的方向。
- en: In addition, the RAG approach (see chapter 1) has become a popular way of providing
    the LLM with a large amount of contextual information to help it perform a task.
    The increasingly large context window of state-of-the-art LLMs has made RAG particularly
    effective.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，RAG方法（见第1章）已成为向LLM提供大量上下文信息以帮助其执行任务的一种流行方式。最先进的LLM的日益增大的上下文窗口使得RAG特别有效。
- en: Improved prompts can help customize image generation. For example, the image
    generator Midjourney lets users upload images as part of their prompts to indicate
    the desired style of the generated images.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的提示可以帮助定制图像生成。例如，图像生成器Midjourney允许用户将图像作为他们提示的一部分上传，以指示生成图像所需的风格。
- en: The other school of thought suggests altering the model to make it more suitable
    for the intended task, which is known as *fine-tuning.* The model’s internal parameters
    are adjusted, so you utilize an altered copy of the original model to generate
    your outputs.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思想流派建议修改模型以使其更适合目标任务，这被称为*微调*。模型的内部参数被调整，因此你使用修改后的原始模型的副本来生成你的输出。
- en: Fine-tuning requires training data, which is used to continue the training of
    the original model for a little longer. For example, to fine-tune an LLM, you
    must create a sample of text in your intended style. This data is fed to the training
    algorithm to refine the LLM. The amount of data used for fine-tuning is usually
    much smaller compared to the data used to train the original LLM—you may need
    just a handful of documents to do so. Open source models are ideal for fine-tuning
    as you have access to the entire model with its parameters, and you can then alter
    the parameters to better suit your needs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 微调需要训练数据，这些数据用于继续对原始模型进行更长时间的训练。例如，为了微调一个LLM，你必须创建一个符合你预期风格的文本样本。这些数据被输入到训练算法中，以细化LLM。用于微调的数据量通常比用于训练原始LLM的数据量小得多——你可能只需要几份文档就能做到这一点。开源模型非常适合微调，因为你可以访问整个模型及其参数，然后你可以调整参数以更好地满足你的需求。
- en: Perhaps the biggest challenge of fine-tuning is overdoing it—if you specialize
    your model too much on your fine-tuning training data, it might end up memorizing
    specific examples present in the data and not perform well with other instances.
    This is known as *overfitting*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 微调可能面临的最大挑战可能是过度调整——如果你在微调训练数据上过度专门化你的模型，它可能会记住数据中存在的特定示例，而无法在其他实例上表现良好。这被称为*过拟合*。
- en: There are a handful of techniques to prevent overfitting (see the sidebar).
    You need to be mindful of these techniques and configure the fine-tuning algorithm
    appropriately to prevent overfitting. We’ll discuss later in this chapter how
    you can use validation and test sets to evaluate and compare different AI models,
    which can help select the best strategy to fine-tune a model and ensure the final
    model hasn’t overfitted the data.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种技术可以防止过拟合（见侧边栏）。你需要注意这些技术，并适当地配置微调算法以防止过拟合。我们将在本章后面讨论如何使用验证集和测试集来评估和比较不同的AI模型，这有助于选择最佳的微调策略，并确保最终模型没有过拟合数据。
- en: Techniques to control overfitting
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 控制过拟合的技术
- en: '*Early stopping**—*You train the model on your fine-tuning data only for a
    few iterations. You stop once performance stops improving, as measured on a separate
    piece of data (called the *validation set*).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*提前停止**—*你只在微调数据上训练模型几次迭代。一旦性能不再提高（在单独的数据上衡量，称为*验证集*），你就停止训练。'
- en: '*Limited scope of updates**—*You only allow some parts of the model to be updated.
    For example, one popular method called LoRA inserts small layers with new learnable
    parameters into the model, while keeping its original parameters intact.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*更新范围有限**—*你只允许模型的一些部分被更新。例如，一种名为LoRA的流行方法将带有新可学习参数的小层插入到模型中，同时保持其原始参数不变。'
- en: '*Regularization**—*You add a term to the loss function that penalizes too high
    or too low parameter values. This reduces the risk of overfitting by preventing
    parameters from being overly specialized to specific training examples.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*正则化**—*你会在损失函数中添加一个项，惩罚过高的或过低的参数值。这通过防止参数过度专门化到特定的训练示例来降低过拟合的风险。'
- en: '*Dropout**—*Pieces of the model are randomly removed on each iteration of the
    training process, which prevents internal units of the model from overly specializing
    to the training examples.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout**—*在训练过程的每次迭代中，随机移除模型的一部分，这防止了模型的内部单元过度专门化到训练示例。'
- en: A method known as LoRA has become popular for fine-tuning (see [https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)).
    LoRA inserts small layers with new learnable parameters to adjust the existing
    model, instead of modifying its original parameters. This makes fine-tuning faster
    as few parameter updates must be calculated on each iteration. It also helps control
    overfitting as you only modify a limited number of parameters (see “Limited scope
    of updates” in the sidebar).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一种称为LoRA的方法已成为微调（见[https://arxiv.org/abs/2106.09685](https://arxiv.org/abs/2106.09685)）的流行方法。LoRA通过插入带有新可学习参数的小层来调整现有模型，而不是修改其原始参数。这使得微调更快，因为每个迭代只需要计算少量参数更新。它还有助于控制过拟合，因为你只修改了有限数量的参数（见侧边栏中的“更新范围有限”）。
- en: The libraries developed by Hugging Face are very popular for fine-tuning existing
    models ([https://huggingface.co/docs/trl/main/en/index](https://huggingface.co/docs/trl/main/en/index)).
    Hugging Face also contains a large inventory of open source models you can fine-tune.
    Many users run their fine-tuning using Jupyter notebooks connected to cloud-computing
    instances. Google Collab is particularly commonly used for this, as it provides
    easy-to-access notebooks and lets you use some of its computing power for free,
    which might be enough to fine-tune some models.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face 开发的库非常受欢迎，用于微调现有模型（[https://huggingface.co/docs/trl/main/en/index](https://huggingface.co/docs/trl/main/en/index)）。Hugging
    Face 还包含大量开源模型，您可以对其进行微调。许多用户通过连接到云计算实例的 Jupyter 笔记本进行微调。Google Collab 特别常用，因为它提供了易于访问的笔记本，并允许您免费使用其部分计算能力，这可能足以微调一些模型。
- en: Fine-tuning requires some specialized machine learning knowledge, so I recommend
    you learn the basics of ML to get it right. You might also require infrastructure
    to run the fine-tuning process, and you’ll then have to use your own customized
    copy of the model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 微调需要一些专门的机器学习知识，所以我建议您学习ML的基础知识以正确进行。您可能还需要基础设施来运行微调过程，然后您将不得不使用自己定制的模型副本。
- en: In some cases, it is also possible to fine-tune proprietary AI. For example,
    OpenAI lets you upload your own fine-tuning dataset and create a fine-tuned version
    of its models, which you can access through the API. The company charges a premium
    for using fine-tuned models compared to using OpenAI’s original models. The process
    is friendly, although not as customizable as fine-tuning open source models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，也可以微调专有AI。例如，OpenAI 允许您上传自己的微调数据集并创建其模型的微调版本，您可以通过API访问。与使用 OpenAI 的原始模型相比，公司对使用微调模型收取额外费用。该过程友好，尽管不如微调开源模型可定制。
- en: How to decide
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何决定
- en: Prompt engineering is the most straightforward way of improving a model’s performance.
    Common advice is that it’s the first thing you should try (check out [https://mng.bz/AQZx](https://mng.bz/AQZx)
    and [https://mng.bz/ZlQA](https://mng.bz/ZlQA) for more info). As context windows
    have become large, prompts can be quite rich. So, it is often advisable to use
    fine-tuning as a last resort when the output still isn’t quite what you expect,
    even after trying multiple ways of improving the prompts. Note, however, that
    prompt engineering works best with the most advanced and costly models, as they
    can adapt better to a wider range of tasks and fit longer prompts within their
    context windows. Table 3.2 compares off-the-shelf with fine-tuned AI.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是提高模型性能的最直接方法。常见的建议是，这是您应该尝试的第一件事（更多信息请参阅[https://mng.bz/AQZx](https://mng.bz/AQZx)
    和 [https://mng.bz/ZlQA](https://mng.bz/ZlQA)）。随着上下文窗口变得很大，提示可以相当丰富。因此，当尝试了多种改进提示的方法后，输出仍然不是您期望的那样时，通常建议将微调作为最后的手段。然而，请注意，提示工程与最先进且成本最高的模型结合得最好，因为它们可以更好地适应更广泛的任务，并在它们的上下文窗口中适应更长的提示。表3.2比较了现成的AI与微调AI。
- en: Table 3.2  Off-the-shelf vs. fine-tuned AI
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.2  现成的AI与微调AI的比较
- en: '| Off-the-shelf AIBest when . . . | Fine-tuned AIBest when . . . |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 现成的AI最佳使用场景 | 微调AI最佳使用场景 |'
- en: '| --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| • Prompt engineering techniques work well.• It is okay to use proprietary
    AI.• You can afford large models.• You prioritize ease of use. | • You want highly
    customized outputs, and you’ve exhausted other options.• You need to use smaller
    models (for example, for self-hosting them).• You have ML expertise and access
    to computing resources. |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| • 提示工程技巧效果良好。• 使用专有AI是可以的。• 您可以负担得起大型模型。• 您优先考虑易用性。 | • 您希望获得高度定制的输出，并且已经用尽了其他选项。•
    您需要使用较小的模型（例如，用于自托管）。• 您拥有机器学习专业知识和访问计算资源。 |'
- en: Fine-tuning can be a good choice for smaller models, for example, because you
    want to reduce your costs. This is particularly relevant when you must self-host
    your own models. In this case, using a small, fine-tuned model might be more effective
    than using prompt engineering with a larger model.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的模型，微调可能是一个不错的选择，例如，因为您想降低成本。这在您必须自托管自己的模型时尤其相关。在这种情况下，使用小型、微调的模型可能比使用大型模型的提示工程更有效。
- en: Customer-facing AI apps vs. foundation models
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向客户的AI应用与基础模型
- en: Customer-facing AI apps help final customers perform tasks. These include general-purpose
    commercial chatbots such as ChatGPT and special-purpose apps such as GitHub Copilot
    and Cursor, which help software engineers write code.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 面向客户的AI APP帮助最终客户完成任务。这些包括通用商业聊天机器人，如ChatGPT，以及特殊用途的APP，如GitHub Copilot和Cursor，这些APP帮助软件工程师编写代码。
- en: In contrast, foundation models are large, multipurpose AI models. These models
    are used behind the scenes to power customer-facing apps. For example, foundation
    models such as GPT-4o are used to power customer-facing ChatGPT.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基础模型是大型、多用途的AI模型。这些模型在幕后用于为面向客户的APP提供动力。例如，GPT-4o等基础模型被用于为面向客户的ChatGPT提供动力。
- en: Some companies build both customer-facing apps and provide access to their underlying
    foundation models through APIs so that software developers can build their own
    apps on top.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一些公司既构建面向客户的APP，又通过API提供其底层基础模型的访问权限，以便软件开发者可以在其上构建自己的APP。
- en: How to decide
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何决定
- en: Customer-facing apps are the most suitable choice when you want AI to assist
    you in performing a specific task, as they’re friendly to use and particularly
    tailored to the task. Foundation models are best used as a building block when
    you want to create your own app based on powerful AI. Table 3.3 compares customer-facing
    AI apps with foundation models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当您希望AI协助您完成特定任务时，面向客户的APP是最合适的选择，因为它们易于使用，并且特别针对任务进行了定制。基础模型在您希望基于强大的AI创建自己的APP时，最好用作构建块。表3.3比较了面向客户的AI
    APP与基础模型。
- en: Table 3.3  Customer-facing AI apps vs. foundation models
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3  面向客户的AI APP与基础模型
- en: '| Customer-facing AI appsSuitable for . . . | Foundation modelsSuitable for
    . . . |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 面向客户的AI APP适用于... | 基础模型适用于... |'
- en: '| --- | --- |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| • Assistance with a specific task• End users | • Powering AI-based apps•
    Engineers |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| • 特定任务的协助• 最终用户 | • 支持基于AI的APP• 工程师 |'
- en: Model validation, selection, and testing
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型验证、选择和测试
- en: If you want to accurately compare and select AI models, it’s a good idea to
    build a benchmark to assess their respective performances. Also, for reasons that
    will become apparent soon, we often overestimate machine learning’s performance,
    so it’s good to follow a well-designed assessment process to prevent bad surprises.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想准确比较和选择AI模型，建立一个基准来评估它们的各自性能是个好主意。此外，由于一些原因将在后面变得明显，我们经常高估机器学习的性能，因此遵循一个精心设计的评估过程以避免意外是个好主意。
- en: This section describes the ideal protocol to evaluate AI’s performance at a
    task. In this protocol, AI models are built and evaluated using three different
    collections of data, known as *datasets.* In the following, we describe the role
    of each type of dataset and how it should be used.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了评估AI在任务上性能的理想协议。在这个协议中，AI模型使用三种不同的数据集进行构建和评估，这些数据集被称为*数据集*。以下，我们将描述每种类型数据集的作用以及如何使用它们。
- en: Training set
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练集
- en: The training set is the dataset used to build the model. It contains a large
    collection of examples of how to perform the task. For example, for image generation,
    it comprises numerous images paired with captions that describe their content.
    For text generation, it comprises a large amount of text. A much smaller training
    set is also used to fine-tune a model.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集是用于构建模型的数据库。它包含大量如何执行任务的示例。例如，对于图像生成，它包括大量与描述其内容的标题配对的图像。对于文本生成，它包括大量文本。还使用了一个较小的训练集来微调模型。
- en: During training or fine-tuning, the training algorithm tries to find model parameters
    that minimize the loss on the training set (see chapter 1). The loss is a mathematical
    function that quantifies how far off the model is from performing the required
    task well, such as predicting the next token in the case of LLMs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练或微调期间，训练算法试图找到最小化训练集损失的模型参数（见第1章）。损失是一个数学函数，它量化了模型在执行所需任务时的偏差程度，例如在LLMs的情况下预测下一个标记。
- en: The loss is usually designed to have nice mathematical properties, such as differentiability,
    so it’s not always the most intuitive way of understanding a model’s performance.
    In addition, the loss does not always quantify how good the model is at your intended
    task. For example, if you want to use AI to solve coding problems, the training
    loss does not explicitly quantify its coding abilities; instead, it quantifies
    how well it autocompletes text, which is only indirectly related to coding abilities.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 损失通常被设计成具有良好的数学属性，例如可微性，因此它不总是理解模型性能的最直观方式。此外，损失并不总是量化模型在你期望的任务上的表现。例如，如果你想使用人工智能来解决编码问题，训练损失并不明确量化其编码能力；相反，它量化了它自动完成文本的能力，这仅与编码能力间接相关。
- en: You don’t have to worry much about creating a training set unless you’re fine-tuning
    a model or training one from scratch. However, you might need to be mindful of
    what data was used for training when creating the validation and test sets (more
    on this in a minute).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你正在微调模型或从头开始训练模型，否则你不必过于担心创建训练集。然而，在创建验证集和测试集时，你可能需要注意用于训练的数据（关于这一点，稍后还会详细介绍）。
- en: Validation set
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 验证集
- en: The validation set is used to compare the performance of different models. For
    example, you could use a validation set to compare the performance of GPT-4o and
    Llama 3 at performing a task. This helps you pick the best model, which is known
    as *model selection.*
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集用于比较不同模型的性能。例如，你可以使用验证集来比较 GPT-4o 和 Llama 3 在执行任务时的性能。这有助于你选择最佳模型，这被称为*模型选择*。
- en: The performance on the validation set is usually calculated using a measure
    close to your actual business objective. For example, you could calculate how
    often the model solves coding problems correctly. Note this is often different
    from the loss function used for training or fine-tuning the model. There’s a list
    of common performance measures later in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集上的性能通常使用接近你实际业务目标的度量来计算。例如，你可以计算模型正确解决编码问题的频率。注意，这通常与用于训练或微调模型的损失函数不同。本章后面将列出常见的性能度量。
- en: It’s important that data in the validation set is not present inside the training
    set. Otherwise, you might overestimate the model’s performance. This is because
    a poor model that overfits the training data (it memorizes specific instances)
    may go undetected, as some of the memorized data will also appear in the validation
    set it’s evaluated on. If the validation set is included in the training set,
    it’s a bit like an exam that contains questions present verbatim in the textbook—students
    could memorize answers without genuinely learning and pass the exam.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 确保验证集中的数据不在训练集中出现非常重要。否则，你可能会高估模型的性能。这是因为一个过度拟合训练数据（它记住了特定实例）的较差模型可能不会被检测到，因为一些被记住的数据也可能出现在它所评估的验证集中。如果验证集包含在训练集中，这就像是一场包含教科书上直接出现的问题的考试——学生可以记住答案而不真正学习并通过考试。
- en: You need to be particularly careful about this when using LLMs as they’re trained
    on a huge amount of publicly available data that includes solutions to many problems.
    Suppose you want to use an LLM to help you solve crossword puzzles. You create
    a validation set by gathering clues from real *New York Times* crosswords published
    in the past. You then count how often the LLM identifies the right word based
    on the clues. The problem is that there are numerous websites that explicitly
    provide the solutions to all past *New York Times* crosswords, clue by clue. So,
    at least in theory, an LLM could memorize the exact solution to each past clue.
    Your validation data would thus assess the LLM’s performance at solving problems
    whose solution it had the answer to. A better way of doing this would be to create
    a validation set containing new clues that haven’t appeared in past puzzles. This
    way, the LLM wouldn’t be able to “cheat.” Alternatively, you could make sure that
    the puzzles in the validation set were published after the LLM’s training data
    cut-off date.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用大型语言模型（LLM）时，你需要特别注意这一点，因为它们是在包括许多问题解决方案在内的海量公开数据上训练的。假设你想使用LLM来帮助你解决填字游戏。你可以通过收集过去出版的真实《纽约时报》填字游戏的线索来创建一个验证集。然后，你统计LLM根据线索识别正确单词的频率。问题是，有众多网站明确提供了所有过去《纽约时报》填字游戏的解决方案，线索逐个列出。因此，至少在理论上，LLM可以记住每个过去线索的确切解决方案。因此，你的验证数据将评估LLM解决其已有答案的问题的性能。更好的方法是创建一个包含过去谜题中未出现的新线索的验证集。这样，LLM就无法“作弊”。或者，你可以确保验证集中的谜题是在LLM的训练数据截止日期之后出版的。
- en: The validation set can also be used to help you make high-level decisions when
    you’re training or fine-tuning your own model. For example, you can train two
    models with different numbers of layers or different learning rates (how much
    the model’s parameters are updated on every training iteration), and then pick
    the model with highest performance on the validation set. You could also use a
    validation set to compare different prompt engineering approaches.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集也可以在你训练或微调自己的模型时帮助你做出高级决策。例如，你可以训练两个具有不同层数或不同学习率（模型参数在每次训练迭代中更新的程度）的模型，然后在验证集上选择性能最高的模型。你也可以使用验证集来比较不同的提示工程方法。
- en: Test set
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试集
- en: Using a validation set is not enough to properly assess a model’s performance.
    Because you’re specifically selecting the model that works best on the validation
    set alone, you might get an overly optimistic idea of its performance. After all,
    you discarded the models that weren’t as good on that specific piece of data.
    What if the selected model only works well on the validation data by chance and
    is not a better model in general?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用验证集不足以正确评估模型的表现。因为你专门选择了在验证集上表现最好的模型，你可能会对其性能有一个过于乐观的看法。毕竟，你丢弃了在该特定数据上表现不佳的模型。如果选定的模型只是偶然在验证数据上表现良好，而不是一个更优秀的模型，那会怎样？
- en: So, after you’re done picking the best model using the validation set, you must
    perform a final check using *another* dataset, called the *test set.* The test
    set gives you an idea of the model’s performance on data that has genuinely never
    been used to make modeling decisions. This final assessment is a sanity check.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你使用验证集挑选出最佳模型之后，你必须使用*另一个*数据集进行最终检查，这个数据集被称为*测试集*。测试集让你了解模型在从未用于建模决策的数据上的性能。这种最终评估是一种理智的检查。
- en: The test set can only be used once. If after the test you find performance disappointing
    and want to update the model or consider alternatives, you must collect a new
    test set to perform a new assessment. Otherwise, you end up using the test set
    repeatedly for model selection, so it turns into a validation set.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集只能使用一次。如果在测试后发现性能令人失望，想要更新模型或考虑其他选择，你必须收集一个新的测试集来进行新的评估。否则，你将反复使用测试集进行模型选择，使其变成验证集。
- en: It is up to you to choose how thorough you want to be when following this process.
    I know of hedge funds that are very stringent about following it, as a lot of
    money is at stake. For example, they try not to even look at the data inside the
    test set to, say, plot a graph. This way, they prevent knowledge about the test
    data from creeping into modeling decisions, so the test data is as independent
    as possible.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循此过程时，您需要决定您希望有多彻底。我知道一些对遵循此过程非常严格的对冲基金，因为涉及的资金量很大。例如，他们甚至试图不去查看测试集中的数据，比如绘制图表。这样，他们可以防止关于测试数据的知识渗透到建模决策中，从而使测试数据尽可能独立。
- en: Performance measures
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能指标
- en: This section describes some common performance measures that can be used to
    evaluate AI’s performance at an intended task.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本节描述了一些可以用来评估AI在预期任务中性能的常见性能指标。
- en: Accuracy
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准确度
- en: '*Accuracy* is the percentage of tasks performed correctly. For example, 90%
    accuracy means that 9 out of 10 solutions are correct, as measured on the validation
    or test sets.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*准确度*是任务执行正确的百分比。例如，90%的准确度意味着在验证集或测试集上测量的解决方案中有9个是正确的。'
- en: Accuracy is commonly used for classification tasks. For example, it is often
    used to assess how good AI is at categorizing an image or detecting a tweet’s
    sentiment. You can also use it for other problem-solving tasks. For instance,
    you could use accuracy to measure an LLM’s ability to solve coding problems—you’d
    need to count the number of correctly solved problems and divide it by the total
    number of problems in your validation or test set.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度通常用于分类任务。例如，它经常用来评估AI在分类图像或检测推文的情感方面的好坏。您还可以将其用于其他问题解决任务。例如，您可以使用准确度来衡量一个LLM解决编码问题的能力——您需要计算正确解决的问题数量，并将其除以验证集或测试集中的总问题数。
- en: Precision and recall
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精确度和召回率
- en: In information retrieval, we are interested in identifying relevant instances
    out of a much larger pool. For example, a law firm may use a RAG approach to retrieve
    relevant legal cases, according to a query, from a large database of past cases.
    As another example, a bank may want to identify fraudulent transactions out of
    a (hopefully) much larger pool of transactions.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在信息检索中，我们感兴趣的是从大量数据中识别相关实例。例如，一家律师事务所可能使用RAG方法根据查询从大量过去的案例数据库中检索相关法律案例。作为另一个例子，一家银行可能希望从（希望是）更大的交易池中识别欺诈交易。
- en: Two common performance measures are recall and precision. However, as we’ll
    discuss in a minute, neither can be used by itself.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 两个常见的性能指标是召回率和精确度。然而，正如我们稍后将讨论的，这两个指标都不能单独使用。
- en: '*Recall* measures how many relevant instances are identified. For example,
    90% recall means that 9 out of 10 relevant instances are retrieved, the remaining
    being missed.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率*衡量识别出的相关实例数量。例如，90%的召回率意味着10个相关实例中有9个被检索到，其余的未被检索到。'
- en: '*Precision* measures how relevant the retrieved instances are. For example,
    90% precision means that 9 out of 10 retrieved instances are truly relevant, the
    remaining being irrelevant or a false positive.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度*衡量检索到的实例的相关性。例如，90%的精确度意味着10个检索到的实例中有9个确实是相关的，其余的要么是不相关的，要么是误报。'
- en: The challenge is that there is a tradeoff between precision and recall. Consider
    a system that retrieves too much stuff. For example, it could determine that almost
    every past legal case is relevant to every query. This system would achieve very
    high recall, perhaps close to 100%. However, it would be plagued with false positives,
    so its precision would be very low.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战在于精确度和召回率之间存在权衡。考虑一个检索过多内容的系统。例如，它可能确定几乎所有过去的法律案例都与每个查询相关。这个系统将实现非常高的召回率，可能接近100%。然而，它将受到大量误报的困扰，因此其精确度会非常低。
- en: In contrast, consider a system that doesn’t retrieve much stuff at all. For
    example, it may consider almost every past legal case irrelevant regardless of
    the query. This system would achieve close to 100% precision, but its recall would
    be very low.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，考虑一个几乎不检索任何内容的系统。例如，它可能认为几乎每个过去的法律案例都与查询无关。这个系统将实现接近100%的精确度，但召回率会非常低。
- en: 'So, to properly quantify AI’s performance at information retrieval, you must
    somehow combine recall and precision into a single measure. A popular way to do
    this is to calculate the *F-measure*, which is the harmonic mean (a sort of average)
    between precision *P* and recall *R*:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了正确量化AI在信息检索方面的性能，您必须以某种方式将召回率和精确度结合成一个单一指标。一种常见的方法是计算*F度量值*，它是精确度*P*和召回率*R*之间的调和平均（一种平均数）：
- en: F = 2(P R)/(P + R)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: F = 2(P R)/(P + R)
- en: The higher the F-measure becomes, the higher the recall and precision. It takes
    its maximum value when recall and precision are both 100%.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: F-measure 越高，召回率和精确率就越高。当召回率和精确率都达到 100% 时，它达到最大值。
- en: I’m not a big fan of the F-measure for two reasons. First, it gives equal importance
    to recall and precision. This is arbitrary. In reality, a business may not care
    equally about them. I advise you to be wary of any promises of a measure that
    is universally good for information retrieval, be it the F-measure or something
    else, as the relative appetite for precision and recall is business specific.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我对 F-measure 不太感冒，原因有两个。首先，它对召回率和精确率给予同等的重要性。这是任意的。在现实中，企业可能不会对它们同等关心。我建议您对任何关于信息检索普遍适用的度量（无论是
    F-measure 还是其他什么）的承诺持谨慎态度，因为精确率和召回率的相对需求是业务特定的。
- en: Second, the F-measure is difficult to interpret, as the harmonic mean is not
    very intuitive. Technically, the F-measure is the reciprocal of the average of
    the reciprocals, which leads to the above formula after some algebraic manipulation.
    Good luck at communicating that to the business!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，F-measure 难以解释，因为调和平均数并不直观。技术上，F-measure 是倒数平均数的倒数，经过一些代数运算后得到上述公式。祝您在与企业沟通这一点时好运！
- en: In my opinion, your best bet is to try to understand the business’s preferences
    with respect to precision and recall and come up with a custom measure that considers
    that. In the following paragraphs, I explain one of my preferred ways of doing
    this.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在我看来，您的最佳选择是尝试理解企业在精确率和召回率方面的偏好，并据此制定一个考虑这些因素的定制度量。在接下来的段落中，我将解释我偏好的这种方法之一。
- en: The first step is to understand the business’s minimum desirable level of recall
    (it can also be done with precision, but we’ll use recall here). For example,
    the business may want to make sure to always recall at least 95% of relevant legal
    cases or fraudulent transactions.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是理解企业对召回率的最小期望水平（也可以用精确率来做，但这里我们使用召回率）。例如，企业可能希望确保至少召回 95% 的相关法律案例或欺诈交易。
- en: Afterward, you tune the system so that it attains the desired level of recall.
    One way to do this is to have AI output relevance as a numerical score, with values
    ranging from 0 (totally irrelevant) to 1 (totally relevant). Instances above a
    certain relevance threshold are considered relevant. You pick the threshold that
    helps you attain the desired level of recall. For example, it could be that setting
    a threshold of, say, 0.7, above which an instance is considered relevant, helps
    you attain the required 95% recall (you can use the validation set to calculate
    the threshold).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您调整系统以达到所需的召回率水平。一种方法是让 AI 输出相关性作为数值分数，分数范围从 0（完全不相关）到 1（完全相关）。高于某个相关性阈值的实例被认为是相关的。您选择有助于您达到所需召回率的阈值。例如，设定一个阈值为
    0.7，高于这个阈值，实例被认为是相关的，可以帮助您达到所需的 95% 召回率（您可以使用验证集来计算阈值）。
- en: Finally, you use the other measure—precision in this case—to report performance.
    You can thus compare different models (all attaining the desired recall) by how
    precise they are.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您使用另一个度量——在本例中是精确率——来报告性能。因此，您可以通过模型（所有达到所需的召回率）的精确度来比较不同的模型。
- en: Mean absolute error and root mean squared error
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 平均绝对误差和均方根误差
- en: If you use AI to make a numerical prediction, such as the amount of rainfall,
    you must calculate how far off predictions are from actual values. One straightforward
    way of doing this is to calculate the absolute difference between predicted and
    known values in the training or test sets and average the results. This is known
    as the *mean absolute error*, or MAE.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用 AI 进行数值预测，例如降雨量，您必须计算预测值与实际值之间的偏差。一种简单的方法是计算训练集或测试集中预测值与已知值之间的绝对差异，并平均结果。这被称为
    *平均绝对误差*，或 MAE。
- en: An alternative is to square the differences, which makes them all positive,
    average the results, and then take the square root of this number to (sort of)
    undo the effect of squaring. This is known as the *root mean squared error*, or
    RMSE. This measure is quite popular owing to its nice mathematical properties
    (in particular, its differentiability) and because it penalizes larger deviations
    more due to the squaring of the difference. However, it’s not as easy to interpret
    as MAE.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是平方差异，使它们都变为正值，然后平均这些结果，最后取这个数的平方根以（某种程度上）抵消平方的影响。这被称为*均方根误差*，或RMSE。由于它具有很好的数学性质（特别是其可微性）并且由于平方差异的平方，它对较大偏差的惩罚更多，因此这个度量相当受欢迎。然而，它不像MAE那样容易解释。
- en: Summary
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Proprietary AI is a good choice when you need an easy-to-use, done-for-you solution.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要一个易于使用、现成的解决方案时，专有AI是一个不错的选择。
- en: Open source AI is a good choice when you need to self-host or customize models.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你需要自行托管或定制模型时，开源AI是一个不错的选择。
- en: If AI isn’t working quite the way you expect, or you need to customize it, it’s
    usually recommended to still use off-the-shelf models and enhance your prompts.
    If that doesn’t work, you may want to fine-tune a model to your own data. Fine-tuning
    is also a good option when you prefer to use a smaller model.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果人工智能没有按你期望的方式工作，或者你需要对其进行定制，通常建议仍然使用现成的模型并增强你的提示。如果这还不行，你可能想对你的数据进行微调。当你更喜欢使用较小的模型时，微调也是一个好选择。
- en: Customer-facing AI apps are designed to be friendly and useful to end users.
    They’re powered by foundation models behind the scenes, which are large, general-purpose
    AI models you can use to build your own AI-based apps.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向客户的AI应用程序旨在对最终用户友好且有用。它们由幕后的大型通用AI模型提供支持，你可以使用这些模型来构建自己的基于AI的应用程序。
- en: Make sure to use a validation set (with data not present in the training set)
    to compare and select models. You should also perform a sanity check afterward
    using a separate test set, once you’ve selected your favorite model. Do not use
    the test set twice.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保使用一个验证集（包含不在训练集中的数据）来比较和选择模型。一旦你选定了你最喜欢的模型，你也应该使用一个单独的测试集进行合理性检查。不要两次使用测试集。
- en: The accuracy of a model measures how often it performs a task correctly. Measures
    such as precision and recall are used for information retrieval (e.g., fetching
    relevant legal cases according to a query from a much larger pool of legal cases).
    Precision and recall cannot be used by themselves; they must be combined in a
    way that matches business preferences about their relative importance. You can
    use the mean absolute error (MAE) or the root mean squared error (RMSE) to evaluate
    the performance of a model at predicting a number (such as the amount of rainfall).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的准确性衡量它正确执行任务的频率。例如，在信息检索中（例如，根据查询从大量法律案件中检索相关法律案例），使用精确度和召回率。精确度和召回率不能单独使用；它们必须以与业务偏好相匹配的方式结合，以反映它们相对重要性的关系。你可以使用平均绝对误差（MAE）或均方根误差（RMSE）来评估模型在预测数字（如降雨量）方面的性能。
