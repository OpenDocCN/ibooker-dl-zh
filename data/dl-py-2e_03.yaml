- en: 3 Introduction to Keras and TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 Keras和TensorFlow简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: A closer look at TensorFlow, Keras, and their relationship
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仔细研究TensorFlow、Keras及它们之间的关系
- en: Setting up a deep learning workspace
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置深度学习工作空间
- en: An overview of how core deep learning concepts translate to Keras and TensorFlow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深入了解核心深度学习概念如何转化为Keras和TensorFlow
- en: This chapter is meant to give you everything you need to start doing deep learning
    in practice. I’ll give you a quick presentation of Keras ([https://keras.io](https://keras.io))
    and TensorFlow ([https://tensorflow.org](https://www.tensorflow.org/)), the Python-based
    deep learning tools that we’ll use throughout the book. You’ll find out how to
    set up a deep learning workspace, with TensorFlow, Keras, and GPU support. Finally,
    building on top of the first contact you had with Keras and TensorFlow in chapter
    2, we’ll review the core components of neural networks and how they translate
    to the Keras and TensorFlow APIs.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为您提供开始实践深度学习所需的一切。我将为您快速介绍Keras（[https://keras.io](https://keras.io)）和TensorFlow（[https://tensorflow.org](https://www.tensorflow.org/)），这是本书中将使用的基于Python的深度学习工具。您���了解如何设置深度学习工作空间，使用TensorFlow、Keras和GPU支持。最后，基于您在第2章中对Keras和TensorFlow的初步接触，我们将回顾神经网络的核心组件以及它们如何转化为Keras和TensorFlow的API。
- en: By the end of this chapter, you’ll be ready to move on to practical, real-world
    applications, which will start with chapter 4.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将准备好进入实际的现实世界应用程序，这将从第4章开始。
- en: 3.1 What’s TensorFlow?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 什么是TensorFlow？
- en: 'TensorFlow is a Python-based, free, open source machine learning platform,
    developed primarily by Google. Much like NumPy, the primary purpose of TensorFlow
    is to enable engineers and researchers to manipulate mathematical expressions
    over numerical tensors. But TensorFlow goes far beyond the scope of NumPy in the
    following ways:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个基于Python的免费、开源的机器学习平台，主要由Google开发。与NumPy类似，TensorFlow的主要目的是使工程师和研究人员能够在数值张量上操作数学表达式。但是TensorFlow在以下方面远远超出了NumPy的范围：
- en: It can automatically compute the gradient of any differentiable expression (as
    you saw in chapter 2), making it highly suitable for machine learning.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以自动计算任何可微表达式的梯度（正如您在第2章中看到的），使其非常适合机器学习。
- en: It can run not only on CPUs, but also on GPUs and TPUs, highly parallel hardware
    accelerators.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不仅可以在CPU上运行，还可以在GPU和TPU上运行，高度并行的硬件加速器。
- en: Computation defined in TensorFlow can be easily distributed across many machines.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中定义的计算可以轻松地分布到许多机器上。
- en: TensorFlow programs can be exported to other runtimes, such as C++, JavaScript
    (for browser-based applications), or TensorFlow Lite (for applications running
    on mobile devices or embedded devices), etc. This makes TensorFlow applications
    easy to deploy in practical settings.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow程序可以导出到其他运行时，例如C++、JavaScript（用于基于浏览器的应用程序）或TensorFlow Lite（用于在移动设备或嵌入式设备上运行的应用程序）等。这使得TensorFlow应用程序在实际环境中易于部署。
- en: It’s important to keep in mind that TensorFlow is much more than a single library.
    It’s really a platform, home to a vast ecosystem of components, some developed
    by Google and some developed by third parties. For instance, there’s TF-Agents
    for reinforcement-learning research, TFX for industry-strength machine learning
    workflow management, TensorFlow Serving for production deployment, and there’s
    the TensorFlow Hub repository of pretrained models. Together, these components
    cover a very wide range of use cases, from cutting-edge research to large-scale
    production applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，TensorFlow远不止是一个单一的库。它实际上是一个平台，拥有庞大的组件生态系统，其中一些由Google开发，一些由第三方开发。例如，有用于强化学习研究的TF-Agents，用于工业强度机器学习工作流管理的TFX，用于生产部署的TensorFlow
    Serving，以及预训练模型的TensorFlow Hub存储库。这些组件共同涵盖了非常广泛的用例，从前沿研究到大规模生产应用。
- en: 'TensorFlow scales fairly well: for instance, scientists from Oak Ridge National
    Lab have used it to train a 1.1 exaFLOPS extreme weather forecasting model on
    the 27,000 GPUs of the IBM Summit supercomputer. Likewise, Google has used TensorFlow
    to develop very compute-intensive deep learning applications, such as the chess-playing
    and Go-playing agent AlphaZero. For your own models, if you have the budget, you
    can realistically hope to scale to around 10 petaFLOPS on a small TPU pod or a
    large cluster of GPUs rented on Google Cloud or AWS. That would still be around
    1% of the peak compute power of the top supercomputer in 2019!'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的扩展性相当不错：例如，奥克岭国家实验室的科学家们已经使用它在IBM Summit超级计算机的27000个GPU上训练了一个1.1艾克斯佛洛普的极端天气预测模型。同样，谷歌已经使用TensorFlow开发了非常计算密集的深度学习应用程序，例如下棋和围棋代理AlphaZero。对于您自己的模型，如果有预算，您可以实际上希望在小型TPU架或在Google
    Cloud或AWS上租用的大型GPU集群上扩展到约10 petaFLOPS。这仍然约占2019年顶级超级计算机峰值计算能力的1%！
- en: 3.2 What’s Keras?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 什么是Keras？
- en: Keras is a deep learning API for Python, built on top of TensorFlow, that provides
    a convenient way to define and train any kind of deep learning model. Keras was
    initially developed for research, with the aim of enabling fast deep learning
    experimentation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Keras是一个基于TensorFlow的Python深度学习API，提供了一种方便的方式来定义和训练任何类型的深度学习模型。Keras最初是为研究而开发的，旨在实现快速的深度学习实验。
- en: Through TensorFlow, Keras can run on top of different types of hardware (see
    figure 3.1)—GPU, TPU, or plain CPU—and can be seamlessly scaled to thousands of
    machines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通过TensorFlow，Keras可以在不同类型的硬件上运行（见图3.1）—GPU、TPU或普通CPU，并且可以无缝地扩展到数千台机器。
- en: '![](../Images/03-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-01.png)'
- en: 'Figure 3.1 Keras and TensorFlow: TensorFlow is a low-level tensor computing
    platform, and Keras is a high-level deep learning API'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 Keras和TensorFlow：TensorFlow是一个低级张量计算平台，而Keras是一个高级深度学习API
- en: 'Keras is known for prioritizing the developer experience. It’s an API for human
    beings, not machines. It follows best practices for reducing cognitive load: it
    offers consistent and simple workflows, it minimizes the number of actions required
    for common use cases, and it provides clear and actionable feedback upon user
    error. This makes Keras easy to learn for a beginner, and highly productive to
    use for an expert.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 以优先考虑开发者体验而闻名。它是为人类而设计的 API，而不是为机器。它遵循减少认知负荷的最佳实践：提供一致简单的工作流程，最小化常见用例所需的操作数量，并在用户出错时提供清晰可行的反馈。这使得
    Keras 对初学者易于学习，对专家使用高效。
- en: Keras has well over a million users as of late 2021, ranging from academic researchers,
    engineers, and data scientists at both startups and large companies to graduate
    students and hobbyists. Keras is used at Google, Netflix, Uber, CERN, NASA, Yelp,
    Instacart, Square, and hundreds of startups working on a wide range of problems
    across every industry. Your YouTube recommendations originate from Keras models.
    The Waymo self-driving cars are developed with Keras models. Keras is also a popular
    framework on Kaggle, the machine learning competition website, where most deep
    learning competitions have been won using Keras.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 截至 2021 年底，Keras 已经拥有超过一百万用户，包括学术研究人员、工程师、数据科学家、初创公司和大公司的研究生和爱好者。Keras 在 Google、Netflix、Uber、CERN、NASA、Yelp、Instacart、Square
    等公司中被使用，以及数百家从事各行各业各种问题的初创公司。你的 YouTube 推荐源自 Keras 模型。Waymo 自动驾驶汽车是使用 Keras 模型开发的。Keras
    也是 Kaggle 上的热门框架，大多数深度学习竞赛都是使用 Keras 赢得的。
- en: Because Keras has a large and diverse user base, it doesn’t force you to follow
    a single “true” way of building and training models. Rather, it enables a wide
    range of different workflows, from the very high level to the very low level,
    corresponding to different user profiles. For instance, you have an array of ways
    to build models and an array of ways to train them, each representing a certain
    trade-off between usability and flexibility. In chapter 5, we’ll review in detail
    a good fraction of this spectrum of workflows. You could be using Keras like you
    would use Scikit-learn—just calling `fit()` and letting the framework do its thing—or
    you could be using it like NumPy—taking full control of every little detail.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Keras 拥有庞大且多样化的用户群，它不会强迫你遵循单一的“正确”模型构建和训练方式。相反，它支持各种不同的工作流程，从非常高级到非常低级，对应不同的用户配置文件。例如，你有多种构建模型和训练模型的方式，每种方式都代表着可用性和灵活性之间的某种权衡。在第
    5 章中，我们将详细审查这种工作流程的一部分。你可以像使用 Scikit-learn 一样使用 Keras——只需调用 `fit()`，让框架自行处理——或者像使用
    NumPy 一样使用它——完全控制每一个细节。
- en: This means that everything you’re learning now as you’re getting started will
    still be relevant once you’ve become an expert. You can get started easily and
    then gradually dive into workflows where you’re writing more and more logic from
    scratch. You won’t have to switch to an entirely different framework as you go
    from student to researcher, or from data scientist to deep learning engineer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你现在学习的所有内容在你成为专家后仍然是相关的。你可以轻松入门，然后逐渐深入到需要从头开始编写更多逻辑的工作流程中。在从学生转变为研究人员，或者从数据科学家转变为深度学习工程师时，你不必切换到完全不同的框架。
- en: 'This philosophy is not unlike that of Python itself! Some languages only offer
    one way to write programs—for instance, object-oriented programming or functional
    programming. Meanwhile, Python is a multiparadigm language: it offers an array
    of possible usage patterns that all work nicely together. This makes Python suitable
    to a wide range of very different use cases: system administration, data science,
    machine learning engineering, web development . . . or just learning how to program.
    Likewise, you can think of Keras as the Python of deep learning: a user-friendly
    deep learning language that offers a variety of workflows to different user profiles.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种哲学与 Python 本身的哲学非常相似！有些语言只提供一种编写程序的方式——例如，面向对象编程或函数式编程。而 Python 是一种多范式语言：它提供了一系列可能的使用模式，它们都可以很好地协同工作。这使得
    Python 适用于各种非常不同的用例：系统管理、数据科学、机器学习工程、Web 开发……或者只是学习如何编程。同样，你可以将 Keras 视为深度学习的
    Python：一种用户友好的深度学习语言，为不同用户配置文件提供各种工作流程。
- en: '3.3 Keras and TensorFlow: A brief history'
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 Keras 和 TensorFlow：简史
- en: Keras predates TensorFlow by eight months. It was released in March 2015, and
    TensorFlow was released in November 2015\. You may ask, if Keras is built on top
    of TensorFlow, how it could exist before TensorFlow was released? Keras was originally
    built on top of Theano, another tensor-manipulation library that provided automatic
    differentiation and GPU support—the earliest of its kind. Theano, developed at
    the Montréal Institute for Learning Algorithms (MILA) at the Université de Montréal,
    was in many ways a precursor of TensorFlow. It pioneered the idea of using static
    computation graphs for automatic differentiation and for compiling code to both
    CPU and GPU.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 比 TensorFlow 早八个月发布。它于 2015 年 3 月发布，而 TensorFlow 则于 2015 年 11 月发布。你可能会问，如果
    Keras 是建立在 TensorFlow 之上的，那么在 TensorFlow 发布之前它是如何存在的？Keras 最初是建立在 Theano 之上的，Theano
    是另一个提供自动微分和 GPU 支持的张量操作库，是最早的之一。Theano 在蒙特利尔大学机器学习算法研究所（MILA）开发，从许多方面来看是 TensorFlow
    的前身。它开创了使用静态计算图进行自动微分和将代码编译到 CPU 和 GPU 的想法。
- en: 'In late 2015, after the release of TensorFlow, Keras was refactored to a multibackend
    architecture: it became possible to use Keras with either Theano or TensorFlow,
    and switching between the two was as easy as changing an environment variable.
    By September 2016, TensorFlow had reached a level of technical maturity where
    it became possible to make it the default backend option for Keras. In 2017, two
    new additional backend options were added to Keras: CNTK (developed by Microsoft)
    and MXNet (developed by Amazon). Nowadays, both Theano and CNTK are out of development,
    and MXNet is not widely used outside of Amazon. Keras is back to being a single-backend
    API—on top of TensorFlow.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 发布后的 2015 年底，Keras 被重构为多后端架构：可以使用 Keras 与 Theano 或 TensorFlow，而在两者之间切换就像更改环境变量一样简单。到
    2016 年 9 月，TensorFlow 达到了技术成熟的水平，使其成为 Keras 的默认后端选项成为可能。2017 年，Keras 添加了两个新的后端选项：CNTK（由微软开发）和
    MXNet（由亚马逊开发）。如今，Theano 和 CNTK 已经停止开发，MXNet 在亚马逊之外并不广泛使用。Keras 又回到了基于 TensorFlow
    的单一后端 API。
- en: Keras and TensorFlow have had a symbiotic relationship for many years. Throughout
    2016 and 2017, Keras became well known as the user-friendly way to develop TensorFlow
    applications, funneling new users into the TensorFlow ecosystem. By late 2017,
    a majority of TensorFlow users were using it through Keras or in combination with
    Keras. In 2018, the TensorFlow leadership picked Keras as TensorFlow’s official
    high-level API. As a result, the Keras API is front and center in TensorFlow 2.0,
    released in September 2019—an extensive redesign of TensorFlow and Keras that
    takes into account over four years of user feedback and technical progress.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，Keras 和 TensorFlow 之间建立了一种共生关系。在 2016 年和 2017 年期间，Keras 成为了开发 TensorFlow
    应用程序的用户友好方式，将新用户引入 TensorFlow 生态系统。到 2017 年底，大多数 TensorFlow 用户都是通过 Keras 或与 Keras
    结合使用。2018 年，TensorFlow 领导层选择了 Keras 作为 TensorFlow 的官方高级 API。因此，Keras API 在 2019
    年 9 月发布的 TensorFlow 2.0 中占据了重要位置——这是 TensorFlow 和 Keras 的全面重新设计，考虑了四年多的用户反馈和技术进步。
- en: By this point, you must be eager to start running Keras and TensorFlow code
    in practice. Let’s get you started.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到这个时候，你一定迫不及待地想要开始实践运行 Keras 和 TensorFlow 代码了。让我们开始吧。
- en: 3.4 Setting up a deep learning workspace
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 设置深度学习工作空间
- en: Before you can get started developing deep learning applications, you need to
    set up your development environment. It’s highly recommended, although not strictly
    necessary, that you run deep learning code on a modern NVIDIA GPU rather than
    your computer’s CPU. Some applications—in particular, image processing with convolutional
    networks—will be excruciatingly slow on CPU, even a fast multicore CPU. And even
    for applications that can realistically be run on CPU, you’ll generally see the
    speed increase by a factor of 5 or 10 by using a recent GPU.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始开发深度学习应用程序之前，你需要设置好你的开发环境。强烈建议，尽管不是绝对必要的，你应该在现代 NVIDIA GPU 上运行深度学习代码，而不是在计算机的
    CPU 上运行。一些应用程序——特别是使用卷积网络进行图像处理的应用程序——在 CPU 上会非常慢，即使是快速的多核 CPU。即使对于可以在 CPU 上运行的应用程序，使用最新
    GPU 通常会使速度提高 5 到 10 倍。
- en: 'To do deep learning on a GPU, you have three options:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 GPU 上进行深度学习，你有三个选择：
- en: Buy and install a physical NVIDIA GPU on your workstation.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的工作站上购买并安装一块物理 NVIDIA GPU。
- en: Use GPU instances on Google Cloud or AWS EC2.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Google Cloud 或 AWS EC2 上的 GPU 实例。
- en: Use the free GPU runtime from Colaboratory, a hosted notebook service offered
    by Google (for details about what a “notebook” is, see the next section).
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Colaboratory 提供的免费 GPU 运行时，这是 Google 提供的托管笔记本服务（有关“笔记本”是什么的详细信息，请参见下一节）。
- en: Colaboratory is the easiest way to get started, as it requires no hardware purchase
    and no software installation—just open a tab in your browser and start coding.
    It’s the option we recommend for running the code examples in this book. However,
    the free version of Colaboratory is only suitable for small workloads. If you
    want to scale up, you’ll have to use the first or second option.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Colaboratory 是最简单的入门方式，因为它不需要购买硬件，也不需要安装软件——只需在浏览器中打开一个标签页并开始编码。这是我们推荐在本书中运行代码示例的选项。然而，Colaboratory
    的免费版本只适用于小型工作负载。如果你想扩大规模，你将不得不使用第一或第二个选项。
- en: If you don’t already have a GPU that you can use for deep learning (a recent,
    high-end NVIDIA GPU), then running deep learning experiments in the cloud is a
    simple, low-cost way for you to move to larger workloads without having to buy
    any additional hardware. If you’re developing using Jupyter notebooks, the experience
    of running in the cloud is no different from running locally.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有可以用于深度学习的 GPU（一块最新的高端 NVIDIA GPU），那么在云中运行深度学习实验是一个简单、低成本的方式，让你能够扩展到更大的工作负载，而无需购买任何额外的硬件。如果你正在使用
    Jupyter 笔记本进行开发，那么在云中运行的体验与本地运行没有任何区别。
- en: 'But if you’re a heavy user of deep learning, this setup isn’t sustainable in
    the long term—or even for more than a few months. Cloud instances aren’t cheap:
    you’d pay $2.48 per hour for a V100 GPU on Google Cloud in mid-2021\. Meanwhile,
    a solid consumer-class GPU will cost you somewhere between $1,500 and $2,500—a
    price that has been fairly stable over time, even as the specs of these GPUs keep
    improving. If you’re a heavy user of deep learning, consider setting up a local
    workstation with one or more GPUs.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你是深度学习的重度用户，这种设置在长期内甚至在几个月内都是不可持续的。云实例并不便宜：在 2021 年中期，你将为 Google Cloud
    上的 V100 GPU 每小时支付 2.48 美元。与此同时，一块可靠的消费级 GPU 的价格在 1500 到 2500 美元之间——即使这些 GPU 的规格不断改进，价格也保持相对稳定。如果你是深度学习的重度用户，请考虑设置一个带有一块或多块
    GPU 的本地工作站。
- en: Additionally, whether you’re running locally or in the cloud, it’s better to
    be using a Unix workstation. Although it’s technically possible to run Keras on
    Windows directly, we don’t recommend it. If you’re a Windows user and you want
    to do deep learning on your own workstation, the simplest solution to get everything
    running is to set up an Ubuntu dual boot on your machine, or to leverage Windows
    Subsystem for Linux (WSL), a compatibility layer that enables you to run Linux
    applications from Windows. It may seem like a hassle, but it will save you a lot
    of time and trouble in the long run.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，无论您是在本地运行还是在云端运行，最好使用 Unix 工作站。虽然在 Windows 上直接运行 Keras 在技术上是可能的，但我们不建议这样做。如果您是
    Windows 用户，并且想在自己的工作站上进行深度学习，最简单的解决方案是在您的机器上设置一个 Ubuntu 双系统引导，或者利用 Windows Subsystem
    for Linux（WSL），这是一个兼容层，使您能够从 Windows 运行 Linux 应用程序。这可能看起来有点麻烦，但从长远来看，这将为您节省大量时间和麻烦。
- en: '3.4.1 Jupyter notebooks: The preferred way to run deep learning experiments'
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 Jupyter 笔记本：运行深度学习实验的首选方式
- en: Jupyter notebooks are a great way to run deep learning experiments—in particular,
    the many code examples in this book. They’re widely used in the data science and
    machine learning communities. A *notebook* is a file generated by the Jupyter
    Notebook app ([https://jupyter.org](https://jupyter.org)) that you can edit in
    your browser. It mixes the ability to execute Python code with rich text-editing
    capabilities for annotating what you’re doing. A notebook also allows you to break
    up long experiments into smaller pieces that can be executed independently, which
    makes development interactive and means you don’t have to rerun all of your previous
    code if something goes wrong late in an experiment.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter 笔记本是运行深度学习实验的绝佳方式，特别是本书中的许多代码示例。它们在数据科学和机器学习社区中被广泛使用。*笔记本*是由 Jupyter
    Notebook 应用程序生成的文件（[https://jupyter.org](https://jupyter.org)），您可以在浏览器中编辑。它结合了执行
    Python 代码的能力和用于注释您正在进行的操作的丰富文本编辑功能。笔记本还允许您将长实验分解为可以独立执行的较小部分，这使得开发交互式，并且意味着如果实验的后期出现问题，您不必重新运行之前的所有代码。
- en: 'I recommend using Jupyter notebooks to get started with Keras, although that
    isn’t a requirement: you can also run standalone Python scripts or run code from
    within an IDE such as PyCharm. All the code examples in this book are available
    as open source notebooks; you can download them from GitHub at [github.com/fchollet/deep-learning-with-python-notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议使用 Jupyter 笔记本来开始使用 Keras，尽管这不是必需的：您也可以运行独立的 Python 脚本或在诸如 PyCharm 这样的 IDE
    中运行代码。本书中的所有代码示例都作为开源笔记本提供；您可以从 GitHub 上下载它们：[github.com/fchollet/deep-learning-with-python-notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)。
- en: 3.4.2 Using Colaboratory
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 使用 Colaboratory
- en: Colaboratory (or Colab for short) is a free Jupyter notebook service that requires
    no installation and runs entirely in the cloud. Effectively, it’s a web page that
    lets you write and execute Keras scripts right away. It gives you access to a
    free (but limited) GPU runtime and even a TPU runtime, so you don’t have to buy
    your own GPU. Colaboratory is what we recommend for running the code examples
    in this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Colaboratory（简称 Colab）是一个免费的 Jupyter 笔记本服务，无需安装，完全在云端运行。实际上，它是一个网页，让您可以立即编写和执行
    Keras 脚本。它为您提供免费（但有限）的 GPU 运行时，甚至还有 TPU 运行时，因此您不必购买自己的 GPU。Colaboratory 是我们推荐用于运行本书中代码示例的工具。
- en: First steps with Colaboratory
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Colaboratory 的第一步
- en: To get started with Colab, go to [https://colab.research.google.com](https://colab.research.google.com)
    and click the New Notebook button. You’ll see the standard Notebook interface
    shown in figure 3.2.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Colab，请访问 [https://colab.research.google.com](https://colab.research.google.com)
    并单击 New Notebook 按钮。您将看到图 3.2 中显示的标准笔记本界面。
- en: '![](../Images/03-02.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-02.png)'
- en: Figure 3.2 A Colab notebook
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 一个 Colab 笔记本
- en: 'You’ll notice two buttons in the toolbar: + Code and + Text. They’re for creating
    executable Python code cells and annotation text cells, respectively. After entering
    code in a code cell, Pressing Shift-Enter will execute it (see figure 3.3).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您会在工具栏中看到两个按钮：+ Code 和 + Text。它们分别用于创建可执行的 Python 代码单元格和注释文本单元格。在代码单元格中输入代码后，按
    Shift-Enter 将执行它（参见图 3.3）。
- en: '![](../Images/03-03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-03.png)'
- en: Figure 3.3 Creating a code cell
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3 创建一个代码单元格
- en: In a text cell, you can use Markdown syntax (see figure 3.4). Pressing Shift-Enter
    on a text cell will render it.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本单元格中，您可以使用 Markdown 语法（参见图 3.4）。按 Shift-Enter 在文本单元格上将渲染它。
- en: '![](../Images/03-04.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-04.png)'
- en: Figure 3.4 Creating a text cell
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 创建一个文本单元格
- en: 'Text cells are useful for giving a readable structure to your notebooks: use
    them to annotate your code with section titles and long explanation paragraphs
    or to embed figures. Notebooks are meant to be a multimedia experience!'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 文本单元格对于为您的笔记本提供可读的结构非常有用：使用它们为您的代码添加部分标题和长说明段落或嵌入图像。笔记本旨在成为一种多媒体体验！
- en: Installing packages with pip
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 pip 安装软件包
- en: 'The default Colab environment already comes with TensorFlow and Keras installed,
    so you can start using it right away without any installation steps required.
    But if you ever need to install something with `pip`, you can do so by using the
    following syntax in a code cell (note that the line starts with `!` to indicate
    that it is a shell command rather than Python code):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 Colab 环境已经安装了 TensorFlow 和 Keras，因此您可以立即开始使用它，无需任何安装步骤。但是，如果您需要使用 `pip`
    安装某些内容，您可以在代码单元格中使用以下语法进行安装（请注意，该行以 `!` 开头，表示这是一个 shell 命令而不是 Python 代码）：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using the GPU runtime
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPU 运行时
- en: To use the GPU runtime with Colab, select Runtime > Change Runtime Type in the
    menu and select GPU for the Hardware Accelerator (see figure 3.5).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Colab 中使用 GPU 运行时，请在菜单中选择 Runtime > Change Runtime Type，并选择 GPU 作为硬件加速器（参见图
    3.5）。
- en: '![](../Images/03-05.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-05.png)'
- en: Figure 3.5 Using the GPU runtime with Colab
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 使用 Colab 的 GPU 运行时
- en: TensorFlow and Keras will automatically execute on GPU if a GPU is available,
    so there’s nothing more you need to do after you’ve selected the GPU runtime.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 GPU 可用，TensorFlow 和 Keras 将自动在 GPU 上执行，所以在选择了 GPU 运行时后，你无需做其他操作。
- en: You’ll notice that there’s also a TPU runtime option in that Hardware Accelerator
    dropdown menu. Unlike the GPU runtime, using the TPU runtime with TensorFlow and
    Keras does require a bit of manual setup in your code. We’ll cover this in chapter
    13\. For the time being, we recommend that you stick to the GPU runtime to follow
    along with the code examples in the book.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在硬件加速器下拉菜单中还有一个 TPU 运行时选项。与 GPU 运行时不同，使用 TensorFlow 和 Keras 的 TPU 运行时需要在代码中进行一些手动设置。我们将在第13章中介绍这个内容。目前，我们建议你选择
    GPU 运行时，以便跟随本书中的代码示例。
- en: You now have a way to start running Keras code in practice. Next, let’s see
    how the key ideas you learned about in chapter 2 translate to Keras and TensorFlow
    code.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了一个开始在实践中运行 Keras 代码的方法。接下来，让我们看看你在第2章学到的关键思想如何转化为 Keras 和 TensorFlow 代码。
- en: 3.5 First steps with TensorFlow
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 TensorFlow 的第一步
- en: 'As you saw in the previous chapters, training a neural network revolves around
    the following concepts:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在之前的章节中看到的，训练神经网络围绕着以下概念展开：
- en: 'First, low-level tensor manipulation—the infrastructure that underlies all
    modern machine learning. This translates to TensorFlow APIs:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，低级张量操作——支撑所有现代机器学习的基础设施。这转化为 TensorFlow API：
- en: '*Tensors*, including special tensors that store the network’s state (*variables*)'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*张量*，包括存储网络状态的特殊张量（*变量*）'
- en: '*Tensor operations* such as addition, `relu`, `matmul`'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*张量操作*，如加法、`relu`、`matmul`'
- en: '*Backpropagation*, a way to compute the gradient of mathematical expressions
    (handled in TensorFlow via the `GradientTape` object)'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反向传播*，一种计算数学表达式梯度的方法（在 TensorFlow 中通过`GradientTape`对象处理）'
- en: 'Second, high-level deep learning concepts. This translates to Keras APIs:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，高级深度学习概念。这转化为 Keras API：
- en: '*Layers*, which are combined into a *model*'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层*，这些层组合成一个*模型*'
- en: A *loss function*, which defines the feedback signal used for learning
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*损失函数*，定义用于学习的反馈信号
- en: An *optimizer*, which determines how learning proceeds
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*优化器*，确定学习如何进行
- en: '*Metrics* to evaluate model performance, such as accuracy'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指标*用于评估模型性能，如准确度'
- en: A *training loop* that performs mini-batch stochastic gradient descent
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行小批量随机梯度下降的*训练循环*
- en: 'In the previous chapter, you already had a first light contact with some of
    the corresponding TensorFlow and Keras APIs: you’ve briefly used TensorFlow’s
    `Variable` class, the `matmul` operation, and the `GradientTape`. You’ve instantiated
    Keras `Dense` layers, packed them into a `Sequential` model, and trained that
    model with the `fit()` method.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你已经初步接触了一些对应的 TensorFlow 和 Keras API：你已经简要使用了 TensorFlow 的`Variable`类、`matmul`操作和`GradientTape`。你实例化了
    Keras 的`Dense`层，将它们打包成一个`Sequential`模型，并用`fit()`方法训练了该模型。
- en: Now let’s take a deeper dive into how all of these different concepts can be
    approached in practice using TensorFlow and Keras.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入了解如何使用 TensorFlow 和 Keras 在实践中处理所有这些不同概念。
- en: 3.5.1 Constant tensors and variables
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 常量张量和变量
- en: To do anything in TensorFlow, we’re going to need some tensors. Tensors need
    to be created with some initial value. For instance, you could create all-ones
    or all-zeros tensors (see listing 3.1), or tensors of values drawn from a random
    distribution (see listing 3.2).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 TensorFlow 中做任何事情，我们需要一些张量。张量需要用一些初始值创建。例如，你可以创建全为 1 或全为 0 的张量（见列表 3.1），或者从随机分布中抽取值的张量（见列表
    3.2）。
- en: Listing 3.1 All-ones or all-zeros tensors
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 全为 1 或全为 0 的张量
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Equivalent to np.ones(shape=(2, 1))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 等同于 np.ones(shape=(2, 1))
- en: ❷ Equivalent to np.zeros(shape=(2, 1))
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 等同于 np.zeros(shape=(2, 1))
- en: Listing 3.2 Random tensors
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 随机张量
- en: '[PRE2]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Tensor of random values drawn from a normal distribution with mean 0 and standard
    deviation 1\. Equivalent to np.random.normal(size=(3, 1), loc=0., scale=1.).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从均值为 0、标准差为 1 的正态分布中抽取的随机值张量。等同于 np.random.normal(size=(3, 1), loc=0., scale=1.)。
- en: ❷ Tensor of random values drawn from a uniform distribution between 0 and 1\.
    Equivalent to np.random.uniform(size=(3, 1), low=0., high=1.).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 0 到 1 之间均匀分布的随机值张量。等同于 np.random.uniform(size=(3, 1), low=0., high=1.)。
- en: 'A significant difference between NumPy arrays and TensorFlow tensors is that
    TensorFlow tensors aren’t assignable: they’re constant. For instance, in NumPy,
    you can do the following.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 数组和 TensorFlow 张量之间的一个重要区别是 TensorFlow 张量不可赋值：它们是常量。例如，在 NumPy 中，你可以这样做。
- en: Listing 3.3 NumPy arrays are assignable
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 NumPy 数组是可赋值的
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Try to do the same thing in TensorFlow, and you will get an error: “EagerTensor
    object does not support item assignment.”'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在 TensorFlow 中做同样的事情，你会得到一个错误：“EagerTensor 对象不支持项目赋值。”
- en: Listing 3.4 TensorFlow tensors are not assignable
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 TensorFlow 张量不可赋值
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ This will fail, as a tensor isn’t assignable.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这将失败，因为张量不可赋值。
- en: To train a model, we’ll need to update its state, which is a set of tensors.
    If tensors aren’t assignable, how do we do it? That’s where *variables* come in.
    `tf.Variable` is the class meant to manage modifiable state in TensorFlow. You’ve
    already briefly seen it in action in the training loop implementation at the end
    of chapter 2.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练一个模型，我们需要更新它的状态，这是一组张量。如果张量不可赋值，我们该怎么办？这就是*变量*发挥作用的地方。`tf.Variable`是 TensorFlow
    中用来管理可修改状态的类。你在第2章末尾的训练循环实现中已经简要看到它的作用。
- en: To create a variable, you need to provide some initial value, such as a random
    tensor.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个变量，你需要提供一些初始值，比如一个随机张量。
- en: Listing 3.5 Creating a TensorFlow variable
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 创建一个 TensorFlow 变量
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The state of a variable can be modified via its `assign` method, as follows.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 变量的状态可以通过其`assign`方法修改，如下所示。
- en: Listing 3.6 Assigning a value to a TensorFlow variable
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 给 TensorFlow 变量赋值
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It also works for a subset of the coefficients.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 它也适用于一部分系数。
- en: Listing 3.7 Assigning a value to a subset of a TensorFlow variable
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 给 TensorFlow 变量的子集赋值
- en: '[PRE7]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Similarly, `assign_add()` and `assign_sub()` are efficient equivalents of `+=`
    and `-=`, as shown next.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，`assign_add()` 和 `assign_sub()` 是`+=` 和 `-=` 的高效等价物，如下所示。
- en: Listing 3.8 Using `assign_add()`
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.8 使用`assign_add()`
- en: '[PRE8]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '3.5.2 Tensor operations: Doing math in TensorFlow'
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 张量操作：在 TensorFlow 中进行数学运算
- en: Just like NumPy, TensorFlow offers a large collection of tensor operations to
    express mathematical formulas. Here are a few examples.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 NumPy 一样，TensorFlow 提供了大量的张量操作来表达数学公式。以下是一些示例。
- en: Listing 3.9 A few basic math operations
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.9 几个基本数学操作
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Take the square.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 求平方。
- en: ❷ Take the square root.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 求平方根。
- en: ❸ Add two tensors (element-wise).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 两个张量相加（逐元素）。
- en: ❹ Take the product of two tensors (as discussed in chapter 2).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 两个张量的乘积（如第 2 章中讨论的）。
- en: ❺ Multiply two tensors (element-wise).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 两个张量相乘（逐元素）。
- en: 'Importantly, each of the preceding operations gets executed on the fly: at
    any point, you can print what the current result is, just like in NumPy. We call
    this *eager execution*.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，前面的每个操作都是即时执行的：在任何时候，你都可以打印出当前的结果，就像在 NumPy 中一样。我们称之为*即时执行*。
- en: 3.5.3 A second look at the GradientTape API
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 再看一下 GradientTape API
- en: 'So far, TensorFlow seems to look a lot like NumPy. But here’s something NumPy
    can’t do: retrieve the gradient of any differentiable expression with respect
    to any of its inputs. Just open a `GradientTape` scope, apply some computation
    to one or several input tensors, and retrieve the gradient of the result with
    respect to the inputs.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，TensorFlow 看起来很像 NumPy。但这里有一件 NumPy 做不到的事情：检索任何可微表达式相对于其任何输入的梯度。只需打开一个`GradientTape`范围，对一个或多个输入张量应用一些计算，并检索结果相对于输入的梯度。
- en: Listing 3.10 Using the `GradientTape`
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.10 使用`GradientTape`
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This is most commonly used to retrieve the gradients of the loss of a model
    with respect to its weights: `gradients` `=` `tape.gradient(loss,` `weights)`.
    You saw this in action in chapter 2.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于检索模型损失相对于其权重的梯度：`gradients` `=` `tape.gradient(loss,` `weights)`。你在第 2
    章中看到了这个过程。
- en: So far, you’ve only seen the case where the input tensors in `tape.gradient()`
    were TensorFlow variables. It’s actually possible for these inputs to be any arbitrary
    tensor. However, only *trainable variables* are tracked by default. With a constant
    tensor, you’d have to manually mark it as being tracked by calling `tape.watch()`
    on it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只看到了`tape.gradient()`中输入张量是 TensorFlow 变量的情况。实际上，这些输入可以是任意张量。然而，默认只有*可训练变量*会被跟踪。对于常量张量，你需要手动调用`tape.watch()`来标记它被跟踪。
- en: Listing 3.11 Using `GradientTape` with constant tensor inputs
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.11 使用带有常量张量输入的`GradientTape`
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Why is this necessary? Because it would be too expensive to preemptively store
    the information required to compute the gradient of anything with respect to anything.
    To avoid wasting resources, the tape needs to know what to watch. Trainable variables
    are watched by default because computing the gradient of a loss with regard to
    a list of trainable variables is the most common use of the gradient tape.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是必要的？因为预先存储计算任何东西相对于任何东西的梯度所需的信息将会太昂贵。为了避免浪费资源，磁带需要知道要观察什么。可训练变量默认会被监视，因为计算损失相对于一组可训练变量的梯度是梯度磁带最常见的用法。
- en: The gradient tape is a powerful utility, even capable of computing *second-order
    gradients*, that is to say, the gradient of a gradient. For instance, the gradient
    of the position of an object with regard to time is the speed of that object,
    and the second-order gradient is its acceleration.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度磁带是一个强大的实用工具，甚至能够计算*二阶梯度*，也就是说，一个梯度的梯度。例如，一个物体的位置相对于时间的梯度是该物体的速度，而二阶梯度是它的加速度。
- en: If you measure the position of a falling apple along a vertical axis over time
    and find that it verifies `position(time)` `=` `4.9` `*` `time` `**` `2`, what
    is its acceleration? Let’s use two nested gradient tapes to find out.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你测量一个沿垂直轴下落的苹果随时间的位置，并发现它验证`position(time)` `=` `4.9` `*` `time` `**` `2`，那么它的加速度是多少？让我们使用两个嵌套的梯度磁带来找出答案。
- en: Listing 3.12 Using nested gradient tapes to compute second-order gradients
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.12 使用嵌套的梯度磁带计算二阶梯度
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ We use the outer tape to compute the gradient of the gradient from the inner
    tape. Naturally, the answer is 4.9 * 2 = 9.8\.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用外部磁带来计算内部磁带的梯度。自然地，答案是 4.9 * 2 = 9.8。
- en: '3.5.4 An end-to-end example: A linear classifier in pure TensorFlow'
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 一个端到端的示例：在纯 TensorFlow 中的线性分类器
- en: You know about tensors, variables, and tensor operations, and you know how to
    compute gradients. That’s enough to build any machine learning model based on
    gradient descent. And you’re only at chapter 3!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了张量、变量和张量操作，也知道如何计算梯度。这足以构建基于梯度下降的任何机器学习模型。而你只是在第 3 章！
- en: 'In a machine learning job interview, you may be asked to implement a linear
    classifier from scratch in TensorFlow: a very simple task that serves as a filter
    between candidates who have some minimal machine learning background and those
    who don’t. Let’s get you past that filter and use your newfound knowledge of TensorFlow
    to implement such a linear classifier.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习工作面试中，你可能会被要求在 TensorFlow 中从头开始实现一个线性分类器：这是一个非常简单的任务，可以作为筛选具有一些最低机器学习背景和没有背景的候选人之间���过滤器。让我们帮你通过这个筛选器，并利用你对
    TensorFlow 的新知识来实现这样一个线性分类器。
- en: 'First, let’s come up with some nicely linearly separable synthetic data to
    work with: two classes of points in a 2D plane. We’ll generate each class of points
    by drawing their coordinates from a random distribution with a specific covariance
    matrix and a specific mean. Intuitively, the covariance matrix describes the shape
    of the point cloud, and the mean describes its position in the plane (see figure
    3.6). We’ll reuse the same covariance matrix for both point clouds, but we’ll
    use two different mean values—the point clouds will have the same shape, but different
    positions.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们想出一些线性可分的合成数据来处理：2D 平面上的两类点。我们将通过从具有特定协方差矩阵和特定均值的随机分布中绘制它们的坐标来生成每一类点。直观地，协方差矩阵描述了点云的形状，均值描述了它在平面上的位置（参见图
    3.6）。我们将为两个点云重复使用相同的协方差矩阵，但我们将使用两个不同的均值值——点云将具有相同的形状，但不同的位置。
- en: Listing 3.13 Generating two classes of random points in a 2D plane
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.13 在 2D 平面上生成两类随机点
- en: '[PRE13]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '❶ Generate the first class of points: 1000 random 2D points. cov=[[1, 0.5],[0.5,
    1]] corresponds to an oval-like point cloud oriented from bottom left to top right.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成第一类点：1000 个随机的 2D 点。cov=[[1, 0.5],[0.5, 1]] 对应于一个从左下到右上方向的椭圆形点云。
- en: ❷ Generate the other class of points with a different mean and the same covariance
    matrix.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用不同均值和相同协方差矩阵生成另一类点。
- en: In the preceding code, `negative_samples` and `positive_samples` are both arrays
    with shape `(1000,` `2)`. Let’s stack them into a single array with shape `(2000,`
    `2)`.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，`negative_samples` 和 `positive_samples` 都是形状为 `(1000,` `2)` 的数组。让我们将它们堆叠成一个形状为
    `(2000,` `2)` 的单一数组。
- en: Listing 3.14 Stacking the two classes into an array with shape (2000, 2)
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.14 将两类堆叠成形状为 (2000, 2) 的数组
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s generate the corresponding target labels, an array of zeros and ones of
    shape `(2000,` `1)`, where `targets[i,` `0]` is 0 if `inputs[i]` belongs to class
    0 (and inversely).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成相应的目标标签，一个形状为 `(2000,` `1)` 的零和一的数组，其中 `targets[i,` `0]` 为 0，如果 `inputs[i]`
    属于类 0（反之亦然）。
- en: Listing 3.15 Generating the corresponding targets (0 and 1)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.15 生成相应的目标值 (0 和 1)
- en: '[PRE15]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, let’s plot our data with Matplotlib.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们用 Matplotlib 绘制我们的数据。
- en: Listing 3.16 Plotting the two point classes (see figure 3.6)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.16 绘制两类点（参见图 3.6）
- en: '[PRE16]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/03-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-06.png)'
- en: 'Figure 3.6 Our synthetic data: two classes of random points in the 2D plane'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 我们的合成数据：2D 平面上的两类随机点
- en: Now let’s create a linear classifier that can learn to separate these two blobs.
    A linear classifier is an affine transformation (`prediction` `=` `W` `•` `input`
    `+` `b`) trained to minimize the square of the difference between predictions
    and the targets.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个线性分类器，它可以学会分离这两个斑点。线性分类器是一个仿射变换（`prediction` `=` `W` `•` `input` `+`
    `b`），训练以最小化预测与目标之间差的平方。
- en: As you’ll see, it’s actually a much simpler example than the end-to-end example
    of a toy two-layer neural network you saw at the end of chapter 2\. However, this
    time you should be able to understand everything about the code, line by line.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将看到的，这实际上比第 2 章末尾看到的玩具两层神经网络的端到端示例要简单得多。然而，这次你应该能够逐行理解代码的一切。
- en: Let’s create our variables, `W` and `b`, initialized with random values and
    with zeros, respectively.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的变量，`W` 和 `b`，分别用随机值和零值初始化。
- en: Listing 3.17 Creating the linear classifier variables
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.17 创建线性分类器变量
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ The inputs will be 2D points.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入将是 2D 点。
- en: ❷ The output predictions will be a single score per sample (close to 0 if the
    sample is predicted to be in class 0, and close to 1 if the sample is predicted
    to be in class 1).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输出预测将是每个样本的单个分数（如果样本被预测为类 0，则接近 0，如果样本被预测为类 1，则接近 1）。
- en: Here’s our forward pass function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的前向传播函数。
- en: Listing 3.18 The forward pass function
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.18 前向传播函数
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Because our linear classifier operates on 2D inputs, `W` is really just two
    scalar coefficients, `w1` and `w2`: `W` `=` `[[w1],` `[w2]]`. Meanwhile, `b` is
    a single scalar coefficient. As such, for a given input point `[x,` `y]`, its
    prediction value is `prediction` `=` `[[w1],` `[w2]]` `•` `[x,` `y]` `+` `b` `=`
    `w1` `*` `x` `+` `w2` `*` `y` `+` `b`.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的线性分类器操作在 2D 输入上，`W` 实际上只是两个标量系数，`w1` 和 `w2`：`W` `=` `[[w1],` `[w2]]`。同时，`b`
    是一个单一的标量系数。因此，对于给定的输入点 `[x,` `y]`，其预测值为 `prediction` `=` `[[w1],` `[w2]]` `•`
    `[x,` `y]` `+` `b` `=` `w1` `*` `x` `+` `w2` `*` `y` `+` `b`。
- en: The following listing shows our loss function.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了我们的损失函数。
- en: Listing 3.19 The mean squared error loss function
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.19 均方误差损失函数
- en: '[PRE19]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ per_sample_losses will be a tensor with the same shape as targets and predictions,
    containing per-sample loss scores.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ per_sample_losses 将是一个与目标和预测相同形状的张量，包含每个样本的损失分数。
- en: '❷ We need to average these per-sample loss scores into a single scalar loss
    value: this is what reduce_mean does.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们需要将这些每个样本的损失���数平均为单个标量损失值：这就是 reduce_mean 所做的。
- en: Next is the training step, which receives some training data and updates the
    weights `W` and `b` so as to minimize the loss on the data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练步骤，它接收一些训练数据并更新权重 `W` 和 `b`，以使数据上的损失最小化。
- en: Listing 3.20 The training step function
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.20 训练步骤函数
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Forward pass, inside a gradient tape scope
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 前向传播，在梯度磁带范围内
- en: ❷ Retrieve the gradient of the loss with regard to weights.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检索损失相对于权重的梯度。
- en: ❸ Update the weights.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 更新权重。
- en: 'For simplicity, we’ll do *batch training* instead of *mini-batch training*:
    we’ll run each training step (gradient computation and weight update) for all
    the data, rather than iterate over the data in small batches. On one hand, this
    means that each training step will take much longer to run, since we’ll compute
    the forward pass and the gradients for 2,000 samples at once. On the other hand,
    each gradient update will be much more effective at reducing the loss on the training
    data, since it will encompass information from all training samples instead of,
    say, only 128 random samples. As a result, we will need many fewer steps of training,
    and we should use a larger learning rate than we would typically use for mini-batch
    training (we’ll use `learning_rate` `=` `0.1`, defined in listing 3.20).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们将进行*批量训练*而不是*小批量训练*：我们将对所有数据运行每个训练步骤（梯度计算和权重更新），而不是在小批量中迭代数据。一方面，这意味着每个训练步骤将需要更长时间运行，因为我们将一次计算2,000个样本的前向传播和梯度。另一方面，每个梯度更新将更有效地减少训练数据上的损失，因为它将包含所有训练样本的信息，而不是仅仅128个随机样本。因此，我们将需要更少的训练步骤，并且我们应该使用比通常用于小批量训练更大的学习率（我们将使用`learning_rate`
    `=` `0.1`，在列表3.20中定义）。
- en: Listing 3.21 The batch training loop
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.21 批量训练循环
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'After 40 steps, the training loss seems to have stabilized around 0.025\. Let’s
    plot how our linear model classifies the training data points. Because our targets
    are zeros and ones, a given input point will be classified as “0” if its prediction
    value is below 0.5, and as “1” if it is above 0.5 (see figure 3.7):'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 经过40步，训练损失似乎已经稳定在0.025左右。让我们绘制我们的线性模型如何对训练数据点进行分类。因为我们的目标是0和1，给定输入点将被分类为“0”，如果其预测值低于0.5，将被分类为“1”，如果高于0.5（见图3.7）：
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/03-07.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-07.png)'
- en: 'Figure 3.7 Our model’s predictions on the training inputs: pretty similar to
    the training targets'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 我们模型对训练输入的预测：与训练目标非常相似
- en: 'Recall that the prediction value for a given point `[x,` `y]` is simply `prediction`
    `==` `[[w1],` `[w2]]` `•` `[x,` `y]` `+` `b` `==` `w1` `*` `x` `+` `w2` `*` `y`
    `+` `b`. Thus, class 0 is defined as `w1` `*` `x` `+` `w2` `*` `y` `+` `b` `<`
    `0.5`, and class 1 is defined as `w1` `*` `x` `+` `w2` `*` `y` `+` `b` `>` `0.5`.
    You’ll notice that what you’re looking at is really the equation of a line in
    the 2D plane: `w1` `*` `x` `+` `w2` `*` `y` `+` `b` `=` `0.5`. Above the line
    is class 1, and below the line is class 0\. You may be used to seeing line equations
    in the format `y` `=` `a` `*` `x` `+` `b`; in the same format, our line becomes
    `y` `=` `-` `w1` `/` `w2` `*` `x` `+` `(0.5` `-` `b)` `/` `w2`.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，给定点`[x,` `y]`的预测值简单地为`prediction` `==` `[[w1],` `[w2]]` `•` `[x,` `y]` `+`
    `b` `==` `w1` `*` `x` `+` `w2` `*` `y` `+` `b`。因此，类0被定义为`w1` `*` `x` `+` `w2`
    `*` `y` `+` `b` `<` `0.5`，类1被定义为`w1` `*` `x` `+` `w2` `*` `y` `+` `b` `>` `0.5`。你会注意到你所看到的实际上是二维平面上的一条直线方程：`w1`
    `*` `x` `+` `w2` `*` `y` `+` `b` `=` `0.5`。在直线上方是类1，在直线下方是类0。你可能习惯于看到直线方程的格式为`y`
    `=` `a` `*` `x` `+` `b`；以相同格式，我们的直线变成了`y` `=` `-` `w1` `/` `w2` `*` `x` `+` `(0.5`
    `-` `b)` `/` `w2`。
- en: 'Let’s plot this line (shown in figure 3.8):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制这条直线（如图3.8所示）：
- en: '[PRE23]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Generate 100 regularly spaced numbers between –1 and 4, which we will use
    to plot our line.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 生成100个在-1到4之间均匀间隔的数字，我们将用它们来绘制我们的直线。
- en: ❷ This is our line’s equation.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是我们直线的方程。
- en: ❸ Plot our line ("-r" means “plot it as a red line”).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制我们的直线（"-r"表示“将其绘制为红色线”）。
- en: ❹ Plot our model’s predictions on the same plot.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制我们模型的预测在同一图中。
- en: '![](../Images/03-08.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-08.png)'
- en: Figure 3.8 Our model, visualized as a line
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 我们模型，可视化为一条直线
- en: 'This is really what a linear classifier is all about: finding the parameters
    of a line (or, in higher-dimensional spaces, a hyperplane) neatly separating two
    classes of data.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这才是线性分类器的真正含义：找到一个线（或者在更高维空间中，一个超平面）的参数，将两类数据清晰地分开。
- en: '3.6 Anatomy of a neural network: Understanding core Keras APIs'
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 神经网络的解剖：理解核心 Keras API
- en: 'At this point, you know the basics of TensorFlow, and you can use it to implement
    a toy model from scratch, such as the batch linear classifier in the previous
    section, or the toy neural network at the end of chapter 2\. That’s a solid foundation
    to build upon. It’s now time to move on to a more productive, more robust path
    to deep learning: the Keras API.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经了解了 TensorFlow 的基础知识，并且可以使用它从头开始实现一个玩具模型，比如前一节中的批量线性分类器，或者第2章末尾的玩具神经网络。这是一个坚实的基础，可以继续建立。现在是时候转向更具生产力、更健壮的深度学习路径了：Keras
    API。
- en: '3.6.1 Layers: The building blocks of deep learning'
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.1 层：深度学习的构建模块
- en: 'The fundamental data structure in neural networks is the *layer*, to which
    you were introduced in chapter 2\. A layer is a data processing module that takes
    as input one or more tensors and that outputs one or more tensors. Some layers
    are stateless, but more frequently layers have a state: the layer’s *weights*,
    one or several tensors learned with stochastic gradient descent, which together
    contain the network’s *knowledge*.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的基本数据结构是*层*，你在第2章中已经介绍过。层是一个数据处理模块，它以一个或多个张量作为输入，并输出一个或多个张量。一些层是无状态的，但更频繁的情况是层有一个状态：层的*权重*，一个或多个使用随机梯度下降学习的张量，它们一起包含网络的*知识*。
- en: Different types of layers are appropriate for different tensor formats and different
    types of data processing. For instance, simple vector data, stored in rank-2 tensors
    of shape `(samples,` `features)`, is often processed by *densely connected* layers,
    also called *fully connected* or *dense* layers (the `Dense` class in Keras).
    Sequence data, stored in rank-3 tensors of shape `(samples,` `timesteps,` `features)`,
    is typically processed by *recurrent* layers, such as an `LSTM` layer, or 1D convolution
    layers (`Conv1D`). Image data, stored in rank-4 tensors, is usually processed
    by 2D convolution layers (`Conv2D`).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的层适用于不同的张量格式和不同类型的数据处理。例如，简单的向量数据，存储在形状为`(samples, features)`的秩-2张量中，通常由*密集连接*层处理，也称为*全连接*或*密集*层（Keras中的`Dense`类）。序列数据，存储在形状为`(samples,
    timesteps, features)`的秩-3张量中，通常由*循环*层处理，例如`LSTM`层，或1D卷积层（`Conv1D`）。图像数据，存储在秩-4张量中，通常由2D卷积层（`Conv2D`）处理。
- en: You can think of layers as the LEGO bricks of deep learning, a metaphor that
    is made explicit by Keras. Building deep learning models in Keras is done by clipping
    together compatible layers to form useful data-transformation pipelines.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把层想象成深度学习的乐高积木，这个比喻在Keras中是明确的。在Keras中构建深度学习模型是通过将兼容的层剪辑在一起形成有用的数据转换流水线。
- en: The base Layer class in Keras
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的基础Layer类
- en: A simple API should have a single abstraction around which everything is centered.
    In Keras, that’s the `Layer` class. Everything in Keras is either a `Layer` or
    something that closely interacts with a `Layer`.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的API应该围绕一个单一的抽象进行中心化。在Keras中，这就是`Layer`类。Keras中的一切都是一个`Layer`或与`Layer`紧密交互的东西。
- en: A `Layer` is an object that encapsulates some state (weights) and some computation
    (a forward pass). The weights are typically defined in a `build()` (although they
    could also be created in the constructor, `__init__()`), and the computation is
    defined in the `call()` method.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`Layer`是一个封装了一些状态（权重）和一些计算（前向传播）的对象。权重通常在`build()`中定义（尽管它们也可以在构造函数`__init__()`中创建），计算在`call()`方法中定义。
- en: In the previous chapter, we implemented a `NaiveDense` class that contained
    two weights `W` and `b` and applied the computation `output` `=` `activation(dot(input,`
    `W)` `+` `b)`. This is what the same layer would look like in Keras.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们实现了一个`NaiveDense`类，其中包含两个权重`W`和`b`，并应用了计算`output = activation(dot(input,
    W) + b)`。这就是在Keras中相同层的样子。
- en: Listing 3.22 A `Dense` layer implemented as a `Layer` subclass
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.22 作为`Layer`子类实现的`Dense`层
- en: '[PRE24]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ All Keras layers inherit from the base Layer class.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有的Keras层都继承自基础的Layer类。
- en: ❷ Weight creation takes place in the build() method.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 权重的创建发生在`build()`方法中。
- en: ❸ add_weight() is a shortcut method for creating weights. It is also possible
    to create standalone variables and assign them as layer attributes, like self.W
    = tf.Variable(tf.random.uniform(w_shape)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `add_weight()`是一个创建权重的快捷方法。也可以创建独立的变量并将它们分配为层属性，如`self.W = tf.Variable(tf.random.uniform(w_shape))`。
- en: ❹ We define the forward pass computation in the call() method.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们在`call()`方法中定义了前向传播计算。
- en: In the next section, we’ll cover in detail the purpose of these `build()` and
    `call()` methods. Don’t worry if you don’t understand everything just yet!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将详细介绍这些`build()`和`call()`方法的目的。如果你现在还不理解，不要担心！
- en: 'Once instantiated, a layer like this can be used just like a function, taking
    as input a TensorFlow tensor:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例化，像这样的层可以像函数一样使用，以TensorFlow张量作为输入：
- en: '[PRE25]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Instantiate our layer, defined previously.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化我们之前定义的层。
- en: ❷ Create some test inputs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一些测试输入。
- en: ❸ Call the layer on the inputs, just like a function.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在输入上调用层，就像调用函数一样。
- en: You’re probably wondering, why did we have to implement `call()` and `build()`,
    since we ended up using our layer by plainly calling it, that is to say, by using
    its `__call__()` method? It’s because we want to be able to create the state just
    in time. Let’s see how that works.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们要实现`call()`和`build()`，因为我们最终只是简单地调用了我们的层，也就是说，使用了它的`__call__()`方法？这是因为我们希望能够及时创建状态。让我们看看它是如何工作的。
- en: 'Automatic shape inference: Building layers on the fly'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 自动形状推断：动态构建层
- en: 'Just like with LEGO bricks, you can only “clip” together layers that are compatible.
    The notion of *layer compatibility* here refers specifically to the fact that
    every layer will only accept input tensors of a certain shape and will return
    output tensors of a certain shape. Consider the following example:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 就像乐高积木一样，你只能“连接”兼容的层。这里的*层兼容性*概念特指每个层只接受特定形状的输入张量，并返回特定形状的输出张量。考虑以下示例：
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ A dense layer with 32 output units
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个具有32个输出单元的密集层
- en: This layer will return a tensor where the first dimension has been transformed
    to be 32\. It can only be connected to a downstream layer that expects 32-dimensional
    vectors as its input.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层将返回一个张量，其中第一个维度已经被转换为32。它只能连接到一个期望32维向量作为输入的下游层。
- en: 'When using Keras, you don’t have to worry about size compatibility most of
    the time, because the layers you add to your models are dynamically built to match
    the shape of the incoming layer. For instance, suppose you write the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Keras时，大多数情况下你不必担心大小的兼容性，因为你添加到模型中的层会动态构建以匹配传入层的形状。例如，假设你写下以下内容：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The layers didn’t receive any information about the shape of their inputs—instead,
    they automatically inferred their input shape as being the shape of the first
    inputs they see.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 层没有接收到关于它们输入形状的任何信息——相反，它们自动推断它们的输入形状为它们看到的第一个输入的形状。
- en: 'In the toy version of the `Dense` layer we implemented in chapter 2 (which
    we named `NaiveDense`), we had to pass the layer’s input size explicitly to the
    constructor in order to be able to create its weights. That’s not ideal, because
    it would lead to models that look like this, where each new layer needs to be
    made aware of the shape of the layer before it:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们在第2章中实现的`Dense`层的玩具版本中（我们称之为`NaiveDense`），我们必须显式地将层的输入大小传递给构造函数，以便能够创建其权重。这并不理想，因为这将导致模型看起来像这样，其中每个新层都需要知道其前一层的形状：
- en: '[PRE28]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: It would be even worse if the rules used by a layer to produce its output shape
    are complex. For instance, what if our layer returned outputs of shape `(batch,`
    `input_ size` `*` `2` `if` `input_size` `%` `2` `==` `0` `else` `input_size` `*`
    `3)`?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个层用于生成其输出形状的规则很复杂，情况会变得更糟。例如，如果我们的层返回形状为`(batch,` `input_ size` `*` `2` `if`
    `input_size` `%` `2` `==` `0` `else` `input_size` `*` `3)`的输出会怎样？
- en: If we were to reimplement our `NaiveDense` layer as a Keras layer capable of
    automatic shape inference, it would look like the previous `SimpleDense` layer
    (see listing 3.22), with its `build()` and `call()` methods.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要将我们的`NaiveDense`层重新实现为一个能够自动推断形状的Keras层，它将看起来像之前的`SimpleDense`层（见列表3.22），具有其`build()`和`call()`方法。
- en: 'In `SimpleDense`, we no longer create weights in the constructor like in the
    `NaiveDense` example; instead, we create them in a dedicated state-creation method,
    `build()`, which receives as an argument the first input shape seen by the layer.
    The `build()` method is called automatically the first time the layer is called
    (via its `__call__()` method). In fact, that’s why we defined the computation
    in a separate `call()` method rather than in the `__call__()` method directly.
    The `__call__()` method of the base layer schematically looks like this:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SimpleDense`中，我们不再像`NaiveDense`示例中那样在构造函数中创建权重；相反，我们在一个专门的状态创建方法`build()`中创建它们，该方法接收层首次看到的第一个输入形状作为参数。`build()`方法在第一次调用层时（通过其`__call__()`方法）会自动调用。事实上，这就是为什么我们将计算定义在单独的`call()`方法中而不是直接在`__call__()`方法中的原因。基础层的`__call__()`方法基本上是这样的：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'With automatic shape inference, our previous example becomes simple and neat:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 有了自动形状推断，我们之前的示例变得简单而整洁：
- en: '[PRE30]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Note that automatic shape inference is not the only thing that the `Layer`
    class’s `__call__()` method handles. It takes care of many more things, in particular
    routing between *eager* and *graph* execution (a concept you’ll learn about in
    chapter 7), and input masking (which we’ll cover in chapter 11). For now, just
    remember: when implementing your own layers, put the forward pass in the `call()`
    method.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，自动形状推断并不是`Layer`类的`__call__()`方法处理的唯一事情。它还处理许多其他事情，特别是在*eager*和*graph*执行之间的路由（这是你将在第7章学习的概念），以及输入掩码（我们将在第11章中介绍）。现在，只需记住：当实现自己的层时，将前向传播放在`call()`方法中。
- en: 3.6.2 From layers to models
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.2 从层到模型
- en: 'A deep learning model is a graph of layers. In Keras, that’s the `Model` class.
    Until now, you’ve only seen `Sequential` models (a subclass of `Model`), which
    are simple stacks of layers, mapping a single input to a single output. But as
    you move forward, you’ll be exposed to a much broader variety of network topologies.
    These are some common ones:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型是一系列层的图。在Keras中，这就是`Model`类。到目前为止，你只看到过`Sequential`模型（`Model`的子类），它们是简单的层堆叠，将单个输入映射到单个输出。但随着你的学习，你将接触到更广泛的网络拓扑。以下是一些常见的拓扑结构：
- en: Two-branch networks
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双分支网络
- en: Multihead networks
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多头网络
- en: Residual connections
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差连接
- en: Network topology can get quite involved. For instance, figure 3.9 shows the
    topology of the graph of layers of a Transformer, a common architecture designed
    to process text data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑可能会变得非常复杂。例如，图3.9显示了Transformer的层图拓扑，这是一种常见的用于处理文本数据的架构。
- en: '![](../Images/03-09.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-09.png)'
- en: Figure 3.9 The Transformer architecture (covered in chapter 11). There’s a lot
    going on here. Throughout the next few chapters, you’ll climb your way up to understanding
    it.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 变压器架构（在第11章中介绍）。这里面有很多内容。在接下来的几章中，你将逐步理解它。
- en: 'There are generally two ways of building such models in Keras: you could directly
    subclass the `Model` class, or you could use the Functional API, which lets you
    do more with less code. We’ll cover both approaches in chapter 7.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中通常有两种构建这种模型的方法：你可以直接子类化`Model`类，或者你可以使用Functional API，它让你用更少的代码做更多的事情。我们将在第7章中涵盖这两种方法。
- en: The topology of a model defines a *hypothesis space*. You may remember that
    in chapter 1 we described machine learning as searching for useful representations
    of some input data, within a predefined *space of possibilities*, using guidance
    from a feedback signal. By choosing a network topology, you constrain your space
    of possibilities (hypothesis space) to a specific series of tensor operations,
    mapping input data to output data. What you’ll then be searching for is a good
    set of values for the weight tensors involved in these tensor operations.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的拓扑定义了一个*假设空间*。你可能还记得，在第1章中我们将机器学习描述为在预定义的*可能性空间*内搜索一些输入数据的有用表示，使用来自反馈信号的指导。通过选择网络拓扑，你将限制你的可能性空间（假设空间）到一系列特定的张量操作，将输入数据映射到输出数据。接下来，你将搜索这些张量操作中涉及的权重张量的良好值集。
- en: To learn from data, you have to make assumptions about it. These assumptions
    define what can be learned. As such, the structure of your hypothesis space—the
    architecture of your model—is extremely important. It encodes the assumptions
    you make about your problem, the prior knowledge that the model starts with. For
    instance, if you’re working on a two-class classification problem with a model
    made of a single `Dense` layer with no activation (a pure affine transformation),
    you are assuming that your two classes are linearly separable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要从数据中学习，您必须对其进行假设。这些假设定义了可以学到的内容。因此，您的假设空间的结构——模型的架构——非常重要。它编码了您对问题的假设，模型开始的先验知识。例如，如果您正在处理一个由单个`Dense`层组成且没有激活函数（纯仿射变换）的模型的两类分类问题，那么您假设您的两类是线性可分的。
- en: Picking the right network architecture is more an art than a science, and although
    there are some best practices and principles you can rely on, only practice can
    help you become a proper neural-network architect. The next few chapters will
    both teach you explicit principles for building neural networks and help you develop
    intuition as to what works or doesn’t work for specific problems. You’ll build
    a solid intuition about what type of model architectures work for different kinds
    of problems, how to build these networks in practice, how to pick the right learning
    configuration, and how to tweak a model until it yields the results you want to
    see.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的网络架构更多地是一门艺术而不是一门科学，尽管有一些最佳实践和原则可以依靠，但只有实践才能帮助你成为一个合格的神经网络架构师。接下来的几章将教授您构建神经网络的明确原则，并帮助您培养对于特定问题的有效性或无效性的直觉。您将建立对于不同类型问题适用的模型架构的坚实直觉，如何在实践中构建这些网络，如何选择正确的学习配置，以及如何调整模型直到产生您想要看到的结果。
- en: '3.6.3 The “compile” step: Configuring the learning process'
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.3 “compile”步骤：配置学习过程
- en: 'Once the model architecture is defined, you still have to choose three more
    things:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型架构被定义，您仍然必须选择另外三个事项：
- en: '*Loss function (objective function)*—The quantity that will be minimized during
    training. It represents a measure of success for the task at hand.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数（目标函数）*—在训练过程中将被最小化的数量。它代表了任务的成功度量。'
- en: '*Optimizer*—Determines how the network will be updated based on the loss function.
    It implements a specific variant of stochastic gradient descent (SGD).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*—根据损失函数确定网络���如何更新。它实现了随机梯度下降（SGD）的特定变体。'
- en: '*Metrics*—The measures of success you want to monitor during training and validation,
    such as classification accuracy. Unlike the loss, training will not optimize directly
    for these metrics. As such, metrics don’t need to be differentiable.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Metrics*—在训练和验证过程中要监视的成功度量，例如分类准确度。与损失不同，训练不会直接为这些指标进行优化。因此，指标不需要可微分。'
- en: Once you’ve picked your loss, optimizer, and metrics, you can use the built-in
    `compile()` and `fit()` methods to start training your model. Alternatively, you
    could also write your own custom training loops—we’ll cover how to do this in
    chapter 7\. It’s a lot more work! For now, let’s take a look at `compile()` and
    `fit()`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您选择了损失、优化器和指标，您可以使用内置的`compile()`和`fit()`方法开始训练您的模型。或者，您也可以编写自己的自定义训练循环——我们将在第七章中介绍如何做到这一点。这是更多的工作！现在，让我们看看`compile()`和`fit()`。
- en: 'The `compile()` method configures the training process—you’ve already been
    introduced to it in your very first neural network example in chapter 2\. It takes
    the arguments `optimizer`, `loss`, and `metrics` (a list):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '`compile()`方法配置训练过程——你在第二章的第一个神经网络示例中已经见过它。它接受`optimizer`、`loss`和`metrics`（一个列表）作为参数：'
- en: '[PRE31]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Define a linear classifier.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个线性分类器。
- en: '❷ Specify the optimizer by name: RMSprop (it’s case-insensitive).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过名称指定优化器：RMSprop（不区分大小写）。
- en: '❸ Specify the loss by name: mean squared error.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过名称指定损失：均方误差。
- en: '❹ Specify a list of metrics: in this case, only accuracy.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 指定一个指标列表：在这种情况下，只有准确度。
- en: 'In the preceding call to `compile()`, we passed the optimizer, loss, and metrics
    as strings (such as `"rmsprop"`). These strings are actually shortcuts that get
    converted to Python objects. For instance, `"rmsprop"` becomes `keras.optimizers.RMSprop()`.
    Importantly, it’s also possible to specify these arguments as object instances,
    like this:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面对`compile()`的调用中，我们将优化器、损失和指标作为字符串传递（例如`"rmsprop"`）。这些字符串实际上是转换为Python对象的快捷方式。例如，`"rmsprop"`变成了`keras.optimizers.RMSprop()`。重要的是，也可以将这些参数指定为对象实例，如下所示：
- en: '[PRE32]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This is useful if you want to pass your own custom losses or metrics, or if
    you want to further configure the objects you’re using—for instance, by passing
    a `learning_rate` argument to the optimizer:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想传递自定义损失或指标，或者如果您想进一步配置您正在使用的对象，例如通过向优化器传递`learning_rate`参数：
- en: '[PRE33]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In chapter 7, we’ll cover how to create custom losses and metrics. In general,
    you won’t have to create your own losses, metrics, or optimizers from scratch,
    because Keras offers a wide range of built-in options that is likely to include
    what you need:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在第七章中，我们将介绍如何创建自定义损失和指标。一般来说，您不必从头开始创建自己的损失、指标或优化器，因为Keras提供了广泛的内置选项，很可能包括您需要的内容：
- en: 'Optimizers:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '优化器： '
- en: '`SGD` (with or without momentum)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SGD`（带有或不带有动量）'
- en: '`RMSprop`'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RMSprop`'
- en: '`Adam`'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Adam`'
- en: '`Adagrad`'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Adagrad`'
- en: Etc.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: 'Losses:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 损失：
- en: '`CategoricalCrossentropy`'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CategoricalCrossentropy`'
- en: '`SparseCategoricalCrossentropy`'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparseCategoricalCrossentropy`'
- en: '`BinaryCrossentropy`'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryCrossentropy`'
- en: '`MeanSquaredError`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MeanSquaredError`'
- en: '`KLDivergence`'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KLDivergence`'
- en: '`CosineSimilarity`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CosineSimilarity`'
- en: Etc.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: 'Metrics:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 指标：
- en: '`CategoricalAccuracy`'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CategoricalAccuracy`'
- en: '`SparseCategoricalAccuracy`'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SparseCategoricalAccuracy`'
- en: '`BinaryAccuracy`'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryAccuracy`'
- en: '`AUC`'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AUC`'
- en: '`Precision`'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Precision`'
- en: '`Recall`'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recall`'
- en: Etc.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: Throughout this book, you’ll see concrete applications of many of these options.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，您将看到许多这些选项的具体应用。
- en: 3.6.4 Picking a loss function
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.4 选择损失函数
- en: 'Choosing the right loss function for the right problem is extremely important:
    your network will take any shortcut it can to minimize the loss, so if the objective
    doesn’t fully correlate with success for the task at hand, your network will end
    up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained
    via SGD with this poorly chosen objective function: “maximizing the average well-being
    of all humans alive.” To make its job easier, this AI might choose to kill all
    humans except a few and focus on the well-being of the remaining ones—because
    average well-being isn’t affected by how many humans are left. That might not
    be what you intended! Just remember that all neural networks you build will be
    just as ruthless in lowering their loss function—so choose the objective wisely,
    or you’ll have to face unintended side effects.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为正确的问题选择正确的损失函数非常重要：你的网络会尽其所能缩小损失，因此如果目标与当前任务的成功并不完全相关，你的网络最终可能会执行一些你不希望的操作。想象一下，通过使用这个选择不当的目标函数（“最大化所有活着人类的平均幸福感”）进行
    SGD 训练的愚蠢、全能的 AI。为了简化工作，这个 AI 可能选择杀死除少数人外的所有人类，并专注于剩下人的幸福感——因为平均幸福感不受剩余人数的影响。这可能不是你想要的结果！请记住，你构建的所有神经网络都会像这样无情地降低它们的损失函数，因此明智地选择目标，否则你将面临意想不到的副作用。
- en: Fortunately, when it comes to common problems such as classification, regression,
    and sequence prediction, there are simple guidelines you can follow to choose
    the correct loss. For instance, you’ll use binary crossentropy for a two-class
    classification problem, categorical crossentropy for a many-class classification
    problem, and so on. Only when you’re working on truly new research problems will
    you have to develop your own loss functions. In the next few chapters, we’ll detail
    explicitly which loss functions to choose for a wide range of common tasks.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于常见问题如分类、回归和序列预测，你可以遵循简单的准则来选择正确的损失函数。例如，对于两类分类问题，你将使用二元交叉熵，对于多类分类问题，你将使用分类交叉熵，依此类推。只有在处理真正新的研究问题时，你才需要开发自己的损失函数。在接下来的几章中，我们将明确详细地介绍为各种常见任务选择哪些损失函数。
- en: 3.6.5 Understanding the fit() method
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.5 理解 fit() 方法
- en: 'After `compile()` comes `fit()`. The `fit()` method implements the training
    loop itself. These are its key arguments:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `compile()` 之后是 `fit()`。`fit()` 方法实现了训练循环本身。以下是它的关键参数：
- en: The *data* (inputs and targets) to train on. It will typically be passed either
    in the form of NumPy arrays or a TensorFlow `Dataset` object. You’ll learn more
    about the `Dataset` API in the next chapters.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于训练的*数据*（输入和目标）。通常会以 NumPy 数组或 TensorFlow `Dataset` 对象的形式传递。你将在接下来的章节中更多地了解
    `Dataset` API。
- en: 'The number of *epochs* to train for: how many times the training loop should
    iterate over the data passed.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的*轮数*：训练循环应该迭代传递的数据多少次。
- en: 'The batch size to use within each epoch of mini-batch gradient descent: the
    number of training examples considered to compute the gradients for one weight
    update step.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个迷你批次梯度下降的 epoch 中使用的批次大小：用于计算一次权重更新步骤的训练示例数量。
- en: Listing 3.23 Calling `fit()` with NumPy data
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3.23 节 使用 NumPy 数据调用 `fit()`
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ The input examples, as a NumPy array
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入示例，作为 NumPy 数组
- en: ❷ The corresponding training targets, as a NumPy array
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 相应的训练目标，作为 NumPy 数组
- en: ❸ The training loop will iterate over the data 5 times.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练循环将在数据上迭代 5 次。
- en: ❹ The training loop will iterate over the data in batches of 128 examples.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练循环将以 128 个示例的批次迭代数据。
- en: The call to `fit()` returns a `History` object. This object contains a `history`
    field, which is a dict mapping keys such as `"loss"` or specific metric names
    to the list of their per-epoch values.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `fit()` 返回一个 `History` 对象。该对象包含一个 `history` 字段，它是一个将诸如 `"loss"` 或特定指标名称映射到每个
    epoch 值列表的字典。
- en: '[PRE35]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 3.6.6 Monitoring loss and metrics on validation data
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.6 监控验证数据上的损失和指标
- en: The goal of machine learning is not to obtain models that perform well on the
    training data, which is easy—all you have to do is follow the gradient. The goal
    is to obtain models that perform well in general, and particularly on data points
    that the model has never encountered before. Just because a model performs well
    on its training data doesn’t mean it will perform well on data it has never seen!
    For instance, it’s possible that your model could end up merely *memorizing* a
    mapping between your training samples and their targets, which would be useless
    for the task of predicting targets for data the model has never seen before. We’ll
    go over this point in much more detail in chapter 5.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的目标不是获得在训练数据上表现良好的模型，这很容易——你只需遵循梯度。目标是获得在一般情况下表现良好的模型，特别是在模型从未遇到过的数据点上表现良好。仅仅因为一个模型在训练数据上表现良好并不意味着它会在从未见过的数据上表现良好！例如，你的模型可能最终只是*记忆*训��样本和它们的目标之间的映射，这对于预测模型从未见过的数据的目标是无用的。我们将在第
    5 章中更详细地讨论这一点。
- en: 'To keep an eye on how the model does on new data, it’s standard practice to
    reserve a subset of the training data as *validation data*: you won’t be training
    the model on this data, but you will use it to compute a loss value and metrics
    value. You do this by using the `validation_data` argument in `fit()`. Like the
    training data, the validation data could be passed as NumPy arrays or as a TensorFlow
    `Dataset` object.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监视模型在新数据上的表现，通常会将训练数据的一个子集保留为*验证数据*：你不会在这些数据上训练模型，但会用它们来计算损失值和指标值。你可以通过在 `fit()`
    中使用 `validation_data` 参数来实现这一点。与训练数据类似，验证数据可以作为 NumPy 数组或 TensorFlow `Dataset`
    对象传递。
- en: Listing 3.24 Using the `validation_data` argument
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 第 3.24 节 使用 `validation_data` 参数
- en: '[PRE36]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ To avoid having samples from only one class in the validation data, shuffle
    the inputs and targets using a random indices permutation.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了避免在验证数据中只有一个类的样本，使用随机索引排列来对输入和目标进行洗牌。
- en: ❷ Reserve 30% of the training inputs and targets for validation (we’ll exclude
    these samples from training and reserve them to compute the validation loss and
    metrics).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保留30%的训练输入和目标用于验证（我们将排除这些样本进行训练，并保留它们来计算验证损失和指标）。
- en: ❸ Training data, used to update the weights of the model
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于更新模型权重的训练数据
- en: ❹ Validation data, used only to monitor the validation loss and metrics
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 仅用于监控验证损失和指标的验证数据
- en: 'The value of the loss on the validation data is called the “validation loss,”
    to distinguish it from the “training loss.” Note that it’s essential to keep the
    training data and validation data strictly separate: the purpose of validation
    is to monitor whether what the model is learning is actually useful on new data.
    If any of the validation data has been seen by the model during training, your
    validation loss and metrics will be flawed.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证数据上的损失值称为“验证损失”，以区别于“训练损失”。请注意，保持训练数据和验证数据严格分开是至关重要的：验证的目的是监测模型学习的内容是否实际上对新数据有用。如果模型在训练过程中看到任何验证数据，您的验证损失和指标将是有缺陷的。
- en: 'Note that if you want to compute the validation loss and metrics after the
    training is complete, you can call the `evaluate()` method:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果您想在训练完成后计算验证损失和指标，可以调用`evaluate()`方法：
- en: '[PRE37]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '`evaluate()` will iterate in batches (of size `batch_size`) over the data passed
    and return a list of scalars, where the first entry is the validation loss and
    the following entries are the validation metrics. If the model has no metrics,
    only the validation loss is returned (rather than a list).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate()`将在传递的数据上以批量（大小为`batch_size`）迭代，并返回一个标量列表，其中第一个条目是验证损失，后���条目是验证指标。如果模型没有指标，只返回验证损失（而不是列表）。'
- en: '3.6.7 Inference: Using a model after training'
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6.7 推断：在训练后使用模型
- en: 'Once you’ve trained your model, you’re going to want to use it to make predictions
    on new data. This is called *inference*. To do this, a naive approach would simply
    be to `__call__()` the model:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您训练好模型，您将想要使用它在新数据上进行预测。这被称为*推断*。为此，一个简单的方法就是简单地`__call__()`模型：
- en: '[PRE38]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Takes a NumPy array or TensorFlow tensor and returns a TensorFlow tensor
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接受一个NumPy数组或TensorFlow张量，并返回一个TensorFlow张量
- en: However, this will process all inputs in `new_inputs` at once, which may not
    be feasible if you’re looking at a lot of data (in particular, it may require
    more memory than your GPU has).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这将一次性处理`new_inputs`中的所有输入，如果你要处理大量数据可能不可行（特别是可能需要比你的GPU更多的内存）。
- en: A better way to do inference is to use the `predict()` method. It will iterate
    over the data in small batches and return a NumPy array of predictions. And unlike
    `__call__()`, it can also process TensorFlow `Dataset` objects.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 进行推断的更好方法是使用`predict()`方法。它将以小批量迭代数据，并返回一个预测的NumPy数组。与`__call__()`不同，它还可以处理TensorFlow的`Dataset`对象。
- en: '[PRE39]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Takes a NumPy array or a Dataset and returns a NumPy array
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接受一个NumPy数组或数据集，并返回一个NumPy数组
- en: 'For instance, if we use `predict()` on some of our validation data with the
    linear model we trained earlier, we get scalar scores that correspond to the model’s
    prediction for each input sample:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们对之前训练过的线性模型使用`predict()`在一些验证数据上，我们会得到对应于模型对每个输入样本的预测的标量分数：
- en: '[PRE40]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: For now, this is all you need to know about Keras models. You are ready to move
    on to solving real-world machine learning problems with Keras in the next chapter.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这就是您需要了解的关于Keras模型的全部内容。您已经准备好在下一章节中使用Keras解决真实世界的机器学习问题了。
- en: Summary
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow is an industry-strength numerical computing framework that can run
    on CPU, GPU, or TPU. It can automatically compute the gradient of any differentiable
    expression, it can be distributed to many devices, and it can export programs
    to various external runtimes—even JavaScript.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow是一个工业强度的数值计算框架，可以在CPU、GPU或TPU上运行。它可以自动计算任何可微表达式的梯度，可以分布到许多设备，还可以将程序导出到各种外部运行时，甚至JavaScript。
- en: Keras is the standard API for doing deep learning with TensorFlow. It’s what
    we’ll use throughout this book.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras是使用TensorFlow进行深度学习的标准API。这是我们将在整本书中使用的。
- en: Key TensorFlow objects include tensors, variables, tensor operations, and the
    gradient tape.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow的关键对象包括张量、变量、张量操作和梯度带。
- en: The central class of Keras is the `Layer`. A *layer* encapsulates some weights
    and some computation. Layers are assembled into *models*.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras的核心类是`Layer`。一个*层*封装了一些权重和一些计算。层被组装成*模型*。
- en: Before you start training a model, you need to pick an *optimizer*, a *loss*,
    and some *metrics*, which you specify via the `model.compile()` method.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始训练模型之前，您需要选择一个*优化器*，一个*损失*和一些*指标*，您可以通过`model.compile()`方法指定。
- en: To train a model, you can use the `fit()` method, which runs mini-batch gradient
    descent for you. You can also use it to monitor your loss and metrics on *validation
    data*, a set of inputs that the model doesn’t see during training.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要训练一个模型，您可以使用`fit()`方法，它为您运行小批量梯度下降。您还可以使用它来监视您在*验证数据*上的损失和指标，这是模型在训练过程中没有看到的一组输入。
- en: Once your model is trained, you use the `model.predict()` method to generate
    predictions on new inputs.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦您的模型训练完成，您可以使用`model.predict()`方法在新输入上生成预测。
