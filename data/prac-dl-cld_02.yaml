- en: 'Chapter 2\. What’s in the Picture: Image Classification with Keras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章。图片中有什么：使用Keras进行图像分类
- en: If you have skimmed through deep learning literature, you might have come across
    a barrage of academic explanations laced with intimidating mathematics. Don’t
    worry. We will ease you into practical deep learning with an example of classifying
    images with just a few lines of code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您浏览过深度学习文献，可能会看到一大堆充斥着令人生畏的数学的学术解释。不用担心。我们将通过一个简单的例子来引导您进入实际的深度学习，只需几行代码就可以对图像进行分类。
- en: In this chapter, we take a closer look at the Keras framework, discuss its place
    in the deep learning landscape, and then use it to classify a few images using
    existing state-of-the-art classifiers. We visually investigate how these classifiers
    operate by using *heatmaps*. With these heatmaps, we make a fun project in which
    we classify objects in videos.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更仔细地研究Keras框架，讨论它在深度学习领域的地位，然后使用它来使用现有的最先进分类器对一些图像进行分类。我们通过使用*热图*来直观地研究这些分类器的运作方式。通过这些热图，我们将在视频中对对象进行分类，做一个有趣的项目。
- en: 'Recall from the [“Recipe for the Perfect Deep Learning Solution”](part0003.html#2RHT2-13fa565533764549a6f0ab7f11eed62b)
    that we need four ingredients to create our deep learning recipe: hardware, dataset,
    framework, and model. Let’s see how each of these comes into play in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[“完美深度学习解决方案的配方”](part0003.html#2RHT2-13fa565533764549a6f0ab7f11eed62b)，我们需要四个要素来创建我们的深度学习配方：硬件、数据集、框架和模型。让我们看看这些在本章中是如何发挥作用的：
- en: 'We begin with the easy one: *hardware*. Even an inexpensive laptop would suffice
    for what we we’re doing in this chapter. Alternatively, you can run the code in
    this chapter by opening the GitHub notebook (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai))
    in Colab. This is just a matter of a few mouse clicks.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们从简单的*硬件*开始。即使是一台廉价的笔记本电脑也足以满足本章的需求。或者，您可以通过在Colab中打开GitHub笔记本（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)）来运行本章中的代码。这只是几次鼠标点击的事情。
- en: Because we won’t be training a neural network just yet, we don’t need a *dataset*
    (other than a handful of sample photos to test with).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因为我们暂时不会训练神经网络，所以我们不需要一个*数据集*（除了一些样本照片用于测试）。
- en: Next, we come to the *framework*. This chapter’s title has Keras in it, so that
    is what we will be using for now. In fact, we use Keras for our training needs
    throughout a good part of the book.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们来看*框架*。本章的标题中包含了Keras，所以我们暂时会使用它。事实上，在本书的很大一部分中，我们都会使用Keras来满足我们的训练需求。
- en: One way to approach a deep learning problem is to obtain a dataset, write the
    code to train it, spend a lot of time and energy (both human and electrical) in
    training that model, and then use it for making predictions. But we are not gluttons
    for punishment. So, we will use a *pretrained model* instead. After all, the research
    community has already spent blood, sweat, and tears training and publishing many
    of the standard models that are now publicly available. We will be reusing one
    of the more famous models called ResNet-50, the little sibling of ResNet-152 that
    won the ILSVRC in 2015.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决深度学习问题的一种方法是获取一个数据集，编写训练代码，花费大量时间和精力（包括人力和电力）来训练模型，然后用它进行预测。但我们不是自寻烦恼的人。因此，我们将使用一个*预训练模型*。毕竟，研究界已经花费了大量心血和泪水来训练和发布许多现在公开可用的标准模型。我们将重复使用其中一个更著名的模型，名为ResNet-50，这是ResNet-152的小兄弟，后者在2015年赢得了ILSVRC比赛。
- en: You will get hands-on with some code in this chapter. As we all know, the best
    way to learn is by doing. You might be wondering, though, what’s the theory behind
    this? That comes in later chapters, in which we delve deeper into the nuts and
    bolts of CNNs using this chapter as a foundation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将亲自动手编写一些代码。众所周知，学习的最佳方式是通过实践。不过，您可能会想知道，这背后的理论是什么？这将在后续章节中介绍，我们将通过本章作为基础，更深入地探讨CNN的细节。
- en: Introducing Keras
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍Keras
- en: As [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b) discussed,
    Keras started in 2015 as an easy-to-use abstraction layer over other libraries,
    making rapid prototyping possible. This made the learning curve a lot less steep
    for beginners of deep learning. At the same time, it made deep learning experts
    more productive by helping them rapidly iterate on experiments. In fact, the majority
    of the winning teams on *[Kaggle.com](http://Kaggle.com)* (which hosts data science
    competitions) have used Keras. Eventually, in 2017, the full implementation of
    Keras was available directly in TensorFlow, thereby combining the high scalability,
    performance, and vast ecosystem of TensorFlow with the ease of Keras. On the web,
    we often see the TensorFlow version of Keras referred to as `tf.keras`.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第1章](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)所讨论的，Keras于2015年作为一个易于使用的抽象层出现，使快速原型设计成为可能。这使得深度学习的初学者的学习曲线陡峭度降低了很多。同时，它通过帮助他们快速迭代实验，使深度学习专家更加高效。事实上，[Kaggle.com](http://Kaggle.com)上的大多数获胜团队都使用了Keras。最终，在2017年，Keras的完整实现直接集成到了TensorFlow中，将TensorFlow的高可扩展性、性能和庞大的生态系统与Keras的易用性结合在一起。在网络上，我们经常看到将TensorFlow版本的Keras称为`tf.keras`。
- en: In this chapter and [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b),
    we write all of the code exclusively in Keras. That includes boilerplate functions
    such as file reading, image manipulation (augmentation), and so on. We do this
    primarily for ease of learning. From [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)
    onward, we begin to gradually use more of the native performant TensorFlow functions
    directly for more configurability and control.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章和[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中，我们完全使用Keras编写所有代码。这包括文件读取、图像处理（增强）等样板函数。我们主要出于学习的便利性。从[第5章](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)开始，我们逐渐开始直接使用更多原生高性能的TensorFlow函数，以获得更多的可配置性和控制。
- en: Predicting an Image’s Category
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测图像的类别
- en: 'In layperson’s terms, image classification answers the question: “what object
    does this image contain?” More specifically, “This image contains *X* object with
    what probability,” where *X* is from a predefined list of categories of objects.
    If the probability is higher than a minimum threshold, the image is likely to
    contain one or more instances of *X*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用通俗的语言来说，图像分类回答了一个问题：“这张图像包含什么对象？”更具体地说，“这张图像包含* X *对象的概率是多少”，其中* X *来自预定义的对象类别列表。如果概率高于最小阈值，则图像很可能包含一个或多个*
    X *实例。
- en: 'A simple image classification pipeline would consist of the following steps:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的图像分类流程包括以下步骤：
- en: Load an image.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一张图像。
- en: Resize it to a predefined size such as 224 x 224 pixels.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其调整为预定义大小，如224 x 224像素。
- en: Scale the values of the pixel to the range [0,1] or [–1,1], a.k.a. normalization.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将像素值缩放到范围[0,1]或[–1,1]，也就是归一化。
- en: Select a pretrained model.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个预训练模型。
- en: Run the pretrained model on the image to get a list of category predictions
    and their respective probabilities.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在图像上运行预训练模型，以获取类别预测列表及其相应的概率。
- en: Display a few of the highest probability categories.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示几个最高概率类别。
- en: Tip
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The GitHub link is provided on the website [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai).
    Navigate to `code/chapter-2` where you will find the Jupyter notebook `1-predict-class.ipynb`
    that has all the steps detailed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub链接在网站[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)上提供。导航到`code/chapter-2`，您将找到详细步骤的Jupyter笔记本`1-predict-class.ipynb`。
- en: 'We begin by importing all of the necessary modules from the Keras and Python
    packages:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从Keras和Python包中导入所有必要的模块：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load and display the image that we want to classify (see [Figure 2-1](part0004.html#plot_showing_the_contents_of_the_input_f)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们加载并显示要分类的图像（参见[图2-1](part0004.html#plot_showing_the_contents_of_the_input_f)）：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Plot showing the contents of the input file](../images/00032.jpeg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![显示输入文件内容的图表](../images/00032.jpeg)'
- en: Figure 2-1\. Plot showing the contents of the input file
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。显示输入文件内容的图表
- en: Yup, it’s a cat (although the filename kind of gave it away). And that’s what
    our model should ideally be predicting.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是一只猫（尽管文件名有点暴露了）。这就是我们的模型理想情况下应该预测的内容。
- en: Before feeding any image to Keras, we want to convert it to a standard format.
    This is because pretrained models expect the input to be of a specific size. The
    standardization in our case involves resizing the image to 224 x 224 pixels.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在将任何图像传递给Keras之前，我们希望将其转换为标准格式。这是因为预训练模型期望输入具有特定大小。在我们的情况下，标准化涉及将图像调整为224 x
    224像素。
- en: Most deep learning models expect a batch of images as input. But what do we
    do when we have just one image? We create a batch of one image, of course! That
    essentially involves making an array consisting of that one object. Another way
    to look at this is to expand the number of dimensions from three (representing
    the three channels of the image) to four (the extra one for the length of the
    array itself).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习模型期望输入一批图像。但是当我们只有一张图像时该怎么办？当然，我们创建一个包含一张图像的批次！这实质上涉及制作一个由该对象组成的数组。另一种看待这个问题的方式是将维度的数量从三（表示图像的三个通道）扩展到四（额外的一个用于数组本身长度）。
- en: 'If that is not clear, consider this scenario: for a batch of 64 images of size
    224 x 224 pixels, each containing three channels (RGB), the object representing
    that batch would have a shape 64 x 224 x 224 x 3\. In the code that follows, where
    we’d be using only one 224 x 224 x 3 image, we’d create a batch of just that image
    by expanding the dimensions from three to four. The shape of this newly created
    batch would be 1 x 224 x 224 x 3:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这不清楚，考虑这种情况：对于一个包含64张尺寸为224 x 224像素的图像的批次，每张图像包含三个通道（RGB），表示该批次的对象将具有形状64
    x 224 x 224 x 3。在接下来的代码中，我们将只使用一张尺寸为224 x 224 x 3的图像，我们将通过将维度从三扩展到四来创建一个只包含该图像的批次。这个新创建的批次的形状将是1
    x 224 x 224 x 3：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In machine learning, models perform best when they are fed with data within
    a consistent range. Ranges typically include [0,1] and [–1,1]. Given that image
    pixel values are between 0 and 255, running the `preprocess_input` function from
    Keras on input images will normalize each pixel to a standard range. *Normalization*
    or *feature scaling* is one of the core steps in preprocessing images to make
    them suitable for deep learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，模型在接收到一致范围内的数据时表现最佳。范围通常包括[0,1]和[–1,1]。鉴于图像像素值在0到255之间，运行Keras的`preprocess_input`函数对输入图像进行归一化，将每个像素归一化到一个标准范围。*归一化*或*特征缩放*是图像预处理的核心步骤之一，使其适用于深度学习。
- en: 'Now comes the model. We will be using a *Convolutional Neural Network* (CNN)
    called ResNet-50\. The very first question we should ask is, “Where will I find
    the model?” Of course, we could hunt for it on the internet to find something
    that is compatible with our deep learning framework (Keras). *But ain’t nobody
    got time for that!* Luckily, Keras loves to make things easy and provides it to
    us in a single function call. After we call this function for the first time,
    the model will be downloaded from a remote server and cached locally:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是模型的时间。我们将使用一个名为ResNet-50的*卷积神经网络*（CNN）。我们应该问的第一个问题是：“我在哪里找到这个模型？”当然，我们可以在互联网上搜索，找到与我们的深度学习框架（Keras）兼容的内容。*但是没人有时间这样做！*幸运的是，Keras喜欢简化事情，并通过一个函数调用将其提供给我们。在第一次调用此函数后，模型将从远程服务器下载并在本地缓存：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When predicting with this model, the results include probability predictions
    for each class. Keras also provides the `decode_predictions` function, which tells
    us the probability of each category of objects contained in the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这个模型进行预测时，结果包括每个类别的概率预测。Keras还提供了`decode_predictions`函数，告诉我们图像中包含的每个对象类别的概率。
- en: 'Now, let’s see the entire code in one handy function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个方便的函数中的整个代码：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The predicted categories for this image are various types of felines. Why doesn’t
    it simply predict the word “cat,” instead? The short answer is that the ResNet-50
    model was trained on a granular dataset with many categories and does not include
    the more general “cat.” We investigate this dataset in more detail a little later,
    but first, let’s load another sample image (see [Figure 2-2](part0004.html#plot_showing_the_contents_of_the_file_do)):'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这幅图像的预测类别是各种类型的猫科动物。为什么它不简单地预测“猫”这个词呢？简短的答案是，ResNet-50模型是在一个包含许多类别的细粒度数据集上训练的，不包括更一般的“猫”。我们稍后会更详细地调查这个数据集，但首先让我们加载另一张样本图像（参见[图2-2](part0004.html#plot_showing_the_contents_of_the_file_do)）：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Plot showing the contents of the file dog.jpg](../images/00236.jpeg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![显示文件dog.jpg内容的图](../images/00236.jpeg)'
- en: Figure 2-2\. Plot showing the contents of the file dog.jpg
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 显示文件dog.jpg内容的图
- en: 'And, again, we run our handy function from earlier:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们再次运行之前的便捷函数：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As expected, we get different breeds of canines (and not just the “dog” category).
    If you are unfamiliar with the Corgi breed of dogs, the word “corgi” literally
    means “dwarf dog” in Welsh. The Cardigan and Pembroke are subbreeds of the Corgi
    family, which happen to look pretty similar to one another. It’s no wonder our
    model thinks that way, too.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如预期的那样，我们得到了不同品种的犬类（不仅仅是“狗”类别）。如果你对柯基品种的狗不熟悉，那么“corgi”这个词在威尔士语中的意思就是“侏儒犬”。卡迪根和彭布罗克是柯基家族的亚品种，它们看起来非常相似。我们的模型也认为是这样，这一点也不奇怪。
- en: Notice the predicted probability of each category. Usually, the prediction with
    the highest probability is considered the answer. Alternatively, any value over
    a predefined threshold can be considered as the answer, too. In the dog example,
    if we set a threshold of 0.5, Cardigan would be our answer.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意每个类别的预测概率。通常，具有最高概率的预测被认为是答案。或者，任何高于预定义阈值的值也可以被视为答案。在狗的例子中，如果我们设置阈值为0.5，那么卡迪根将是我们的答案。
- en: '![Running the notebook on Google Colab using the browser](../images/00183.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![在浏览器中使用Google Colab运行笔记本](../images/00183.jpeg)'
- en: Figure 2-3\. Running the notebook on Google Colab using the browser
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 在浏览器中使用Google Colab运行笔记本
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can follow along with the code in this chapter and execute it interactively
    without any installations in the browser itself with Google Colab. Simply find
    the “Run on Colab” link at the top of each notebook on GitHub that you’d like
    to experiment with. Then, click the “Run Cell” button; this should execute the
    code within that cell, as shown in [Figure 2-3](part0004.html#running_the_notebook_on_google_colab_usi).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章中跟随代码并在浏览器中交互式地执行，而无需进行任何安装，只需使用Google Colab。只需在GitHub上每个您想要尝试的笔记本顶部找到“在Colab上运行”链接。然后，点击“运行单元格”按钮；这应该执行该单元格中的代码，如[图2-3](part0004.html#running_the_notebook_on_google_colab_usi)所示。
- en: Investigating the Model
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查模型
- en: 'We got the predictions from our model, great! But what factors led to those
    predictions? There are a few questions that we need to ask here:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从模型中得到了预测，太棒了！但是是什么因素导致了这些预测？这里有一些问题我们需要问：
- en: What dataset was the model trained on?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是在哪个数据集上训练的？
- en: Are there other models that I can use? How good are they? Where can I get them?
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我可以使用其他模型吗？它们有多好？我可以在哪里获取它们？
- en: Why does my model predict what it predicts?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我的模型会做出这样的预测？
- en: We look into the answers to each of these questions in this section.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本节中探讨这些问题的答案。
- en: ImageNet Dataset
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ImageNet数据集
- en: Let’s investigate the ImageNet dataset on which ResNet-50 was trained. [ImageNet](http://www.image-net.org/),
    as the name suggests, is a network of images; that is, a dataset of images organized
    as a network, as demonstrated in [Figure 2-4](part0004.html#the_categories_and_subcategories_in_the).
    It is arranged in a hierarchical manner (like the WordNet hierarchy) such that
    the parent node encompasses a collection of images of all different varieties
    possible within that parent. For example, within the “animal” parent node, there
    are fish, birds, mammals, invertebrates, and so on. Each category has multiple
    subcategories, and these have subsubcategories, and so forth. For example, the
    category “American water spaniel” is eight levels from the root. The dog category
    contains 189 total subcategories in five hierarchical levels.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查ResNet-50训练的ImageNet数据集。正如其名称所示，[ImageNet](http://www.image-net.org/)是一个图像网络；也就是说，这是一个以网络形式组织的图像数据集，如[图2-4](part0004.html#the_categories_and_subcategories_in_the)所示。它以分层方式排列（类似于WordNet层次结构），使得父节点包含该父节点内所有可能的各种图像的集合。例如，在“动物”父节点内，有鱼类、鸟类、哺乳动物、无脊椎动物等。每个类别都有多个子类别，这些子类别又有子子类别，依此类推。例如，“美国水猎犬”类别距离根节点有八个级别。狗类别包含了总共五个层次中的189个子类别。
- en: Visually, we developed the tree diagram shown in [Figure 2-5](part0004.html#tree_map_of_imagenet_and_its_classes)
    to help you to understand the wide variety of high-level entities that the ImageNet
    dataset contains. This treemap also shows the relative percentage of different
    categories that make up the ImageNet dataset.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们制作了[图2-5](part0004.html#tree_map_of_imagenet_and_its_classes)中显示的树状图，以帮助您了解ImageNet数据集包含的各种高级实体。这个树状图还显示了构成ImageNet数据集的不同类别的相对百分比。
- en: '![The categories and subcategories in the ImageNet dataset](../images/00258.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![ImageNet数据集中的类别和子类别](../images/00258.jpeg)'
- en: Figure 2-4\. The categories and subcategories in the ImageNet dataset
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. ImageNet数据集中的类别和子类别
- en: '![Treemap of ImageNet and its classes](../images/00319.jpeg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![ImageNet及其类别的树状图](../images/00319.jpeg)'
- en: Figure 2-5\. Treemap of ImageNet and its classes
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. ImageNet及其类别的树状图
- en: The ImageNet dataset was the basis for the famous ILSVRC that started in 2010
    to benchmark progress in computer vision and challenge researchers to innovate
    on tasks including object classification. Recall from [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)
    that the ImageNet challenge saw submissions that drastically improved in accuracy
    each year. When it started out, the error rate was nearly 30%. And now, it is
    2.2%, already better than how an average human would perform at this task. This
    dataset and challenge are considered the single biggest reasons for the recent
    advancements in computer vision.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet数据集是著名的ILSVRC的基础，该比赛始于2010年，旨在评估计算机视觉的进展并挑战研究人员在包括对象分类在内的任务上进行创新。回想一下[第1章](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)中提到的，ImageNet挑战看到每年提交的结果在准确性上有了显著提高。当它刚开始时，错误率接近30%。现在，它是2.2%，已经优于普通人在这项任务上的表现。这个数据集和挑战被认为是计算机视觉最近进展的最重要原因。
- en: 'Wait, AI has better-than-human accuracy? If the dataset was created by humans,
    won’t humans have 100% accuracy? Well, the dataset was created by experts, with
    each image verified by multiple people. Then Stanford researcher (and now of Tesla
    fame) Andrej Karpathy attempted to figure out how much a normal human would fare
    on ImageNet-1000\. Turns out he achieved an accuracy of 94.9%, well short of the
    100% we all expected. Andrej painstakingly spent a week going over 1,500 images,
    spending approximately one minute per image in tagging it. How did he misclassify
    5.1% of the images? The reasons are a bit subtle:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，人工智能的准确率比人类还高？如果数据集是由人类创建的，那么人类不应该有100%的准确率吗？事实上，数据集是由专家创建的，每个图像都经过多人验证。然后，斯坦福研究人员（现在是特斯拉的著名人物）Andrej
    Karpathy尝试了解一个普通人在ImageNet-1000上的表现。结果他取得了94.9%的准确率，远低于我们所有人期望的100%。Andrej费了一周的时间查看了1500张图像，每张图像花费大约一分钟的时间进行标记。他是如何将5.1%的图像分类错误的呢？原因有点微妙：
- en: Fine-grained recognition
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度识别
- en: For many people, it is really tough to distinguish a Siberian husky from a Alaskan
    Malamute. Someone who is really familiar with dog breeds would be able to tell
    them apart because they look for finer-level details that distinguish both breeds.
    It turns out that neural networks are capable of learning those finer-level details
    much more easily than humans.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多人来说，很难区分西伯利亚哈士奇和阿拉斯加雪橇犬。真正熟悉狗品种的人能够区分它们，因为他们寻找区分这两种品种的更细节的细节。事实证明，神经网络能够更容易地学习这些更细节的细节，而不是人类。
- en: Category unawareness
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 类别无知
- en: Not everyone is aware of all the 120 breeds of dogs and most certainly not each
    one of the 1,000 classes. But the AI is. After all, it was trained on it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个人都知道所有120种狗的品种，当然也不会知道其中的每一种。但人工智能知道。毕竟，它是经过训练的。
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注
- en: Similar to ImageNet, speech datasets like Switchboard report a 5.1% error rate
    for speech transcription (coincidentally the same number as ImageNet). It’s clear
    that humans have a limit, and AI is gradually beating us.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与ImageNet类似，像Switchboard这样的语音数据集报告了5.1%的语音转录错误率（巧合地与ImageNet相同）。很明显，人类有极限，而人工智能正在逐渐超越我们。
- en: One of the other key reasons for this fast pace of improvement was that researchers
    were openly sharing models trained on datasets like ImageNet. In the next section,
    we learn about model reuse in more detail.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种快速改进的另一个关键原因之一是研究人员公开分享了在像ImageNet这样的数据集上训练的模型。在下一节中，我们将更详细地了解模型重用。
- en: Model Zoos
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型动物园
- en: A model zoo is a place where organizations or individuals can publicly upload
    models that they have built for others to reuse and improve upon. These models
    can be trained using any framework (e.g., Keras, TensorFlow, MXNet), for any task
    (classification, detection, etc.), or trained on any dataset (e.g., ImageNet,
    Street View House Numbers (SVHN)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型动物园是一个组织或个人可以公开上传他们构建的模型供他人重用和改进的地方。这些模型可以使用任何框架（例如Keras、TensorFlow、MXNet），用于任何任务（分类、检测等），或者在任何数据集上进行训练（例如ImageNet、街景房屋号码（SVHN））。
- en: The tradition of model zoos started with Caffe, one of the first deep learning
    frameworks, developed at the University of California, Berkeley. Training a deep
    learning model from scratch on a multimillion-image database requires weeks of
    training time and lots of GPU computational energy, making it a difficult task.
    The research community recognized this as a bottleneck, and the organizations
    that participated in the ImageNet competition open sourced their trained models
    on Caffe’s website. Other frameworks soon followed suit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型动物园的传统始于Caffe，这是第一个深度学习框架之一，由加州大学伯克利分校开发。从头开始在一个拥有数百万图像的数据库上训练深度学习模型需要数周的训练时间和大量的GPU计算能量，这使得这项任务变得困难。研究界认识到这是一个瓶颈，参加ImageNet比赛的组织在Caffe的网站上开源了他们训练过的模型。其他框架很快也效仿。
- en: When starting out on a new deep learning project, it’s a good idea to first
    explore whether there’s already a model that performs a similar task and was trained
    on a similar dataset.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始新的深度学习项目时，首先探索是否已经有一个执行类似任务并在类似数据集上训练的模型是一个好主意。
- en: The [model zoo](https://keras.io/applications/) in Keras is a collection of
    various architectures trained using the Keras framework on the ImageNet dataset.
    We tabulate their details in [Table 2-1](part0004.html#architectural_details_of_select_pretrain).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的[模型动物园](https://keras.io/applications/)是使用Keras框架在ImageNet数据集上训练的各种架构的集合。我们在[表2-1](part0004.html#architectural_details_of_select_pretrain)中列出它们的详细信息。
- en: Table 2-1\. Architectural details of select pretrained ImageNet models
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1. 选择预训练的ImageNet模型的架构细节
- en: '| **Model** | **Size** | **Top-1 accuracy** | **Top-5 accuracy** | **Parameters**
    | **Depth** |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **模型** | **大小** | **Top-1准确率** | **Top-5准确率** | **参数** | **深度** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| VGG16 | 528 MB | 0.713 | 0.901 | 138,357,544 | 23 |'
- en: '| VGG19 | 549 MB | 0.713 | 0.9 | 143,667,240 | 26 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| VGG19 | 549 MB | 0.713 | 0.9 | 143,667,240 | 26 |'
- en: '| ResNet-50 | 98 MB | 0.749 | 0.921 | 25,636,712 | 50 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | 98 MB | 0.749 | 0.921 | 25,636,712 | 50 |'
- en: '| ResNet-101 | 171 MB | 0.764 | 0.928 | 44,707,176 | 101 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-101 | 171 MB | 0.764 | 0.928 | 44,707,176 | 101 |'
- en: '| ResNet-152 | 232 MB | 0.766 | 0.931 | 60,419,944 | 152 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-152 | 232 MB | 0.766 | 0.931 | 60,419,944 | 152 |'
- en: '| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| InceptionV3 | 92 MB | 0.779 | 0.937 | 23,851,784 | 159 |'
- en: '| InceptionResNetV2 | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| InceptionResNetV2 | 215 MB | 0.803 | 0.953 | 55,873,736 | 572 |'
- en: '| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 | — |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| NASNetMobile | 23 MB | 0.744 | 0.919 | 5,326,716 | — |'
- en: '| NASNetLarge | 343 MB | 0.825 | 0.96 | 88,949,818 | — |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| NASNetLarge | 343 MB | 0.825 | 0.96 | 88,949,818 | — |'
- en: '| MobileNet | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet | 16 MB | 0.704 | 0.895 | 4,253,864 | 88 |'
- en: '| MobileNetV2 | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MobileNetV2 | 14 MB | 0.713 | 0.901 | 3,538,984 | 88 |'
- en: 'The column “Top-1 accuracy” indicates how many times the best guess was the
    correct answer, and the column “Top-5 accuracy” indicates how many times at least
    one out of five guesses were correct. The “Depth” of the network indicates how
    many layers are present in the network. The “Parameters” column indicates the
    size of the model; that is, how many individual weights the model has: the more
    parameters, the “heavier” the model is, and the slower it is to make predictions.
    In this book, we often use ResNet-50 (the most common architecture cited in research
    papers for high accuracy) and MobileNet (for a good balance between speed, size,
    and accuracy).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: “Top-1准确率”列指示最佳猜测正确答案的次数，“Top-5准确率”列指示五次猜测中至少有一次正确的次数。网络的“深度”指示网络中存在多少层。“参数”列指示模型的大小；也就是说，模型有多少个独立的权重：参数越多，模型越“重”，预测速度越慢。在本书中，我们经常使用ResNet-50（在研究论文中引用最多的常见架构，以获得高准确性）和MobileNet（在速度、大小和准确性之间取得良好平衡）。
- en: Class Activation Maps
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类激活图
- en: Image saliency, usually famous in UX research, is trying to answer the question
    “What part of the image are users paying attention to?” This is facilitated with
    the help of eye-tracking studies and represented in heatmaps. For example, big,
    bold fonts or people’s faces usually get more attention than backgrounds. It’s
    easy to guess how useful these heatmaps would be to designers and advertisers,
    who can then adapt their content to maximize users’ attention. Taking inspiration
    from this human version of saliency, wouldn’t it be great to learn which part
    of the image the neural network is paying attention to? That’s precisely what
    we will be experimenting with.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图像显著性，在UX研究中通常很有名，试图回答“用户注意力集中在图像的哪个部分？”这是通过眼动研究来实现的，并以热图表示。例如，大号、粗体字或人脸通常比背景更受关注。可以猜想这些热图对设计师和广告商会有多有用，他们可以根据最大化用户注意力来调整内容。受到这种人类显著性版本的启发，了解神经网络关注图像的哪个部分会很有趣，这正是我们将要进行的实验。
- en: In our experiment, we will be overlaying a *class activation map* (or colloquially
    a *heatmap*) on top of a video in order to understand what the network pays attention
    to. The heatmap tells us something like “In this picture, these pixels were responsible
    for the prediction of the class `dog` where “dog” was the category with the highest
    probability. The “hot” pixels are represented with warmer colors such as red,
    orange, and yellow, whereas the “cold” pixels are represented using blue. The
    “hotter” a pixel is, the higher the signal it provides toward the prediction.
    [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate) gives us
    a clearer picture. (If you’re reading the print version, refer to the book’s GitHub
    for the original color image.)
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们将在视频上叠加一个*类激活图*（或俗称的*热图*），以便了解网络关注的内容。热图告诉我们类似于“在这张图片中，这些像素负责预测类别`dog`，其中“dog”是概率最高的类别。
    “热”像素用红色、橙色和黄色等暖色表示，而“冷”像素则用蓝色表示。像素越“热”，提供的信号越高，指向预测的方向。[图2-6](part0004.html#original_image_of_a_dog_and_its_generate)给我们一个更清晰的图像。（如果您正在阅读印刷版本，请参考书的GitHub获取原始彩色图片。）
- en: '![Original image of a dog and its generated heatmap](../images/00148.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![狗的原始图像及其生成的热图](../images/00148.jpeg)'
- en: Figure 2-6\. Original image of a dog and its generated heatmap
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. 狗的原始图像及其生成的热图
- en: 'In the GitHub repository (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    navigate to *code/chapter-2*. There, you’ll find a handy Jupyter notebook, *2-class-activation-map-on-video.ipynb,*
    which describes the following steps:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub存储库（参见[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)），导航至*code/chapter-2*。在那里，您会找到一个方便的Jupyter笔记本，*2-class-activation-map-on-video.ipynb*，描述以下步骤：
- en: 'First, we need to install `keras-vis` using `pip`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用`pip`安装`keras-vis`：
- en: '[PRE9]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then run the visualization script on a single image to generate the heatmap
    for it:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在单个图像上运行可视化脚本，生成其热图：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We should see a newly created file called *dog-output.jpg* that shows a side-by-side
    view of the original image and its heatmap. As we can see from [Figure 2-6](part0004.html#original_image_of_a_dog_and_its_generate),
    the right half of the image indicates the “areas of heat” along with the correct
    prediction of a “Cardigan” (i.e., Welsh Corgi).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个名为*dog-output.jpg*的新创建的文件，显示原始图像及其热图的并排视图。正如我们从[图2-6](part0004.html#original_image_of_a_dog_and_its_generate)中看到的，图像的右半部分指示了“热区”，以及对“Cardigan”（即威尔士柯基）的正确预测。
- en: Next, we want to visualize the heatmap for frames in a video. For that, we need
    `FFmpeg`, an open source multimedia framework. You can find the download binary
    as well as the installation instructions for your operating system at [*https://www.ffmpeg.org*](https://www.ffmpeg.org).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要可视化视频中帧的热图。为此，我们需要`FFmpeg`，这是一个开源的多媒体框架。您可以在[*https://www.ffmpeg.org*](https://www.ffmpeg.org)找到下载二进制文件以及您操作系统的安装说明。
- en: 'We use `ffmpeg` to split up a video into individual frames (at 25 frames per
    second) and then run our visualization script on each of those frames. We must
    first create a directory to store these frames and pass its name as part of the
    `ffmpeg` command:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`ffmpeg`将视频拆分为单独的帧（每秒25帧），然后在每个帧上运行我们的可视化脚本。我们必须首先创建一个目录来存储这些帧，并将其名称作为`ffmpeg`命令的一部分传递：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then run the visualization script with the path of the directory containing
    the frames from the previous step:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用包含上一步帧的目录路径运行可视化脚本：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We should see a newly created *kitchen-output* directory that contains all of
    the heatmaps for the frames from the input directory.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到一个新创建的*kitchen-output*目录，其中包含来自输入目录的所有帧的热图。
- en: 'Finally, compile a video from those frames using `ffmpeg`:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`ffmpeg`从这些帧编译一个视频：
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Perfect! The result is the original video side by side with a copy of the heatmap
    overlaid on it. This is a useful tool, in particular, to discover whether the
    model has learned the correct features or if it picked up stray artifacts during
    its training.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！结果是原始视频与覆盖在其上的热图的副本并排显示。这是一个有用的工具，特别是用来发现模型是否学习了正确的特征，或者在训练过程中是否捕捉到了杂散的人工制品。
- en: Imagine generating heatmaps to analyze the strong points and shortfalls of our
    trained model or a pretrained model.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下生成热图来分析我们训练模型或预训练模型的优点和不足。
- en: You should try this experiment out on your own by shooting a video with your
    smartphone camera and running the aforementioned scripts on the file. Don’t forget
    to post your videos on Twitter, tagging [@PracticalDLBook](https://www.twitter.com/PracticalDLBook)!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用智能手机摄像头拍摄视频并在文件上运行上述脚本来自己尝试这个实验。别忘了在Twitter上发布您的视频，标记[@PracticalDLBook](https://www.twitter.com/PracticalDLBook)！
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Heatmaps are a great way to visually detect bias in the data. The quality of
    a model’s predictions depends heavily on the data on which it was trained. If
    the data is biased, that will reflect in the predictions. A great example of this
    is (although probably an urban legend) one in which the US Army wanted to use
    neural networks to detect enemy tanks camouflaged in trees.^([1](part0004.html#ch02fn2))
    The researchers who were building the model took photographs—50% containing camouflaged
    tanks and 50% with just trees. Model training yielded 100% accuracy. A cause for
    celebration? That sadly wasn’t the case when the US Army tested it. The model
    had performed very poorly—no better than random guesses. Investigation revealed
    that photos with the tanks were taken on cloudy (overcast) days and those without
    the tanks on clear, sunny days. And the neural network model began looking for
    the sky instead of the tank. If the researchers had visualized the model using
    heatmaps, they would have caught that issue pretty early.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 热图是一种在数据中可视化检测偏见的好方法。模型预测的质量严重依赖于其训练的数据。如果数据存在偏见，那将反映在预测中。一个很好的例子是（尽管可能是一个都市传说），美国军方想要使用神经网络来检测伪装在树木中的敌方坦克。^([1](part0004.html#ch02fn2))
    构建模型的研究人员拍摄了照片——50%包含伪装的坦克，50%只有树木。模型训练得到了100%的准确率。值得庆祝吗？遗憾的是，当美国军方进行测试时情况并非如此。该模型表现得非常糟糕——不比随机猜测好。调查发现，带有坦克的照片是在多云（阴天）拍摄的，而没有坦克的照片是在晴朗的天气拍摄的。神经网络模型开始寻找天空而不是坦克。如果研究人员使用热图来可视化模型，他们会很早就发现这个问题。
- en: As we collect data, we must be vigilant at the outset of potential bias that
    can pollute our model’s learning. For example, when collecting images to build
    a food classifier, we should verify that the other artifacts such as plates and
    utensils are not being learned as food. Otherwise, the presence of chopsticks
    might get our food classified as chow mein. Another term to define this is *co-occurrence*.
    Food very frequently co-occurs with cutlery. So watch out for these artifacts
    seeping into your classifier’s training.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据时，我们必须警惕潜在的偏见，这可能会影响我们模型的学习。例如，当收集图像来构建食物分类器时，我们应该验证其他人工制品（如盘子和餐具）是否被学习为食物。否则，筷子的存在可能会导致我们的食物被分类为炒面。另一个术语来定义这个问题是*共现性*。食物经常与餐具共同出现。因此要注意这些人工制品是否渗入到分类器的训练中。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we got a glimpse of the deep learning universe using Keras.
    It’s an easy-to-use yet powerful framework that we use in the next several chapters.
    We observed that there is often no need to collect millions of images and use
    powerful GPUs to train a custom model because we can use a pretrained model to
    predict the category of an image. By diving deeper into datasets like ImageNet,
    we learned the kinds of categories these pretrained models can predict. We also
    learned about finding these models in model zoos that exist for most frameworks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过Keras对深度学习宇宙有了一瞥。这是一个易于使用但功能强大的框架，我们将在接下来的几章中使用。我们观察到通常不需要收集数百万张图片并使用强大的GPU来训练自定义模型，因为我们可以使用预训练模型来预测图像的类别。通过深入研究像ImageNet这样的数据集，我们了解了这些预训练模型可以预测的类别。我们还了解到在大多数框架中存在的模型动物园中可以找到这些模型。
- en: In [Chapter 3](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b), we explore
    how we can tweak an existing pretrained model to make predictions on classes of
    input for which it was not originally intended. As with the current chapter, our
    approach is geared toward obtaining output without needing millions of images
    and lots of hardware resources to train a classifier.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0005.html#4OIQ3-13fa565533764549a6f0ab7f11eed62b)中，我们探讨了如何调整现有的预训练模型，以便对其原始意图之外的输入类别进行预测。与当前章节一样，我们的方法旨在获得输出，而无需数百万张图片和大量硬件资源来训练分类器。
- en: ^([1](part0004.html#ch02fn2-marker)) [“Artificial Intelligence as a Positive
    and Negative Factor in Global Risk”](https://oreil.ly/-svD0) by Eliezer Yudkowsky
    in *Global Catastrophic Risks* (Oxford University Press).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](part0004.html#ch02fn2-marker)) [“人工智能作为全球风险中的积极和消极因素”](https://oreil.ly/-svD0)
    作者Eliezer Yudkowsky在《全球灾难性风险》（牛津大学出版社）中。
