- en: '5 RAG evaluation: Accuracy, relevance, and faithfulness'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 RAG评估：准确性、相关性和忠实度。
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The need and requirements for evaluating RAG pipelines
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估RAG管道的需求和需求。
- en: Metrics, frameworks, and benchmarks for RAG evaluation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的指标、框架和基准。
- en: Current limitations and future course of RAG evaluation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的当前局限性和未来发展方向。
- en: Chapters 3 and 4 discussed the development of retrieval-augmented generation
    (RAG) systems using the indexing and generation pipelines. RAG promises to reduce
    hallucinations and ground the large language model (LLM) responses in the provided
    context, which is done by creating a non-parametric memory or knowledge base for
    the system and then retrieving information from it.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章和第4章讨论了使用索引和生成管道开发检索增强生成（RAG）系统的进展。RAG承诺减少幻觉，并将大型语言模型（LLM）的响应基于提供的环境，这是通过为系统创建一个非参数化记忆或知识库，然后从中检索信息来实现的。
- en: This chapter covers the methods used to evaluate how well the RAG system is
    functioning. We need to make sure that the components of the two RAG pipelines
    are performing per the expectations. At a high level, we need to ensure that the
    information being retrieved is relevant to the input query and that the LLM is
    generating responses grounded in the retrieved context. To this end, there have
    been several frameworks developed over time. Here we discuss some popular frameworks
    and the metrics they calculate.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了评估RAG系统功能良好程度的方法。我们需要确保两个RAG管道的组件按预期运行。在较高层次上，我们需要确保检索到的信息与输入查询相关，并且LLM生成的响应基于检索到的环境。为此，随着时间的推移已经开发出几个框架。在这里，我们讨论了一些流行的框架以及它们计算的指标。
- en: There is also a second aspect to evaluation. While the frameworks allow for
    the calculation of metrics, how do you make sure that your RAG pipelines are working
    better than those developed by other developers? The evaluations cannot be done
    in isolation. For this purpose, several benchmarks have been established. These
    benchmarks evaluate the RAG systems on preset data, such as question–answer sets,
    for accurate comparison of different RAG pipelines. These benchmarks help developers
    evaluate the performance of their systems vis-à-vis those developed by other developers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 评估还有第二个方面。虽然框架允许计算指标，但如何确保你的RAG管道比其他开发者开发的管道表现更好？评估不能孤立进行。为此，已经建立了一些基准。这些基准评估了RAG系统在预设数据上的表现，如问答集，以便对不同RAG管道进行准确比较。这些基准帮助开发者评估他们的系统与其他开发者开发的系统相比的性能。
- en: Finally, like RAG techniques, the research on RAG evaluations is still in progress.
    There are still some limitations in the current set of evaluation parameters.
    We discuss these limitations and some ideas on the way forward for RAG evaluations.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，像RAG技术一样，RAG评估的研究仍在进行中。当前评估参数集中仍存在一些局限性。我们讨论了这些局限性以及RAG评估未来发展的想法。
- en: By the end of this chapter, you should
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该
- en: Know the fundamentals of RAG evaluations.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RAG评估的基本原理。
- en: Be aware of the popular frameworks, metrics, and benchmarks for RAG evaluation.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解RAG评估的流行框架、指标和基准。
- en: Understand the limitations and best practices.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解局限性和最佳实践。
- en: Be able to evaluate the RAG pipeline in Python.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在Python中评估RAG管道。
- en: For RAG to live up to the promise of grounding the LLM responses in data, you
    will need to go beyond the simple implementation of indexing, retrieval, augmentation,
    and generation. We will discuss these advanced strategies in chapter 6\. However,
    to improve something, you need to first measure the performance. RAG evaluations
    help in setting up the baseline of your RAG system performance for you to then
    improve it. First, we look at the fundamental aspects of RAG systems evaluation.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让RAG实现将LLM响应基于数据的承诺，你需要超越简单的索引、检索、增强和生成实现。我们将在第6章中讨论这些高级策略。然而，要改进某物，你首先需要衡量其性能。RAG评估有助于为你设置RAG系统性能的基线，然后你可以对其进行改进。首先，我们来看RAG系统评估的基本方面。
- en: 5.1 Key aspects of RAG evaluation
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 RAG评估的关键方面
- en: Building a PoC RAG pipeline is not overtly complex. It is achievable through
    brief training and verification of a limited set of examples. However, to enhance
    its robustness, thorough testing on a dataset that accurately mirrors the production
    use case is imperative. RAG pipelines can suffer from hallucinations of their
    own. This can be because
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个PoC RAG管道并不特别复杂。通过简短的培训和验证一组有限的示例即可实现。然而，为了增强其鲁棒性，在准确反映生产用例的数据集上进行彻底测试是必不可少的。RAG管道可能会出现自己的幻觉。这可能是由于
- en: The retriever fails to retrieve the entire context or retrieves irrelevant context.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索器未能检索到完整上下文，或者检索到了无关的上下文。
- en: Despite being provided the context, the LLM does not consider it.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管提供了上下文，但大型语言模型（LLM）并没有考虑它。
- en: The LLM picks irrelevant information from the context instead of answering the
    query.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM从上下文中选择无关信息而不是回答查询。
- en: 'Retrieval and generation are two processes that need special focus from an
    evaluation perspective. This is because these two steps produce outputs that can
    be evaluated. (While indexing and augmentation will have a bearing on the outputs,
    they do not produce measurable outcomes). Here are several questions we need to
    ask ourselves about these two processes:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 检索和生成是两个需要从评估角度特别关注的过程。这是因为这两个步骤会产生可评估的输出。（虽然索引和增强会影响输出，但它们不会产生可测量的结果）。以下是关于这两个过程我们需要问自己的几个问题：
- en: How good is the retrieval of the context from the knowledge base?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从知识库中检索上下文的效果如何？
- en: Is it relevant to the query?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否与查询相关？
- en: How much noise (irrelevant information) is present?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在多少噪声（无关信息）？
- en: How good is the generated response?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的响应有多好？
- en: Is the response grounded in the provided context?
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应是否基于提供的上下文？
- en: Is the response relevant to the query?
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 响应是否与查询相关？
- en: You can ask many more questions such as these to assess the performance of your
    RAG system. Contemporary research has discovered certain scores to assess the
    quality and abilities of a RAG system. The following sections discuss three predominant
    quality scores and four main abilities.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以问更多类似的问题来评估你的RAG系统的性能。当代研究已经发现某些评分来评估RAG系统的质量和能力。以下几节将讨论三个主要的质量评分和四种主要能力。
- en: 5.1.1 Quality scores
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 质量评分
- en: 'There are three quality score dimensions prevalent in the discourse on RAG
    evaluation. They measure the quality of retrieval and generation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在RAG评估的讨论中，有三个质量评分维度很常见。它们衡量检索和生成的质量：
- en: '*Context relevanc**e*—This dimension evaluates how relevant the retrieved information
    or context is to the user query. It calculates metrics such as the precision and
    recall with which context is retrieved from the knowledge base.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*上下文相关性*—这个维度评估检索到的信息或上下文与用户查询的相关性。它计算从知识库中检索上下文的精确度和召回率等指标。'
- en: '*Answer faithfulness (also called groundedness**)*—This dimension evaluates
    whether the answer generated by the system is using the retrieved information.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*答案忠实度（也称为扎根性**）*—这个维度评估系统生成的答案是否使用了检索到的信息。'
- en: '*Answer relevanc**e*—This dimension evaluates how relevant the answer generated
    by the system is to the original user query.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*答案相关性*—这个维度评估系统生成的答案与原始用户查询的相关性。'
- en: We discuss how these scores are calculated in section 5.2
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5.2节讨论这些评分是如何计算的。
- en: 5.1.2 Required abilities
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 必需的能力
- en: 'The quality scores are important for measuring how well the retrieval and the
    generation components of the RAG system are performing. At an overall level, there
    are certain critical abilities that a RAG system should possess:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 质量评分对于衡量RAG系统的检索和生成组件表现如何非常重要。在整体层面上，RAG系统应该具备某些关键能力：
- en: '*Noise robustnes**s*—It is impractical to assume that the information stored
    in the knowledge base for RAG systems is perfectly curated to answer the questions
    that can be potentially asked. It is very probable that a document is related
    to the user query but does not have any meaningful information to answer it. The
    ability of the RAG system to separate these noisy documents from the relevant
    ones is termed noise robustness.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*噪声鲁棒性*—假设RAG系统存储的知识库中的信息被完美整理以回答可能提出的问题是不切实际的。很可能一个文档与用户查询相关，但没有任何有意义的答案信息。RAG系统从相关文档中分离出这些噪声文档的能力被称为噪声鲁棒性。'
- en: '*Negative rejectio**n*—By nature, LLMs always generate text. There may be no
    information about the user query in the documents in the knowledge base. The ability
    of the RAG system not to give an answer when there is no relevant information
    is called negative rejection.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负拒绝*—本质上，LLMs总是生成文本。知识库中的文档可能没有关于用户查询的信息。RAG系统在没有相关信息时不给出答案的能力被称为负拒绝。'
- en: '*Information integratio**n*—To obtain a comprehensive answer to a user query,
    it is also very likely the information must be retrieved from multiple documents.
    This ability of the system to assimilate information from multiple documents is
    called information integration.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息整合*—为了对用户查询给出全面的答案，很可能需要从多个文档中检索信息。系统从多个文档中吸收信息的能力被称为信息整合。'
- en: '*Counterfactual robustnes**s*—Sometimes the information in the knowledge base
    might itself be inaccurate. A high-quality RAG system should be able to address
    this problem and reject known inaccuracies in the retrieved information. This
    ability is known as counterfactual robustness.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反事实鲁棒性*—有时知识库中的信息本身可能是不准确的。一个高质量的RAG系统应该能够解决这个问题，并拒绝检索到的信息中的已知错误。这种能力被称为反事实鲁棒性。'
- en: Noise robustness is an ability that the retrieval component should possess,
    and other abilities are largely related to the generation component.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声鲁棒性是检索组件应该具备的能力，而其他能力大多与生成组件相关。
- en: Apart from these, *latency* is another often-mentioned capability. Although
    it is a non-functional requirement, it is quite critical in generative AI applications.
    Latency is the delay that happens between the user query and the response. You
    may have observed that LLMs themselves have considerable latency before the final
    response is generated. Add to it the task of retrieval and augmentation, and the
    latency is bound to increase. Therefore, it is important to monitor how much time
    your RAG system takes from user input to response.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，*延迟*是另一个经常提到的能力。尽管它是一个非功能性需求，但在生成AI应用中却非常关键。延迟是用户查询和响应之间的延迟。你可能已经注意到，LLMs在生成最终响应之前有相当大的延迟。再加上检索和增强的任务，延迟肯定会增加。因此，重要的是要监控你的RAG系统从用户输入到响应所需的时间。
- en: Ethical considerations are also at the forefront of generative AI adoption.
    For some RAG applications, it is important to measure the degree of *bias* and
    *toxicity*in the system responses. This is also influenced by the underlying data
    in the knowledge base. While it is not specific to RAG, it is important to evaluate
    the outputs for bias and toxicity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理考量也是生成AI应用的前沿。对于某些RAG应用，测量系统响应中的*偏差*和*毒性*的程度很重要。这也会受到知识库中底层数据的影响。虽然这并不是RAG特有的，但评估输出中的偏差和毒性是很重要的。
- en: Another aspect to check is the *robustness* of the system, that is, its ability
    to handle different types of queries. Some queries may be simple, while others
    may involve complex reasoning. Some queries may require comparing two pieces of
    information, while others may involve complex post-processing, like mathematical
    calculations. We will look at some types of queries when we discuss CRAG, a benchmark,
    in section 5.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要检查的方面是系统的*鲁棒性*，即其处理不同类型查询的能力。一些查询可能很简单，而另一些可能涉及复杂的推理。一些查询可能需要比较两块信息，而另一些可能涉及复杂的后处理，如数学计算。当我们讨论第5.4节中的基准CRAG时，我们将探讨一些查询类型。
- en: Finally, it is important to mention that these are scores and abilities that
    approach RAG at the core technique level. RAG, after all, is a means to solving
    the end use case. Therefore, you may have to build a *use case-specific*evaluation
    criteria for your RAG system. For example, a question-answering system may use
    an exact match (EM) or F1 score as a metric, and a summarization service may use
    ROUGE scores. Modern search engines using RAG may look at user interaction metrics,
    accuracy of source attribution, and similar.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是要提到，这些是接近RAG核心技术层面的分数和能力。毕竟，RAG是一种解决最终用例的手段。因此，你可能需要为你的RAG系统构建*特定用例*的评估标准。例如，问答系统可能使用精确匹配（EM）或F1分数作为指标，而摘要服务可能使用ROUGE分数。使用RAG的现代搜索引擎可能会考虑用户交互指标、源归属的准确性以及类似指标。
- en: 'This is the main idea behind evaluating RAG pipelines. The quality scores and
    the abilities that we discussed before need to be measured and benchmarked. There
    are two critical enablers of RAG evaluations: frameworks and benchmarks.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是评估RAG管道背后的主要思想。我们之前讨论的质量分数和能力需要被衡量和基准化。RAG评估的两个关键推动者是框架和基准。
- en: '*Frameworks* are tools designed to facilitate evaluation, offering automation
    of the evaluation process and data generation. They are used to streamline the
    evaluation process by providing a structured environment for testing different
    aspects of RAG systems. They are flexible and can be adapted to different datasets
    and metrics. We will discuss the popular evaluation frameworks in section 5.3.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*框架*是设计用来促进评估的工具，提供评估过程和数据生成的自动化。它们通过提供一个结构化的环境来测试RAG系统的不同方面，从而简化评估过程。它们是灵活的，可以适应不同的数据集和指标。我们将在第5.3节讨论流行的评估框架。'
- en: '*Benchmarks* are standardized datasets and their evaluation metrics used to
    measure the performance of RAG systems. Benchmarks provide a common ground for
    comparing different RAG approaches. They ensure consistency across the evaluations
    by considering a fixed set of tasks and their evaluation criteria. For example,
    HotpotQA focuses on multi-hop reasoning and retrieval capabilities using metrics
    such as Exact Match and F1 scores.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '*基准*是标准化的数据集及其评估指标，用于衡量RAG系统的性能。基准为比较不同的RAG方法提供了一个共同基础。它们通过考虑一组固定的任务及其评估标准来确保评估的一致性。例如，HotpotQA侧重于多跳推理和检索能力，使用精确匹配和F1分数等指标。'
- en: Benchmarks are used to establish a baseline for performance and identify strengths/weaknesses
    in specific tasks or domains. We will discuss a few benchmarks and their characteristics
    in section 5.4
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基准用于建立性能基准并识别特定任务或领域中的优势/劣势。我们将在第5.4节讨论一些基准及其特性。
- en: Developers can use frameworks to integrate evaluation in their development process
    and use benchmarks to compare their development with established standards. The
    frameworks and benchmarks both calculate *metrics* that focus on retrieval and
    the RAG quality scores. We will begin our discussion about the metrics in the
    next section before moving on to the popular benchmarks and frameworks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以使用框架将评估集成到他们的开发过程中，并使用基准来比较他们的开发与既定标准。框架和基准都计算关注检索和RAG质量分数的*指标*。我们将在下一节开始讨论指标，然后再讨论流行的基准和框架。
- en: 5.2 Evaluation metrics
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 评估指标
- en: 'Metrics quantify the assessment of the RAG system performance. We will classify
    the evaluation metrics into two broad groups:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 指标量化了对RAG系统性能的评估。我们将评估指标分为两大类：
- en: Retrieval metrics that are commonly used in information retrieval tasks
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在信息检索任务中常用的检索指标
- en: RAG-specific metrics that have evolved as RAG has found more application
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着RAG应用的增加而演变的RAG特定指标
- en: It is noteworthy that there are natural-language-generation-specific metrics
    such as BLEU, ROUGE, and METEOR that focus on fluency and measure relevance and
    semantic similarity. They play an important role in analyzing and benchmarking
    the performance of LLMs. This book discusses metrics specific to retrieval and
    RAG.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，还有一些针对自然语言生成的特定指标，如BLEU、ROUGE和METEOR，它们侧重于流畅性，并衡量相关性和语义相似度。它们在分析和基准化LLMs性能方面发挥着重要作用。本书讨论了针对检索和RAG的特定指标。
- en: 5.2.1 Retrieval metrics
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 检索指标
- en: The retrieval component of RAG can be evaluated independently to determine how
    well the retrievers are satisfying the user query. The primary retrieval evaluation
    metrics include accuracy, precision, recall, F1-score, mean reciprocal rank (MRR),
    mean average precision (MAP), and normalized discounted cumulative gain (nDCG).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的检索组件可以独立评估，以确定检索器在满足用户查询方面的表现如何。主要的检索评估指标包括准确度、精确度、召回率、F1分数、平均倒数排名（MRR）、平均平均精度（MAP）和归一化折现累积增益（nDCG）。
- en: Accuracy
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 准确度
- en: Accuracy is typically defined as the proportion of correct predictions (both
    true positives and true negatives) among the total number of cases examined. In
    the context of information retrieval, it could be interpreted as
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度通常定义为在考察的总案例中，正确预测（包括真阳性和真阴性）的比例。在信息检索的背景下，它可以解释为
- en: '![A diagram of a machine'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-0x.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-0x.png)
- en: Although accuracy is a simple, intuitive metric, it is not the primary metric
    for retrieval. In a large knowledge base, a majority of documents are usually
    irrelevant to any given query, which can lead to misleadingly high accuracy scores.
    It does not consider the ranking of the retrieved results.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确度是一个简单直观的指标，但它不是检索的主要指标。在一个大型知识库中，大多数文档通常与任何给定的查询无关，这可能导致误导性高的准确度分数。它不考虑检索结果的排名。
- en: Precision
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Precision
- en: Precision focuses on the quality of the retrieved results. It measures the proportion
    of retrieved documents relevant to the user query. It answers the question, “Of
    all the documents that were retrieved, how many were relevant?”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回收率关注检索结果的品质。它衡量检索到的文档中与用户查询相关的比例。它回答了这样的问题：“在所有检索到的文档中，有多少是相关的？”
- en: '![A diagram of a machine'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-1x.png)
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-1x.png)
- en: A higher precision means that the retriever is performing well and retrieving
    mostly relevant documents.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 更高的精确度意味着检索器表现良好，主要检索到相关文档。
- en: Precision@k
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Precision@k
- en: 'Precision@k is a variation of precision that measures the proportion of relevant
    documents among the top ‘k’ retrieved results. It is particularly important because
    it focuses on the top results rather than all the retrieved documents. For RAG,
    it is important because only the top results are most likely to be used for augmentation.
    For example, if you restrict your RAG system to use only the top five retrieved
    documents for context augmentation, Precision@5 will be the metric to calculate:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Precision@k 是精确度的一个变体，它衡量在检索到的前‘k’个结果中相关文档的比例。它特别重要，因为它关注的是顶部结果，而不是所有检索到的文档。对于RAG来说，它很重要，因为只有顶部结果最有可能被用于增强。例如，如果你限制你的RAG系统只使用前五个检索到的文档进行上下文增强，Precision@5
    将是计算该指标的度量：
- en: '![A diagram of a machine'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-2x.png)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-2x.png)
- en: where ‘k’ is a chosen cut-off point. A precision@5 of .8 means that out of the
    top five retrieved documents, four were relevant.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中‘k’是一个选择的截止点。Precision@5 为 .8 意味着在顶部五个检索到的文档中，有四个是相关的。
- en: Precision@k is also useful to compare systems when the total number of results
    retrieved may be different in different systems. However, the limitation is that
    the choice of ‘k’ can be arbitrary, and this metric doesn’t look beyond the chosen
    ‘k’.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Precision@k 也用于比较系统，当不同系统检索到的结果总数可能不同时。然而，局限性在于‘k’的选择可能是任意的，并且这个指标不会超出选择的‘k’。
- en: Recall
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 回收率
- en: Recall focuses on the coverage that the retriever provides. It measures the
    proportion of the relevant documents retrieved from all the relevant documents
    in the corpus. It answers the question, “Of all the relevant documents, how many
    were retrieved?”
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 回收率关注检索器提供的覆盖率。它衡量从语料库中所有相关文档中检索到的相关文档的比例。它回答了这样的问题：“在所有相关文档中，有多少被检索到了？”
- en: '![A diagram of a machine'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-3x.png)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-3x.png)
- en: Note that, unlike precision, calculation of recall requires prior knowledge
    of the total number of relevant documents. This requirement can become challenging
    in large-scale systems, which have many documents in the knowledge base.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，与精确度不同，计算回收率需要先了解相关文档的总数。在具有大量文档的知识库中的大规模系统中，这一要求可能具有挑战性。
- en: Like precision, recall also doesn’t consider the ranking of the retrieved documents.
    It can also be misleading as retrieving all documents in the knowledge base will
    result in a perfect recall value. Figure 5.1 visualizes various precision and
    recall scenarios.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确度一样，回收率也不考虑检索文档的排名。它也可能具有误导性，因为检索知识库中的所有文档将导致完美的回收率值。图5.1可视化了各种精确度和回收率场景。
- en: '![](../Images/CH05_F01_Kimothi.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![机器的示意图](../Images/CH05_F01_Kimothi.png)'
- en: Figure 5.1  Precision and recall
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 精确度和回收率
- en: F1-score
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F1分数
- en: 'F1-score is the harmonic mean of precision and recall. It provides a single
    metric that balances both the quality and coverage of the retriever:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数是精确度和回收率的调和平均数。它提供了一个平衡检索器质量和覆盖率的单一指标：
- en: '![A diagram of a machine'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-4x.png)
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-4x.png)
- en: The equation is such that the F1-score penalizes either variable having a low
    score; a high F1 score is only possible when both recall and precision values
    are high. This means that the score cannot be positively skewed by a single variable.
    Figure 5.2 illustrates how the F1-score balances precision and recall.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程式使得F1分数惩罚任何得分低的变量；只有当召回率和精确度值都很高时，才能得到高F1分数。这意味着分数不能由单个变量正偏斜。图5.2说明了F1分数如何平衡精确度和召回率。
- en: '![A diagram of a graph'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图的示意图'
- en: AI-generated content may be incorrect.](../Images/CH05_F02_Kimothi.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F02_Kimothi.png)
- en: Figure 5.2  F1-score balances precision and recall. A medium value of both precision
    and recall gets a higher F1-score than if one value is very high and the other
    is very low.
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2  F1分数平衡了精确度和召回率。精确度和召回率的中间值比一个非常高而另一个非常低的值得到的F1分数要高。
- en: F1-score provides a single, balanced measure that can be used to easily compare
    different systems. However, it does not take ranking into account and gives equal
    weight to precision and recall, which might not always be ideal.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数提供了一个单一、平衡的度量，可以用来轻松比较不同的系统。然而，它不考虑排名，并且对精确度和召回率给予相同的权重，这可能并不总是理想的。
- en: Mean reciprocal rank
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均倒数排名
- en: 'Mean reciprocal rank, or MRR, is particularly useful in evaluating the rank
    of the relevant document. It measures the reciprocal of the ranks of the first
    relevant document in the list of results. MRR is calculated over a set of queries:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 平均倒数排名，或MRR，在评估相关文档排名时特别有用。它测量列表中第一个相关文档的排名的倒数。MRR在查询集上计算：
- en: '![A diagram of a machine'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-5x.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-5x.png)
- en: where N is the total number of queries, and ranki  is the rank of the first
    relevant document of the i-th query.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 其中N是查询总数，ranki是第i个查询的第一个相关文档的排名。
- en: MRR is particularly useful when you’re interested in how quickly the system
    can find a relevant document and consider the ranking of the results. However,
    since it doesn’t look at anything beyond the first relevant result, it may not
    be useful when multiple relevant results are important. Figure 5.3 shows how the
    mean reciprocal rank is calculated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: MRR在你想了解系统如何快速找到相关文档并考虑结果排名时特别有用。然而，由于它没有考虑第一个相关结果之外的内容，当多个相关结果都很重要时，它可能没有用。图5.3展示了平均倒数排名的计算方法。
- en: '![A paper with a number of results'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有多个结果的论文'
- en: AI-generated content may be incorrect.](../Images/CH05_F03_Kimothi.png)
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F03_Kimothi.png)
- en: Figure 5.3  MRR considers the ranking but doesn’t consider all the documents.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3  MRR考虑了排名，但没有考虑所有文档。
- en: Mean average precision
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均值平均精度
- en: 'Mean average precision, or MAP, is a metric that combines precision and recall
    at different cut-off levels of ‘k’, that is, the cut-off number for the top results.
    It calculates a measure called average precision and then averages it across all
    queries:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 均值平均精度，或MAP，是一个度量，它结合了不同“k”截止水平的精确度和召回率，即顶部结果的截止数。它计算一个称为平均精度的度量，然后对所有查询进行平均：
- en: '![A diagram of a machine'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-6x.png)
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-6x.png)
- en: where Ri is the number of relevant documents for query i, Precision@k is the
    precision at cut-off ‘k’, and rel@k is a binary flag indicating the relevance
    of the document at rank k.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中Ri是查询i的相关文档数，Precision@k是截止“k”处的精确度，rel@k是一个二进制标志，表示排名k的文档的相关性。
- en: 'Mean average precision is the mean of the average precision over all the N
    queries:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 均值平均精度是所有N个查询的平均精度：
- en: '![A diagram of a machine'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-7x.png)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-7x.png)
- en: MAP provides a single measure of quality across recall levels. It is quite suitable
    when result ranking is important but complex to calculate. Let’s look at an example
    MAP calculation in figure 5.4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: MAP提供了一种在召回率水平上的单一质量度量。当结果排序很重要但计算复杂时，它非常合适。让我们看看图5.4中的MAP计算示例。
- en: '![A screenshot of a computer'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![计算机屏幕截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F04_Kimothi.png)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能不正确。](../Images/CH05_F04_Kimothi.png)
- en: Figure 5.4  MAP considers all the retrieved documents and gives a higher score
    for better ranking
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4  MAP考虑了所有检索到的文档，并为更好的排名给出更高的分数
- en: Normalized discounted cumulative gain
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 归一化折现累积增益
- en: 'Normalized discounted cumulative gain (nDCG) evaluates the ranking quality
    by considering the position of relevant documents in the result list and assigning
    higher scores to relevant documents appearing earlier. It is particularly effective
    for scenarios where documents have varying degrees of relevance. To calculate
    discounted cumulative gain (DCG), each document in the retrieved list is assigned
    a relevance score, rel, and a discount factor reduces the weight of documents
    as their rank position increases:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化折现累积增益（nDCG）通过考虑相关文档在结果列表中的位置并给予较早出现的相关文档更高的分数来评估排名质量。它在文档具有不同相关度级别的场景中特别有效。为了计算折现累积增益（DCG），检索列表中的每个文档都被分配一个相关性分数
    rel，以及一个折扣因子减少文档的权重，随着其排名位置的上升：
- en: '![A diagram of a machine'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-8x.png)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-8x.png)
- en: where reli is the graded relevance of the document at position I, and IDCG is
    the ideal DCG, which is the DCG for perfect ranking.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 reli 是文档在位置 I 的分级相关性，IDCG 是理想DCG，即完美排名的DCG。
- en: 'nDCG is calculated as the ratio between actual DCG and the IDCG:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: nDCG是实际DCG与IDCG的比率：
- en: '![A diagram of a machine'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器的示意图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-9x.png)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-9x.png)
- en: Figure 5.5 shows an example of nDCG calculation.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5展示了nDCG计算的示例。
- en: '![A paper with numbers and letters'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![带有数字和字母的论文'
- en: AI-generated content may be incorrect.](../Images/CH05_F05_Kimothi.png)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F05_Kimothi.png)
- en: Figure 5.5  nDCG addresses degrees of relevance in documents and penalizes incorrect
    ranking.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5  nDCG处理文档中的相关度级别，并惩罚错误的排名。
- en: nDCG is a complex metric to calculate. It requires documents to have a relevance
    score, which may lead to subjectivity, and the choice of the discount factor affects
    the values significantly, but it accounts for varying degrees of relevance in
    documents and gives more weight to higher-ranked items.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: nDCG是一个复杂的指标，计算起来很复杂。它需要文档具有相关性分数，这可能导致主观性，折扣因子的选择对值有显著影响，但它考虑了文档中不同级别的相关性，并给予排名较高的项目更高的权重。
- en: Retrieval systems are not just used in RAG but also in a variety of other application
    areas such as web and enterprise search engines, e-commerce product search and
    personalized recommendations, social media ad retrieval, archival systems, databases,
    virtual assistants, and more. The retrieval metrics help in assessing and improving
    the performance to effectively meet user needs. Table 5.1 summarizes different
    retrieval metrics.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 检索系统不仅用于RAG，还用于各种其他应用领域，如网络和搜索引擎、电子商务产品搜索和个性化推荐、社交媒体广告检索、归档系统、数据库、虚拟助手等。检索指标有助于评估和改进性能，以有效满足用户需求。表5.1总结了不同的检索指标。
- en: Table 5.1 Retrieval metrics
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1 检索指标
- en: '| Metric | What it measures | Strengths | Use cases | Considerations |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 衡量内容 | 优点 | 用例 | 考虑事项 |'
- en: '| Accuracy | Overall correctness of retrieval | Simple to understand; includes
    true negatives | General performance in balanced datasets | Can be misleading
    in imbalanced datasets; doesn’t consider ranking |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Accuracy | 检索的整体正确性 | 容易理解；包括真实负例 | 平衡数据集的一般性能 | 在不平衡数据集中可能具有误导性；不考虑排名 |'
- en: '| Precision | Quality of retrieved results | Easy to understand and calculate
    | General retrieval quality assessment | Doesn’t consider ranking or completeness
    of retrieval |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Precision | 检索结果的品质 | 容易理解和计算 | 通用检索质量评估 | 不考虑排名或检索的完整性 |'
- en: '| Precision@k | Quality of top k retrieved results | Focuses on most relevant
    results for RAG | When only top k results are used for augmentation | Choose k
    based on your RAG system’s usage |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Precision@k | 前 k 个检索结果的品质 | 专注于RAG中最相关的结果 | 仅使用前 k 个结果进行增强 | 根据你的RAG系统使用情况选择
    k |'
- en: '| Recall | Coverage of relevant documents | Measures completeness of retrieval
    | Assessing if important information is missed | Requires knowing all relevant
    documents in the corpus |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Recall | 相关文档的覆盖率 | 衡量检索的完整性 | 评估是否遗漏了重要信息 | 需要知道语料库中所有相关文档 |'
- en: '| F1-score | Balance between precision and recall | Single metric combining
    quality and coverage | Overall retrieval performance | May obscure tradeoffs between
    precision and recall |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| F1-score | 精确率和召回率的平衡 | 结合质量和覆盖率的单一指标 | 通用检索性能 | 可能会模糊精确率和召回率之间的权衡 |'
- en: '| Mean reciprocal rank (MRR) | How quickly a relevant document is found | Emphasizes
    finding at least one relevant result quickly | When finding one good result is
    sufficient | Less useful when multiple relevant results are needed |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 平均倒数排名（MRR） | 快速找到相关文档的速度 | 强调快速找到至少一个相关结果 | 当找到一个好结果就足够时 | 当需要多个相关结果时不太有用
    |'
- en: '| Mean average precision (MAP) | Precision at different recall levels | Considers
    both precision and ranking | Comprehensive evaluation of ranked retrieval results
    | More complex to calculate and interpret |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 平均平均精度（MAP） | 不同召回率水平的精度 | 考虑精度和排名 | 对排名检索结果的全面评估 | 计算和解释更复杂 |'
- en: '| Normalized discounted cumulative gain (nDCG) | Ranking quality with graded
    relevance | Accounts for varying degrees of relevance and ranking | When documents
    have different levels of relevance | Requires relevance scoring for documents
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 标准化折现累积增益（nDCG） | 带有分级相关性的排名质量 | 考虑不同程度的相关性和排名 | 当文档具有不同级别的相关性时 | 需要对文档进行相关性评分
    |'
- en: Not all retrieval metrics are popular for evaluation. Often, the more complex
    metrics are overlooked for the sake of explainability. The usage of these metrics
    depends on the stage of improvement in the evolution of system performance you
    are in. For example, to start with, you may just be trying to improve precision,
    while at an evolved stage, you may be looking for better ranking.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有检索指标都适用于评估。通常，为了可解释性，更复杂的指标会被忽视。这些指标的使用取决于你在系统性能演变过程中的改进阶段。例如，一开始你可能只是试图提高精确度，而在一个进化的阶段，你可能正在寻找更好的排名。
- en: While these metrics focus on retrieval in general, some metrics have been created
    specifically for RAG applications. These metrics focus on the three quality scores
    discussed in section 5.1\.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些指标主要关注检索，但一些指标是专门为RAG应用创建的。这些指标专注于第5.1节中讨论的三个质量评分。
- en: 5.2.2 RAG-specific metrics
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 RAG特定指标
- en: 'The three quality scores used to evaluate RAG applications are context relevance,
    answer relevance, and answer faithfulness. These scores specifically answer the
    following three questions:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 用于评估RAG应用的三个质量评分是语境相关性、答案相关性和答案忠实度。这些评分具体回答以下三个问题：
- en: Is the information retrieval relevant to the user query?
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息检索是否与用户查询相关？
- en: Is the generated answer rooted in the retrieved information?
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的答案是否基于检索到的信息？
- en: Is the generated answer relevant to the user query?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的答案是否与用户查询相关？
- en: Let’s now take a look at each of these scores.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些评分中的每一个。
- en: Context relevance
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语境相关性
- en: Context relevance evaluates how well the retrieved documents relate to the original
    query. The key aspects are topical alignment, information usefulness, and redundancy.
    There are human evaluation methods, as well as semantic similarity measures to
    calculate context relevance.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 语境相关性评估检索到的文档与原始查询的相关程度。关键方面包括主题一致性、信息有用性和冗余。存在人类评估方法和语义相似度度量来计算语境相关性。
- en: 'One such measure is employed by the Retrieval-Augmented Generation Assessment
    (RAGAs) framework (further discussed in section 5.3). The retrieved context should
    contain information only relevant to the query or the prompt. For context relevance,
    a metric S is estimated, where S is the number of sentences in the retrieved context
    relevant for responding to the query or the prompt:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标被检索增强生成评估（RAGAs）框架采用（将在第5.3节中进一步讨论）。检索到的上下文应只包含与查询或提示相关的信息。对于语境相关性，估计一个指标S，其中S是检索上下文中与回答查询或提示相关的句子数量：
- en: '![A diagram of a machine'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器图示'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-10x.png)
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成描述](../Images/kimothi-ch5-eqs-10x.png)
- en: Figure 5.6 is an illustrative example of high and low context relevance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6是高语境和低语境相关性的一个示例。
- en: '![A screenshot of a test'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![测试截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F06_Kimothi.png)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F06_Kimothi.png)
- en: Figure 5.6  Context relevance evaluates the degree to which the retrieved information
    is relevant
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6  语境相关性评估检索信息的相关程度
- en: to the query.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 与查询的相关性。
- en: The number of relevant sentences is also sometimes customized to the sum of
    similarity scores of each of the sentences with the query. Context relevance ensures
    that the generation component has access to appropriate information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 相关句子的数量有时也会根据每个句子与查询的相似度得分之和进行定制。语境相关性确保生成组件可以访问适当的信息。
- en: Answer faithfulness
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 答案忠实度
- en: 'Answer faithfulness is the measure of the extent to which the response is factually
    grounded in the retrieved context. Faithfulness ensures that the facts in the
    response do not contradict the context and can be traced back to the source. It
    also ensures that the LLM is not hallucinating. In the RAGAs framework, faithfulness
    first identifies the number of claims made in the response and calculates the
    proportion of those claims present in the context:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 答案忠诚度是衡量响应在检索到的上下文中事实基础的程度的指标。忠诚度确保响应中的事实不与上下文矛盾，并且可以追溯到源头。它还确保 LLM 没有产生幻觉。在
    RAGAs 框架中，忠诚度首先识别响应中提出的声明数量，并计算这些声明在上下文中存在的比例：
- en: '![A diagram of a machine'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-11x.png)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-11x.png)
- en: Let’s look at an example in figure 5.7
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看图 5.7 中的例子
- en: '![A white paper with text and numbers'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含文本和数字的白色文档'
- en: AI-generated content may be incorrect.](../Images/CH05_F07_Kimothi.png)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的内 容可能是不正确的。](../Images/CH05_F07_Kimothi.png)
- en: Figure 5.7  Answer faithfulness evaluates the closeness of the generated response
    to the retrieved context.
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.7  答案忠诚度评估生成的响应与检索到的上下文之间的接近程度。
- en: Faithfulness is not a complete measure of factual accuracy but only evaluates
    the groundedness to the context. An inverse metric for faithfulness is also the
    *hallucination rate***,** which can calculate the proportion of generated claims
    in the response that are not present in the retrieved context.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 忠诚度并非衡量事实准确性的完整指标，它仅评估对上下文的扎根程度。忠诚度的逆指标是**幻觉率**，它可以计算出响应中生成的声明在检索到的上下文中不存在的比例。
- en: 'Another related metric to faithfulness is *coverage*. Coverage measures the
    number of relevant claims in the context and calculates the proportion of relevant
    claims present in the generated response. It measures how much of the relevant
    information from the retrieved passages is included in the generated answer:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与忠诚度相关的另一个相关指标是**覆盖率**。覆盖率衡量上下文中相关声明的数量，并计算生成的响应中存在的相关声明的比例。它衡量检索到的段落中的相关信息包含在生成的答案中的程度：
- en: '![A diagram of a machine'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-12x.png)
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-12x.png)
- en: Answer relevance
  id: totrans-166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 答案相关性
- en: Like context relevance measures the relevance of the retrieved context to the
    query, answer relevance is the measure of the extent to which the response is
    relevant to the query. This metric focuses on key aspects such as the system’s
    ability to comprehend the query, the response being pertinent to the query, and
    the completeness of the response.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 与上下文相关性度量检索到的上下文与查询的相关性一样，答案相关性是衡量响应与查询相关程度的一个指标。此指标侧重于关键方面，例如系统理解查询的能力、响应与查询的相关性以及响应的完整性。
- en: 'In RAGAs, for this metric, a response is generated for the initial query or
    prompt. To compute the score, the LLM is then prompted to generate questions for
    the generated response several times. The mean cosine similarity between these
    questions and the original one is then calculated. The concept is that if the
    answer addresses the initial question correctly, the LLM should generate questions
    from it that match the original question:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RAGAs 中，对于此指标，为初始查询或提示生成一个响应。为了计算分数，然后提示 LLM 多次生成针对生成的响应的问题。然后计算这些问题和原始问题之间的平均余弦相似度。其概念是，如果答案正确地回答了初始问题，LLM
    应该能够从中生成与原始问题匹配的问题：
- en: '![A diagram of a machine'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![机器图'
- en: Description automatically generated](../Images/kimothi-ch5-eqs-13x.png)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](../Images/kimothi-ch5-eqs-13x.png)
- en: where N is the number of queries generated by the LLM.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 N 是 LLM 生成的查询数量。
- en: 'Note that answer relevance is not a measure of truthfulness but only of relevance.
    The response may or may not be factually accurate, but it may be relevant. Figure
    5.8 is an illustration of the answer relevance calculation. Can you find the reason
    why the relevance is not very high? (Hint: The answer may have some irrelevant
    facts.) Answer relevance ensures that the RAG system provides useful and appropriate
    responses, enhancing user satisfaction and the system’s practical utility.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，答案相关性不是一个衡量真实性的指标，而只是一个衡量相关性的指标。响应可能或可能不是事实准确的，但它可能是相关的。图 5.8 是答案相关性计算的说明。你能找到相关性不高的原因吗？（提示：答案可能包含一些无关的事实。）答案相关性确保
    RAG 系统能够提供有用且适当的响应，从而提高用户满意度和系统的实用价值。
- en: Tradeoffs and other considerations
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 权衡和其他考虑因素
- en: These three metrics and their derivatives form the core of RAG quality evaluation.
    Furthermore, these metrics are interconnected and sometimes involve tradeoffs.
    High context relevance usually leads to better faithfulness, as the system has
    access to more pertinent information. However, high faithfulness doesn’t always
    guarantee high answer relevance. A system might faithfully reproduce information
    from the retrieved passages but fail to directly address the query. Optimizing
    for answer relevance without considering faithfulness might lead to responses
    that seem appropriate but contain hallucinated or incorrect information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个指标及其导数构成了RAG质量评估的核心。此外，这些指标相互关联，有时还涉及权衡。高上下文相关性通常会导致更高的忠实度，因为系统可以访问更多相关信息。然而，高忠实度并不总是保证高答案相关性。一个系统可能忠实于检索到的段落中的信息，但未能直接回答查询。在不考虑忠实度的情况下优化答案相关性可能会导致看似合适的响应，但包含幻觉或错误信息。
- en: We have discussed quite a few metrics in this section. Effective interpretation
    of these metrics is crucial for performance improvement. As creators of RAG systems,
    you should use these metrics to compare with similar systems. You can also look
    at consistent trends to identify the strengths and weaknesses of your system.
    A low-precision high-recall system may indicate that your system is retrieving
    a lot of documents, and you may need to make your retriever more selective. A
    low-precision low-recall system points out fundamental problems with retrieval,
    and you may need to reassess the indexing pipeline itself. The same problem may
    be indicated by a low MAP or a low context-relevance score. Similarly, a low MRR
    or a low nDCG value may indicate a problem with the ranking algorithm of the retriever.
    To address low-answer faithfulness or low-answer relevance, you may need to improve
    your prompts or fine-tune the LLM.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论了许多指标。有效解释这些指标对于性能提升至关重要。作为RAG系统的创建者，您应该使用这些指标来与其他类似系统进行比较。您还可以观察一致的趋势，以识别您系统的优势和劣势。低精确度高召回率的系统可能表明您的系统检索了大量文档，您可能需要使检索器更加选择性地检索。低精确度低召回率的系统指出检索存在根本问题，您可能需要重新评估索引管道本身。低MAP或低上下文相关性分数可能表明存在相同的问题。同样，低MRR或低nDCG值可能表明检索器的排名算法存在问题。为了解决低答案忠实度或低答案相关性，您可能需要改进您的提示或微调LLM。
- en: '![A screenshot of a diagram'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![一个图表的截图'
- en: AI-generated content may be incorrect.](../Images/CH05_F08_Kimothi.png)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是错误的。](../Images/CH05_F08_Kimothi.png)
- en: Figure 5.8  Answer relevance is calculated as the mean of cosine similarity
    between the original and synthetic questions.
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8 答案相关性是通过计算原始问题和合成问题之间的余弦相似度的平均值来计算的。
- en: There may also exist some tradeoffs that you will need to balance. Improving
    precision often reduces recall and vice-versa. Highly relevant but brief contexts
    may lead to incomplete answers, and high answer faithfulness may sometimes come
    at the cost of answer relevance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 也可能存在一些需要您权衡的权衡。提高精确度通常会降低召回率，反之亦然。高度相关但简短的上下文可能导致答案不完整，而高答案忠实度有时可能以牺牲答案相关性为代价。
- en: The relative importance of each metric will depend on your use case and user
    requirements. You may need to include other metrics specific to your downstream
    use case, such as summarization to measure conciseness, and chatbots to emphasize
    conversation coherence.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每个指标相对的重要性将取决于您的用例和用户需求。您可能需要包括针对您的下游用例特定的其他指标，例如用于衡量简洁性的摘要，以及用于强调对话连贯性的聊天机器人。
- en: Developers can code these metrics from scratch and integrate them in the development
    and deployment process of their RAG system. However, you’ll find evaluation frameworks
    that are readily available quite handy. We discuss three popular frameworks in
    the next section.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者可以从头开始编写这些指标并将其集成到他们RAG系统的开发和部署过程中。然而，您会发现现成的评估框架非常方便。我们将在下一节讨论三个流行的框架。
- en: Human evaluations and ground truth data
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 人工评估和真实数据
- en: Most of the metrics we discussed talk about a concept of relevant documents.
    For example, precision is calculated as the number of relevant documents retrieved,
    divided by the total number of retrieved documents. The question that arises is,
    how does one establish that a document is relevant?
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中讨论的大多数指标都涉及相关文档的概念。例如，精确度是检索到的相关文档数量除以检索到的总文档数量。出现的问题是，一个人如何确定一个文档是相关的？
- en: The simple answer is a human evaluation approach. A subject matter expert looks
    at the documents and determines the relevance. Human evaluation brings in subjectivity,
    and therefore, human evaluations are done by a panel of experts rather than an
    individual. But human evaluations are restrictive from a scale and a cost perspective.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的答案是采用人工评估方法。主题专家查看文档并确定相关性。人工评估引入了主观性，因此，人工评估是由专家小组而不是个人进行的。但人工评估在规模和成本方面存在限制。
- en: Any data that can reliably establish relevance becomes extremely useful consequently.
    Ground truth is information known to be real or true. In RAG, and the generative
    AI domain in general, ground truth is a prepared set of prompt–context–response
    or question–context–response examples, akin to labeled data in supervised machine
    learning parlance. Ground truth data created for your knowledge base can be used
    for the evaluation of your RAG system.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可以可靠地建立相关性的数据都变得极其有用。地面实据是已知为真实或正确的信息。在RAG和生成式AI领域，地面实据是一组准备好的提示-上下文-响应或问题-上下文-响应示例，类似于监督机器学习中的标记数据。为您的知识库创建的地面实据数据可以用于评估您的RAG系统。
- en: How does one go about creating the ground truth data? It can be viewed as a
    one-time exercise where a group of experts creates this data. However, generating
    hundreds of QCA (question–context–answer) samples from documents manually can
    be a time-consuming and labor-intensive task. Additionally, if the knowledge base
    is dynamic, the ground truth data will also need updates. Questions created by
    humans may face challenges in achieving the necessary level of complexity for
    a comprehensive evaluation, potentially affecting the overall quality of the assessment.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如何创建地面实据数据？这可以被视为一次性的练习，其中一组专家创建这些数据。然而，从文档中手动生成数百个QCA（问题-上下文-答案）样本可能是一个耗时且劳动密集型的任务。此外，如果知识库是动态的，地面实据数据也需要更新。由人类创建的问题可能面临达到全面评估所需复杂性的挑战，这可能会影响评估的整体质量。
- en: LLMs can be used to address these challenges. Synthetic data generation uses
    LLMs to generate diverse questions and answers from the documents in the knowledge
    base. LLMs can be prompted to create questions such as simple questions, multi-context
    questions, conditional questions, reasoning questions, and similar using the documents
    from the knowledge base as context.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）可以用来解决这些挑战。合成数据生成使用LLMs从知识库中的文档生成多样化的问题和答案。LLMs可以被提示创建问题，如简单问题、多上下文问题、条件问题、推理问题等，使用知识库中的文档作为上下文。
- en: 5.3 Frameworks
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 框架
- en: 'Frameworks provide a structured approach to RAG evaluations. They can be used
    to automate the evaluation process. Some go beyond and assist in the synthetic
    ground truth data generation. While new evaluation frameworks continue to be introduced,
    there are two popular ones that we discuss here:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 框架为RAG评估提供了一个结构化的方法。它们可以用来自动化评估过程。一些框架甚至超越了这一点，并帮助生成合成地面实据数据。虽然新的评估框架仍在不断推出，但这里我们讨论两个流行的框架：
- en: RAGAs (Retrieval-Augmented Generation Assessment)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs（检索增强生成评估）
- en: ARES (Automated RAG Evaluation System)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARES（自动RAG评估系统）
- en: 5.3.1 RAGAs
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 RAGAs
- en: Retrieval-Augmented Generation Assessment, or RAGAs, is a framework developed
    by Exploding Gradients that assesses the retrieval and generation components of
    RAG systems without relying on extensive human annotations. RAGAs
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Retrieval-Augmented Generation Assessment（RAGAs，检索增强生成评估），是由Exploding Gradients开发的一个框架，它评估RAG系统的检索和生成组件，而不依赖于大量的人工标注。RAGAs
- en: Synthetically generate a test dataset that can be used to evaluate a RAG pipeline.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过合成生成一个可用于评估RAG管道的测试数据集。
- en: Use metrics to measure the performance of the pipeline.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用指标来衡量管道的性能。
- en: Monitor the quality of the application in production.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控生产中的应用质量。
- en: We will continue with our example of the Wikipedia page of the 2023 Cricket
    World Cup, but we first create a synthetic test dataset using RAGAs and then use
    the RAGAs metrics to evaluate the performance of the RAG pipeline we created in
    chapters 3 and 4.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续以2023年板球世界杯的维基百科页面为例，但首先使用RAGAs创建一个合成测试数据集，然后使用RAGAs指标来评估我们在第3章和第4章中创建的RAG管道的性能。
- en: Synthetic test dataset generation (ground truths)
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合成测试数据集生成（地面实据）
- en: Section 5.2 pointed out that ground truths data is necessary to calculate evaluation
    metrics for assessing the quality of RAG pipelines. While this data can be manually
    curated, RAGAs provides the functionality of generating this dataset from the
    documents in the knowledge base.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 第5.2节指出，为了计算评估RAG管道质量的评估指标，需要真实数据。虽然这些数据可以手动整理，但RAGAs提供了从知识库中的文档生成此数据集的功能。
- en: RAGAs does this using an LLM. It analyses the documents in the knowledge base
    and uses an LLM to generate seed questions from chunks in the knowledge base.
    These questions are based on the document chunks from the knowledge base. These
    chunks act as the context for the questions. Another LLM is used to generate the
    answer to these questions. This is how it generates a question–context–answer
    data based on the documents in the knowledge base. RAGAs also has an evolver module
    that creates more difficult questions (e.g., multi-context, reasoning, and conditional)
    for a more comprehensive evaluation. Figure 5.9 illustrates the process of synthetic
    data generation using RAGAs.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: RAGAs使用一个LLM来完成这项工作。它分析知识库中的文档，并使用LLM从知识库的片段中生成种子问题。这些问题基于知识库中的文档片段。这些片段作为问题的上下文。另一个LLM用于生成这些问题的答案。这就是它如何根据知识库中的文档生成问题-上下文-答案数据的方式。RAGAs还有一个进化模块，它创建更难的问题（例如，多上下文、推理和条件）以进行更全面的评估。图5.9说明了使用RAGAs生成合成数据的过程。
- en: '![A diagram of questions and arrows'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![问题和箭头的图示'
- en: AI-generated content may be incorrect.](../Images/CH05_F09_Kimothi.png)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F09_Kimothi.png)
- en: Figure 5.9  Synthetic ground truths data generation using RAGAs
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9 使用RAGAs生成合成真实数据
- en: 'To evaluate our RAG pipeline, let’s recreate the documents from the Wikipedia
    page like we did in chapter 3\. Note that we will have to install the packages
    used in the previous chapters to continue with the following code:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们的RAG管道，让我们像在第3章中做的那样重新创建Wikipedia页面的文档。请注意，为了继续以下代码，我们不得不安装前几章中使用的包：
- en: '[PRE0]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The `html_data_transformed` contains the necessary document format of the Wikipedia
    page. We will use RAGAs library to generate the dataset from these documents.
    For that, we will first need to install the RAGAs library:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`html_data_transformed`包含Wikipedia页面的必要文档格式。我们将使用RAGAs库从这些文档生成数据集。为此，我们首先需要安装RAGAs库：'
- en: '[PRE1]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `testset` that we created contains 20 questions based on our document, along
    with the chunk of the document that the question was based on, and the ground
    truth answer. A screenshot of the dataset is shown in figure 5.10.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的`testset`包含基于我们文档的20个问题，以及问题所基于的文档片段和真实答案。数据集的截图显示在图5.10中。
- en: '![A close-up of a table'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![表格的特写'
- en: AI-generated content may be incorrect.](../Images/CH05_F10_Kimothi.png)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: AI生成的内容可能是不正确的。](../Images/CH05_F10_Kimothi.png)
- en: Figure 5.10  Synthetic test data generated using RAGAs
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10 使用RAGAs生成的合成测试数据
- en: We will use this dataset to evaluate our RAG pipeline.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个数据集来评估我们的RAG管道。
- en: Recreating the RAG pipeline
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 重新创建RAG管道
- en: 'From the created test dataset, we use the `question` and the `ground_truth`
    information. We pass the questions to our RAG pipeline and generate answers. We
    compare these answers with the `ground_truth` to calculate the evaluation metrics.
    First, we recreate our RAG pipeline. Again, it is important to note that we will
    have to install the packages we used in the previous chapters to continue with
    the code:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 从创建的测试数据集中，我们使用`问题`和`真实答案`信息。我们将问题传递给我们的RAG管道并生成答案。我们将这些答案与`真实答案`进行比较，以计算评估指标。首先，我们重新创建我们的RAG管道。再次强调，为了继续编写代码，我们不得不安装前几章中使用的包：
- en: '[PRE2]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We can try this pipeline to generate answers.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试这个管道来生成答案。
- en: '[PRE3]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the RAG pipeline function, we can evaluate this pipeline using
    the questions that have been synthetically generated.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了RAG管道函数，我们可以使用合成的这些问题来评估这个管道。
- en: Evaluations
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估
- en: 'We first generate answers to the questions in the synthetic test data using
    our RAG pipeline. We then compare the answers to the ground truth answers. We
    first generate the answers:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用我们的RAG管道对合成测试数据集中的问题生成答案。然后，我们将这些答案与真实答案进行比较。我们首先生成答案：
- en: '[PRE4]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For RAGAs, the evaluation set needs to be in the `Dataset` format:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAGAs，评估集需要以`Dataset`格式：
- en: '[PRE5]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have the complete evaluation dataset, we can invoke the metrics:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了完整的评估数据集，我们可以调用指标：
- en: '[PRE6]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You can also check the official documentation of RAGAs for more information
    ([https://docs.ragas.io/en/stable/](https://docs.ragas.io/en/stable/)). RAGAs
    calculates a bunch of metrics that are useful for assessing the quality of the
    RAG pipeline. RAGAs uses an LLM to do this, somewhat subjective, task. For example,
    to calculate faithfulness for a given question–context–answer record, RAGAs first
    breaks down the answer into simple statements. Then, for each statement, it asks
    the LLM whether the statement can be inferred from the context. The LLM provides
    a 0 or 1 response along with a reason. This process is repeated a couple of times.
    Finally, faithfulness is calculated as the proportion of statements judged by
    the LLM as faithful (i.e., 1). Several other metrics are calculated using this
    LLM-based approach. This approach, where an LLM is used in evaluating a task,
    is also popularly called *LLM as a judge* approach. An important point to note
    here is that the accuracy of this evaluation is also dependent on the quality
    of the LLM being used as the judge.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以查看RAGAs的官方文档以获取更多信息（[https://docs.ragas.io/en/stable/](https://docs.ragas.io/en/stable/)）。RAGAs计算一系列对评估RAG管道质量有用的指标。RAGAs使用一个LLM来完成这个相对主观的任务。例如，为了计算给定问题-上下文-答案记录的忠实度，RAGAs首先将答案分解成简单的陈述。然后，对于每个陈述，它会询问LLM该陈述是否可以从上下文中推断出来。LLM会提供一个0或1的响应，并附上原因。这个过程会重复几次。最后，忠实度被计算为LLM判断为忠实（即1）的陈述的比例。使用基于LLM的方法还计算了其他几个指标。这种在评估任务中使用LLM的方法也被称为*LLM作为评委*方法。需要注意的是，这种评估的准确性也取决于作为评委使用的LLM的质量。
- en: 5.3.2 Automated RAG evaluation system
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 自动RAG评估系统
- en: 'Automated RAG evaluation system, or ARES, is a framework developed by researchers
    at Stanford University and Databricks. Like RAGAs, ARES uses an LLM as a judge
    approach for evaluations. Both request a language model to classify answer relevance,
    context relevance, and faithfulness for a given query. However, there are some
    differences:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 自动RAG评估系统，或ARES，是由斯坦福大学和Databricks的研究人员开发的一个框架。与RAGAs类似，ARES在评估中使用LLM作为评委的方法。两者都要求语言模型对给定查询的答案相关性、上下文相关性和忠实度进行分类。然而，也有一些不同之处：
- en: RAGAs relies on heuristically written prompts sent to the LLM for evaluation.
    ARES, in contrast, trains a classifier using a language model.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs依赖于发送给LLM进行评估的启发式提示。相比之下，ARES使用语言模型训练一个分类器。
- en: RAGAs aggregates the responses from the LLM to arrive at a score. ARES provides
    confidence intervals for the scores using a framework called Prediction-Powered
    Inference (PPI).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs通过汇总LLM的响应来得出分数。ARES使用一个名为预测增强推理（PPI）的框架为分数提供置信区间。
- en: RAGAs generates a simple synthetic question–context–answer dataset for evaluation
    from the documents. ARES generate synthetic datasets comprising both positive
    and negative examples of query–passage–answer triples.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs从文档中生成一个简单的合成问题-上下文-答案数据集用于评估。ARES生成包含查询-段落-答案三元组的正负例的合成数据集。
- en: 'ARES requires more data than RAGAs. To use ARES, you need the following three
    datasets:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ARES需要比RAGAs更多的数据。要使用ARES，您需要以下三个数据集：
- en: '*In-domain passage se**t*—This is a collection of passages relevant to the
    specific domain being evaluated. The passages should be suitable for generating
    queries and answers. In our case, it will be the documents that we created from
    the Wikipedia article.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*领域内段落集*——这是一组与被评估的具体领域相关的段落。这些段落应该适合生成查询和答案。在我们的案例中，它将是来自维基百科文章的文档。'
- en: '*Human preference validation se**t*—A minimum of approximately 150 annotated
    data points is required. This set is used to validate the preferences of human
    annotators regarding the relevance of the generated query-passage–answer triples.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*人类偏好验证集*——需要至少大约150个标注数据点。这个集合用于验证人类标注者对生成的查询-段落-答案三元组的关联性的偏好。'
- en: '*Few-shot example**s*—At least five examples of in-domain queries and answers
    are needed. These examples help prompt the LLMs during the synthetic data generation
    process.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*少量示例*——至少需要五个领域内查询和答案的示例。这些示例有助于在合成数据生成过程中提示LLMs。'
- en: The need for a human-preference validation set and fine-tuning of language models
    for classification makes applying ARES more complex. The application of ARES is
    out of the scope of this book. However, ARES is a robust framework. It provides
    a detailed analysis of system performance with statistical confidence intervals,
    making it suitable for in-depth RAG system evaluations. RAGAs promises a faster
    evaluation cycle without reliance on human annotations. More details on the ARES
    application can be found in the official GitHub repository ([https://github.com/stanford-futuredata/ARES](https://github.com/stanford-futuredata/ARES)).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类偏好验证集的需求以及对语言模型进行分类的微调使得应用ARES更加复杂。ARES的应用超出了本书的范围。然而，ARES是一个鲁棒的框架。它提供了对系统性能的详细分析，包括统计置信区间，使其适合深入评估RAG系统。RAGAs承诺在没有依赖人类标注的情况下实现更快的评估周期。有关ARES应用的更多详细信息，请参阅官方GitHub仓库（[https://github.com/stanford-futuredata/ARES](https://github.com/stanford-futuredata/ARES)）。
- en: While RAGAs and ARES have gained popularity, there are other frameworks, such
    as TruLens, DeepEval, and RAGChecker, that have also gotten acceptance amongst
    RAG developers.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 RAGAs 和 ARES 已经获得了流行，但还有其他框架，如 TruLens、DeepEval 和 RAGChecker，这些框架也得到了 RAG
    开发者的认可。
- en: Frameworks provide a standardized method of automating the evaluation of your
    RAG pipelines. Your choice of the evaluation framework should depend on your use
    case requirements. For quick and easy evaluations that are widely understood,
    RAGAs may be your choice. For robustness across diverse domains and question types,
    ARES might suit better. Most of the proprietary service providers (vector DBs,
    LLMs, etc.) have their evaluation features you may use. You can also develop your
    metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 框架提供了一种标准化的方法来自动化评估您的RAG管道。您选择的评估框架应取决于您的用例需求。对于快速且易于理解的评估，RAGAs可能是您的选择。对于跨多个领域和问题类型的鲁棒性，ARES可能更适合。大多数专有服务提供商（向量数据库、LLMs等）都有您可能使用的评估功能。您也可以开发自己的指标。
- en: Next, we look at benchmarks. Benchmarks are used to compare competing RAG systems
    with one another.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨基准测试。基准测试用于比较相互竞争的RAG系统。
- en: 5.4 Benchmarks
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 基准测试
- en: Benchmarks provide a standard point of reference to evaluate the quality and
    performance of a system. RAG benchmarks are a set of standardized tasks, and a
    dataset used to compare the efficiency of different RAG systems in retrieving
    relevant information and generating accurate responses. There has been a surge
    in creating benchmarks since 2023, when RAG started gaining popularity, but there
    have been benchmarks on question-answering tasks that were introduced before that.
    Benchmarks such as Stanford Question Answering Dataset (SQuAD), WikiQA, Natural
    Question (NQ), and HotpotQA are open domain question-answering datasets that primarily
    evaluate the retriever component using metrics such as Exact Match (EM) and F1-score.
    BEIR or benchmarking information retrieval is a comprehensive, heterogeneous benchmark
    based on 9 IR tasks and 19 question–answer datasets. This section discusses three
    of the popular RAG-specific benchmarks and their evaluation.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试提供了一种标准参考点，用于评估系统的质量和性能。RAG基准测试是一组标准化任务和用于比较不同RAG系统在检索相关信息和生成准确响应效率的数据库。自2023年以来，随着RAG开始流行，基准测试的数量急剧增加，但在那之前就已经有用于问答任务的基准测试。例如，斯坦福问答数据集（SQuAD）、WikiQA、自然问题（NQ）和HotpotQA都是开放域问答数据集，主要使用精确匹配（EM）和F1分数等指标来评估检索组件。BEIR或基准信息检索是一个基于9个信息检索任务和19个问答数据集的全面、异构基准。本节讨论了三个流行的RAG特定基准及其评估。
- en: 5.4.1 RGB
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 RGB
- en: 'Retrieval-augmented generation benchmark (RGB) was introduced in a December
    2023 paper ([https://arxiv.org/pdf/2309.01431](https://arxiv.org/pdf/2309.01431)).
    It comprises 600 base questions and 400 additional questions, evenly split between
    English and Chinese. The corpus was constructed using a multistep process that
    involved collecting recent news articles, generating questions and answers using
    ChatGPT, retrieving relevant web pages through Google’s API, and selecting the
    most pertinent text chunks using a dense retrieval model. It is a benchmark that
    focuses on four key abilities of a RAG system: noise robustness, negative rejection,
    information integration, and counterfactual robustness, as illustrated in figure
    5.11.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强生成基准（RGB）在 2023 年 12 月的一篇论文中提出([https://arxiv.org/pdf/2309.01431](https://arxiv.org/pdf/2309.01431))。它包含
    600 个基础问题和 400 个附加问题，英语和中文各占一半。语料库是通过一个多步骤过程构建的，包括收集最近的新闻文章，使用 ChatGPT 生成问题和答案，通过
    Google 的 API 检索相关网页，并使用密集检索模型选择最相关的文本片段。它是一个专注于 RAG 系统四个关键能力的基准：噪声鲁棒性、负拒绝、信息整合和反事实鲁棒性，如图
    5.11 所示。
- en: 'RGB focuses on the following metrics for evaluation:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: RGB 专注于以下指标进行评估：
- en: '*Accurac**y*—Used for noise robustness and information integration. It is based
    on the exact matching of the generated text with the correct answer.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性*—用于噪声鲁棒性和信息整合。它基于生成的文本与正确答案的精确匹配。'
- en: '*Rejection rat**e*—Used for negative rejection. It is measured by exact matching
    of the model’s output with a specific rejection phrase. The rejection rate is
    also'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拒绝率*—用于负拒绝。它通过模型输出与特定拒绝短语的精确匹配来衡量。拒绝率也'
- en: '![A group of text boxes'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![使用 ChatGPT 评估的一组文本框'
- en: AI-generated content may be incorrect.](../Images/CH05_F11_Kimothi.png)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能生成的内容可能是不正确的。](../Images/CH05_F11_Kimothi.png)
- en: 'Figure 5.11  Four abilities required of RAG systems. Source: Benchmarking Large
    Language Models in Retrieval-Augmented Generation by Chen et al., [https://arxiv.org/pdf/2309.0143](https://arxiv.org/pdf/2309.0143).'
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.11  RAG 系统所需的四种能力。来源：陈等人发表的《检索增强生成中的大型语言模型基准测试》，[https://arxiv.org/pdf/2309.0143](https://arxiv.org/pdf/2309.0143)。
- en: evaluated using ChatGPT to determine whether the responses contain rejection
    information.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以确定响应是否包含拒绝信息。
- en: '*Error detection rat**e*—Used for counterfactual robustness. It is measured
    by exact matching of the model’s output with a specific error-detection phrase
    and is also evaluated using ChatGPT.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误检测率*—用于反事实鲁棒性。它通过模型输出与特定错误检测短语的精确匹配来衡量，并使用 ChatGPT 进行评估。'
- en: '*Error correction rat**e*—Used for counterfactual robustness. It measures whether
    the model can provide the correct answer after identifying errors.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误纠正率*—用于反事实鲁棒性。它衡量模型在识别错误后是否可以提供正确答案。'
- en: You can use the GitHub repository to implement RGB ([https://github.com/chen700564/RGB](https://github.com/chen700564/RGB)).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 GitHub 仓库来实施 RGB ([https://github.com/chen700564/RGB](https://github.com/chen700564/RGB))。
- en: Multi-hop RAG
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多跳 RAG
- en: 'Curated by researchers at HKUST, multi-hop RAG contains 2556 queries, with
    evidence for each query distributed across two to four documents. The queries
    also involve document metadata, reflecting complex scenarios commonly found in
    real-world RAG applications. It contains four types of queries:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由香港科技大学的研究人员整理的多跳 RAG 包含 2556 个查询，每个查询的证据分布在两到四份文档中。查询还涉及文档元数据，反映了在现实世界的 RAG
    应用中常见的复杂场景。它包含四种类型的查询：
- en: '*Inferenc**e*—Synthesizing information across multiple sources (e.g., Which
    report discusses the supply chain risk of Apple—the 2019 annual report or the
    2020 annual report?)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*推理*—跨多个来源综合信息（例如，哪份报告讨论了苹果的供应链风险——2019 年年度报告还是 2020 年年度报告？）'
- en: '*Compariso**n*—Comparing facts from different sources (e.g., Did Netflix or
    Google report higher revenue for the year 2023?)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*比较*—比较不同来源的事实（例如，Netflix 或 Google 是否在 2023 年报告了更高的收入？）'
- en: '*Tempora**l*—Analyzing the temporal ordering of events (e.g., e.g. Did Apple
    introduce the AirTag tracking device before or after the launch of the 5th generation
    iPad Pro?)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间性*—分析事件的时序（例如，例如，苹果是在第五代 iPad Pro 发布前还是之后推出 AirTag 跟踪设备？）'
- en: '*Nul**l*—Queries not answerable from the knowledge base'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*空值*—无法从知识库中回答的查询'
- en: Full implementation code is available at [https://github.com/yixuantt/MultiHop-RAG](https://github.com/yixuantt/MultiHop-RAG).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的实现代码可在 [https://github.com/yixuantt/MultiHop-RAG](https://github.com/yixuantt/MultiHop-RAG)
    找到。
- en: CRAG
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CRAG
- en: Comprehensive RAG benchmark (CRAG), curated by Meta and HKUST, is a factual
    question-answering benchmark of 4,409 question–answer pairs and mock APIs to simulate
    web and knowledge graph (KG) search. It contains eight types (simple, conditions,
    comparison questions, aggregation questions, multi-hop questions, set queries,
    post-processing-heavy questions, and false-premise questions, as illustrated in
    figure 5.12) of queries across five domains (finance, sports, music, movie, and
    open domain).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Meta 和 HKUST 精心制作的综合 RAG 基准（CRAG）是一个包含 4,409 个问答对和模拟 API 的实际问答基准，用于模拟网络和知识图谱（KG）搜索。它包含八个类型（简单、条件、比较问题、聚合问题、多跳问题、集合查询、后处理密集型问题和错误前提问题，如图
    5.12 所示）的查询，涵盖五个领域（金融、体育、音乐、电影和开放领域）。
- en: 'For each question in the evaluation set, CRAG labels the answer with one of
    four classes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估集中的每个问题，CRAG 使用四个类别之一对答案进行标记：
- en: '*Perfec**t*—The response correctly answers the user’s question and contains
    no hallucinated content (scored as +1).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完美*—响应正确回答了用户的问题，且不包含任何幻觉内容（评分为 +1）。'
- en: '*Acceptabl**e*—The response provides a useful answer to the user’s question
    but may contain minor errors that do not harm the usefulness of the answer (scored
    as +0.5).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可接受*—响应提供了对用户问题的有用答案，但可能包含一些不影响答案有用性的小错误（评分为 +0.5）。'
- en: '*Missin**g*—The response is “I don’t know”, “I’m sorry I can’t find ...”, a
    system error such as an empty response, or a request from the system to clarify
    the original question (scored as 0).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺失*—响应是“我不知道”、“很抱歉我找不到...”，系统错误（如空响应）或系统请求澄清原始问题的请求（评分为 0）。'
- en: '*Incorrec**t*—The response provides wrong or irrelevant information to answer
    the user’s question (scored as −1).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误*—响应提供了错误或不相关的信息来回答用户的问题（评分为 -1）。'
- en: '![A table of questions with text'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含文本的问题表'
- en: AI-generated content may be incorrect.](../Images/CH05_F12_Kimothi.png)
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: AI 生成的内容可能是不正确的。](../Images/CH05_F12_Kimothi.png)
- en: Figure 5.12  Eight question types in CRAG
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.12  CRAG 中的八种问题类型
- en: For automatic evaluation, CRAG classifies an answer as perfect if it exactly
    matches the ground truth. If not, then it asks an LLM to do the classification.
    It uses two LLM evaluators. You can read more about CRAG at [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动评估，CRAG 将答案精确匹配到基准真实值时将其分类为完美。如果不匹配，则要求一个 LLM 进行分类。它使用了两个 LLM 评估器。您可以在 [https://arxiv.org/pdf/2406.04744](https://arxiv.org/pdf/2406.04744)
    上了解更多关于 CRAG 的信息。
- en: Other noteworthy benchmark datasets are MedRAG ([https://github.com/Teddy-XiongGZ/MedRAG](https://github.com/Teddy-XiongGZ/MedRAG)),
    which focuses on Medical Information, CRUD-RAG ([https://arxiv.org/pdf/2401.17043](https://arxiv.org/pdf/2401.17043)),
    which focuses on the Chinese language, and FeB4RAG ([https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891)),
    which focuses on federated search. If you’re developing an LLM application that
    has accurate and contextual generation as its core proposition, you’ll be able
    to communicate the quality of your application by showing how it performs on different
    benchmarks. Table 5.2 compares the different benchmarks.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 其他值得注意的基准数据集包括专注于医疗信息的 MedRAG ([https://github.com/Teddy-XiongGZ/MedRAG](https://github.com/Teddy-XiongGZ/MedRAG))，专注于中文的
    CRUD-RAG ([https://arxiv.org/pdf/2401.17043](https://arxiv.org/pdf/2401.17043))，以及专注于联邦搜索的
    FeB4RAG ([https://arxiv.org/abs/2402.11891](https://arxiv.org/abs/2402.11891))。如果您正在开发一个以准确和上下文生成为核心主张的
    LLM 应用程序，您将通过展示它在不同基准上的表现来展示您应用程序的质量。表 5.2 比较了不同的基准。
- en: Table 5.2 RAG benchmarks
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.2 RAG 基准
- en: '| Benchmark | Dataset | Task | Metrics | Applicability |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 数据集 | 任务 | 指标 | 适用性 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SQuAD | Stanford Question Answering Dataset | Open domain QA | Exact match
    (EM), F1-score | General QA tasks, model evaluation on comprehension accuracy
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD | 斯坦福问答数据集 | 开放域问答 | 精确匹配 (EM), F1-score | 一般问答任务，对理解准确性的模型评估 |'
- en: '| Natural questions | Real Google search queries | Open domain QA | F1-score
    | Real-world QA, information retrieval from large corpora |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 自然问题 | 真实 Google 搜索查询 | 开放域问答 | F1-score | 真实世界的问答，从大型语料库中检索信息 |'
- en: '| HotpotQA | Wikipedia-based QA | Multi-hop QA | EM, F1-score | QA involving
    multiple documents, complex reasoning tasks |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 火锅问答 | 基于维基百科的问答 | 多跳问答 | EM, F1-score | 涉及多个文档、复杂推理任务的问答 |'
- en: '| BEIR | Multiple datasets | Information retrieval | nDCG@10 | Comprehensive
    IR model evaluation across multiple domains |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| BEIR | 多个数据集 | 信息检索 | nDCG@10 | 多个领域的综合信息检索模型评估 |'
- en: '| RGB | News articles, ChatGPT-generated QA | Robust QA | Accuracy, rejection
    rate, error detection rate, error correction rate | Robustness and reliability
    of RAG systems |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| RGB | 新闻文章、ChatGPT生成的问答 | 强健问答 | 准确率、拒绝率、错误检测率、错误纠正率 | RAG系统的鲁棒性和可靠性 |'
- en: '| Multi-hop RAG | HKUST-curated queries | Complex QA | Various | RAG applications
    requiring multi-source synthesis |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 多跳RAG | 香港科技大学整理的查询 | 复杂问答 | 各种 | 需要跨源综合的RAG应用 |'
- en: '| CRAG | Multiple sources (finance, sports, music, etc.) | Factual QA | Four-class
    evaluation (perfect, acceptable, missing, and incorrect) | Evaluating factual
    QA with diverse question types |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| CRAG | 多个来源（金融、体育、音乐等） | 事实问答 | 四类评估（完美、可接受、缺失和错误） | 使用不同问题类型评估事实问答 |'
- en: We have looked frameworks that help in automating the calculation of evaluation
    metrics and benchmarks that enable comparisons across different implementations
    and approaches. Frameworks will assist you in improving the performance of your
    system, and benchmarks will facilitate comparing it with other systems available
    in the market.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了帮助自动化计算评估指标和基准框架，这些基准框架能够实现不同实现和方法的比较。框架将帮助您提高系统的性能，而基准将有助于将其与其他市场上可用的系统进行比较。
- en: However, as with any evolving field, there are some limitations and challenges
    to consider. The next section examines these limitations and discusses best practices
    that have emerged to address them, ensuring a more holistic and nuanced approach
    to RAG evaluation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与任何不断发展的领域一样，有一些限制和挑战需要考虑。下一节将探讨这些限制，并讨论为解决这些问题而出现的最佳实践，以确保对RAG评估采取更全面和细致的方法。
- en: 5.5 Limitations and best practices
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 限制和最佳实践
- en: There has been a lot of progress made in the frameworks and benchmarks used
    for RAG evaluation. The complexity in evaluation arises due to the interplay between
    the retrieval and generation components. In practice, there’s a significant reliance
    on human judgements, which are subjective and difficult to scale. What follows
    are a few common challenges and some guidelines to navigate them.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于RAG评估的框架和基准方面已经取得了很大的进展。评估的复杂性源于检索和生成组件之间的相互作用。在实践中，对人类判断的依赖很大，这些判断是主观的，难以扩展。以下是一些常见的挑战和一些指导方针，以帮助应对这些挑战。
- en: Lack of standardized metrics
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺乏标准化指标
- en: There’s no consensus on what the best metrics are to evaluate RAG systems. Precision,
    recall, and F1-score are commonly measured for retrieval but do not fully capture
    the nuances of generative response. Similarly, commonly used generation metrics
    such as BLEU, ROUGE, and similar do not fully capture the context awareness required
    for RAG. Using RAG-specific metrics such as answer relevance, context relevance,
    and faithfulness for evaluation brings in the necessary nuances required for RAG
    evaluation. However, even for these metrics, there’s no standard way of calculation
    and each framework brings in its methodology.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估RAG系统，没有共识认为哪些是最好的指标。对于检索，通常测量精确率、召回率和F1分数，但它们并不能完全捕捉生成响应的细微差别。同样，常用的生成指标如BLEU、ROUGE等也不能完全捕捉RAG所需的上下文意识。使用针对RAG的特定指标，如答案相关性、上下文相关性和忠实度进行评估，可以引入RAG评估所需的必要细微差别。然而，即使是这些指标，也没有标准的计算方法，每个框架都带来了自己的方法。
- en: '*Best practice*: Compare the results on RAG specific metrics from different
    frameworks. Sometimes, it may be warranted to change the calculation method with
    respect to the use case.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳实践*：比较不同框架在RAG特定指标上的结果。有时，根据用例更改计算方法可能是合理的。'
- en: Overreliance on LLM as a judge
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过度依赖LLM作为评判者
- en: The evaluation of RAG-specific metrics (in RAGAs, ARES, etc.) relies on using
    an LLM as a judge. An LLM is prompted or fine-tuned to classify a response as
    relevant or not. This adds to the complexity of the LLMs’ ability to do this task.
    It may be possible that the LLM may not be very accurate in judging for your specific
    documents and knowledge bases. Another problem that arises is that of self-reference.
    It is possible that if the judge LLM is the same as the generation LLM in your
    system, you will get a more favorable evaluation.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: RAG特定指标（在RAGAs、ARES等中）的评估依赖于使用一个LLM作为评判者。LLM被提示或微调以将响应分类为相关或不相关。这增加了LLM执行此任务复杂性的程度。可能的情况是，LLM在评判您特定的文档和知识库时可能不太准确。另一个问题是自我参照。如果评判LLM与您的系统中的生成LLM相同，您可能会得到更有利的结果。
- en: '*Best practice*: Sample a few results from the judge LLM and evaluate whether
    the results are in line with commonly understood business practice. To avoid the
    self-reference problem, make sure to use a judge LLM different from the generation
    LLM. It may also help if you use multiple judge LLMs and aggregate their results.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳实践*：从裁判LLM中抽取一些结果并评估这些结果是否与普遍理解的企业实践一致。为了避免自引用问题，请确保使用与生成LLM不同的裁判LLM。如果你使用多个裁判LLM并汇总他们的结果，这也可能有所帮助。'
- en: Lack of use case subjectivity
  id: totrans-293
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 缺乏用例主观性
- en: Most frameworks have a generalized approach to evaluation. They may not capture
    the subjective nature of the task relevant to your use case (content generation
    versus chatbot versus question-answering, etc.)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数框架在评估时采用通用方法。它们可能无法捕捉与你用例相关的任务的主体性质（内容生成与聊天机器人、问答等）。
- en: '*Best practice*: Focus on use-case-specific metrics to assess quality, coherence,
    usefulness, and similar. Incorporate human judgements in your workflow with techniques
    such as user feedback, crowd-sourcing, or expert ratings.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳实践*：关注特定用例的指标来评估质量、连贯性、有用性和类似方面。在你的工作流程中结合人类判断，使用用户反馈、众包或专家评分等技术。'
- en: Benchmarks are static
  id: totrans-296
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准是静态的
- en: Most benchmarks are static and do not account for the evolving nature of information.
    RAG systems need to adapt to real-time information changes, which are not currently
    tested effectively. There is a lack of evaluation for how well RAG models learn
    and adapt to new data over time. Most benchmarks are domain-agnostic, which may
    not reflect the performance of RAG systems in your specific domain.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基准都是静态的，没有考虑到信息的演变性质。RAG系统需要适应实时信息的变化，而这些变化目前还没有得到有效测试。缺乏对RAG模型如何随时间学习并适应新数据的评估。大多数基准都是领域无关的，这可能无法反映你特定领域的RAG系统的性能。
- en: '*Best practice*: Use a benchmark that is tailored to your domain. The static
    nature of benchmarks is limiting. Do not overly rely on benchmarks, and augment
    the use of benchmarks with regularly updating data.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳实践*：使用针对你领域定制的基准。基准的静态性质是有限的。不要过度依赖基准，并使用定期更新的数据来补充基准的使用。'
- en: Scalability and cost
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 可扩展性和成本
- en: Evaluating large-scale RAG systems is more complex than evaluating basic RAG
    pipelines. It requires significant computational resources. Benchmarks and frameworks
    also generally do not account for metrics such as latency and efficiency, which
    are critical for real-world applications.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 评估大规模RAG系统比评估基本的RAG管道更复杂。它需要大量的计算资源。基准和框架通常也不考虑如延迟和效率等对现实应用至关重要的指标。
- en: '*Best practice*: Employ careful sampling of test cases for evaluation. Incorporate
    workflows to measure latency and efficiency.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*最佳实践*：在评估时，仔细采样测试用例。结合工作流程来衡量延迟和效率。'
- en: Apart from these, you should also carefully consider the aspects of bias and
    toxicity, focusing on information integration and negative rejection, which the
    frameworks do not evaluate well. It is also important to keep an eye on how these
    evaluation frameworks and benchmarks evolve.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，你还应仔细考虑偏差和毒性等方面，重点关注信息整合和负面拒绝，这些框架评估得并不好。同时，也要关注这些评估框架和基准的演变。
- en: In this chapter, we comprehensively examined the evaluation metrics, frameworks,
    and benchmarks that will help you evaluate your RAG pipelines. We used RAGAs to
    evaluate the pipeline that we have been building.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们全面考察了评估指标、框架和基准，这些将帮助你评估你的RAG管道。我们使用了RAGAs来评估我们一直在构建的管道。
- en: Until now, we have looked at building and evaluating a simple RAG system. This
    also marks the second part 2 of this book. You are now familiar with the creation
    of the RAG knowledge brain using the indexing pipeline, enabling real-time interaction
    using the generation pipeline and evaluating your RAG system using frameworks
    and benchmarks.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了构建和评估一个简单的RAG系统。这也标志着本书的第二部分2。你现在已经熟悉了使用索引管道创建RAG知识大脑，使用生成管道实现实时交互，以及使用框架和基准来评估你的RAG系统。
- en: In the next part, we will move toward discussing the production aspects of RAG
    systems. In chapter 6, we will look at strategies and advanced techniques to improve
    our RAG pipeline, which should also reflect in better evaluation metrics. In chapter
    7, we will look at the LLMOps stack that enables RAG in production.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将转向讨论RAG系统的生产方面。在第6章中，我们将探讨改进RAG管道的策略和高级技术，这些也应反映在更好的评估指标中。在第7章中，我们将探讨使RAG在生产中成为可能的LLMOps堆栈。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: RAG evaluation fundamentals
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RAG评估基础
- en: RAG evaluation assesses how well systems reduce hallucinations and ground responses
    in the provided context.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估评估系统在提供的上下文中减少幻觉和地面响应的能力。
- en: Three key quality scores for RAG evaluation are context relevance, answer faithfulness,
    and answer relevance.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的三个关键质量分数是上下文相关性、答案忠实度和答案相关性。
- en: Four critical abilities required of RAG systems include noise robustness, negative
    rejection, information integration, and counterfactual robustness.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG系统所需的四个关键能力包括噪声鲁棒性、负拒绝、信息整合和反事实鲁棒性。
- en: Additional considerations include latency, robustness, bias, and toxicity of
    responses.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要考虑的额外因素包括响应的延迟、鲁棒性、偏差和毒性。
- en: Custom use-case-specific metrics should be developed to evaluate performance.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该开发定制化的特定用例指标来评估性能。
- en: Evaluation metrics
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估指标
- en: Retrieval metrics include precision, recall, F1-score, mean reciprocal rank
    (MRR), mean average precision (MAP), and normalized discounted cumulative gain
    (nDCG).
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索指标包括精确率、召回率、F1分数、平均倒数排名（MRR）、平均平均精度（MAP）和归一化折现累积增益（nDCG）。
- en: Accuracy, precision, recall, and F1-score do not consider the ranking order
    of the results.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率、精确率、召回率和F1分数不考虑结果的排名顺序。
- en: RAG-specific metrics focus on context relevance, answer faithfulness, and answer
    relevance.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG特定指标关注上下文相关性、答案忠实度和答案相关性。
- en: Human evaluations and ground truth data play a crucial role in RAG assessment.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人工评估和真实数据在RAG评估中起着至关重要的作用。
- en: Evaluation frameworks
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估框架
- en: RAGAs is an easy-to-implement framework that can be used for quick evaluation
    of RAG pipelines.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGAs是一个易于实现的框架，可用于快速评估RAG管道。
- en: ARES uses a more complex approach, including classifier training and confidence
    interval calculations.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARES采用更复杂的方法，包括分类器训练和置信区间计算。
- en: Benchmarks
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基准
- en: Benchmarks provide standardized datasets and metrics for comparing different
    RAG implementations on specific tasks.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准为比较特定任务上不同RAG实现提供了标准化的数据集和指标。
- en: Popular benchmarks such as SQuAD, natural questions, HotpotQA, and BEIR focus
    on retrieval quality.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流行基准如SQuAD、自然问题、HotpotQA和BEIR专注于检索质量。
- en: Recent benchmarks such as RGB, multi-hop RAG, and CRAG are more holistic from
    a RAG perspective.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近期基准如RGB、多跳RAG和CRAG从RAG的角度看更为全面。
- en: Benchmarks focus on different aspects of RAG performance, such as multi-hop
    reasoning or specific domains.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准关注RAG性能的不同方面，如多跳推理或特定领域。
- en: Limitations and best practices
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局限性和最佳实践
- en: Challenges in RAG evaluation include lack of standardized metrics, overreliance
    on LLMs as judges, and static nature of benchmarks.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估的挑战包括缺乏标准化指标、过度依赖LLM作为评判者以及基准的静态性质。
- en: Best practices include using multiple frameworks, incorporating use-case-specific
    metrics, and regularly updating evaluation data.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳实践包括使用多个框架、结合特定用例指标和定期更新评估数据。
- en: Balancing automated metrics with human judgment and considering use-case-specific
    requirements is crucial.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平衡自动化指标与人工判断，并考虑特定用例的要求至关重要。
- en: The field of RAG evaluation is evolving, with new frameworks and benchmarks
    constantly emerging.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAG评估领域正在发展，新的框架和基准不断涌现。
- en: Developers should stay informed about new developments and adapt their evaluation
    strategies accordingly.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发者应关注新进展并相应地调整评估策略。
