- en: 2 The mathematical building blocks of neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 神经网络的数学基础
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: A first example of a neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的第一个例子
- en: Tensors and tensor operations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张量和张量操作
- en: How neural networks learn via backpropagation and gradient descent
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络如何通过反向传播和梯度下降学习
- en: 'Understanding deep learning requires familiarity with many simple mathematical
    concepts: *tensors*, *tensor operations*, *differentiation*, *gradient descent*,
    and so on. Our goal in this chapter will be to build up your intuition about these
    notions without getting overly technical. In particular, we’ll steer away from
    mathematical notation, which can introduce unnecessary barriers for those without
    any mathematics background and isn’t necessary to explain things well. The most
    precise, unambiguous description of a mathematical operation is its executable
    code.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 理解深度学习需要熟悉许多简单的数学概念：*张量*、*张量操作*、*微分*、*梯度���降*等。本章的目标是在不过于技术化的情况下建立您对这些概念的直觉。特别是，我们将避开数学符号，这可能会给没有数学背景的人带来不必要的障碍，并且不是解释事物的必要条件。数学操作的最精确、明确的描述是其可执行代码。
- en: To provide sufficient context for introducing tensors and gradient descent,
    we’ll begin the chapter with a practical example of a neural network. Then we’ll
    go over every new concept that’s been introduced, point by point. Keep in mind
    that these concepts will be essential for you to understand the practical examples
    in the following chapters!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为引入张量和梯度下降提供足够的背景，我们将从一个神经网络的实际例子开始本章。然后我们将逐点地讨论每个新引入的概念。请记住，这些概念对于您理解以下章节中的实际示例至关重要！
- en: After reading this chapter, you’ll have an intuitive understanding of the mathematical
    theory behind deep learning, and you’ll be ready to start diving into Keras and
    TensorFlow in chapter 3.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读完本章后，您将对深度学习背后的数学理论有直观的理解，并准备好在第三章开始深入研究Keras和TensorFlow。
- en: 2.1 A first look at a neural network
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 神经网络的初步了解
- en: Let’s look at a concrete example of a neural network that uses the Python library
    Keras to learn to classify handwritten digits. Unless you already have experience
    with Keras or similar libraries, you won’t understand everything about this first
    example right away. That’s fine. In the next chapter, we’ll review each element
    in the example and explain them in detail. So don’t worry if some steps seem arbitrary
    or look like magic to you! We’ve got to start somewhere.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子，一个使用Python库Keras学习分类手写数字的神经网络。除非您已经有使用Keras或类似库的经验，否则您不会立即理解这个第一个例子的所有内容。没关系。在下一章中，我们将逐个审查示例中的每个元素并详细解释它们。所以如果有些步骤看起来随意或对您来说像魔术一样，请不要担心！我们必须从某个地方开始。
- en: The problem we’re trying to solve here is to classify grayscale images of handwritten
    digits (28 × 28 pixels) into their 10 categories (0 through 9). We’ll use the
    MNIST dataset, a classic in the machine learning community, which has been around
    almost as long as the field itself and has been intensively studied. It’s a set
    of 60,000 training images, plus 10,000 test images, assembled by the National
    Institute of Standards and Technology (the NIST in MNIST) in the 1980s. You can
    think of “solving” MNIST as the “Hello World” of deep learning—it’s what you do
    to verify that your algorithms are working as expected. As you become a machine
    learning practitioner, you’ll see MNIST come up over and over again in scientific
    papers, blog posts, and so on. You can see some MNIST samples in figure 2.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要解决的问题是将手写数字的灰度图像（28×28像素）分类为它们的10个类别（0到9）。我们将使用MNIST数据集，这是机器学习社区中的经典数据集，几乎与该领域本身一样久远并受到密切研究。这是由国家标准技术研究所（MNIST中的NIST）在上世纪80年代汇编的一组60000个训练图像和10000个测试图像。您可以将“解决”MNIST看作是深度学习的“Hello
    World” - 这是您验证算法是否按预期工作的方法。随着您成为机器学习从业者，您会发现MNIST在科学论文、博客文章等中反复出现。您可以在图2.1中看到一些MNIST样本。
- en: '![](../Images/02-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-01.png)'
- en: Figure 2.1 MNIST sample digits
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 MNIST样本数字
- en: Note In machine learning, a *category* in a classification problem is called
    a *class*. Data points are called *samples*. The class associated with a specific
    sample is called a *label*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，分类问题中的*类别*称为*类*。数据点称为*样本*。与特定样本相关联的类称为*标签*。
- en: You don’t need to try to reproduce this example on your machine just now. If
    you wish to, you’ll first need to set up a deep learning workspace, which is covered
    in chapter 3.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在不需要在您的机器上尝试复制这个例子。如果您希望这样做，您首先需要设置一个深度学习工作空间，这在第三章中有介绍。
- en: The MNIST dataset comes preloaded in Keras, in the form of a set of four NumPy
    arrays.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集已经预装在Keras中，以四个NumPy数组的形式存在。
- en: Listing 2.1 Loading the MNIST dataset in Keras
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 在Keras中加载MNIST数据集
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`train_images` and `train_labels` form the training set, the data that the
    model will learn from. The model will then be tested on the test set, `test_images`
    and `test_labels`. The images are encoded as NumPy arrays, and the labels are
    an array of digits, ranging from 0 to 9\. The images and labels have a one-to-one
    correspondence.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`train_images`和`train_labels`组成训练集，模型将从中学习。然后模型将在测试集`test_images`和`test_labels`上进行测试。图像被编码为NumPy数组，标签是一个从0到9的数字数组。图像和标签之间有一一对应关系。'
- en: 'Let’s look at the training data:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下训练数据：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'And here’s the test data:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是测试数据：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The workflow will be as follows: First, we’ll feed the neural network the training
    data, `train_images` and `train_labels`. The network will then learn to associate
    images and labels. Finally, we’ll ask the network to produce predictions for `test_images`,
    and we’ll verify whether these predictions match the labels from `test_labels`.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程如下：首先，我们将向神经网络提供训练数据`train_images`和`train_labels`。然后网络将学习将图像和标签关联起来。最后，我们将要求网络为`test_images`生成预测，并验证这些预测是否与`test_labels`中的标签匹配。
- en: Let’s build the network—again, remember that you aren’t expected to understand
    everything about this example yet.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建网络—再次提醒您，您不必完全理解这个示例的所有内容。
- en: Listing 2.2 The network architecture
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 网络架构
- en: '[PRE3]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The core building block of neural networks is the *layer*. You can think of
    a layer as a filter for data: some data goes in, and it comes out in a more useful
    form. Specifically, layers extract *representations* out of the data fed into
    them—hopefully, representations that are more meaningful for the problem at hand.
    Most of deep learning consists of chaining together simple layers that will implement
    a form of progressive *data distillation*. A deep learning model is like a sieve
    for data processing, made of a succession of increasingly refined data filters—the
    layers.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的核心构建块是*层*。您可以将层视为数据的过滤器：一些数据进入，以更有用的形式输出。具体来说，层从输入的数据中提取*表示*，希望这些表示对手头的问题更有意义。大部分深度学习都是将一些简单层链接在一起，这些层将实现一种渐进*数据精炼*形式。深度学习模型就像是数据处理的筛子，由一系列越来越精细的数据过滤器（层）组成。
- en: Here, our model consists of a sequence of two `Dense` layers, which are densely
    connected (also called *fully connected*) neural layers. The second (and last)
    layer is a 10-way *softmax classification* layer, which means it will return an
    array of 10 probability scores (summing to 1). Each score will be the probability
    that the current digit image belongs to one of our 10 digit classes.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的模型由两个`Dense`层的序列组成，这些层是密集连接（也称为*全连接*）的神经层。第二（也是最后）层是一个10路*softmax分类*层，这意味着它将返回一个总和为1的10个概率分数数组。每个分数将是当前数字图像属于我们的10个数字类别之一的概率。
- en: 'To make the model ready for training, we need to pick three more things as
    part of the *compilation* step:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使模型准备好进行训练，我们需要在*编译*步骤中选择另外三个事项：
- en: '*An optimizer*—The mechanism through which the model will update itself based
    on the training data it sees, so as to improve its performance.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*—模型将根据其看到的训练数据更新自身的机制，以提高其性能。'
- en: '*A loss function*—How the model will be able to measure its performance on
    the training data, and thus how it will be able to steer itself in the right direction.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失函数*—模型如何能够衡量其在训练数据上的表现，从而如何能够引导自己朝着正确的方向前进。'
- en: '*Metrics to monitor during training and testing*—Here, we’ll only care about
    accuracy (the fraction of the images that were correctly classified).'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练和测试过程中监控的指标*—在这里，我们只关心准确率（被正确分类的图像的比例）。'
- en: The exact purpose of the loss function and the optimizer will be made clear
    throughout the next two chapters.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数和优化器的确切目的将在接下来的两章中明确。
- en: Listing 2.3 The compilation step
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 编译步骤
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Before training, we’ll preprocess the data by reshaping it into the shape the
    model expects and scaling it so that all values are in the `[0,` `1]` interval.
    Previously, our training images were stored in an array of shape `(60000,` `28,`
    `28)` of type `uint8` with values in the `[0,` `255]` interval. We’ll transform
    it into a `float32` array of shape `(60000,` `28` `*` `28)` with values between
    0 and 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们将通过重新调整数据的形状以及缩放数据，使所有值都在`[0，` `1]`区间内来预处理数据。之前，我们的训练图像存储在一个形状为`(60000，`
    `28，` `28)`的`uint8`类型数组中，值在`[0，` `255]`区间内。我们将其转换为一个形状为`(60000，` `28` `*` `28)`的`float32`数组，值在0到1之间。
- en: Listing 2.4 Preparing the image data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.4 准备图像数据
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’re now ready to train the model, which in Keras is done via a call to the
    model’s `fit()` method—we *fit* the model to its training data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练模型，在Keras中通过调用模型的`fit()`方法来完成——我们将模型与其训练数据*拟合*。
- en: Listing 2.5 “Fitting” the model
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.5 “拟合”模型
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Two quantities are displayed during training: the loss of the model over the
    training data, and the accuracy of the model over the training data. We quickly
    reach an accuracy of 0.989 (98.9%) on the training data.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中显示两个量：模型在训练数据上的损失和模型在训练数据上的准确率。我们很快就在训练数据上达到了0.989（98.9%）的准确率。
- en: Now that we have a trained model, we can use it to predict class probabilities
    for *new* digits—images that weren’t part of the training data, like those from
    the test set.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个经过训练的模型，我们可以使用它来预测*新*数字的类别概率—这些图像不是训练数据的一部分，就像测试集中的那些图像一样。
- en: Listing 2.6 Using the model to make predictions
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.6 使用模型进行预测
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each number of index `i` in that array corresponds to the probability that digit
    image `test_digits[0]` belongs to class `i`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 数组中索引`i`处的每个数字对应于数字图像`test_digits[0]`属于类别`i`的概率。
- en: 'This first test digit has the highest probability score (0.99999106, almost
    1) at index 7, so according to our model, it must be a 7:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个测试数字在索引7处具有最高的概率分数（0.99999106，接近1），因此根据我们的模型，它必须是一个7：
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can check that the test label agrees:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查测试标签是否一致：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: On average, how good is our model at classifying such never-before-seen digits?
    Let’s check by computing average accuracy over the entire test set.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在对这些以前从未见过的数字进行分类时，平均表现如何？让我们通过计算整个测试集上的平均准确率来检查。
- en: Listing 2.7 Evaluating the model on new data
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.7 在新数据上评估模型
- en: '[PRE10]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The test-set accuracy turns out to be 97.8%—that’s quite a bit lower than the
    training-set accuracy (98.9%). This gap between training accuracy and test accuracy
    is an example of *overfitting*: the fact that machine learning models tend to
    perform worse on new data than on their training data. Overfitting is a central
    topic in chapter 3.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集准确率为97.8%—这比训练集准确率（98.9%）要低得多。训练准确率和测试准确率之间的差距是*过拟合*的一个例子：机器学习模型在新数据上的表现往往不如在其训练数据上。过拟合是第3章的一个核心主题。
- en: This concludes our first example—you just saw how you can build and train a
    neural network to classify handwritten digits in less than 15 lines of Python
    code. In this chapter and the next, we’ll go into detail about every moving piece
    we just previewed and clarify what’s going on behind the scenes. You’ll learn
    about tensors, the data-storing objects going into the model; tensor operations,
    which layers are made of; and gradient descent, which allows your model to learn
    from its training examples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们的第一个示例——你刚刚看到如何构建和训练一个神经网络来对手写数字进行分类，只需不到15行的Python代码。在本章和下一章中，我们将详细介绍我们刚刚预览的每个移动部分，并澄清幕后发生的事情。你将了解张量，这些数据存储对象进入模型；张量操作，层是由什么组成的；以及梯度下降，它允许你的模型从训练示例中学习。
- en: 2.2 Data representations for neural networks
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 神经网络的数据表示
- en: In the previous example, we started from data stored in multidimensional NumPy
    arrays, also called *tensors*. In general, all current machine learning systems
    use tensors as their basic data structure. Tensors are fundamental to the field—so
    fundamental that TensorFlow was named after them. So what’s a tensor?
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们从存储在多维NumPy数组中的数据开始，也称为*张量*。一般来说，所有当前的机器学习系统都使用张量作为它们的基本数据结构。张量对于这个领域是基础的——以至于TensorFlow就是以它们命名的。那么什么是张量？
- en: 'At its core, a tensor is a container for data—usually numerical data. So, it’s
    a container for numbers. You may be already familiar with matrices, which are
    rank-2 tensors: tensors are a generalization of matrices to an arbitrary number
    of *dimensions* (note that in the context of tensors, a dimension is often called
    an *axis*).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，张量是数据的容器——通常是数值数据。因此，它是一个数字的容器。你可能已经熟悉矩阵，它们是秩为2的张量：张量是对矩阵到任意数量的*维度*的泛化（请注意，在张量的上下文中，维度通常被称为*轴*）。
- en: 2.2.1 Scalars (rank-0 tensors)
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 标量（秩为0的张量）
- en: 'A tensor that contains only one number is called a *scalar* (or scalar tensor,
    or rank-0 tensor, or 0D tensor). In NumPy, a `float32` or `float64` number is
    a scalar tensor (or scalar array). You can display the number of axes of a NumPy
    tensor via the `ndim` attribute; a scalar tensor has 0 axes (`ndim` `==` `0`).
    The number of axes of a tensor is also called its *rank*. Here’s a NumPy scalar:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 只包含一个数字的张量称为*标量*（或标量张量，或秩为0的张量，或0D张量）。在NumPy中，`float32`或`float64`数字是标量张量（或标量数组）。你可以通过`ndim`属性显示NumPy张量的轴数；标量张量有0个轴（`ndim`
    `==` `0`）。张量的轴数也称为其*秩*。这是一个NumPy标量：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 2.2.2 Vectors (rank-1 tensors)
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 向量（秩为1的张量）
- en: 'An array of numbers is called a *vector*, or rank-1 tensor, or 1D tensor. A
    rank-1 tensor is said to have exactly one axis. Following is a NumPy vector:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一组数字称为*向量*，或秩为1的张量，或1D张量。秩为1的张量被称为具有一个轴。以下是一个NumPy向量：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This vector has five entries and so is called a *5-dimensional vector*. Don’t
    confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five
    dimensions along its axis, whereas a 5D tensor has five axes (and may have any
    number of dimensions along each axis). *Dimensionality* can denote either the
    number of entries along a specific axis (as in the case of our 5D vector) or the
    number of axes in a tensor (such as a 5D tensor), which can be confusing at times.
    In the latter case, it’s technically more correct to talk about a *tensor of rank
    5* (the rank of a tensor being the number of axes), but the ambiguous notation
    *5D tensor* is common regardless.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个向量有五个条目，因此被称为*5维向量*。不要混淆5D向量和5D张量！一个5D向量只有一个轴，并且沿着轴有五个维度，而一个5D张量有五个轴（并且可以有任意数量的维度沿着每个轴）。*维度*可以表示沿着特定轴的条目数（如我们的5D向量的情况），或者张量中轴的数量（比如5D张量），这有时可能会令人困惑。在后一种情况下，从技术上讲，谈论*秩为5的张量*更正确（张量的秩是轴的数量），但是不明确的符号*5D张量*是常见的。
- en: 2.2.3 Matrices (rank-2 tensors)
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 矩阵（秩为2的张量）
- en: 'An array of vectors is a *matrix*, or rank-2 tensor, or 2D tensor. A matrix
    has two axes (often referred to as *rows* and *columns*). You can visually interpret
    a matrix as a rectangular grid of numbers. This is a NumPy matrix:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 一组向量是一个*矩阵*，或秩为2的张量，或2D张量。矩阵有两个轴（通常称为*行*和*列*）。你可以将矩阵视为一个数字矩形网格。这是一个NumPy矩阵：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The entries from the first axis are called the *rows*, and the entries from
    the second axis are called the *columns*. In the previous example, `[5,` `78,`
    `2,` `34,` `0]` is the first row of `x`, and `[5,` `6,` `7]` is the first column.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个轴的条目称为*行*，第二个轴的条目称为*列*。在前面的示例中，`[5,` `78,` `2,` `34,` `0]`是`x`的第一行，`[5,`
    `6,` `7]`是第一列。
- en: 2.2.4 Rank-3 and higher-rank tensors
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 秩为3及更高秩的张量
- en: 'If you pack such matrices in a new array, you obtain a rank-3 tensor (or 3D
    tensor), which you can visually interpret as a cube of numbers. Following is a
    NumPy rank-3 tensor:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将这些矩阵打包到一个新数组中，你将得到一个秩为3的张量（或3D张量），你可以将其视为一个数字立方体。以下是一个NumPy秩为3的张量：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By packing rank-3 tensors in an array, you can create a rank-4 tensor, and so
    on. In deep learning, you’ll generally manipulate tensors with ranks 0 to 4, although
    you may go up to 5 if you process video data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数组中打包秩为3的张量，你可以创建一个秩为4的张量，依此类推。在深度学习中，你通常会处理秩为0到4的张量，尽管如果处理视频数据可能会升到5。
- en: 2.2.5 Key attributes
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 关键属性
- en: 'A tensor is defined by three key attributes:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个张量由三个关键属性定义：
- en: '*Number of axes (rank)*—For instance, a rank-3 tensor has three axes, and a
    matrix has two axes. This is also called the tensor’s `ndim` in Python libraries
    such as NumPy or TensorFlow.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*轴的数量（秩）*—例如，一个秩为3的张量有三个轴，一个矩阵有两个轴。这在Python库（如NumPy或TensorFlow）中也被称为张量的`ndim`。'
- en: '*Shape*—This is a tuple of integers that describes how many dimensions the
    tensor has along each axis. For instance, the previous matrix example has shape
    `(3,` `5)`, and the rank-3 tensor example has shape `(3,` `3,` `5)`. A vector
    has a shape with a single element, such as `(5,)`, whereas a scalar has an empty
    shape, `()`.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*形状*—这是一个描述张量沿着每个轴有多少维度的整数元组。例如，前面的矩阵示例的形状为`(3,` `5)`，而秩为3的张量示例的形状为`(3,` `3,`
    `5)`。一个向量的形状有一个单一元素，如`(5,)`，而一个标量的形状为空，`()`。'
- en: '*Data type (usually called* `dtype` *in Python libraries)*—This is the type
    of the data contained in the tensor; for instance, a tensor’s type could be `float16`,
    `float32`, `float64`, `uint8`, and so on. In TensorFlow, you are also likely to
    come across `string` tensors.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据类型（通常在Python库中称为* `dtype` *）*—这是张量中包含的数据类型；例如，张量的类型可以是`float16`、`float32`、`float64`、`uint8`等。在TensorFlow中，您也可能会遇到`string`张量。'
- en: 'To make this more concrete, let’s look back at the data we processed in the
    MNIST example. First, we load the MNIST dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更具体地说明这一点，让我们回顾一下在MNIST示例中处理的数据。首先，我们加载MNIST数据集：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we display the number of axes of the tensor `train_images`, the `ndim`
    attribute:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们显示张量`train_images`的轴数，`ndim`属性：
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here’s its shape:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的形状：
- en: '[PRE17]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And this is its data type, the `dtype` attribute:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的数据类型，`dtype`属性：
- en: '[PRE18]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So what we have here is a rank-3 tensor of 8-bit integers. More precisely, it’s
    an array of 60,000 matrices of 28 × 28 integers. Each such matrix is a grayscale
    image, with coefficients between 0 and 255.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们这里有一个8位整数的秩-3张量。更准确地说，它是一个由60,000个28×28整数矩阵组成的数组。每个这样的矩阵都是一个灰度图像，系数介于0和255之间。
- en: Let’s display the fourth digit in this rank-3 tensor, using the Matplotlib library
    (a well-known Python data visualization library, which comes preinstalled in Colab);
    see figure 2.2.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Matplotlib库（Colab中预装的著名Python数据可视化库）显示这个秩-3张量中的第四个数字；参见图2.2。
- en: '![](../Images/02-02.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-02.png)'
- en: Figure 2.2 The fourth sample in our dataset
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 数据集中的第四个样本
- en: Listing 2.8 Displaying the fourth digit
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 显示第四个数字
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Naturally, the corresponding label is the integer 9:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，相应的标签是整数9：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 2.2.6 Manipulating tensors in NumPy
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 在NumPy中操作张量
- en: In the previous example, we selected a specific digit alongside the first axis
    using the syntax `train_images[i]`. Selecting specific elements in a tensor is
    called *tensor slicing*. Let’s look at the tensor-slicing operations you can do
    on NumPy arrays.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的示例中，我们使用语法`train_images[i]`沿着第一个轴选择了一个特定的数字。在张量中选择特定元素称为*张量切片*。让我们看看您可以在NumPy数组上进行的张量切片操作。
- en: 'The following example selects digits #10 to #100 (#100 isn’t included) and
    puts them in an array of shape `(90,` `28,` `28)`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例选择了第10到第100个数字（不包括第100个）并将它们放入形状为`(90,` `28,` `28)`的数组中：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'It’s equivalent to this more detailed notation, which specifies a start index
    and stop index for the slice along each tensor axis. Note that `:` is equivalent
    to selecting the entire axis:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这等同于更详细的表示法，它为每个张量轴上的切片指定了起始索引和停止索引。请注意，`:`等同于选择整个轴：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Equivalent to the previous example
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 等同于前面的示例
- en: ❷ Also equivalent to the previous example
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 也等同于前面的示例
- en: 'In general, you may select slices between any two indices along each tensor
    axis. For instance, in order to select 14 × 14 pixels in the bottom-right corner
    of all images, you would do this:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您可以在每个张量轴上选择任意两个索引之间的切片。例如，为了选择所有图像右下角的14×14像素，您可以这样做：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It’s also possible to use negative indices. Much like negative indices in Python
    lists, they indicate a position relative to the end of the current axis. In order
    to crop the images to patches of 14 × 14 pixels centered in the middle, you’d
    do this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用负索引。与Python列表中的负索引类似，它们表示相对于当前轴末尾的位置。为了将图像裁剪为中心14×14像素的补丁，您可以这样做：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 2.2.7 The notion of data batches
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 数据批次的概念
- en: In general, the first axis (axis 0, because indexing starts at 0) in all data
    tensors you’ll come across in deep learning will be the *samples axis* (sometimes
    called the *samples dimension*). In the MNIST example, “samples” are images of
    digits.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在深度学习中您会遇到的所有数据张量中的第一个轴（轴0，因为索引从0开始）将是*样本轴*（有时称为*样本维度*）。在MNIST示例中，“样本”是数字的图像。
- en: 'In addition, deep learning models don’t process an entire dataset at once;
    rather, they break the data into small batches. Concretely, here’s one batch of
    our MNIST digits, with a batch size of 128:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，深度学习模型不会一次处理整个数据集；相反，它们将数据分成小批次。具体来说，这是我们MNIST数字的一个批次，批量大小为128：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And here’s the next batch:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是下一个批次：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'And the *n*th batch:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 和第*n*批次：
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: When considering such a batch tensor, the first axis (axis 0) is called the
    *batch axis* or *batch dimension*. This is a term you’ll frequently encounter
    when using Keras and other deep learning libraries.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这样一个批量张量时，第一个轴（轴0）被称为*批量轴*或*批量维度*。这是您在使用Keras和其他深度学习库时经常遇到的术语。
- en: 2.2.8 Real-world examples of data tensors
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.8 数据张量的现实世界示例
- en: 'Let’s make data tensors more concrete with a few examples similar to what you’ll
    encounter later. The data you’ll manipulate will almost always fall into one of
    the following categories:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个类似于您以后会遇到的示例来更具体地说明数据张量。您将处理的数据几乎总是属于以下类别之一：
- en: '*Vector data*—Rank-2 tensors of shape `(samples,` `features)`, where each sample
    is a vector of numerical attributes (“features”)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*向量数据*—形状为`(samples,` `features)`的秩-2张量，其中每个样本是一个数值属性（“特征”）向量'
- en: '*Timeseries data or sequence data*—Rank-3 tensors of shape `(samples,` `timesteps,`
    `features)`, where each sample is a sequence (of length `timesteps`) of feature
    vectors'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间序列数据或序列数据*—形状为`(samples,` `timesteps,` `features)`的秩-3张量，其中每个样本是一个长度为`timesteps`的特征向量序列'
- en: '*Images*—Rank-4 tensors of shape `(samples,` `height,` `width,` `channels)`,
    where each sample is a 2D grid of pixels, and each pixel is represented by a vector
    of values (“channels”)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图像*—形状为`(samples,` `height,` `width,` `channels)`的秩-4张量，其中每个样本是一个像素网格，每个像素由一组值（“通道”）表示'
- en: '*Video*—Rank-5 tensors of shape `(samples,` `frames,` `height,` `width,` `channels)`,
    where each sample is a sequence (of length `frames`) of images'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*视频*—形状为`(samples,` `frames,` `height,` `width,` `channels)`的秩-5张量，其中每个样本是一个图像序列（长度为`frames`）'
- en: 2.2.9 Vector data
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.9 向量数据
- en: This is one of the most common cases. In such a dataset, each single data point
    can be encoded as a vector, and thus a batch of data will be encoded as a rank-2
    tensor (that is, an array of vectors), where the first axis is the *samples axis*
    and the second axis is the *features axis*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常见的情况之一。在这样的数据集中，每个单个数据点可以被编码为一个向量，因此数据的批次将被编码为一个二阶张量（即向量数组），其中第一个轴是*样本轴*，第二个轴是*特征轴*。
- en: 'Let’s take a look at two examples:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看两个例子：
- en: An actuarial dataset of people, where we consider each person’s age, gender,
    and income. Each person can be characterized as a vector of 3 values, and thus
    an entire dataset of 100,000 people can be stored in a rank-2 tensor of shape
    `(100000,` `3)`.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个人们的精算数据集，我们考虑每个人的年龄、性别和收入。每个人可以被描述为一个包含3个值的向量，因此一个包含10万人的完整数据集可以存储在形状为`(100000,
    3)`的二阶张量中。
- en: A dataset of text documents, where we represent each document by the counts
    of how many times each word appears in it (out of a dictionary of 20,000 common
    words). Each document can be encoded as a vector of 20,000 values (one count per
    word in the dictionary), and thus an entire dataset of 500 documents can be stored
    in a tensor of shape `(500,` `20000)`.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个文本文档数据集，我们通过每个单词在文档中出现的次数（在一个包含2万个常见单词的字典中）来表示每个文档。每个文档可以被编码为一个包含2万个值的向量（字典中每个单词的计数），因此一个包含500个文档的完整数据集可以存储在形状为`(500,
    20000)`的张量中。
- en: 2.2.10 Timeseries data or sequence data
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.10 时间序列数据或序列数据
- en: Whenever time matters in your data (or the notion of sequence order), it makes
    sense to store it in a rank-3 tensor with an explicit time axis. Each sample can
    be encoded as a sequence of vectors (a rank-2 tensor), and thus a batch of data
    will be encoded as a rank-3 tensor (see figure 2.3).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每当数据中涉及时间（或序列顺序的概念）时，将其存储在具有显式时间轴的三阶张量中是有意义的。每个样本可以被编码为一系列向量（一个二阶张量），因此数据的批次将被编码为一个三阶张量（见图2.3）。
- en: '![](../Images/02-03.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-03.png)'
- en: Figure 2.3 A rank-3 timeseries data tensor
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 一个三阶时间序列数据张量
- en: 'The time axis is always the second axis (axis of index 1) by convention. Let’s
    look at a few examples:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，时间轴始终是第二轴（索引为1的轴）。让我们看几个例子：
- en: A dataset of stock prices. Every minute, we store the current price of the stock,
    the highest price in the past minute, and the lowest price in the past minute.
    Thus, every minute is encoded as a 3D vector, an entire day of trading is encoded
    as a matrix of shape `(390,` `3)` (there are 390 minutes in a trading day), and
    250 days’ worth of data can be stored in a rank-3 tensor of shape `(250,` `390,`
    `3)`. Here, each sample would be one day’s worth of data.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个股票价格数据集。每分钟，我们存储股票的当前价格、过去一分钟内的最高价格和最低价格。因此，每分钟被编码为一个三维向量，整个交易日被编码为形状为`(390,
    3)`的矩阵（一个交易日有390分钟），250天的数据可以存储在形状为`(250, 390, 3)`的三阶张量中。在这里，每个样本将是一天的数据。
- en: A dataset of tweets, where we encode each tweet as a sequence of 280 characters
    out of an alphabet of 128 unique characters. In this setting, each character can
    be encoded as a binary vector of size 128 (an all-zeros vector except for a 1
    entry at the index corresponding to the character). Then each tweet can be encoded
    as a rank-2 tensor of shape `(280,` `128)`, and a dataset of 1 million tweets
    can be stored in a tensor of shape `(1000000,` `280,` `128)`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个推文数据集，我们将每条推文编码为一个由128个唯一字符组成的字母表中的280个字符序列。在这种情况下，每个字符可以被编码为一个大小为128的二进制向量（除了在对应字符的索引处有一个1条目外，其他都是全零向量）。然后，每条推文可以被编码为形状为`(280,
    128)`的二阶张量，100万条推文的数据集可以存储在形状为`(1000000, 280, 128)`的张量中。
- en: 2.2.11 Image data
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.11 图像数据
- en: 'Images typically have three dimensions: height, width, and color depth. Although
    grayscale images (like our MNIST digits) have only a single color channel and
    could thus be stored in rank-2 tensors, by convention image tensors are always
    rank-3, with a one-dimensional color channel for grayscale images. A batch of
    128 grayscale images of size 256 × 256 could thus be stored in a tensor of shape
    `(128,` `256,` `256,` `1)`, and a batch of 128 color images could be stored in
    a tensor of shape `(128,` `256,` `256,` `3)` (see figure 2.4).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图像通常具有三个维度：高度、宽度和颜色深度。尽管灰度图像（如我们的MNIST数字）只有一个颜色通道，因此可以存储在二阶张量中，但按照惯例，图像张量始终是三阶的，对于灰度图像有一个一维颜色通道。因此，一个包含128个尺寸为256×256的灰度图像的批次可以存储在形状为`(128,
    256, 256, 1)`的张量中，而一个包含128个彩色图像的批次可以存储在形状为`(128, 256, 256, 3)`的张量中（见图2.4）。
- en: '![](../Images/02-04.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-04.png)'
- en: Figure 2.4 A rank-4 image data tensor
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 一个四阶图像数据张量
- en: 'There are two conventions for shapes of image tensors: the *channels-last*
    convention (which is standard in TensorFlow) and the *channels-first* convention
    (which is increasingly falling out of favor).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图像张量的形状有两种约定：*通道最后*约定（在TensorFlow中是标准的）和*通道优先*约定（越来越不受青睐）。
- en: 'The channels-last convention places the color-depth axis at the end: `(samples,`
    `height,` `width,` `color_depth)`. Meanwhile, the channels-first convention places
    the color depth axis right after the batch axis: `(samples,` `color_depth,` `height,`
    `width)`. With the channels-first convention, the previous examples would become
    `(128,` `1,` `256,` `256)` and `(128,` `3,` `256,` `256)`. The Keras API provides
    support for both formats.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通道最后的约定将颜色深度轴放在最后：`(样本数, 高度, 宽度, 颜色深度)`。与此同时，通道优先的约定将颜色深度轴放在批次轴之后：`(样本数, 颜色深度,
    高度, 宽度)`。使用通道优先的约定，前面的例子将变为`(128, 1, 256, 256)`和`(128, 3, 256, 256)`。Keras API支持这两种格式。
- en: 2.2.12 Video data
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.12 视频数据
- en: Video data is one of the few types of real-world data for which you’ll need
    rank-5 tensors. A video can be understood as a sequence of frames, each frame
    being a color image. Because each frame can be stored in a rank-3 tensor `(height,`
    `width,` `color_ depth)`, a sequence of frames can be stored in a rank-4 tensor
    `(frames,` `height,` `width,` `color_depth)`, and thus a batch of different videos
    can be stored in a rank-5 tensor of shape `(samples,` `frames,` `height,` `width,`
    `color_depth)`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 视频数据是少数几种需要使用五阶张量的真实世界数据之一。视频可以被理解为一系列帧，每一帧都是一幅彩色图像。因为每一帧可以存储在一个三阶张量中`(height,`
    `width,` `color_ depth)`，一系列帧可以存储在一个四阶张量中`(frames,` `height,` `width,` `color_depth)`，因此一批不同视频可以存储在一个形状为`(samples,`
    `frames,` `height,` `width,` `color_depth)`的五阶张量中。
- en: For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames
    per second would have 240 frames. A batch of four such video clips would be stored
    in a tensor of shape `(4,` `240,` `144,` `256,` `3)`. That’s a total of 106,168,320
    values! If the `dtype` of the tensor was `float32`, each value would be stored
    in 32 bits, so the tensor would represent 405 MB. Heavy! Videos you encounter
    in real life are much lighter, because they aren’t stored in `float32`, and they’re
    typically compressed by a large factor (such as in the MPEG format).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个 60 秒、144 × 256 的 YouTube 视频剪辑，每秒采样 4 帧，将有 240 帧。四个这样的视频剪辑批次将存储在一个形状为`(4,`
    `240,` `144,` `256,` `3)`的张量中。总共有 106,168,320 个值！如果张量的`dtype`是`float32`，每个值将以
    32 位存储，因此张量将表示 405 MB。非常庞大！在现实生活中遇到的视频要轻得多，因为它们不是以`float32`存储的，通常会被大幅压缩（例如 MPEG
    格式）。
- en: '2.3 The gears of neural networks: Tensor operations'
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 神经网络的齿轮：张量操作
- en: Much as any computer program can be ultimately reduced to a small set of binary
    operations on binary inputs (AND, OR, NOR, and so on), all transformations learned
    by deep neural networks can be reduced to a handful of *tensor operations* (or
    *tensor functions*) applied to tensors of numeric data. For instance, it’s possible
    to add tensors, multiply tensors, and so on.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何计算机程序最终都可以简化为对二进制输入进行的一小组二进制操作（AND、OR、NOR等）一样，深度神经网络学习到的所有变换都可以简化为应用于数值数据张量的一小组*张量操作*（或*张量函数*）。例如，可以对张量进行加法、乘法等操作。
- en: 'In our initial example, we built our model by stacking `Dense` layers on top
    of each other. A Keras layer instance looks like this:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初始示例中，我们通过将`Dense`层堆叠在一起来构建我们的模型。一个 Keras 层实例看起来像这样：
- en: '[PRE28]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This layer can be interpreted as a function, which takes as input a matrix
    and returns another matrix—a new representation for the input tensor. Specifically,
    the function is as follows (where `W` is a matrix and `b` is a vector, both attributes
    of the layer):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层可以被解释为一个函数，它以一个矩阵作为输入并返回另一个矩阵——输入张量的新表示。具体来说，函数如下（其中`W`是矩阵，`b`是向量，都是该层的属性）：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s unpack this. We have three tensor operations here:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细解释一下。这里有三个张量操作：
- en: A dot product (`dot`) between the input tensor and a tensor named `W`
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入张量和名为`W`的张量之间的点积（`dot`）
- en: An addition (`+`) between the resulting matrix and a vector `b`
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果矩阵和向量`b`之间的加法（`+`）
- en: 'A `relu` operation: `relu(x)` is `max(x,` `0)`; “relu” stands for “rectified
    linear unit”'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 `relu` 操作：`relu(x)` 是 `max(x,` `0)`；“relu”代表“修正线性单元”
- en: Note Although this section deals entirely with linear algebra expressions, you
    won’t find any mathematical notation here. I’ve found that mathematical concepts
    can be more readily mastered by programmers with no mathematical background if
    they’re expressed as short Python snippets instead of mathematical equations.
    So we’ll use NumPy and TensorFlow code throughout.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 尽管本节完全涉及线性代数表达式，但这里不会找到任何数学符号。我发现，如果将数学概念表达为简短的 Python 代码片段而不是数学方程式，那么没有数学背景的程序员更容易掌握。因此，我们将在整个过程中使用
    NumPy 和 TensorFlow 代码。
- en: 2.3.1 Element-wise operations
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 逐元素操作
- en: 'The `relu` operation and addition are element-wise operations: operations that
    are applied independently to each entry in the tensors being considered. This
    means these operations are highly amenable to massively parallel implementations
    (*vectorized* implementations, a term that comes from the *vector processor* supercomputer
    architecture from the 1970–90 period). If you want to write a naive Python implementation
    of an element-wise operation, you use a `for` loop, as in this naive implementation
    of an element-wise `relu` operation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`relu` 操作和加法都是逐元素操作：这些操作独立应用于所考虑张量中的每个条目。这意味着这些操作非常适合于高度并行的实现（*矢量化*实现，这个术语来自于20世纪70-90年代的*矢量处理器*超级计算机架构）。如果你想编写一个逐元素操作的朴素
    Python 实现，你会使用一个`for`循环，就像这个逐元素`relu`操作的朴素实现中所示：'
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ x is a rank-2 NumPy tensor.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x 是一个二阶 NumPy 张量。
- en: ❷ Avoid overwriting the input tensor.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 避免覆盖输入张量。
- en: 'You could do the same for addition:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以对加法做同样的操作：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ x and y are rank-2 NumPy tensors.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x 和 y 是二阶 NumPy 张量。
- en: ❷ Avoid overwriting the input tensor.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 避免覆盖输入张量。
- en: On the same principle, you can do element-wise multiplication, subtraction,
    and so on.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在同样的原则下，你可以进行逐元素乘法、减法等操作。
- en: In practice, when dealing with NumPy arrays, these operations are available
    as well-optimized built-in NumPy functions, which themselves delegate the heavy
    lifting to a Basic Linear Algebra Subprograms (BLAS) implementation. BLAS are
    low-level, highly parallel, efficient tensor-manipulation routines that are typically
    implemented in Fortran or C.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，处理 NumPy 数组时，这些操作也作为优化良好的内置 NumPy 函数可用，它们本身将繁重的工作委托给基本线性代数子程序（BLAS）实现。BLAS
    是低级别、高度并行、高效的张量操作例程，通常用 Fortran 或 C 实现。
- en: 'So, in NumPy, you can do the following element-wise operation, and it will
    be blazing fast:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在 NumPy 中，你可以进行以下逐元素操作，速度将非常快：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Element-wise addition
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 逐元素加法
- en: ❷ Element-wise relu
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 逐元素 relu
- en: 'Let’s actually time the difference:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际计算一下时间差异：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This takes 0.02 s. Meanwhile, the naive version takes a stunning 2.45 s:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要 0.02 秒。与此同时，朴素版本需要惊人的 2.45 秒：
- en: '[PRE34]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Likewise, when running TensorFlow code on a GPU, element-wise operations are
    executed via fully vectorized CUDA implementations that can best utilize the highly
    parallel GPU chip architecture.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，在GPU上运行TensorFlow代码时，通过完全向量化的CUDA实现执行元素级操作，可以最好地利用高度并行的GPU芯片架构。
- en: 2.3.2 Broadcasting
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 广播
- en: Our earlier naive implementation of `naive_add` only supports the addition of
    rank-2 tensors with identical shapes. But in the `Dense` layer introduced earlier,
    we added a rank-2 tensor with a vector. What happens with addition when the shapes
    of the two tensors being added differ?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前天真的实现`naive_add`仅支持具有相同形状的秩为2的张量的加法。但在之前介绍的`Dense`层中，我们添加了一个秩为2的张量和一个向量。当被加的两个张量的形状不同时，加法会发生什么？
- en: 'When possible, and if there’s no ambiguity, the smaller tensor will be *broadcast*
    to match the shape of the larger tensor. Broadcasting consists of two steps:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，如果没有歧义，较小的张量将被*广播*以匹配较大张量的形状。广播包括两个步骤：
- en: Axes (called *broadcast axes*) are added to the smaller tensor to match the
    `ndim` of the larger tensor.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 轴（称为*广播轴*）被添加到较小的张量中，以匹配较大张量的`ndim`。
- en: The smaller tensor is repeated alongside these new axes to match the full shape
    of the larger tensor.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较小的张量沿着这些新轴重复，以匹配较大张量的完整形状。
- en: 'Let’s look at a concrete example. Consider `X` with shape `(32,` `10)` and
    `y` with shape `(10,)`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个具体的例子。考虑形状为`(32,` `10)`的`X`和形状为`(10,)`的`y`：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ X is a random matrix with shape (32, 10).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ X是一个形状为(32, 10)的随机矩阵。
- en: ❷ y is a random vector with shape (10,).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y是一个NumPy向量。
- en: 'First, we add an empty first axis to `y`, whose shape becomes `(1,` `10)`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们向`y`添加一个空的第一个轴，其形状变为`(1,` `10)`：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ The shape of y is now (1, 10).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ y的形状现在是(1, 10)。
- en: 'Then, we repeat `y` 32 times alongside this new axis, so that we end up with
    a tensor `Y` with shape `(32,` `10)`, where `Y[i,` `:]` `==` `y` for `i` in `range(0,`
    `32)`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们沿着这个新轴重复`y` 32次，这样我们就得到了一个形状为`(32,` `10)`的张量`Y`，其中`Y[i,` `:]` `==` `y`，对于`i`
    在 `range(0,` `32)`：
- en: '[PRE37]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Repeat y 32 times along axis 0 to obtain Y, which has shape (32, 10).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 沿着轴0重复y 32次，得到形状为(32, 10)的Y。
- en: At this point, we can proceed to add `X` and `Y`, because they have the same
    shape.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以继续添加`X`和`Y`，因为它们具有相同的形状。
- en: 'In terms of implementation, no new rank-2 tensor is created, because that would
    be terribly inefficient. The repetition operation is entirely virtual: it happens
    at the algorithmic level rather than at the memory level. But thinking of the
    vector being repeated 10 times alongside a new axis is a helpful mental model.
    Here’s what a naive implementation would look like:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，不会创建新的秩为2的张量，因为那样会非常低效。重复操作完全是虚拟的：它发生在算法级别而不是内存级别。但想象向量沿着新轴重复10次是一个有用的心理模型。以下是天真实现的样子：
- en: '[PRE38]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ x is a rank-2 NumPy tensor.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x是一个秩为2的NumPy张量。
- en: ❷ y is a NumPy vector.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y是一个NumPy向量。
- en: ❸ Avoid overwriting the input tensor.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 避免覆盖输入张量。
- en: With broadcasting, you can generally perform element-wise operations that take
    two inputs tensors if one tensor has shape `(a,` `b,` `...` `n,` `n` `+` `1,`
    `...` `m)` and the other has shape `(n,` `n` `+` `1,` `...` `m)`. The broadcasting
    will then automatically happen for axes `a` through `n` `-` `1`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用广播，如果一个张量的形状为`(a,` `b,` `...` `n,` `n` `+` `1,` `...` `m)`，另一个张量的形状为`(n,`
    `n` `+` `1,` `...` `m)`，通常可以执行元素级操作。广播将自动发生在轴`a`到`n` `-` `1`。
- en: 'The following example applies the element-wise `maximum` operation to two tensors
    of different shapes via broadcasting:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例通过广播将两个不同形状的张量应用于元素级`maximum`操作：
- en: '[PRE39]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ x is a random tensor with shape (64, 3, 32, 10).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x是一个形状为(64, 3, 32, 10)的随机张量。
- en: ❷ y is a random tensor with shape (32, 10).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y是一个形状为(32, 10)的随机张量。
- en: ❸ The output z has shape (64, 3, 32, 10) like x.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出z的形状与x相同，为(64, 3, 32, 10)。
- en: 2.3.3 Tensor product
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 张量积
- en: The *tensor product*, or *dot product* (not to be confused with an element-wise
    product, the `*` operator), is one of the most common, most useful tensor operations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*张量积*或*点积*（不要与逐元素乘积，即`*`运算符混淆）是最常见、最有用的张量操作之一。'
- en: 'In NumPy, a tensor product is done using the `np.dot` function (because the
    mathematical notation for tensor product is usually a dot):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在NumPy中，使用`np.dot`函数进行张量积（因为张量积的数学表示通常是一个点）：
- en: '[PRE40]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In mathematical notation, you’d note the operation with a dot (•):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学表示中，您会用一个点（•）表示该操作：
- en: '[PRE41]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Mathematically, what does the dot operation do? Let’s start with the dot product
    of two vectors, `x` and `y`. It’s computed as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，点操作是什么？让我们从两个向量`x`和`y`的点积开始。计算如下：
- en: '[PRE42]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ x and y are NumPy vectors.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x和y是NumPy向量。
- en: You’ll have noticed that the dot product between two vectors is a scalar and
    that only vectors with the same number of elements are compatible for a dot product.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到两个向量之间的点积是一个标量，只有元素数量相同的向量才适用于点积。
- en: 'You can also take the dot product between a matrix `x` and a vector `y`, which
    returns a vector where the coefficients are the dot products between `y` and the
    rows of `x`. You implement it as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以计算矩阵`x`和��量`y`之间的点积，返回一个向量，其中系数是`y`和`x`的行之间的点积。您可以按如下方式实现它：
- en: '[PRE43]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ x is a NumPy matrix.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x是一个NumPy矩阵。
- en: ❷ y is a NumPy vector.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ y是一个NumPy向量。
- en: ❸ The first dimension of x must be the same as the 0th dimension of y!
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ x的第一个维度必须与y的第0维度相同！
- en: ❹ This operation returns a vector of 0s with the same shape as y.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 此操作返回一个与y形状相同的0向量。
- en: 'You could also reuse the code we wrote previously, which highlights the relationship
    between a matrix-vector product and a vector product:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以重用我们之前编写的代码，这突显了矩阵-向量乘积与向量乘积之间的关系：
- en: '[PRE44]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note that as soon as one of the two tensors has an `ndim` greater than 1, `dot`
    is no longer *symmetric*, which is to say that `dot(x,` `y)` isn’t the same as
    `dot(y,` `x)`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，只要两个张量中的一个的`ndim`大于1，`dot`就不再是*对称*的，也就是说`dot(x,` `y)`不等同于`dot(y,` `x)`。
- en: 'Of course, a dot product generalizes to tensors with an arbitrary number of
    axes. The most common applications may be the dot product between two matrices.
    You can take the dot product of two matrices `x` and `y` (`dot(x,` `y)`) if and
    only if `x.shape[1]` `==` `y.shape[0]`. The result is a matrix with shape `(x.shape[0],`
    `y.shape[1])`, where the coefficients are the vector products between the rows
    of `x` and the columns of `y`. Here’s the naive implementation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，点积可以推广到具有任意数量轴的张量。最常见的应用可能是两个矩阵之间的点积。只有当 `x.shape[1]` `==` `y.shape[0]` 时，你才能计算两个矩阵
    `x` 和 `y` 的点积（`dot(x,` `y)`）。结果是一个形状为 `(x.shape[0],` `y.shape[1])` 的矩阵，其中系数是 `x`
    的行和 `y` 的列之间的向量积。这是一个简单的实现：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ x and y are NumPy matrices.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ x 和 y 是 NumPy 矩阵。
- en: ❷ The first dimension of x must be the same as the 0th dimension of y!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ x 的第一个维度必须与 y 的第 0 维度相同！
- en: ❸ This operation returns a matrix of 0s with a specific shape.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 此操作返回一个具有特定形状的零矩阵。
- en: ❹ Iterates over the rows of x . . .
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 迭代 x 的行 . . .
- en: ❺ . . . and over the columns of y.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ . . . 并在 y 的列上。
- en: To understand dot-product shape compatibility, it helps to visualize the input
    and output tensors by aligning them as shown in figure 2.5.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解点积形状兼容性，有助于通过将输入和输出张量对齐来可视化它们，如图 2.5 所示。
- en: '![](../Images/02-05.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-05.png)'
- en: Figure 2.5 Matrix dot-product box diagram
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 矩阵点积框图
- en: In the figure, `x`, `y`, and `z` are pictured as rectangles (literal boxes of
    coefficients). Because the rows of `x` and the columns of `y` must have the same
    size, it follows that the width of `x` must match the height of `y`. If you go
    on to develop new machine learning algorithms, you’ll likely be drawing such diagrams
    often.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，`x`、`y` 和 `z` 被描绘为矩形（系数的字面框）。因为 `x` 的行和 `y` 的列必须具有相同的大小，所以 `x` 的宽度必须与 `y`
    的高度匹配。如果你继续开发新的机器学习算法，你可能会经常画这样的图。
- en: 'More generally, you can take the dot product between higher-dimensional tensors,
    following the same rules for shape compatibility as outlined earlier for the 2D
    case:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地，你可以按照前面为 2D 情况概述的相同形状兼容性规则，计算更高维度张量之间的点积：
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: And so on.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。
- en: 2.3.4 Tensor reshaping
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.4 张量重塑
- en: 'A third type of tensor operation that’s essential to understand is *tensor
    reshaping*. Although it wasn’t used in the `Dense` layers in our first neural
    network example, we used it when we preprocessed the digits data before feeding
    it into our model:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 理解的第三种张量操作是*张量重塑*。虽然在我们第一个神经网络示例中的`Dense`层中没有使用它，但在将手写数字数据输入模型之前对数据进行预处理时使用了它：
- en: '[PRE47]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Reshaping a tensor means rearranging its rows and columns to match a target
    shape. Naturally, the reshaped tensor has the same total number of coefficients
    as the initial tensor. Reshaping is best understood via simple examples:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 重塑张量意味着重新排列其行和列以匹配目标形状。显然，重塑后的张量与初始张量具有相同数量的系数。通过简单的例子最容易理解重塑：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'A special case of reshaping that’s commonly encountered is *transposition*.
    *Transposing* a matrix means exchanging its rows and its columns, so that `x[i,`
    `:]` becomes `x[:,` `i]`:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的重塑的一个特殊情况是*转置*。*转置*矩阵意味着交换其行和列，使得 `x[i,` `:]` 变为 `x[:,` `i]`：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Creates an all-zeros matrix of shape (300, 20)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个形状为 (300, 20) 的全零矩阵
- en: 2.3.5 Geometric interpretation of tensor operations
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.5 张量操作的几何解释
- en: 'Because the contents of the tensors manipulated by tensor operations can be
    interpreted as coordinates of points in some geometric space, all tensor operations
    have a geometric interpretation. For instance, let’s consider addition. We’ll
    start with the following vector:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 因为张量操作中的张量内容可以被解释为某个几何空间中点的坐标，所以所有张量操作都有几何解释。例如，让我们从以下向量开始：
- en: '[PRE50]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector
    as an arrow linking the origin to the point, as shown in figure 2.7.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这是二维空间中的一个点（参见图 2.6）。通常将向量描绘为连接原点和点的箭头，如图 2.7 所示。
- en: '![](../Images/02-06.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-06.png)'
- en: Figure 2.6  A point in a 2D space
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6  二维空间中的一个点
- en: '![](../Images/02-07.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-07.png)'
- en: Figure 2.7 A point in a 2D space pictured as an arrow
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 作为箭头的二维空间中的点
- en: Let’s consider a new point, `B` `=` `[1,` `0.25]`, which we’ll add to the previous
    one. This is done geometrically by chaining together the vector arrows, with the
    resulting location being the vector representing the sum of the previous two vectors
    (see figure 2.8). As you can see, adding a vector B to a vector A represents the
    action of copying point A in a new location, whose distance and direction from
    the original point A is determined by the vector B. If you apply the same vector
    addition to a group of points in the plane (an “object”), you would be creating
    a copy of the entire object in a new location (see figure 2.9). Tensor addition
    thus represents the action of *translating an object* (moving the object without
    distorting it) by a certain amount in a certain direction.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个新点，`B` `=` `[1,` `0.25]`，我们将其添加到之前的点上。这是通过将向量箭头链接在一起几何地完成的，结果位置是代表前两个向量之和的向量（参见图
    2.8）。如你所见，将向量 B 添加到向量 A 表示将点 A 复制到一个新位置，其距离和方向从原始点 A 确定为向量 B。如果你将相同的向量加法应用于平面上的一组点（一个“对象”），你将在一个新位置创建整个对象的副本（参见图
    2.9）。因此，张量加法表示*平移对象*（在不扭曲对象的情况下移动对象）到某个方向的某个距离。
- en: '![](../Images/02-08.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-08.png)'
- en: Figure 2.8 Geometric interpretation of the sum of two vectors
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 两个向量之和的几何解释
- en: 'In general, elementary geometric operations such as translation, rotation,
    scaling, skewing, and so on can be expressed as tensor operations. Here are a
    few examples:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，诸如平移、旋转、缩放、倾斜等基本几何操作可以表示为张量操作。以下是一些例子���
- en: '*Translation*: As you just saw, adding a vector to a point will move the point
    by a fixed amount in a fixed direction. Applied to a set of points (such as a
    2D object), this is called a “translation” (see figure 2.9).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平移*：正如你刚刚看到的，向点添加一个向量将使点沿着固定方向移动固定量。应用于一组点（如2D对象），这称为“平移”（见图2.9）。'
- en: '![](../Images/02-09.png)'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02-09.png)'
- en: Figure 2.9 2D translation as a vector addition
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.9 2D平移作为向量相加
- en: '*Rotation*: A counterclockwise rotation of a 2D vector by an angle theta (see
    figure 2.10) can be achieved via a dot product with a 2 × 2 matrix `R` `=` `[[cos(theta),`
    `-sin(theta)],` `[sin(theta),` `cos(theta)]]`.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*旋转*：通过角度θ逆时针旋转2D向量（见图2.10）可以通过与2 × 2矩阵`R` `=` `[[cos(theta),` `-sin(theta)],`
    `[sin(theta),` `cos(theta)]]`进行点积实现。'
- en: '![](../Images/02-10.png)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02-10.png)'
- en: Figure 2.10 2D rotation (counterclockwise) as a dot product
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.10 2D旋转（逆时针）作为点积
- en: '*Scaling*: A vertical and horizontal scaling of the image (see figure 2.11)
    can be achieved via a dot product with a 2 × 2 matrix `S` `=` `[[horizontal_factor,`
    `0],` `[0,` `vertical_factor]]` (note that such a matrix is called a “diagonal
    matrix,” because it only has non-zero coefficients in its “diagonal,” going from
    the top left to the bottom right).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩放*：图像的垂直和水平缩放（见图2.11）可以通过与2 × 2矩阵`S` `=` `[[horizontal_factor,` `0],` `[0,`
    `vertical_factor]]`进行点积实现（请注意，这样的矩阵称为“对角矩阵”，因为它只在从左上到右下的“对角线”上有非零系数）。'
- en: '![](../Images/02-11.png)'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02-11.png)'
- en: Figure 2.11 2D scaling as a dot product
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.11 2D缩放作为点积
- en: '*Linear transform*: A dot product with an arbitrary matrix implements a linear
    transform. Note that *scaling* and *rotation*, listed previously, are by definition
    linear transforms.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*线性变换*：与任意矩阵进行点积实现了线性变换。请注意，前面列出的*缩放*和*旋转*按定义都是线性变换。'
- en: '*Affine transform*: An affine transform (see figure 2.12) is the combination
    of a linear transform (achieved via a dot product with some matrix) and a translation
    (achieved via a vector addition). As you have probably recognized, that’s exactly
    the `y` `=` `W` `•` `x` `+` `b` computation implemented by the `Dense` layer!
    A `Dense` layer without an activation function is an affine layer.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*仿射变换*：仿射变换（见图2.12）是线性变换（通过与某些矩阵进行点积实现）和平移（通过向量相加实现）的组合。你可能已经意识到，这正是`Dense`层实现的`y`
    `=` `W` `•` `x` `+` `b`计算！没有激活函数的`Dense`层就是一个仿射层。'
- en: '![](../Images/02-12.png)'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02-12.png)'
- en: Figure 2.12 Affine transform in the plane
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.12 平面中的仿射变换
- en: '*Dense layer with* `relu` *activation*: An important observation about affine
    transforms is that if you apply many of them repeatedly, you still end up with
    an affine transform (so you could just have applied that one affine transform
    in the first place). Let’s try it with two: `affine2(affine1(x))` `=` `W2` `•`
    `(W1` `•` `x` `+` `b1)` `+` `b2` `=` `(W2` `•` `W1)` `•` `x` `+` `(W2` `•` `b1`
    `+` `b2)`. That’s an affine transform where the linear part is the matrix `W2`
    `•` `W1` and the translation part is the vector `W2` `•` `b1` `+` `b2`. As a consequence,
    a multilayer neural network made entirely of `Dense` layers without activations
    would be equivalent to a single `Dense` layer. This “deep” neural network would
    just be a linear model in disguise! This is why we need activation functions,
    like `relu` (seen in action in figure 2.13). Thanks to activation functions, a
    chain of `Dense` layers can be made to implement very complex, non-linear geometric
    transformations, resulting in very rich hypothesis spaces for your deep neural
    networks. We’ll cover this idea in more detail in the next chapter.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*带有* `relu` *激活的Dense层*：关于仿射变换的一个重要观察是，如果你重复应用许多次，最终仍然得到一个仿射变换（因此你可以一开始就应用那一个仿射变换）。让我们尝试两次：`affine2(affine1(x))`
    `=` `W2` `•` `(W1` `•` `x` `+` `b1)` `+` `b2` `=` `(W2` `•` `W1)` `•` `x` `+`
    `(W2` `•` `b1` `+` `b2)`。这是一个仿射变换，其中线性部分是矩阵`W2` `•` `W1`，平移部分是向量`W2` `•` `b1`
    `+` `b2`。因此，一个完全由`Dense`层组成且没有激活函数的多层神经网络等效于单个`Dense`层。这种“深度”神经网络实际上只��一个伪装的线性模型！这就是为什么我们需要激活函数，比如`relu`（在图2.13中展示）。由于激活函数，一系列`Dense`层可以实现非常复杂、非线性的几何变换，为你的深度神经网络提供非常丰富的假设空间。我们将在下一章更详细地讨论这个想法。'
- en: '![](../Images/02-13.png)'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](../Images/02-13.png)'
- en: Figure 2.13 Affine transform followed by `relu` activation
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.13 仿射变换后跟`relu`激活
- en: 2.3.6 A geometric interpretation of deep learning
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.6 深度学习的几何解释
- en: You just learned that neural networks consist entirely of chains of tensor operations,
    and that these tensor operations are just simple geometric transformations of
    the input data. It follows that you can interpret a neural network as a very complex
    geometric transformation in a high-dimensional space, implemented via a series
    of simple steps.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚学到神经网络完全由张量操作链组成，而这些张量操作只是输入数据的简单几何变换。由此可见，你可以将神经网络解释为在高维空间中非常复杂的几何变换，通过一系列简单步骤实现。
- en: 'In 3D, the following mental image may prove useful. Imagine two sheets of colored
    paper: one red and one blue. Put one on top of the other. Now crumple them together
    into a small ball. That crumpled paper ball is your input data, and each sheet
    of paper is a class of data in a classification problem. What a neural network
    is meant to do is figure out a transformation of the paper ball that would uncrumple
    it, so as to make the two classes cleanly separable again (see figure 2.14). With
    deep learning, this would be implemented as a series of simple transformations
    of the 3D space, such as those you could apply on the paper ball with your fingers,
    one movement at a time.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D中，以下心理形象可能会有所帮助。想象两张彩纸：一张红色，一张蓝色。将它们叠在一起。现在将它们一起揉成一个小球。那个揉皱的纸球就是你的输入数据，每张纸是分类问题中的一个数据类别。神经网络的目的是找出一个可以展开纸球的变换，使得两个类别再次清晰可分（见图2.14）。通过深度学习，这将被实现为对3D空间的一系列简单变换，就像你可以用手指在纸球上一次移动一次一样。
- en: '![](../Images/02-14.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-14.png)'
- en: Figure 2.14 Uncrumpling a complicated manifold of data
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 展开复杂数据流形
- en: 'Uncrumpling paper balls is what machine learning is about: finding neat representations
    for complex, highly folded data *manifolds* in high-dimensional spaces (a manifold
    is a continuous surface, like our crumpled sheet of paper). At this point, you
    should have a pretty good intuition as to why deep learning excels at this: it
    takes the approach of incrementally decomposing a complicated geometric transformation
    into a long chain of elementary ones, which is pretty much the strategy a human
    would follow to uncrumple a paper ball. Each layer in a deep network applies a
    transformation that disentangles the data a little, and a deep stack of layers
    makes tractable an extremely complicated disentanglement process.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 展开纸团就是机器学习的目的：在高维空间中找到复杂、高度折叠数据*流形*的整洁表示（流形是一个连续的表面，就像我们折叠的纸张）。此时，你应该对为什么深度学习擅长这一点有很好的直觉：它采用逐步将复杂的几何变换分解为一长串基本变换的方法，这几乎就是人类展开纸团时会遵循的策略。深度网络中的每一层应用一个能稍微解开数据的变换，而深层堆叠的层使得一个极其复杂的解开过程变得可行。
- en: '2.4 The engine of neural networks: Gradient-based optimization'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 神经网络的引擎：基于梯度的优化
- en: 'As you saw in the previous section, each neural layer from our first model
    example transforms its input data as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在前一节中看到的，我们第一个模型示例中的每个神经层将其输入数据转换如下：
- en: '[PRE51]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In this expression, `W` and `b` are tensors that are attributes of the layer.
    They’re called the *weights* or *trainable parameters* of the layer (the `kernel`
    and `bias` attributes, respectively). These weights contain the information learned
    by the model from exposure to training data.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中，`W`和`b`是层的属性的张量。它们被称为层的*权重*或*可训练参数*（分别是`kernel`和`bias`属性）。这些权重包含了模型从训练数据中学到的信息。
- en: Initially, these weight matrices are filled with small random values (a step
    called *random initialization*). Of course, there’s no reason to expect that `relu(dot(input,`
    `W)` `+` `b)`, when `W` and `b` are random, will yield any useful representations.
    The resulting representations are meaningless—but they’re a starting point. What
    comes next is to gradually adjust these weights, based on a feedback signal. This
    gradual adjustment, also called *training*, is the learning that machine learning
    is all about.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，这些权重矩阵被填充了小的随机值（这一步被称为*随机初始化*）。当`W`和`b`是随机的时候，当然没有理由期望`relu(dot(input,` `W)`
    `+` `b)`会产生任何有用的表示。得到的表示是毫无意义的，但它们是一个起点。接下来要做的是逐渐调整这些权重，基于一个反馈信号。这种逐渐调整，也称为*训练*，就是机器学习的学习过程。
- en: 'This happens within what’s called a *training loop*, which works as follows.
    Repeat these steps in a loop, until the loss seems sufficiently low:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这发生在所谓的*训练循环*中，其工作方式如下。重复这些步骤直到损失看起来足够低：
- en: Draw a batch of training samples, `x`, and corresponding targets, `y_true`.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一批训练样本`x`和相应的目标`y_true`。
- en: Run the model on `x` (a step called the *forward pass*) to obtain predictions,
    `y_pred`.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`x`上运行模型（称为*前向传播*）以获得预测值`y_pred`。
- en: Compute the loss of the model on the batch, a measure of the mismatch between
    `y_pred` and `y_true`.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在批次上的损失，这是`y_pred`和`y_true`之间的不匹配度的度量。
- en: Update all weights of the model in a way that slightly reduces the loss on this
    batch.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型的所有权重，以稍微减少这一批次上的损失。
- en: 'You’ll eventually end up with a model that has a very low loss on its training
    data: a low mismatch between predictions, `y_pred`, and expected targets, `y_true`.
    The model has “learned” to map its inputs to correct targets. From afar, it may
    look like magic, but when you reduce it to elementary steps, it turns out to be
    simple.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你会得到一个在训练数据上损失非常低的模型：预测值`y_pred`与期望目标`y_true`之间的匹配度很低。模型已经“学会”将其输入映射到正确的目标。从远处看，这可能看起来像魔术，但当你将其简化为基本步骤时，它其实很简单。
- en: 'Step 1 sounds easy enough—just I/O code. Steps 2 and 3 are merely the application
    of a handful of tensor operations, so you could implement these steps purely from
    what you learned in the previous section. The difficult part is step 4: updating
    the model’s weights. Given an individual weight coefficient in the model, how
    can you compute whether the coefficient should be increased or decreased, and
    by how much?'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步听起来足够简单——只是I/O代码。第二步和第三步仅仅是应用少量张量操作，所以你可以纯粹根据你在前一节中学到的内容来实现这些步骤。困难的部分在于第四步：更新模型的权重。给定模型中的一个单独权重系数，你如何计算这个系数应该增加还是减少，以及增加多少？
- en: One naive solution would be to freeze all weights in the model except the one
    scalar coefficient being considered, and try different values for this coefficient.
    Let’s say the initial value of the coefficient is 0.3\. After the forward pass
    on a batch of data, the loss of the model on the batch is 0.5\. If you change
    the coefficient’s value to 0.35 and rerun the forward pass, the loss increases
    to 0.6\. But if you lower the coefficient to 0.25, the loss falls to 0.4\. In
    this case, it seems that updating the coefficient by –0.05 would contribute to
    minimizing the loss. This would have to be repeated for all coefficients in the
    model.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 一个天真的解决方案是冻结模型中除了正在考虑的一个标量系数之外的所有权重，并尝试不同的值来��整这个系数。假设系数的初始值是0.3。在一批数据上进行前向传播后，模型在该批次上的损失为0.5。如果你将系数的值更改为0.35并重新运行前向传播，损失增加到0.6。但如果你将系数降低到0.25，损失降至0.4。在这种情况下，似乎通过减小系数-0.05来有助于最小化损失。这将需要对模型中的所有系数重复进行。
- en: 'But such an approach would be horribly inefficient, because you’d need to compute
    two forward passes (which are expensive) for every individual coefficient (of
    which there are many, usually thousands and sometimes up to millions). Thankfully,
    there’s a much better approach: *gradient descent*.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 但这样的方法将非常低效，因为你需要为每个单独的系数（通常有成千上万甚至数百万个）计算两次前向传播（这是昂贵的）。幸运的是，有一个更好的方法：*梯度下降*。
- en: 'Gradient descent is the optimization technique that powers modern neural networks.
    Here’s the gist of it. All of the functions used in our models (such as `dot`
    or `+`) transform their input in a smooth and continuous way: if you look at `z`
    `=` `x` `+` `y`, for instance, a small change in `y` only results in a small change
    in `z`, and if you know the direction of the change in `y`, you can infer the
    direction of the change in `z`. Mathematically, you’d say these functions are
    *differentiable*. If you chain together such functions, the bigger function you
    obtain is still differentiable. In particular, this applies to the function that
    maps the model’s coefficients to the loss of the model on a batch of data: a small
    change in the model’s coefficients results in a small, predictable change in the
    loss value. This enables you to use a mathematical operator called the *gradient*
    to describe how the loss varies as you move the model’s coefficients in different
    directions. If you compute this gradient, you can use it to move the coefficients
    (all at once in a single update, rather than one at a time) in a direction that
    decreases the loss.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是现代神经网络的优化技术。这是其要点。我们模型中使用的所有函数（如 `dot` 或 `+`）以平滑连续的方式转换其输入：例如，如果你看 `z`
    `=` `x` `+` `y`，那么 `y` 的微小变化只会导致 `z` 的微小变化，如果你知道 `y` 变化的方向，你就可以推断出 `z` 变化的方向。从数学上讲，你会说这些函数是*可导*的。如果你将这些函数链接在一起，你得到的更大函数仍然是可导的。特别是，这适用于将模型系数映射到批量数据上的模型损失的函数：模型系数的微小变化导致损失值的微小、可预测的变化。这使你能够使用一种称为*梯度*的数学运算符描述损失随着你将模型系数朝不同方向移动而变化的方式。如果你计算这个梯度，你可以使用它来移动系数（一次性全部更新，而不是逐个更新），朝着减小损失的方向移动系数。
- en: If you already know what *differentiable* means and what a *gradient* is, you
    can skip to section 2.4.3\. Otherwise, the following two sections will help you
    understand these concepts.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经知道*可导*的含义和*梯度*是什么，你可以跳到第 2.4.3 节。否则，接下来的两节将帮助你理解这些概念。
- en: 2.4.1 What’s a derivative?
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 什么是导数？
- en: Consider a continuous, smooth function `f(x)` `=` `y`, mapping a number, `x`,
    to a new number, `y`. We can use the function in figure 2.15 as an example.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个连续、平滑的函数 `f(x)` `=` `y`，将一个数字 `x` 映射到一个新的数字 `y`。我们可以以图 2.15 中的函数作为例子。
- en: '![](../Images/02-15.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-15.png)'
- en: Figure 2.15 A continuous, smooth function
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.15 一个连续、平滑的函数
- en: 'Because the function is *continuous*, a small change in `x` can only result
    in a small change in `y`—that’s the intuition behind *continuity*. Let’s say you
    increase `x` by a small factor, `epsilon_x`: this results in a small `epsilon_y`
    change to `y`, as shown in figure 2.16.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 因为函数是*连续*的，`x` 的微小变化只会导致 `y` 的微小变化——这就是*连续性*背后的直觉。假设你将 `x` 增加一个小因子 `epsilon_x`：这会导致
    `y` 有一个小的 `epsilon_y` 变化，如图 2.16 所示。
- en: '![](../Images/02-16.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-16.png)'
- en: Figure 2.16 With a continuous function, a small change in `x` results in a small
    change in `y`.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.16 对于连续函数，`x` 的微小变化导致 `y` 的微小变化。
- en: 'In addition, because the function is *smooth* (its curve doesn’t have any abrupt
    angles), when `epsilon_x` is small enough, around a certain point `p`, it’s possible
    to approximate `f` as a linear function of slope `a`, so that `epsilon_y` becomes
    `a` `*` `epsilon_x`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，因为函数是*平滑*的（其曲线没有任何突然的角度），当 `epsilon_x` 足够小，围绕某一点 `p`，可以将 `f` 近似为斜率 `a` 的线性函数，使得
    `epsilon_y` 变为 `a` `*` `epsilon_x`：
- en: '[PRE52]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Obviously, this linear approximation is valid only when `x` is close enough
    to `p`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种线性近似仅在 `x` 足够接近 `p` 时才有效。
- en: The slope `a` is called the *derivative* of `f` in `p`. If `a` is negative,
    it means a small increase in `x` around `p` will result in a decrease of `f(x)`
    (as shown in figure 2.17), and if `a` is positive, a small increase in `x` will
    result in an increase of `f(x)`. Further, the absolute value of `a` (the *magnitude*
    of the derivative) tells you how quickly this increase or decrease will happen.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率 `a` 在 `p` 处被称为 `f` 的*导数*。如果 `a` 是负的，这意味着在 `p` 附近将 `x` 稍微增加会导致 `f(x)` 减少（如图
    2.17 所示），如果 `a` 是正的，将 `x` 稍微增加会导致 `f(x)` 增加。此外，`a` 的绝对值（导数的*大小*）告诉你这种增加或减少会有多快发生。
- en: '![](../Images/02-17.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-17.png)'
- en: Figure 2.17 Derivative of `f` in `p`
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.17 在 `p` 处的 `f` 的导数
- en: 'For every differentiable function `f(x)` (*differentiable* means “can be derived”:
    for example, smooth, continuous functions can be derived), there exists a derivative
    function `f''(x)`, that maps values of `x` to the slope of the local linear approximation
    of `f` in those points. For instance, the derivative of `cos(x)` is `-sin(x)`,
    the derivative of `f(x)` `=` `a` `*` `x` is `f''(x)` `=` `a`, and so on.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个可导函数 `f(x)`（*可导*意味着“可以求导”：例如，平滑、连续函数可以求导），都存在一个导数函数 `f'(x)`，将 `x` 的值映射到这些点上
    `f` 的局部线性近似的斜率。例如，`cos(x)` 的导数是 `-sin(x)`，`f(x)` `=` `a` `*` `x` 的导数是 `f'(x)`
    `=` `a`，等等。
- en: 'Being able to derive functions is a very powerful tool when it comes to *optimization*,
    the task of finding values of `x` that minimize the value of `f(x)`. If you’re
    trying to update `x` by a factor `epsilon_x` in order to minimize `f(x)`, and
    you know the derivative of `f`, then your job is done: the derivative completely
    describes how `f(x)` evolves as you change `x`. If you want to reduce the value
    of `f(x)`, you just need to move `x` a little in the opposite direction from the
    derivative.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 能够求导函数是在*优化*方面非常强大的工具，即找到使 `f(x)` 最小化的 `x` 的值的任务。如果你试图通过一个因子 `epsilon_x` 更新
    `x` 以最小化 `f(x)`，并且你知道 `f` 的导数，那么你的任务就完成了：导数完全描述了当你改变 `x` 时 `f(x)` 的演变方式。如果你想减小
    `f(x)` 的值，你只需要将 `x` 沿着导数的相反方向移动一点。
- en: '2.4.2 Derivative of a tensor operation: The gradient'
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 张量操作的导数：梯度
- en: 'The function we were just looking at turned a scalar value `x` into another
    scalar value `y`: you could plot it as a curve in a 2D plane. Now imagine a function
    that turns a tuple of scalars `(x,` `y)` into a scalar value `z`: that would be
    a vector operation. You could plot it as a 2D *surface* in a 3D space (indexed
    by coordinates `x,` `y,` `z`). Likewise, you can imagine functions that take matrices
    as inputs, functions that take rank-3 tensors as inputs, etc.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看的函数将标量值`x`转换为另一个标量值`y`：你可以将其绘制为二维平面上的曲线。现在想象一个将标量元组`(x,` `y)`转换为标量值`z`的函数：那将是一个矢量操作。你可以将其绘制为三维空间中的二维*表面*（由坐标`x,`
    `y,` `z`索引）。同样，你可以想象将矩阵作为输入的函数，将秩-3张量作为输入的函数等。
- en: The concept of derivation can be applied to any such function, as long as the
    surfaces they describe are continuous and smooth. The derivative of a tensor operation
    (or tensor function) is called a *gradient*. Gradients are just the generalization
    of the concept of derivatives to functions that take tensors as inputs. Remember
    how, for a scalar function, the derivative represents the *local slope* of the
    curve of the function? In the same way, the gradient of a tensor function represents
    the *curvature* of the multidimensional surface described by the function. It
    characterizes how the output of the function varies when its input parameters
    vary.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 导数的概念可以应用于任何这样的函数，只要它们描述的表面是连续且光滑的。张量操作（或张量函数）的导数称为*梯度*。梯度只是将导数的概念推广到以张量作为输入的函数。还记得对于标量函数，导数代表函数曲线的*局部斜率*吗？同样，张量函数的梯度代表函数描述的多维表面的*曲率*。它描述了当输入参数变化时函数输出如何变化。
- en: Let’s look at an example grounded in machine learning. Consider
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个基于机器学习的例子。
- en: An input vector, `x` (a sample in a dataset)
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输入向量`x`（数据集中的样本）
- en: A matrix, `W` (the weights of a model)
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个矩阵`W`（模型的权重）
- en: A target, `y_true` (what the model should learn to associate to `x`)
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个目标`y_true`��模型应该学会将其与`x`关联起来的内容）
- en: A loss function, `loss` (meant to measure the gap between the model’s current
    predictions and `y_true`)
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个损失函数`loss`（旨在衡量模型当前预测与`y_true`之间的差距）
- en: 'You can use `W` to compute a target candidate `y_pred`, and then compute the
    loss, or mismatch, between the target candidate `y_pred` and the target `y_true`:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`W`计算目标候选`y_pred`，然后计算目标候选`y_pred`与目标`y_true`之间的损失或不匹配：
- en: '[PRE53]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ We use the model weights, W, to make a prediction for x.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用模型权重`W`来对`x`进行预测。
- en: ❷ We estimate how far off the prediction was.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们估计预测有多大偏差。
- en: Now we’d like to use gradients to figure out how to update `W` so as to make
    `loss_value` smaller. How do we do that?
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要使用梯度来找出如何更新`W`以使`loss_value`变小。我们该如何做？
- en: 'Given fixed inputs `x` and `y_true`, the preceding operations can be interpreted
    as a function mapping values of `W` (the model’s weights) to loss values:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 给定固定的输入`x`和`y_true`，前述操作可以解释为将`W`（模型的权重）的值映射到损失值的函数：
- en: '[PRE54]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: ❶ f describes the curve (or high-dimensional surface) formed by loss values
    when W varies.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ f描述了当W变化时损失值形成的曲线（或高维表面）。
- en: Let’s say the current value of `W` is `W0`. Then the derivative of `f` at the
    point `W0` is a tensor `grad(loss_value,` `W0)`, with the same shape as `W`, where
    each coefficient `grad(loss_value,` `W0)[i,` `j]` indicates the direction and
    magnitude of the change in `loss_value` you observe when modifying `W0[i,` `j]`.
    That tensor `grad(loss_value,` `W0)` is the gradient of the function `f(W)` `=`
    `loss_value` in `W0`, also called “gradient of `loss_value` with respect to `W`
    around `W0`.”
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 假设当前`W`的值为`W0`。那么在点`W0`处的`f`的导数是一个张量`grad(loss_value,` `W0)`，与`W`具有相同的形状，其中每个系数`grad(loss_value,`
    `W0)[i,` `j]`指示修改`W0[i,` `j]`时观察到的`loss_value`变化的方向和大小。该张量`grad(loss_value,` `W0)`是函数`f(W)`
    `=` `loss_value`在`W0`处的梯度，也称为“关于`W`在`W0`周围的`loss_value`的梯度”。
- en: Partial derivatives
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数
- en: The tensor operation `grad(f(W),` `W)` (which takes as input a matrix `W`) can
    be expressed as a combination of scalar functions, `grad_ij(f(W),` `w_ij)`, each
    of which would return the derivative of `loss_value` `=` `f(W)` with respect to
    the coefficient `W[i,` `j]` of `W`, assuming all other coefficients are constant.
    `grad_ij` is called the *partial derivative* of `f` with respect to `W[i,` `j]`.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 张量操作`grad(f(W),` `W)`（以矩阵`W`为输入）可以表示为标量函数的组合，`grad_ij(f(W),` `w_ij)`，每个函数将返回`loss_value`
    `=` `f(W)`相对于`W[i,` `j]`系数的导数，假设所有其他系数都是常数。`grad_ij`称为相对于`W[i,` `j]`的*f的偏导数*。
- en: Concretely, what does `grad(loss_value,` `W0)` represent? You saw earlier that
    the derivative of a function `f(x)` of a single coefficient can be interpreted
    as the slope of the curve of `f`. Likewise, `grad(loss_value,` `W0)` can be interpreted
    as the tensor describing the *direction of steepest ascent* of `loss_value` `=`
    `f(W)` around `W0`, as well as the slope of this ascent. Each partial derivative
    describes the slope of `f` in a specific direction.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，`grad(loss_value,` `W0)`代表什么？你之前看到函数`f(x)`的导数可以解释为`f`的曲线的斜率。同样，`grad(loss_value,`
    `W0)`可以解释为描述`loss_value` `=` `f(W)`在`W0`周围的*最陡上升方向*的张量，以及这种上升的斜率。每个偏导数描述了特定方向上`f`的斜率。
- en: 'For this reason, in much the same way that, for a function `f(x)`, you can
    reduce the value of `f(x)` by moving `x` a little in the opposite direction from
    the derivative, with a function `f(W)` of a tensor, you can reduce `loss_value`
    `=` `f(W)` by moving `W` in the opposite direction from the gradient: for example,
    `W1` `=` `W0` `-` `step` `*` `grad(f(W0),` `W0)` (where `step` is a small scaling
    factor). That means going against the direction of steepest ascent of `f`, which
    intuitively should put you lower on the curve. Note that the scaling factor `step`
    is needed because `grad(loss_value,` `W0)` only approximates the curvature when
    you’re close to `W0`, so you don’t want to get too far from `W0`.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 出于同样的原因，就像对于函数`f(x)`，您可以通过将`x`稍微朝着导数的相反方向移动来减小`f(x)`的值一样，对于张量的函数`f(W)`，您可以通过将`W`朝着梯度的相反方向移动来减小`loss_value`
    `=` `f(W)`：例如，`W1` `=` `W0` `-` `step` `*` `grad(f(W0),` `W0)`（其中`step`是一个小的缩放因子）。这意味着沿着`f`的最陡上升方向的相反方向，直观上应该使您在曲线上更低。请注意，缩放因子`step`是必需的，因为当您接近`W0`时���`grad(loss_value,`
    `W0)`仅近似曲率，因此您不希望离`W0`太远。
- en: 2.4.3 Stochastic gradient descent
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 随机梯度下降
- en: 'Given a differentiable function, it’s theoretically possible to find its minimum
    analytically: it’s known that a function’s minimum is a point where the derivative
    is 0, so all you have to do is find all the points where the derivative goes to
    0 and check for which of these points the function has the lowest value.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于可微函数，从理论上讲，可以通过分析找到其最小值：已知函数的最小值是导数为0的点，因此您只需找到所有导数为0的点，并检查这些点中哪个点的函数值最低。
- en: Applied to a neural network, that means finding analytically the combination
    of weight values that yields the smallest possible loss function. This can be
    done by solving the equation `grad(f(W),` `W)` `=` `0` for `W`. This is a polynomial
    equation of `N` variables, where `N` is the number of coefficients in the model.
    Although it would be possible to solve such an equation for `N` `=` `2` or `N`
    `=` `3`, doing so is intractable for real neural networks, where the number of
    parameters is never less than a few thousand and can often be several tens of
    millions.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 应用于神经网络，意味着找到分析上产生最小可能损失函数的权重值的组合。这可以通过解方程`grad(f(W),` `W)` `=` `0`来实现`W`。这是一个`N`个变量的多项式方程，其中`N`是模型中的系数数量。虽然对于`N`
    `=` `2`或`N` `=` `3`可以解决这样的方程，但对于真实的神经网络来说，这是不可行的，因为参数数量从不少于几千个，通常可以达到几千万个。
- en: 'Instead, you can use the four-step algorithm outlined at the beginning of this
    section: modify the parameters little by little based on the current loss value
    for a random batch of data. Because you’re dealing with a differentiable function,
    you can compute its gradient, which gives you an efficient way to implement step
    4\. If you update the weights in the opposite direction from the gradient, the
    loss will be a little less every time:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以使用本节开头概述的四步算法：根据随机数据批次的当前损失值逐渐修改参数。因为您正在处理可微函数，所以可以计算其梯度，这为您实现第4步提供了一种高效的方法。如果您根据梯度的相反方向更新权重，那么每次损失都会减少一点：
- en: Draw a batch of training samples, `x`, and corresponding targets, `y_true`.
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一批训练样本`x`和相应的目标`y_true`。
- en: Run the model on `x` to obtain predictions, `y_pred` (this is called the *forward
    pass*).
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`x`上运行模型以获得预测值`y_pred`（这称为*前向传递*）。
- en: Compute the loss of the model on the batch, a measure of the mismatch between
    `y_pred` and `y_true`.
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在批次上的损失，即`y_pred`和`y_true`之间的不匹配度的度量。
- en: Compute the gradient of the loss with regard to the model’s parameters (this
    is called the *backward pass*).
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于模型参数的梯度（这称为*反向传递*）。
- en: Move the parameters a little in the opposite direction from the gradient—for
    example, `W` `-=` `learning_rate` `*` `gradient`—thus reducing the loss on the
    batch a bit. The *learning rate* (`learning_rate` here) would be a scalar factor
    modulating the “speed” of the gradient descent process.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数稍微朝着梯度的相反方向移动，例如`W` `-=` `learning_rate` `*` `gradient`，从而在批次上减少一点损失。*学习率*（这里是`learning_rate`）将是一个标量因子，调节梯度下降过程的“速度”。
- en: Easy enough! What we just described is called *mini-batch stochastic gradient
    descent* (mini-batch SGD). The term *stochastic* refers to the fact that each
    batch of data is drawn at random (*stochastic* is a scientific synonym of *random*).
    Figure 2.18 illustrates what happens in 1D, when the model has only one parameter
    and you have only one training sample.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单！我们刚刚描述的是*小批量随机梯度下降*（mini-batch SGD）。术语*随机*指的是每个数据批次都是随机抽取的（*随机*是*随机*的科学同义词）。图2.18说明了在1D中发生的情况，当模型只有一个参数且您只有一个训练样本时。
- en: '![](../Images/02-18.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-18.png)'
- en: Figure 2.18 SGD down a 1D loss curve (one learnable parameter)
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 SGD沿着1D损失曲线下降（一个可学习参数）
- en: As you can see, intuitively it’s important to pick a reasonable value for the
    `learning_ rate` factor. If it’s too small, the descent down the curve will take
    many iterations, and it could get stuck in a local minimum. If `learning_rate`
    is too large, your updates may end up taking you to completely random locations
    on the curve.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，直观上选择合理的`learning_rate`因子值很重要。如果太小，曲线下降将需要许多迭代，并且可能会陷入局部最小值。如果`learning_rate`太大，您的更新可能会使您完全随机地移动到曲线上的位置。
- en: Note that a variant of the mini-batch SGD algorithm would be to draw a single
    sample and target at each iteration, rather than drawing a batch of data. This
    would be *true* SGD (as opposed to *mini-batch* SGD). Alternatively, going to
    the opposite extreme, you could run every step on *all* data available, which
    is called *batch gradient descent*. Each update would then be more accurate, but
    far more expensive. The efficient compromise between these two extremes is to
    use mini-batches of reasonable size.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，小批量SGD算法的一个变体是在每次迭代中绘制单个样本和目标，而不是绘制一批数据。这将是*真正的*SGD（而不是*小批量*SGD）。或者，走向相反的极端，您可以在*所有*可用数据上运行每一步，这被称为*批量梯度下降*。然后，每次更新将更准确，但成本更高。在这两个极端之间的有效折衷方案是使用合理大小的小批量。
- en: 'Although figure 2.18 illustrates gradient descent in a 1D parameter space,
    in practice you’ll use gradient descent in highly dimensional spaces: every weight
    coefficient in a neural network is a free dimension in the space, and there may
    be tens of thousands or even millions of them. To help you build intuition about
    loss surfaces, you can also visualize gradient descent along a 2D loss surface,
    as shown in figure 2.19\. But you can’t possibly visualize what the actual process
    of training a neural network looks like—you can’t represent a 1,000,000-dimensional
    space in a way that makes sense to humans. As such, it’s good to keep in mind
    that the intuitions you develop through these low-dimensional representations
    may not always be accurate in practice. This has historically been a source of
    issues in the world of deep learning research.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图2.18展示了在1D参数空间中的梯度下降，但在实践中，您将在高维空间中使用梯度下降：神经网络中的每个权重系数都是空间中的一个自由维度，可能有成千上万甚至数百万个。为了帮助您建立对损失曲面的直觉，您还可以将梯度下降可视化为2D损失曲面上的过程，如图2.19所示。但您不可能可视化训练神经网络的实际过程——您无法以人类能理解的方式表示一个1000000维空间。因此，要记住通过这些低维表示形成的直觉在实践中可能并不总是准确的。这在深度学习研究领域历史上一直是一个问题。
- en: '![](../Images/02-19.png)'
  id: totrans-343
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-19.png)'
- en: Figure 2.19 Gradient descent down a 2D loss surface (two learnable parameters)
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 梯度下降在2D损失曲面上（两个可学习参数）
- en: 'Additionally, there exist multiple variants of SGD that differ by taking into
    account previous weight updates when computing the next weight update, rather
    than just looking at the current value of the gradients. There is, for instance,
    SGD with momentum, as well as Adagrad, RMSprop, and several others. Such variants
    are known as *optimization methods* or *optimizers*. In particular, the concept
    of *momentum*, which is used in many of these variants, deserves your attention.
    Momentum addresses two issues with SGD: convergence speed and local minima. Consider
    figure 2.20, which shows the curve of a loss as a function of a model parameter.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，还有多种SGD的变体，它们在计算下一个权重更新时考虑了先前的权重更新，而不仅仅是查看梯度的当前值。例如，有带有动量的SGD，以及Adagrad、RMSprop等几种。这些变体被称为*优化方法*或*优化器*。特别是，许多这些变体中使用的*动量*概念值得关注。动量解决了SGD的两个问题：收敛速度和局部最小值。考虑图2.20，显示了损失作为模型参数函数的曲线。
- en: '![](../Images/02-20.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-20.png)'
- en: Figure 2.20 A local minimum and a global minimum
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 一个局部最小值和一个全局最小值
- en: 'As you can see, around a certain parameter value, there is a *local minimum*:
    around that point, moving left would result in the loss increasing, but so would
    moving right. If the parameter under consideration were being optimized via SGD
    with a small learning rate, the optimization process could get stuck at the local
    minimum instead of making its way to the global minimum.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在某个参数值附近，存在一个*局部最小值*：在该点附近，向左移动会导致损失增加，但向右移动也是如此。如果正在通过具有较小学习率的SGD优化考虑的参数，则优化过程可能会卡在局部最小值处，而不是朝着全局最小值前进。
- en: 'You can avoid such issues by using momentum, which draws inspiration from physics.
    A useful mental image here is to think of the optimization process as a small
    ball rolling down the loss curve. If it has enough momentum, the ball won’t get
    stuck in a ravine and will end up at the global minimum. Momentum is implemented
    by moving the ball at each step based not only on the current slope value (current
    acceleration) but also on the current velocity (resulting from past acceleration).
    In practice, this means updating the parameter `w` based not only on the current
    gradient value but also on the previous parameter update, such as in this naive
    implementation:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用动量来避免这些问题，动量从物理学中汲取灵感。在这里一个有用的心理形象是将优化过程视为一个小球沿着损失曲线滚动。如果它有足够的动量，小球就不会卡在峡谷中，最终会到达全局最小值。动量的实现是基于每一步移动小球的不仅仅是当前斜率值（当前加速度），还有当前速度（由过去加速度产生）。在实践中，这意味着根据不仅仅是当前梯度值，还有先前参数更新来更新参数`w`，就像在这个简单实现中一样：
- en: '[PRE55]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: ❶ Constant momentum factor
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 恒定的动量因子
- en: ❷ Optimization loop
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 优化循环
- en: '2.4.4 Chaining derivatives: The Backpropagation algorithm'
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 链式求导：反向传播算法
- en: In the preceding algorithm, we casually assumed that because a function is differentiable,
    we can easily compute its gradient. But is that true? How can we compute the gradient
    of complex expressions in practice? In the two-layer model we started the chapter
    with, how can we get the gradient of the loss with regard to the weights? That’s
    where the *Backpropagation algorithm* comes in.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的算法中，我们随意假设因为一个函数是可微的，我们可以轻松计算它的梯度。但这是真的吗？在实践中如何计算复杂表达式的梯度？在我们本章开始的两层模型中，如何计算损失相对于权重的梯度？这就是*反向传播算法*的作用。
- en: The chain rule
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则
- en: 'Backpropagation is a way to use the derivatives of simple operations (such
    as addition, relu, or tensor product) to easily compute the gradient of arbitrarily
    complex combinations of these atomic operations. Crucially, a neural network consists
    of many tensor operations chained together, each of which has a simple, known
    derivative. For instance, the model defined in listing 2.2 can be expressed as
    a function parameterized by the variables `W1`, `b1`, `W2`, and `b2` (belonging
    to the first and second `Dense` layers respectively), involving the atomic operations
    `dot`, `relu`, `softmax`, and `+`, as well as our loss function `loss`, which
    are all easily differentiable:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是一种利用简单操作的导数（如加法、relu或张量乘积）来轻松计算这些原子操作任意复杂组合的梯度的方法。关键是，神经网络由许多张量操作链在一起组成，每个操作都有简单的已知导数。例如，列表2.2中定义的模型可以表示为由变量`W1`、`b1`、`W2`和`b2`（分别属于第一和第二个`Dense`层）参数化的函数，涉及原子操作`dot`、`relu`、`softmax`和`+`，以及我们的损失函数`loss`，这些都很容易可微：
- en: '[PRE56]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Calculus tells us that such a chain of functions can be derived using the following
    identity, called the *chain rule*.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 微积分告诉我们，这样的函数链可以使用以下恒等式导出，称为*链式法则*。
- en: 'Consider two functions `f` and `g`, as well as the composed function `fg` such
    that `fg(x)` `==` `f(g(x))`:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个函数`f`和`g`，以及组合函数`fg`，使得`fg(x)` `==` `f(g(x))`：
- en: '[PRE57]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Then the chain rule states that `grad(y,` `x)` `==` `grad(y,` `x1)` `*` `grad(x1,`
    `x)`. This enables you to compute the derivative of `fg` as long as you know the
    derivatives of `f` and `g`. The chain rule is named as it is because when you
    add more intermediate functions, it starts looking like a chain:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 然后链式法则表明`grad(y,` `x)` `==` `grad(y,` `x1)` `*` `grad(x1,` `x)`。只要您知道`f`和`g`的导数，就可以计算`fg`的导数。链式法则之所以被命名为链式法则，是因为当您添加更多中间函数时，它开始看起来像一个链条：
- en: '[PRE58]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Applying the chain rule to the computation of the gradient values of a neural
    network gives rise to an algorithm called *backpropagation*. Let’s see how that
    works, concretely.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 将链式法则应用于神经网络梯度值的计算会产生一种称为*反向传播*的算法。让我们看看具体是如何工作的。
- en: Automatic differentiation with computation graphs
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 使用计算图进行自动微分
- en: A useful way to think about backpropagation is in terms of *computation graphs*.
    A computation graph is the data structure at the heart of TensorFlow and the deep
    learning revolution in general. It’s a directed acyclic graph of operations—in
    our case, tensor operations. For instance, figure 2.21 shows the graph representation
    of our first model.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 以*计算图*的方式思考反向传播是一种有用的方式。计算图是TensorFlow和深度学习革命的核心数据结构。它是操作的有向无环图 - 在我们的情况下，是张量操作。例如，图2.21显示了我们第一个模型的图表示。
- en: '![](../Images/02-21.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-21.png)'
- en: Figure 2.21 The computation graph representation of our two-layer model
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 我们两层模型的计算图表示
- en: 'Computation graphs have been an extremely successful abstraction in computer
    science because they enable us to *treat computation as data*: a computable expression
    is encoded as a machine-readable data structure that can be used as the input
    or output of another program. For instance, you could imagine a program that receives
    a computation graph and returns a new computation graph that implements a large-scale
    distributed version of the same computation—this would mean that you could distribute
    any computation without having to write the distribution logic yourself. Or imagine
    a program that receives a computation graph and can automatically generate the
    derivative of the expression it represents. It’s much easier to do these things
    if your computation is expressed as an explicit graph data structure rather than,
    say, lines of ASCII characters in a .py file.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 计算图在计算机科学中是一个非常成功的抽象，因为它使我们能够*将计算视为数据*：可计算表达式被编码为一种可用作另一个程序的输入或输出的机器可读数据结构。例如，您可以想象一个接收计算图并返回实现相同计算的大规模分布式版本的新计算图的程序
    - 这意味着您可以分发任何计算而无需自己编写分发逻辑。或者想象一个接收计算图并可以自动生成其表示的表达式的导数的程序。如果您的计算表达为显式图数据结构而不是.py文件中的ASCII字符行，这些事情要容易得多。
- en: 'To explain backpropagation clearly, let’s look at a really basic example of
    a computation graph (see figure 2.22). We’ll consider a simplified version of
    figure 2.21, where we only have one linear layer and where all variables are scalar.
    We’ll take two scalar variables `w` and `b`, a scalar input `x`, and apply some
    operations to them to combine them into an output `y`. Finally, we’ll apply an
    absolute value error-loss function: `loss_val` `=` `abs(y_true` `-` `y)`. Since
    we want to update `w` and `b` in a way that will minimize `loss_val`, we are interested
    in computing `grad(loss_val,` `b)` and `grad(loss _val,` `w)`.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清楚地解释反向传播，让我们看一个计算图的真正基本的例子（见图2.22）。我们将考虑图2.21的简化版本，其中只有一个线性层，所有变量都是标量。我们将取两个标量变量`w`和`b`，一个标量输入`x`，并对它们应用一些操作将它们组合成输出`y`。最后，我们将应用一个绝对值误差损失函数：`loss_val`
    `=` `abs(y_true` `-` `y)`。由于我们希望以最小化`loss_val`的方式更新`w`和`b`，我们有兴趣计算`grad(loss_val,`
    `b)`和`grad(loss _val,` `w)`。
- en: '![](../Images/02-22.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-22.png)'
- en: Figure 2.22 A basic example of a computation graph
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22 计算图的基本示例
- en: Let’s set concrete values for the “input nodes” in the graph, that is to say,
    the input `x`, the target `y_true`, `w`, and `b`. We’ll propagate these values
    to all nodes in the graph, from top to bottom, until we reach `loss_val`. This
    is the *forward pass* (see figure 2.23).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为图中的“输入节点”设置具体值，也就是说，输入`x`、目标`y_true`、`w`和`b`。我们将这些值从顶部传播到图中的所有节点，直到达到`loss_val`。���是*前向传递*（见图2.23）。
- en: '![](../Images/02-23.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-23.png)'
- en: Figure 2.23 Running a forward pass
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 运行前向传递
- en: 'Now let’s “reverse” the graph: for each edge in the graph going from `A` to
    `B`, we will create an opposite edge from `B` to `A`, and ask, how much does `B`
    vary when `A` varies? That is to say, what is `grad(B,` `A)`? We’ll annotate each
    inverted edge with this value. This backward graph represents the *backward pass*
    (see figure 2.24).'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们“反转”图表：对于图表中从`A`到`B`的每条边，我们将创建一个从`B`到`A`的相反边，并问，当`A`变化时`B`变化多少？也就是说，`grad(B,`
    `A)`是多少？我们将用这个值注释每个反转边。这个反向图代表了*反向传递*（见图2.24）。
- en: '![](../Images/02-24.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-24.png)'
- en: Figure 2.24 Running a backward pass
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.24 运行反向传播
- en: 'We have the following:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下内容：
- en: '`grad(loss_val,` `x2)` `=` `1`, because as `x2` varies by an amount epsilon,
    `loss_val` `=` `abs(4` `-` `x2)` varies by the same amount.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val,` `x2)` `=` `1`，因为当`x2`变化一个epsilon时，`loss_val` `=` `abs(4` `-`
    `x2)`也会变化相同的量。'
- en: '`grad(x2,` `x1)` `=` `1`, because as `x1` varies by an amount epsilon, `x2`
    `=` `x1` `+` `b` `=` `x1` `+` `1` varies by the same amount.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x2,` `x1)` `=` `1`，因为当`x1`变化一个epsilon时，`x2` `=` `x1` `+` `b` `=` `x1`
    `+` `1`也会变化相同的量。'
- en: '`grad(x2,` `b)` `=` `1`, because as `b` varies by an amount epsilon, `x2` `=`
    `x1` `+` `b` `=` `6` `+` `b` varies by the same amount.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x2,` `b)` `=` `1`，因为当`b`变化一个epsilon时，`x2` `=` `x1` `+` `b` `=` `6` `+`
    `b`也会变化相同的量。'
- en: '`grad(x1,` `w)` `=` `2`, because as `w` varies by an amount epsilon, `x1` `=`
    `x` `*` `w` `=` `2` `*` `w` varies by `2` `*` `epsilon`.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(x1,` `w)` `=` `2`，因为当`w`变化一个epsilon时，`x1` `=` `x` `*` `w` `=` `2` `*`
    `w`也会变化`2` `*` `epsilon`。'
- en: What the chain rule says about this backward graph is that you can obtain the
    derivative of a node with respect to another node by *multiplying the derivatives
    for each edge along the path linking the two nodes*. For instance, `grad(loss_val,`
    `w)` `=` `grad(loss_val,` `x2)` `*` `grad(x2,` `x1)` `*` `grad(x1,` `w)` (see
    figure 2.25).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则关于这个反向图的含义是，你可以通过*乘以连接两个节点路径上的每个边的导数*来获得一个节点相对于另一个节点的导数。例如，`grad(loss_val,`
    `w)` `=` `grad(loss_val,` `x2)` `*` `grad(x2,` `x1)` `*` `grad(x1,` `w)`（见图2.25）。
- en: '![](../Images/02-25.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-25.png)'
- en: Figure 2.25 Path from `loss_val` to `w` in the backward graph
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.25 从`loss_val`到`w`的反向图路径
- en: 'By applying the chain rule to our graph, we obtain what we were looking for:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将链式法则应用于我们的图表，我们得到了我们要找的内容：
- en: '`grad(loss_val,` `w)` `=` `1` `*` `1` `*` `2` `=` `2`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val,` `w)` `=` `1` `*` `1` `*` `2` `=` `2`'
- en: '`grad(loss_val,` `b)` `=` `1` `*` `1` `=` `1`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`grad(loss_val,` `b)` `=` `1` `*` `1` `=` `1`'
- en: Note If there are multiple paths linking the two nodes of interest, `a` and
    `b`, in the backward graph, we would obtain `grad(b,` `a)` by summing the contributions
    of all the paths.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果在反向图中存在多条连接两个感兴趣节点`a`和`b`的路径，我们可以通过对所有路径的贡献求和来得到`grad(b,` `a)`。
- en: 'And with that, you just saw backpropagation in action! Backpropagation is simply
    the application of the chain rule to a computation graph. There’s nothing more
    to it. Backpropagation starts with the final loss value and works backward from
    the top layers to the bottom layers, computing the contribution that each parameter
    had in the loss value. That’s where the name “backpropagation” comes from: we
    “back propagate” the loss contributions of different nodes in a computation graph.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，你刚刚看到了反向传播的过程！反向传播简单地是将链式法则应用于计算图。没有更多了。反向传播从最终损失值开始，从顶层向底层向后计算每个参数对损失值的贡献。这就是“反向传播”这个名字的由来：我们在计算图中“反向传播”不同节点的损失贡献。
- en: Nowadays people implement neural networks in modern frameworks that are capable
    of *automatic differentiation*, such as TensorFlow. Automatic differentiation
    is implemented with the kind of computation graph you’ve just seen. Automatic
    differentiation makes it possible to retrieve the gradients of arbitrary compositions
    of differentiable tensor operations without doing any extra work besides writing
    down the forward pass. When I wrote my first neural networks in C in the 2000s,
    I had to write my gradients by hand. Now, thanks to modern automatic differentiation
    tools, you’ll never have to implement backpropagation yourself. Consider yourself
    lucky!
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，人们在现代框架中实现神经网络，这些框架能够进行*自动微分*，例如TensorFlow。自动微分是使用你刚刚看到的计算图实现的。自动微分使得能够检索任意可微张量操作组合的梯度成为可能，而无需额外工作，只需编写前向传播。在2000年代我用C语言编写我的第一个神经网络时，我不得不手动编写梯度。现在，由于现代自动微分工具，你永远不必自己实现反向传播。算你运气好！
- en: The gradient tape in TensorFlow
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的梯度磁带
- en: The API through which you can leverage TensorFlow’s powerful automatic differentiation
    capabilities is the `GradientTape`. It’s a Python scope that will “record” the
    tensor operations that run inside it, in the form of a computation graph (sometimes
    called a “tape”). This graph can then be used to retrieve the gradient of any
    output with respect to any variable or set of variables (instances of the `tf.Variable`
    class). A `tf.Variable` is a specific kind of tensor meant to hold mutable state—for
    instance, the weights of a neural network are always `tf.Variable` instances.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以利用TensorFlow强大的自动微分功能的API是`GradientTape`。它是一个Python范围，将在其中运行的张量操作“记录”为计算图（有时称为“磁带”）。然后可以使用此图检索任何输出相对于任何变量或一组变量（`tf.Variable`类的实例）的梯度。`tf.Variable`是一种特定类型的张量，用于保存可变状态，例如神经网络的权重始终是`tf.Variable`实例。
- en: '[PRE59]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ Instantiate a scalar Variable with an initial value of 0.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个初始值为0的标量变量。
- en: ❷ Open a GradientTape scope.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打开一个GradientTape范围。
- en: ❸ Inside the scope, apply some tensor operations to our variable.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在范围内，对我们的变量应用一些张量操作。
- en: ❹ Use the tape to retrieve the gradient of the output y with respect to our
    variable x.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用磁带检索输出y相对于我们的变量x的梯度。
- en: 'The `GradientTape` works with tensor operations:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '`GradientTape`与张量操作一起工作：'
- en: '[PRE60]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: ❶ Instantiate a Variable with shape (2, 2) and an initial value of all zeros.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实例化一个形状为(2, 2)且初始值全为零的变量。
- en: ❷ grad_of_y_wrt_x is a tensor of shape (2, 2) (like x) describing the curvature
    of y = 2 * a + 3 around x = [[0, 0], [0, 0]].
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `grad_of_y_wrt_x`是一个形状为(2, 2)（像x一样）的张量，描述了y = 2 * a + 3在x = [[0, 0], [0, 0]]周围的曲率。
- en: 'It also works with lists of variables:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 它也适用于变量列表：
- en: '[PRE61]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: ❶ matmul is how you say “dot product” in TensorFlow.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ matmul 是在 TensorFlow 中表示“点积”的方式。
- en: ❷ grad_of_y_wrt_W_and_b is a list of two tensors with the same shapes as W and
    b, respectively.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ grad_of_y_wrt_W_and_b 是两个张量列表，形状与 W 和 b 相同。
- en: You will learn about the gradient tape in the next chapter.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在下一章学习关于梯度带的知识。
- en: 2.5 Looking back at our first example
  id: totrans-408
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 回顾我们的第一个例子
- en: 'You’re nearing the end of this chapter, and you should now have a general understanding
    of what’s going on behind the scenes in a neural network. What was a magical black
    box at the start of the chapter has turned into a clearer picture, as illustrated
    in figure 2.26: the model, composed of layers that are chained together, maps
    the input data to predictions. The loss function then compares these predictions
    to the targets, producing a loss value: a measure of how well the model’s predictions
    match what was expected. The optimizer uses this loss value to update the model’s
    weights.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经接近本章的结束，现在应该对神经网络背后的运作有一个大致的了解。在本章开始时是一个神奇的黑匣子，现在已经变成了一个更清晰的画面，如图2.26所示：模型由相互链接的层组成，将输入数据映射到预测结果。损失函数然后将这些预测与目标进行比较，产生一个损失值：衡量模型预测与预期值匹配程度的指标。优化器使用这个损失值来更新模型的权重。
- en: '![](../Images/02-26.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-26.png)'
- en: Figure 2.26 Relationship between the network, layers, loss function, and optimizer
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.26 网络、层、损失函数和优化器之间的关系
- en: Let’s go back to the first example in this chapter and review each piece of
    it in the light of what you’ve learned since.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章的第一个例子，并根据你学到的知识来逐一审查每个部分。
- en: 'This was the input data:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输入数据：
- en: '[PRE62]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Now you understand that the input images are stored in NumPy tensors, which
    are here formatted as `float32` tensors of shape `(60000,` `784)` (training data)
    and `(10000,` `784)` (test data) respectively.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了输入图像存储在NumPy张量中，这里格式化为`(60000,` `784)`（训练数据）和`(10000,` `784)`（测试数据）的`float32`张量。
- en: 'This was our model:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的模型：
- en: '[PRE63]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now you understand that this model consists of a chain of two `Dense` layers,
    that each layer applies a few simple tensor operations to the input data, and
    that these operations involve weight tensors. Weight tensors, which are attributes
    of the layers, are where the *knowledge* of the model persists.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了这个模型由两个`Dense`层的链条组成，每个层对输入数据应用了一些简单的张量操作，并且这些操作涉及权重张量。权重张量是属于层的属性，是模型的*知识*所在。
- en: 'This was the model-compilation step:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 这是模型编译步骤：
- en: '[PRE64]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Now you understand that `sparse_categorical_crossentropy` is the loss function
    that’s used as a feedback signal for learning the weight tensors, and which the
    training phase will attempt to minimize. You also know that this reduction of
    the loss happens via mini-batch stochastic gradient descent. The exact rules governing
    a specific use of gradient descent are defined by the `rmsprop` optimizer passed
    as the first argument.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了`sparse_categorical_crossentropy`是用作学习权重张量的反馈信号的损失函数，训练阶段将尝试最小化它。你还知道这种损失的减少是通过小批量随机梯度下降来实现的。具体规则由作为第一个参数传递的`rmsprop`优化器定义。
- en: 'Finally, this was the training loop:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是训练循环：
- en: '[PRE65]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now you understand what happens when you call `fit`: the model will start to
    iterate on the training data in mini-batches of 128 samples, 5 times over (each
    iteration over all the training data is called an *epoch*). For each batch, the
    model will compute the gradient of the loss with regard to the weights (using
    the Backpropagation algorithm, which derives from the chain rule in calculus)
    and move the weights in the direction that will reduce the value of the loss for
    this batch.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你明白了当你调用`fit`时会发生什么：模型将开始在128个样本的小批量数据上进行5次迭代（每次迭代所有训练数据都被称为*epoch*）。对于每个批次，模型将计算损失相对于权重的梯度（使用源自微积分链式法则的反向传播算法），并将权重朝着减少该批次损失值的方向移动。
- en: After these 5 epochs, the model will have performed 2,345 gradient updates (469
    per epoch), and the loss of the model will be sufficiently low that the model
    will be capable of classifying handwritten digits with high accuracy.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在这5个epoch之后，模型将执行2,345次梯度更新（每个epoch 469次），并且模型的损失将足够低，以至于模型能够以高准确度对手写数字进行分类。
- en: At this point, you already know most of what there is to know about neural networks.
    Let’s prove it by reimplementing a simplified version of that first example “from
    scratch” in TensorFlow, step by step.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你已经了解了大部分关于神经网络的��识。让我们通过逐步在 TensorFlow 中“从头开始”重新实现那个第一个例子来证明它。
- en: 2.5.1 Reimplementing our first example from scratch in TensorFlow
  id: totrans-427
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 在 TensorFlow 中从头开始重新实现我们的第一个例子
- en: 'What better demonstrates full, unambiguous understanding than implementing
    everything from scratch? Of course, what “from scratch” means here is relative:
    we won’t reimplement basic tensor operations, and we won’t implement backpropagation.
    But we’ll go to such a low level that we will barely use any Keras functionality
    at all.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 有什么比从头开始实现一切更能展示出完全、明确的理解呢？当然，“从头开始”在这里是相对的：我们不会重新实现基本的张量操作，也不会实现反向传播。但我们会降到一个低到几乎不使用任何Keras功能的水平。
- en: Don’t worry if you don’t understand every little detail in this example just
    yet. The next chapter will dive in more detail into the TensorFlow API. For now,
    just try to follow the gist of what’s going on—the intent of this example is to
    help crystalize your understanding of the mathematics of deep learning using a
    concrete implementation. Let’s go!
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在还不理解这个例子中的每一个细节，不要担心。下一章将更详细地深入探讨 TensorFlow API。现在，只需尝试理解正在发生的事情的要点——这个例子的目的是帮助你通过具体实现来澄清对深度学习数学的理解。让我们开始吧！
- en: A simple Dense class
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的 Dense 类
- en: 'You’ve learned earlier that the `Dense` layer implements the following input
    transformation, where `W` and `b` are model parameters, and `activation` is an
    element-wise function (usually `relu`, but it would be `softmax` for the last
    layer):'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 你之前学过`Dense`层实现以下输入转换，其中`W`和`b`是模型参数，`activation`是逐元素函数（通常是`relu`，但对于最后一层可能是`softmax`）：
- en: '[PRE66]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Let’s implement a simple Python class, `NaiveDense`, that creates two TensorFlow
    variables, `W` and `b`, and exposes a `__call__()` method that applies the preceding
    transformation.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现一个简单的Python类`NaiveDense`，它创建两个TensorFlow变量`W`和`b`，并公开一个`__call__()`方法，应用前述转换。
- en: '[PRE67]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ❶ Create a matrix, W, of shape (input_size, output_size), initialized with random
    values.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个形状为(input_size, output_size)的矩阵W，用随机值初始化。
- en: ❷ Create a vector, b, of shape (output_size,), initialized with zeros.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个形状为(output_size,)的向量b，用零初始化。
- en: ❸ Apply the forward pass.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 应用前向传播。
- en: ❹ Convenience method for retrieving the layer’s weights
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 用于检索层权重的便利方法
- en: A simple Sequential class
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的Sequential类
- en: Now, let’s create a `NaiveSequential` class to chain these layers. It wraps
    a list of layers and exposes a `__call__()` method that simply calls the underlying
    layers on the inputs, in order. It also features a `weights` property to easily
    keep track of the layers’ parameters.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个`NaiveSequential`类来链接这些层。它包装了一系列层，并公开一个`__call__()`方法，简单地按顺序在输入上调用底层层。它还具有一个`weights`属性，方便跟踪层的参数。
- en: '[PRE68]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Using this `NaiveDense` class and this `NaiveSequential` class, we can create
    a mock Keras model:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个`NaiveDense`类和这个`NaiveSequential`类，我们可以创建一个模拟的Keras模型：
- en: '[PRE69]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: A batch generator
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 一个批生成器
- en: 'Next, we need a way to iterate over the MNIST data in mini-batches. This is
    easy:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一种方法以小批量迭代MNIST数据。这很容易：
- en: '[PRE70]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 2.5.2 Running one training step
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 运行一个训练步骤
- en: 'The most difficult part of the process is the “training step”: updating the
    weights of the model after running it on one batch of data. We need to'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程中最困难的部分是“训练步骤”：在一个数据批次上运行模型后更新模型的权重。我们需要
- en: Compute the predictions of the model for the images in the batch.
  id: totrans-449
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型对批次中图像的预测。
- en: Compute the loss value for these predictions, given the actual labels.
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算这些预测的损失值，给定实际标签。
- en: Compute the gradient of the loss with regard to the model’s weights.
  id: totrans-451
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算损失相对于模型权重的梯度。
- en: Move the weights by a small amount in the direction opposite to the gradient.
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重沿着梯度相反的方向移动一小步。
- en: 'To compute the gradient, we will use the TensorFlow `GradientTape` object we
    introduced in section 2.4.4:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算梯度，我们将使用在第2.4.4节中介绍的TensorFlow `GradientTape`对象：
- en: '[PRE71]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: ❶ Run the “forward pass” (compute the model’s predictions under a GradientTape
    scope).
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 运行“前向传播”（在GradientTape范围内计算模型的预测）。
- en: ❷ Compute the gradient of the loss with regard to the weights. The output gradients
    is a list where each entry corresponds to a weight from the model.weights list.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算损失相对于权重的梯度。输出梯度是一个列表，其中每个条目对应于模型权重列表中的一个权重。
- en: ❸ Update the weights using the gradients (we will define this function shortly).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用梯度更新权重（我们将很快定义这个函数）。
- en: 'As you already know, the purpose of the “weight update” step (represented by
    the preceding `update_weights` function) is to move the weights by “a bit” in
    a direction that will reduce the loss on this batch. The magnitude of the move
    is determined by the “learning rate,” typically a small quantity. The simplest
    way to implement this `update_weights` function is to subtract `gradient` `*`
    `learning_rate` from each weight:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，“权重更新”步骤的目的（由前面的`update_weights`函数表示）是将权重向“减少此批次上的损失”的方向移动一点。移动的大小由“学习率”确定，通常是一个小量。实现这个`update_weights`函数的最简单方法是从每个权重中减去`gradient`
    `*` `learning_rate`：
- en: '[PRE72]'
  id: totrans-459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: ❶ assign_sub is the equivalent of -= for TensorFlow variables.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `assign_sub`是TensorFlow变量的`-=`的等效操作。
- en: 'In practice, you would almost never implement a weight update step like this
    by hand. Instead, you would use an `Optimizer` instance from Keras, like this:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你几乎永远不会手动实现这样的权重更新步骤。相反，你会使用Keras中的`Optimizer`实例，就像这样：
- en: '[PRE73]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: Now that our per-batch training step is ready, we can move on to implementing
    an entire epoch of training.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的每批训练步骤已经准备��，我们可以继续实现整个训练时期。
- en: 2.5.3 The full training loop
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 完整的训练循环
- en: 'An epoch of training simply consists of repeating the training step for each
    batch in the training data, and the full training loop is simply the repetition
    of one epoch:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的一个时期简单地包括对训练数据中的每个批次重复进行训练步骤，完整的训练循环只是一个时期的重复：
- en: '[PRE74]'
  id: totrans-466
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Let’s test drive it:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试一下：
- en: '[PRE75]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 2.5.4 Evaluating the model
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 评估模型
- en: 'We can evaluate the model by taking the `argmax` of its predictions over the
    test images, and comparing it to the expected labels:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对测试图像的预测取`argmax`，并将其与预期标签进行比较来评估模型：
- en: '[PRE76]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: ❶ Calling .numpy() on a TensorFlow tensor converts it to a NumPy tensor.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在TensorFlow张量上调用`.numpy()`会将其转换为NumPy张量。
- en: All done! As you can see, it’s quite a bit of work to do “by hand” what you
    can do in a few lines of Keras code. But because you’ve gone through these steps,
    you should now have a crystal clear understanding of what goes on inside a neural
    network when you call `fit()`. Having this low-level mental model of what your
    code is doing behind the scenes will make you better able to leverage the high-level
    features of the Keras API.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了！正如你所看到的，手动完成“几行Keras代码可以完成的工作”需要做很多工作。但是因为你已经经历了这些步骤，现在应该对在调用`fit()`时神经网络内部发生的事情有一个清晰的理解。拥有这种低级别的心智模型，了解代码在幕后执行的操作，将使你更能利用Keras
    API的高级功能。
- en: Summary
  id: totrans-474
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: '*Tensors* form the foundation of modern machine learning systems. They come
    in various flavors of `dtype`, `rank`, and `shape`.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*张量*构成现代机器学习系统的基础。它们具有各种`dtype`、`rank`和`shape`。'
- en: You can manipulate numerical tensors via *tensor operations* (such as addition,
    tensor product, or element-wise multiplication), which can be interpreted as encoding
    geometric transformations. In general, everything in deep learning is amenable
    to a geometric interpretation.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以通过*张量操作*（如加法、张量积或逐元素乘法）来操作数值张量，这可以被解释为编码几何变换。总的来说，深度学习中的一切都可以被解释为几何解释。
- en: Deep learning models consist of chains of simple tensor operations, parameterized
    by *weights*, which are themselves tensors. The weights of a model are where its
    “knowledge” is stored.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型由一系列简单的张量操作组成，由*权重*参数化，它们本身也是张量。模型的权重是存储其“知识”的地方。
- en: '*Learning* means finding a set of values for the model’s weights that minimizes
    a *loss function* for a given set of training data samples and their corresponding
    targets.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习*意味着找到一组值，使模型的权重最小化给定一组训练数据样本及其对应目标的*损失函数*。'
- en: Learning happens by drawing random batches of data samples and their targets,
    and computing the gradient of the model parameters with respect to the loss on
    the batch. The model parameters are then moved a bit (the magnitude of the move
    is defined by the learning rate) in the opposite direction from the gradient.
    This is called *mini-batch stochastic gradient descent*.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习是通过随机抽取数据样本及其目标，并计算模型参数相对于批次上的损失的梯度来实现的。然后，模型参数向相反方向移动一点（移动的大小由学习率定义）。这被称为*小批量随机梯度下降*。
- en: The entire learning process is made possible by the fact that all tensor operations
    in neural networks are differentiable, and thus it’s possible to apply the chain
    rule of derivation to find the gradient function mapping the current parameters
    and current batch of data to a gradient value. This is called *backpropagation*.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个学习过程之所以可能，是因为神经网络中的所有张量操作都是可微的，因此可以应用导数的链式法则来找到将当前参数和当前数据批次映射到梯度值的梯度函数。这被称为*反向传播*。
- en: Two key concepts you’ll see frequently in future chapters are *loss* and *optimizers*.
    These are the two things you need to define before you begin feeding data into
    a model.
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将经常在未来章节中看到的两个关键概念是*损失*和*优化器*。这是在开始向模型输入数据之前需要定义的两件事。
- en: The *loss* is the quantity you’ll attempt to minimize during training, so it
    should represent a measure of success for the task you’re trying to solve.
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失*是在训练过程中你将尝试最小化的量，因此它应该代表你尝试解决的任务的成功度量。'
- en: 'The *optimizer* specifies the exact way in which the gradient of the loss will
    be used to update parameters: for instance, it could be the RMSProp optimizer,
    SGD with momentum, and so on.'
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器*指定了损失的梯度将如何用于更新参数的确切方式：例如，可以是RMSProp优化器、带动量的SGD等。'
