- en: Appendix B. Mixed Precision and Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. 混合精度和量化
- en: 'By default, PyTorch uses 32-bit floats to represent model parameters: that’s
    4 bytes per parameter. If your model has 1 billion parameters, then you need at
    least 4 GB of RAM just to hold the model. At inference time you also need enough
    RAM to store the activations, and at training time you need enough RAM to store
    all the intermediate activations as well (for the backward pass), and to store
    the optimizer parameters (e.g., Adam needs two additional parameters for each
    model parameter—that’s an extra 8 GB). This is a lot of RAM, and it’s also plenty
    of time spent transferring data between the CPU and the GPU, not to mention storage
    space, download time, and energy consumption.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch使用32位浮点数来表示模型参数：每个参数4个字节。如果你的模型有10亿个参数，那么你至少需要4GB的RAM来存储模型。在推理时，你还需要足够的RAM来存储激活，而在训练时，你需要足够的RAM来存储所有中间激活（用于反向传播），以及存储优化器参数（例如，Adam为每个模型参数需要两个额外的参数——这额外需要8GB）。这需要大量的RAM，而且还需要大量的时间在CPU和GPU之间传输数据，更不用说存储空间、下载时间和能耗了。
- en: So how can we reduce the model’s size? A simple option is to use a reduced precision
    float representation—typically 16-bit floats instead of 32-bit floats. If you
    train a 32-bit model then shrink it to 16-bits after training, its size will be
    halved, with little impact on its quality. Great!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何减小模型的大小呢？一个简单的选择是使用降低精度的浮点数表示——通常是16位浮点数而不是32位浮点数。如果你训练一个32位的模型，然后在训练后将其缩小到16位，其大小将减半，对质量的影响很小。太棒了！
- en: However, if you try to train the model using 16-bit floats, you may run into
    convergence issues, as we will see. So a common strategy is *mixed-precision training*
    (MPT), where we keep the weights and weight updates at 32-bit precision during
    training, but the rest of the computations use 16-bit precision. After training,
    we shrink the weights down to 16-bits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试使用16位浮点数来训练模型，你可能会遇到收敛问题，正如我们将看到的。因此，一种常见的策略是*混合精度训练*（MPT），在训练过程中保持权重和权重更新为32位精度，但其余的计算使用16位精度。在训练后，我们将权重缩小到16位。
- en: 'Finally, to shrink the model even further, you can use *quantization*: the
    parameters are discretized and represented as 8-bit integers, or even 4-bit integers
    or less. This is harder, and it degrades the model’s quality a bit more, but it
    reduces the model size by a factor of 4 or more, and speeds it up significantly.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了进一步缩小模型的大小，你可以使用*量化*：参数被离散化，并以8位整数表示，甚至可以更少，如4位整数。这比较困难，并且会稍微降低模型的质量，但它可以将模型大小减少4倍以上，并显著提高其速度。
- en: In this appendix, we will cover reduced precision, mixed-precision training,
    and quantization. But to fully understand these, we must first discuss common
    number representations in machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将介绍降低精度、混合精度训练和量化。但要完全理解这些概念，我们首先必须讨论机器学习中常见的数字表示方法。
- en: Common Number Representations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见数字表示方法
- en: By default, PyTorch represents weights and activations using 32-bit floats based
    on the *IEEE Standard for Floating-Point Arithmetic* (IEEE 754), which specifies
    how floating-point numbers are represented in memory. It’s a flexible and efficient
    format which can represent tiny values and huge values, as well as special values
    such as ±0,⁠^([1](app02.html#id4346)) ±infinity, and NaN (i.e., Not a Number).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch根据*IEEE浮点算术标准*（IEEE 754）使用32位浮点数来表示权重和激活，该标准指定了浮点数在内存中的表示方式。它是一种灵活且高效的格式，可以表示极小的值和极大的值，以及特殊的值，如±0、±infinity和NaN（即非数字）。
- en: 'The float32 data type (fp32 for short) can hold numbers as small as ±1.4e^(–45)
    and as large as ±3.4e^(38). It is represented at the top of [Figure B-1](#number_representations_diagram).
    The first bit determines the *sign* *S*: 0 means positive, 1 means negative. The
    next 8 bits hold the *exponent* *E*, ranging from 0 to 255\. And the last 23 bits
    represent the *fraction* *F*, ranging from 0 to 2^(23) – 1\. Here is how to compute
    the value:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: float32数据类型（简称fp32）可以存储从±1.4e^(–45)到±3.4e^(38)的数字。它在[图B-1](#number_representations_diagram)的顶部表示。第一个比特位确定*符号*
    *S*：0表示正数，1表示负数。接下来的8位位表示*指数* *E*，范围从0到255。最后23位表示*分数* *F*，范围从0到2^(23) – 1。以下是计算值的方法：
- en: 'If *E* is between 1 and 254, then the number is called *normalized*: this is
    the most common scenario. In this case, the value *v* can be computed using *v*
    = (–1)^(*S*)⋅2^(*E*–127)⋅(1 + *F*⋅2^(–23)). The last term (1 + *F*⋅2^(–23)) corresponds
    to the most significant digits, so it’s called the *significand*.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* 在 1 和 254 之间，则该数字称为 *normalized*：这是最常见的情况。在这种情况下，值 *v* 可以通过 *v* = (–1)^(*S*)⋅2^(*E*–127)⋅(1
    + *F*⋅2^(–23)) 来计算。最后一个项 (1 + *F*⋅2^(–23)) 对应于最高有效位，因此称为 *significand*。
- en: 'If *E* = 0 and *F* > 0, then the number is called *subnormal*: it is useful
    to represent the tiniest values.⁠^([2](app02.html#id4353)) In this case, *v* =
    (–1)^(*S*)⋅2^(*E*+1–127)⋅(0 + *F*⋅2^(–23)) = (–1)^(*S*)⋅*F*⋅2^(–149).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 0 且 *F* > 0，则该数字称为 *subnormal*：它用于表示极小的值。⁠^([2](app02.html#id4353))
    在这种情况下，*v* = (–1)^(*S*)⋅2^(*E*+1–127)⋅(0 + *F*⋅2^(–23)) = (–1)^(*S*)⋅*F*⋅2^(–149)。
- en: If *E* = 0 and *F* = 0, then *v* = ±0.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 0 且 *F* = 0，则 *v* = ±0。
- en: If *E* = 255 and *F* > 0, then *v* = NaN.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 255 且 *F* > 0，则 *v* = NaN。
- en: If *E* = 255 and *F* = 0, then *v* = ±infinity.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 255 且 *F* = 0，则 *v* = ±infinity。
- en: The other floating-point formats represented in [Figure B-1](#number_representations_diagram)
    differ only by the number of bits used for the exponent and the fraction. For
    example float16 uses 5 bits for the exponent (i.e., it ranges from 0 to 31) and
    10 bits for the fraction (ranging from 0 to 1,023), while float8 uses 4 bits for
    the exponent (from 0 to 15) and 3 bits for the fraction, so it’s often denoted
    fp8 E4M3.⁠^([3](app02.html#id4362)) The equations to compute the value are adjusted
    accordingly, for example normalized float16 values are computed using *v* = (–1)^(*S*)⋅2^(*E*–15)⋅(1
    + *F*⋅2^(–10)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 B-1](#number_representations_diagram) 中表示的其他浮点数格式仅通过用于指数和小数部分的位数不同而有所不同。例如，float16
    使用 5 位用于指数（即范围从 0 到 31）和 10 位用于小数（范围从 0 到 1,023），而 float8 使用 4 位用于指数（从 0 到 15）和
    3 位用于小数，因此通常表示为 fp8 E4M3。⁠^([3](app02.html#id4362)) 计算值的方程相应调整，例如，归一化的 float16
    值通过 *v* = (–1)^(*S*)⋅2^(*E*–15)⋅(1 + *F*⋅2^(–10)) 来计算。'
- en: '![Diagram illustrating common number representations in machine learning, focusing
    on the bit structure of various floating-point and integer formats such as float32,
    float16, float8, and int8, with their respective ranges and components.](assets/hmls_ab01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图示机器学习中常见的数字表示，重点关注各种浮点数和整数格式（如 float32、float16、float8 和 int8）的位结构，以及它们的相应范围和组成部分。](assets/hmls_ab01.png)'
- en: Figure B-1\. Common number representations in machine learning
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-1\. 机器学习中常见的数字表示
- en: The bfloat16 and bfloat8 formats were proposed by Google Brain (hence the *b*),
    and they offer a wider range for the values, at the cost of a significantly reduced
    precision. We will come back to that.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: bfloat16 和 bfloat8 格式是由 Google Brain 提出的（因此有 *b*），它们提供了更宽的值范围，但代价是精度显著降低。我们稍后会回到这一点。
- en: Integers are often represented using 64 bits, with values ranging from 0 to
    2^(64) – 1 (about 1.8e^(19)) for unsigned integers, or –2^32 to 2^32 – 1 (about
    ±4.3e⁹) for signed integers. Integers are also frequently represented using 32
    bits, 16 bits, or 8 bits depending on the use case. In [Figure B-1](#number_representations_diagram),
    I only represented the integer types frequently used for quantization, such as
    8-bit integers (which can be unsigned or signed).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 整数通常使用 64 位表示，无符号整数的值范围从 0 到 2^(64) – 1（约 1.8e^(19)），或有符号整数的值范围从 –2^32 到 2^32
    – 1（约 ±4.3e⁹）。根据用例，整数也经常使用 32 位、16 位或 8 位表示。在 [图 B-1](#number_representations_diagram)
    中，我只表示了经常用于量化的整数类型，例如 8 位整数（可以是无符号或带符号）。
- en: 'When quantizing down to 4 bits, we usually pack 2 weights per byte, and when
    quantizing down to 2 bits, we pack 4 weights per byte. It’s even possible to quantize
    down to ternary values, where each weight can only be equal to –1, 0, or +1\.
    In this case, it’s common to store five weights per byte. For example, the byte
    178 can be written as 20121 in base 3 (since 178 = 2⋅3⁴ + 0⋅3³ + 1⋅3² + 2⋅3¹ +
    1⋅3⁰), and if we subtract 1 from each digit, we get 1, –1, 0, 1, 0: these are
    the 5 ternary weights stored in this single byte. Since 3⁵ = 243, which is less
    than 256, we can fit five ternary values into one byte. This format only uses
    1.6 bits per weight on average, which is 20 times less than using 32-bit floats!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当量化到 4 位时，我们通常每字节打包 2 个权重，当量化到 2 位时，我们每字节打包 4 个权重。甚至可以将量化降低到三进制值，其中每个权重只能等于
    –1、0 或 +1。在这种情况下，通常每字节存储 5 个权重。例如，字节 178 可以写成 20121（三进制），因为 178 = 2⋅3⁴ + 0⋅3³
    + 1⋅3² + 2⋅3¹ + 1⋅3⁰），如果我们从每个数字中减去 1，我们得到 1、–1、0、1、0：这些是存储在这个单一字节中的 5 个三进制权重。由于
    3⁵ = 243，小于 256，我们可以将五个三进制值放入一个字节中。这种格式平均每个权重只使用 1.6 位，这比使用 32 位浮点数少 20 倍！
- en: 'It’s technically possible to quantize weights down to a single bit each, storing
    8 weights per byte: each bit represents a weight equal to either –1 or +1 (or
    sometimes 0 or 1). However, it’s very difficult to get reasonable accuracy using
    such severe quantization.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 技术上可以将权重量化到每个位只有一位，每字节存储8个权重：每个位代表一个等于-1或+1（有时是0或1）的权重。然而，使用这种极端的量化很难获得合理的精度。
- en: 'As you can see, PyTorch’s default weight representation (32-bit floats) takes
    up a *lot* of space compared to other representations: there is room for us to
    shrink our models quite a bit! Let’s start by reducing the precision from 32 bits
    down to 16 bits.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，PyTorch的默认权重表示（32位浮点数）与其他表示相比占用*很多*空间：我们有相当大的空间来缩小我们的模型！让我们从将精度从32位降低到16位开始。
- en: Reduced Precision Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精度降低的模型
- en: 'If you have a 32-bit PyTorch model, you can convert all of its parameters to
    16-bit floats—which is called *half-precision*—by calling the model’s `half()`
    method:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个32位的PyTorch模型，你可以通过调用模型的`half()`方法将所有参数转换为16位浮点数——这被称为*半精度*。
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a quick and easy way to halve the size of a trained model, usually without
    much impact on its quality. Moreover, since many GPUs have 16-bit float optimizations,
    and since there will be less data to transfer between the CPU and the GPU, the
    model will typically run almost twice as fast.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速简单的方法，可以将训练模型的尺寸减半，通常对质量的影响不大。此外，由于许多GPU具有16位浮点优化，并且CPU和GPU之间的数据传输量将减少，因此模型通常可以运行得快近两倍。
- en: Tip
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When downloading a pretrained model using the Transformers library’s `from_pretrained()`
    method, you can set `dtype="auto"` to let the library choose the optimal float
    representation for your hardware.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Transformers库的`from_pretrained()`方法下载预训练模型时，你可以设置`dtype="auto"`以让库为你的硬件选择最佳的浮点表示。
- en: 'To use the model, you now need to feed it 16-bit inputs, and it will output
    16-bit outputs as well:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用该模型，你现在需要提供16位输入，它也将输出16位输出：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But what if you want to build and train a 16-bit model right from the start?
    In this case, you can set `dtype=torch.float16` whenever you create a tensor or
    a module with parameters, for example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想在一开始就构建和训练一个16位模型呢？在这种情况下，你可以在创建张量或具有参数的模块时设置`dtype=torch.float16`，例如：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you prefer to avoid repeating `dtype=torch.float16` everywhere, then you
    can instead set the default data type to `torch.float16` using `torch.set_default_dtype(torch.float16)`.
    Be careful: this will apply to *all* tensors and modules created after that.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望避免在所有地方重复`dtype=torch.float16`，那么你可以通过使用`torch.set_default_dtype(torch.float16)`将默认数据类型设置为`torch.float16`。请注意：这将应用于之后创建的所有张量和模块。
- en: 'However, the reduced precision can cause some issues during training. Indeed,
    16-bit floats have a limited *dynamic range* (i.e., the ratio between the largest
    and smallest positive representable values): the smallest positive representable
    value is about 0.00000006 (i.e., 6.0e^(–8)), while the largest is 65,504 (i.e.,
    ~6.5e⁴). This implies that any gradient update smaller than ~6.0e^(–8) will *underflow*,
    meaning it will be rounded down to zero, and thus ignored. And conversely, any
    value larger than ~6.5e⁴ will *overflow*, meaning it will be rounded up to infinity,
    causing training to fail (once some weights are infinite, the loss will be infinite
    or NaN).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，降低精度可能会在训练期间引起一些问题。实际上，16位浮点数具有有限的*动态范围*（即，最大和最小可表示正值的比率）：最小的可表示正值约为0.00000006（即，6.0e^(–8)），而最大值为65,504（即，~6.5e⁴）。这意味着任何小于~6.0e^(–8)的梯度更新将*下溢*，这意味着它将被舍入到零，因此被忽略。相反，任何大于~6.5e⁴的值将*溢出*，这意味着它将被舍入到无穷大，导致训练失败（一旦某些权重变为无穷大，损失将变为无穷大或NaN）。
- en: 'To avoid underflows, one solution is to scale up the loss by a large factor
    (e.g., multiply it by 256): this will automatically scale up the gradients by
    the same factor during the backward pass, which will prevent them from being smaller
    than the smallest 16-bit representable value. However, you must scale the gradients
    back down before performing an optimizer step, and at this point you may get an
    underflow. Also, if you scale up the loss too much, you will run into overflows.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免下溢，一种解决方案是将损失放大一个很大的因子（例如，乘以256）：这将自动在反向传播过程中以相同的因子放大梯度，从而防止它们小于最小的16位可表示值。然而，在执行优化器步骤之前，你必须将梯度缩小回原来的规模，这时你可能会遇到下溢。此外，如果你将损失放大得太多，你可能会遇到溢出。
- en: 'If you can’t find a good scaling factor that avoids both underflows and overflows,
    you can try to use `torch.bfloat16` rather than `torch.float16`, since bloat16
    has more bits for the exponent: the smallest value is ~9.2e^(–41), while the largest
    is ~3.4e^(38), so there’s less risk of any significant gradient updates being
    ignored, or reasonable values being rounded up to infinity.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你找不到一个既能避免下溢又能避免上溢的合适缩放因子，你可以尝试使用 `torch.bfloat16` 而不是 `torch.float16`，因为
    bfloat16 的指数位更多：最小值约为 ~9.2e^(–41)，最大值约为 ~3.4e^(38)，因此不太可能忽略任何显著的梯度更新，或者将合理的值四舍五入到无穷大。
- en: 'However, bfloat16 has historically had less hardware support (although this
    is improving), and it offers fewer bits for the fraction, which can cause some
    gradient updates to be ignored when the corresponding parameter values are much
    larger, causing training to stall. For example, if the gradient update is 4.5e^(–2)
    (i.e., 0.045) and the corresponding parameter value is equal to 1.23e² (i.e.,
    123), then the sum should be 1.23045e² (i.e., 123.045) but bfloat16 does not have
    enough fraction bits to store all these digits, so it must round the result to
    1.23e² (i.e., 123): as you can see, the gradient update is completely ignored.
    With regular 16-bit floats, the result would be 123.0625, which is not exactly
    right due to floating-point precision errors, but at least the parameter makes
    a step in the right direction. That said, if the gradient update was a bit smaller
    (e.g., 0.03), it would be ignored even in regular 16-bit float precision.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，bfloat16 历史上硬件支持较少（尽管情况正在改善），并且它提供的分数位更少，这可能导致当相应的参数值很大时，一些梯度更新被忽略，导致训练停滞。例如，如果梯度更新是
    4.5e^(–2)（即 0.045）且相应的参数值等于 1.23e²（即 123），那么总和应该是 1.23045e²（即 123.045），但 bfloat16
    没有足够的分数位来存储所有这些数字，因此它必须将结果四舍五入到 1.23e²（即 123）：正如你所看到的，梯度更新被完全忽略。使用常规 16 位浮点数时，结果将是
    123.0625，这并不完全正确，因为浮点精度错误，但至少参数向正确的方向迈出了一步。话虽如此，如果梯度更新稍微小一点（例如，0.03），即使在常规 16
    位浮点精度下也会被忽略。
- en: So if you try float16 and bfloat16 but you still encounter convergence issues
    during training, then you can try *mixed-precision training* instead.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你尝试了 float16 和 bfloat16 但在训练过程中仍然遇到收敛问题，那么你可以尝试进行 *混合精度训练*。
- en: Mixed-Precision Training
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: '[*Mixed-precision training* (MPT)](https://homl.info/mpt) was proposed by Baidu
    and Nvidia researchers in 2017,⁠^([4](app02.html#id4388)) to address the issues
    often observed with 16-bit training. Here’s how it works:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[*混合精度训练*（MPT）](https://homl.info/mpt) 由百度和英伟达的研究人员在 2017 年提出，以解决通常观察到的 16
    位训练问题。以下是它是如何工作的：'
- en: MPT stores a primary copy of the model parameters as 32-bit floats, and at each
    training iteration, it creates a 16-bit copy of these model parameters (see step
    1 in [Figure B-2](#mpt_diagram)), and uses them for the forward pass (step 2).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPT 存储模型参数的主副本为 32 位浮点数，并在每次训练迭代中创建这些模型参数的 16 位副本（参见[图 B-2](#mpt_diagram)中的步骤
    1），然后使用它们进行正向传递（步骤 2）。
- en: The loss is then scaled up by a large factor (step 3) to avoid underflows, as
    we discussed earlier.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，损失被放大一个很大的因子（步骤 3），以避免下溢，正如我们之前讨论的那样。
- en: 'Lastly, we switch back to 32-bit precision to scale the gradients back down:
    this greater precision avoids the risk of underflow. Next we use the gradients
    to perform one optimizer step, improving the primary parameters (step 5). Performing
    the actual optimizer step in 32-bit precision ensures that small weight updates
    are not ignored when applied to much larger parameter values, since 32-bit floats
    have a very large fraction (23 bits).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们切换回 32 位精度以将梯度缩放回原来的大小：这种更高的精度避免了下溢的风险。接下来，我们使用梯度执行一个优化器步骤，改进主参数（步骤 5）。在
    32 位精度下执行实际的优化器步骤确保了当应用于较大的参数值时，不会忽略小的权重更新，因为 32 位浮点数有非常大的分数部分（23 位）。
- en: '![Diagram illustrating mixed-precision training, showing the process of copying
    32-bit parameters to 16-bit, performing forward and backward passes, scaling losses,
    and completing with an optimizer step.](assets/hmls_ab02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![展示混合精度训练过程的图，包括将 32 位参数复制到 16 位，执行正向和反向传递，缩放损失，以及完成优化器步骤。](assets/hmls_ab02.png)'
- en: Figure B-2\. Mixed-precision training
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-2\. 混合精度训练
- en: MPT offers almost all of the benefits of 16-bit training, without the instabilities.
    However, the model parameters take 50% more space than in 32-bit training because
    of the 16-bit copy at each training iteration, so how is this any better? Well,
    during training, most of the RAM is used to store the activations, not the model
    parameters, so in practice MPT requires just a bit more than half the RAM used
    by regular 32-bit training. And it typically runs twice as fast. Moreover, once
    training is finished, we no longer need 32-bit parameters, we can convert them
    to 16 bits, and we get a pure 16-bit model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MPT提供了几乎与16位训练相同的所有好处，但没有不稳定性。然而，由于每次训练迭代中的16位复制，模型参数比32位训练多占用50%的空间，那么这有什么优势呢？好吧，在训练过程中，大部分RAM用于存储激活，而不是模型参数，所以实际上MPT只需要比常规32位训练多一点点RAM。而且它通常运行速度要快两倍。此外，一旦训练完成，我们就不再需要32位参数，我们可以将它们转换为16位，从而得到一个纯16位模型。
- en: Warning
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'MPT does not always accelerate training: it depends on the model, the batch
    size, and the hardware. That said, most large transformers are trained using MPT.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MPT并不总是加速训练：这取决于模型、批量大小和硬件。话虽如此，大多数大型Transformer都是使用MPT进行训练的。
- en: 'Rather than finding the best scaling factor by trial and error, you can run
    training in 32-bit precision for a little while (assuming you have enough RAM)
    and measure the gradient statistics to find the optimal scaling factor for your
    task: it should be large enough to avoid underflows, and small enough to avoid
    overflows.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通过试错来找到最佳缩放因子，您可以在32位精度下运行训练一段时间（假设您有足够的RAM）并测量梯度统计信息，以找到适合您任务的最佳缩放因子：它应该足够大以避免下溢，同时足够小以避免上溢。
- en: 'Alternatively, your training script can adapt the factor dynamically during
    training: if some gradients are infinite or NaN, this means that an overflow occurred
    so the factor must be reduced (e.g., halved) and the training step must be skipped,
    but if no overflow is detected then the scaling factor can be gradually increased
    (e.g., doubled every 2,000 training steps). PyTorch provides a `torch.amp.GradScaler`
    class that implements this approach, and also scales down the learning rate appropriately.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您的训练脚本可以在训练过程中动态调整因子：如果某些梯度是无穷大或NaN，这意味着发生了溢出，因此因子必须减少（例如减半），并且必须跳过训练步骤，但如果未检测到溢出，则缩放因子可以逐渐增加（例如，每2,000个训练步骤加倍）。PyTorch提供了一个`torch.amp.GradScaler`类来实现这种方法，并且还会适当地降低学习率。
- en: PyTorch also provides a `torch.autocast()` function that returns a context within
    which many operations will automatically run in 16-bit precision. This includes
    operations that typically benefit the most from 16-bit precision, such as matrix
    multiplication and convolutions, but it does not include operations like reductions
    (e.g., `torch.sum()`) since running these in half precision offers no significant
    benefit and can damage precision.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了一个`torch.autocast()`函数，它返回一个上下文，在该上下文中许多操作将自动以16位精度运行。这包括通常从16位精度中受益最大的操作，如矩阵乘法和卷积，但不包括像
    reductions（例如`torch.sum()`）这样的操作，因为这些操作在半精度下运行不会带来显著的好处，并且可能会损害精度。
- en: 'Let’s update our training function to run the forward pass within an autocast
    context and use a `GradScaler` to dynamically scale the loss:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的训练函数，使其在autocast上下文中运行前向传递，并使用`GradScaler`动态缩放损失：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When fine-tuning a transformer using the Hugging Face Transformers library,
    you can set `fp16=True` or `bf16=True` in the `TrainingArguments` to activate
    mixed-precision training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Hugging Face Transformers库微调Transformer时，您可以在`TrainingArguments`中将`fp16=True`或`bf16=True`设置为激活混合精度训练。
- en: Reducing precision down to 16-bits often works great, but can we shrink our
    models even further? Yes, we can, using quantization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将精度降低到16位通常效果很好，但我们能否进一步缩小我们的模型？是的，我们可以，使用量化。
- en: Quantization
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: 'Quantization means mapping continuous values to discrete ones. In deep learning,
    this typically involves converting parameters, and often activations as well,
    from floats to integers—usually 32-bit floats to 8-bit integers. More generally,
    the goal is to shrink and speed up our model by reducing the number of bits used
    in parameters, and often in activations as well. Moreover, some embedded devices
    (e.g., ARM Cortex-M0) do not support floating-point operations at all (in part
    to reduce their cost and energy consumption), so models have to be quantized entirely
    (both weights and activations) before you can use them on the device. Modern smartphones
    do support floating point operations but still benefit significantly from quantization:
    int8 operations are 2 to 4 times faster and use 5 to 10 times less energy than
    FP32.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 量化意味着将连续值映射到离散值。在深度学习中，这通常涉及将参数，以及通常激活，从浮点数转换为整数——通常是 32 位浮点数到 8 位整数。更普遍地说，目标是通过减少参数（以及通常激活）中使用的位数来缩小和加速我们的模型。此外，一些嵌入式设备（例如，ARM
    Cortex-M0）根本不支持浮点运算（部分是为了降低其成本和能耗），因此模型必须在设备上使用之前完全量化（包括权重和激活）。现代智能手机支持浮点运算，但仍然可以从量化中受益良多：int8
    运算比 FP32 快 2 到 4 倍，并且使用的能量少 5 到 10 倍。
- en: The simplest approach is *linear quantization*, so we’ll discuss it now. We
    will discuss a few nonlinear quantization methods later in this appendix.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是 *线性量化*，因此我们现在将讨论它。我们将在本附录的后面讨论几种非线性量化方法。
- en: Linear Quantization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性量化
- en: 'Linear quantization dates back to digital signal processing in the 1950s, but
    it has become particularly important in machine learning over the past decade
    since models have become gigantic, and yet we wish to run them on mobile phones
    and other limited devices. It has two variants: asymmetric and symmetric. In *asymmetric
    linear quantization*, float values are simply mapped linearly to unsigned bytes
    with values ranging from 0 to 255 (or more generally from 0 to 2^(*n*) – 1 when
    quantizing to *n*-bit integers). For example, if the weights range between *a*
    = –0.1 and *b* = 0.6, then the float –0.1 will be mapped to the byte 0, the float
    0.0 to integer 36, 0.1 to 72, …​, 0.6 to 255, and more generally, the float tensor
    **w** will be mapped to the integer tensor **q** using [Equation B-1](#asymmetric_linear_quantization_equation).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性量化可以追溯到 20 世纪 50 年代的数字信号处理，但自从模型变得巨大，而我们又希望在手机和其他有限设备上运行它们以来，在过去十年中它在机器学习中变得尤为重要。它有两个变体：非对称和对称。在
    *非对称线性量化* 中，浮点值简单地线性映射到无符号字节，其值范围从 0 到 255（或者更普遍地，当量化到 *n*-位整数时，从 0 到 2^(*n*)
    – 1）。例如，如果权重范围在 *a* = –0.1 和 *b* = 0.6 之间，那么浮点数 –0.1 将映射到字节 0，浮点数 0.0 映射到整数 36，0.1
    映射到 72，……，0.6 映射到 255，并且更普遍地，浮点张量 **w** 将使用 [方程 B-1](#asymmetric_linear_quantization_equation)
    映射到整数张量 **q**。
- en: Equation B-1\. Asymmetric linear quantization
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 B-1\. 非对称线性量化
- en: $StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis
    StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus
    z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript
    n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction
    a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals
    min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript
    i Endscripts w Subscript i Baseline EndLayout$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis
    StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus
    z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript
    n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction
    a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals
    min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript
    i Endscripts w Subscript i Baseline EndLayout$
- en: 'In this equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*w*[*i*] is the *i*^(th) float in the original tensor **w**.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*i*] 是原始张量 **w** 中的第 *i* 个浮点数。'
- en: '*q*[i] is the *i*^(th) integer in the quantized tensor **q**. It is clamped
    between 0 and 2^(*n*) – 1 (e.g., 255 for 8-bit quantization).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q*[i] 是量化张量 **q** 中的第 *i* 个整数。它在 0 和 2^(*n*) – 1 之间夹紧（例如，对于 8 位量化，为 255）。'
- en: '*s* is the *quantization scale*. Note that some authors define it as 1 / *s*
    and adapt the equation accordingly (i.e., they multiply rather than divide).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* 是 *量化尺度*。请注意，一些作者将其定义为 1 / *s* 并相应地调整方程（即，他们乘以而不是除以）。'
- en: '*z* is the *quantization bias* or *zero point*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z* 是 *量化偏差* 或 *零点*。'
- en: '*a* is the minimum value of **w**, and *b* is the maximum value of **w**.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是 **w** 的最小值，*b* 是 **w** 的最大值。'
- en: The range [*a*, *b*] is known for weights, since their values do not change
    after training. However, the range of activation values depends on the inputs
    we feed to the model. As a result, for each activation that we want to quantize
    (e.g., the inputs of each layer), we will either have to compute *a* and *b* on
    the fly for each new input batch (this is called *dynamic quantization*) or run
    a calibration dataset once through the model to determine the typical range of
    activation values, then use this range to quantize the activations of all subsequent
    batches (this is called *static quantization*). Static quantization is a faster
    but less precise.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重，[*a*, *b*] 的范围是已知的，因为它们的值在训练后不会改变。然而，激活值的范围取决于我们输入到模型中的输入。因此，对于我们要量化的每个激活（例如，每层的输入），我们可能必须为每个新的输入批次动态计算
    *a* 和 *b*（这称为动态量化）或者运行一个校准数据集一次通过模型来确定激活值的典型范围，然后使用这个范围来量化所有后续批次的激活（这称为静态量化）。静态量化更快但精度较低。
- en: 'To approximately recover the original value *w*[*i*] from a quantized value
    *q*[*i*], we can compute *w*[*i*] ≈ s × (*q*[*i*] – *z*). This is called *dequantization*.
    For example, if *q*[*i*] = 72, then we get *w*[*i*] ≈ 0.0988, which is indeed
    close to 0.1\. The difference between the dequantized value (0.0988) and the original
    value (0.1) is called the *quantization noise*: with 8-bit quantization, the quantization
    noise usually leads to a slightly degraded accuracy. With 6-bit, 4-bit, or less,
    the quantization noise can hurt even more, especially since it has a cumulative
    effect: the deeper the network, the stronger the impact.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从量化值 *q*[*i*] 近似恢复原始值 *w*[*i*]，我们可以计算 *w*[*i*] ≈ s × (*q*[*i*] – *z*)。这被称为反量化。例如，如果
    *q*[*i*] = 72，那么我们得到 *w*[*i*] ≈ 0.0988，这确实接近 0.1。反量化值（0.0988）与原始值（0.1）之间的差异称为量化噪声：在
    8 位量化中，量化噪声通常会导致略微降低的精度。在 6 位、4 位或更少的情况下，量化噪声的伤害可能更大，尤其是因为它具有累积效应：网络越深，影响越强。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Equation B-1](#asymmetric_linear_quantization_equation) guarantees that any
    float equal to 0.0 can be quantized and dequantized back to 0.0 exactly: indeed,
    if *w*[*i*] = 0.0 then *q*[*i*] = *z*, and dequantizing *q*[*i*] gives back *w*[*i*]
    = *s* × (*z* – *z*) = 0.0\. This is particularly useful for sparse models where
    many weights are equal to zero. It is also important when using activations like
    ReLU which produce many zero activations.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 B-1](#asymmetric_linear_quantization_equation) 保证任何等于 0.0 的浮点数都可以精确量化并反量化回
    0.0：确实，如果 *w*[*i*] = 0.0，则 *q*[*i*] = *z*，反量化 *q*[*i*] 会返回 *w*[*i*] = *s* × (*z*
    – *z*) = 0.0。这对于稀疏模型特别有用，其中许多权重等于零。当使用如 ReLU 这样的激活函数时，它会产生许多零激活，这也非常重要。'
- en: 'In PyTorch, the `torch.quantize_per_tensor()` function lets you create a quantized
    tensor: this is a special kind of tensor that contains the quantized values (i.e.,
    integers), as well as the *quantization parameters* (i.e., the scale and zero
    point). Let’s use this function to quantize a tensor, then dequantize it. In this
    example we will use the data type `torch.quint8`, which uses 8-bit unsigned integers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，`torch.quantize_per_tensor()` 函数允许你创建一个量化张量：这是一种特殊的张量，它包含量化值（即整数），以及量化参数（即缩放和零点）。让我们使用这个函数来量化一个张量，然后进行反量化。在这个例子中，我们将使用数据类型
    `torch.quint8`，它使用 8 位无符号整数：
- en: '[PRE4][PRE5]`` `>>>` `qw` `=` `torch``.``quantize_per_tensor``(``w``,` `scale``=``s``,`
    `zero_point``=``z``,` `dtype``=``torch``.``quint8``)` [PRE6] `tensor([ 0.0988,
    -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,`  `quantization_scheme=torch.per_tensor_affine,
    scale=0.002745098201557994,`  `zero_point=36)` `>>>` `qw``.``dequantize``()`  `#
    back to 32-bit floats (close to the original tensor)` `` `tensor([ 0.0988, -0.0988,  0.6012,  0.0000])`
    `` [PRE7]` [PRE8][PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE4][PRE5]`` `>>>` `qw` `=` `torch``.``quantize_per_tensor``(``w``,` `scale``=``s``,`
    `zero_point``=``z``,` `dtype``=``torch``.``quint8``)` [PRE6] `tensor([ 0.0988,
    -0.0988,  0.6012,  0.0000], size=(4,), dtype=torch.quint8,`  `quantization_scheme=torch.per_tensor_affine,
    scale=0.002745098201557994,`  `zero_point=36)` `>>>` `qw``.``dequantize``()`  `#
    返回 32 位浮点数（接近原始张量)` `` `tensor([ 0.0988, -0.0988,  0.6012,  0.0000])` `` [PRE7]`
    [PRE8][PRE9]'
- en: '[PRE10]` [PRE11] [PRE12]`py [PRE13]py [PRE14]`py  [PRE15]'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE10]` [PRE11] [PRE12]`py [PRE13]py [PRE14]`py  [PRE15]'
