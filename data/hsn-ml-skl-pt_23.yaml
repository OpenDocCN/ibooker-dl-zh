- en: Appendix B. Mixed Precision and Quantization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录B. 混合精度和量化
- en: 'By default, PyTorch uses 32-bit floats to represent model parameters: that’s
    4 bytes per parameter. If your model has 1 billion parameters, then you need at
    least 4 GB of RAM just to hold the model. At inference time you also need enough
    RAM to store the activations, and at training time you need enough RAM to store
    all the intermediate activations as well (for the backward pass), and to store
    the optimizer parameters (e.g., Adam needs two additional parameters for each
    model parameter—that’s an extra 8 GB). This is a lot of RAM, and it’s also plenty
    of time spent transferring data between the CPU and the GPU, not to mention storage
    space, download time, and energy consumption.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch使用32位浮点数来表示模型参数：每个参数4个字节。如果你的模型有10亿个参数，那么你至少需要4GB的RAM来存储模型。在推理时，你需要足够的RAM来存储激活，在训练时，你需要足够的RAM来存储所有中间激活（用于反向传播），以及存储优化器参数（例如，Adam为每个模型参数需要两个额外的参数——这额外需要8GB）。这需要大量的RAM，而且还需要大量的时间在CPU和GPU之间传输数据，更不用说存储空间、下载时间和能耗了。
- en: So how can we reduce the model’s size? A simple option is to use a reduced precision
    float representation—typically 16-bit floats instead of 32-bit floats. If you
    train a 32-bit model then shrink it to 16-bits after training, its size will be
    halved, with little impact on its quality. Great!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何减小模型的大小呢？一个简单的选择是使用降低精度的浮点数表示——通常是16位浮点数而不是32位浮点数。如果你训练一个32位模型，然后在训练后将其缩小到16位，其大小将减半，对质量的影响很小。太棒了！
- en: However, if you try to train the model using 16-bit floats, you may run into
    convergence issues, as we will see. So a common strategy is *mixed-precision training*
    (MPT), where we keep the weights and weight updates at 32-bit precision during
    training, but the rest of the computations use 16-bit precision. After training,
    we shrink the weights down to 16-bits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试使用16位浮点数训练模型，你可能会遇到收敛问题，正如我们将看到的。因此，一个常见的策略是*混合精度训练*（MPT），在训练过程中保持权重和权重更新为32位精度，但其余的计算使用16位精度。训练完成后，我们将权重缩小到16位。
- en: 'Finally, to shrink the model even further, you can use *quantization*: the
    parameters are discretized and represented as 8-bit integers, or even 4-bit integers
    or less. This is harder, and it degrades the model’s quality a bit more, but it
    reduces the model size by a factor of 4 or more, and speeds it up significantly.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了进一步缩小模型，你可以使用*量化*：参数被离散化，并以8位整数表示，甚至可以更少，如4位整数。这比较困难，并且会稍微降低模型的质量，但它可以将模型大小减少4倍或更多，并显著提高速度。
- en: In this appendix, we will cover reduced precision, mixed-precision training,
    and quantization. But to fully understand these, we must first discuss common
    number representations in machine learning.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本附录中，我们将介绍降低精度、混合精度训练和量化。但要完全理解这些，我们首先必须讨论机器学习中常见的数字表示。
- en: Common Number Representations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见数字表示
- en: By default, PyTorch represents weights and activations using 32-bit floats based
    on the *IEEE Standard for Floating-Point Arithmetic* (IEEE 754), which specifies
    how floating-point numbers are represented in memory. It’s a flexible and efficient
    format which can represent tiny values and huge values, as well as special values
    such as ±0,⁠^([1](app02.html#id4346)) ±infinity, and NaN (i.e., Not a Number).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PyTorch使用基于*IEEE浮点算术标准*（IEEE 754）的32位浮点数来表示权重和激活，该标准规定了浮点数在内存中的表示方式。这是一个灵活且高效的格式，可以表示极小的值和极大的值，以及±0、⁠^([1](app02.html#id4346))
    ±infinity和NaN（即非数字）等特殊值。
- en: 'The float32 data type (fp32 for short) can hold numbers as small as ±1.4e^(–45)
    and as large as ±3.4e^(38). It is represented at the top of [Figure B-1](#number_representations_diagram).
    The first bit determines the *sign* *S*: 0 means positive, 1 means negative. The
    next 8 bits hold the *exponent* *E*, ranging from 0 to 255\. And the last 23 bits
    represent the *fraction* *F*, ranging from 0 to 2^(23) – 1\. Here is how to compute
    the value:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: float32数据类型（简称fp32）可以容纳从±1.4e^(–45)到±3.4e^(38)的数值。它在[图B-1](#number_representations_diagram)的顶部表示。第一个比特位确定*符号*
    *S*：0表示正数，1表示负数。接下来的8位包含*指数* *E*，范围从0到255。最后23位代表*分数* *F*，范围从0到2^(23) – 1。以下是计算值的方法：
- en: 'If *E* is between 1 and 254, then the number is called *normalized*: this is
    the most common scenario. In this case, the value *v* can be computed using *v*
    = (–1)^(*S*)⋅2^(*E*–127)⋅(1 + *F*⋅2^(–23)). The last term (1 + *F*⋅2^(–23)) corresponds
    to the most significant digits, so it’s called the *significand*.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* 在 1 和 254 之间，则该数字称为 *规范化*：这是最常见的情况。在这种情况下，值 *v* 可以通过 *v* = (–1)^(*S*)⋅2^(*E*–127)⋅(1
    + *F*⋅2^(–23)) 来计算。最后一个项 (1 + *F*⋅2^(–23)) 对应于最高有效位，因此称为 *尾数*。
- en: 'If *E* = 0 and *F* > 0, then the number is called *subnormal*: it is useful
    to represent the tiniest values.⁠^([2](app02.html#id4353)) In this case, *v* =
    (–1)^(*S*)⋅2^(*E*+1–127)⋅(0 + *F*⋅2^(–23)) = (–1)^(*S*)⋅*F*⋅2^(–149).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 0 且 *F* > 0，则该数字称为 *次规范化*：它用于表示极小的值。⁠^([2](app02.html#id4353)) 在这种情况下，*v*
    = (–1)^(*S*)⋅2^(*E*+1–127)⋅(0 + *F*⋅2^(–23)) = (–1)^(*S*)⋅*F*⋅2^(–149)。
- en: If *E* = 0 and *F* = 0, then *v* = ±0.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 0 且 *F* = 0，则 *v* = ±0。
- en: If *E* = 255 and *F* > 0, then *v* = NaN.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 255 且 *F* > 0，则 *v* = NaN。
- en: If *E* = 255 and *F* = 0, then *v* = ±infinity.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 *E* = 255 且 *F* = 0，则 *v* = ±infinity。
- en: The other floating-point formats represented in [Figure B-1](#number_representations_diagram)
    differ only by the number of bits used for the exponent and the fraction. For
    example float16 uses 5 bits for the exponent (i.e., it ranges from 0 to 31) and
    10 bits for the fraction (ranging from 0 to 1,023), while float8 uses 4 bits for
    the exponent (from 0 to 15) and 3 bits for the fraction, so it’s often denoted
    fp8 E4M3.⁠^([3](app02.html#id4362)) The equations to compute the value are adjusted
    accordingly, for example normalized float16 values are computed using *v* = (–1)^(*S*)⋅2^(*E*–15)⋅(1
    + *F*⋅2^(–10)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 B-1](#number_representations_diagram) 中表示的其他浮点数格式仅通过指数和分数使用的位数不同。例如，float16
    使用 5 位用于指数（即，范围从 0 到 31）和 10 位用于分数（范围从 0 到 1,023），而 float8 使用 4 位用于指数（从 0 到 15）和
    3 位用于分数，因此通常表示为 fp8 E4M3。⁠^([3](app02.html#id4362)) 计算值的方程相应调整，例如规范化 float16 值通过
    *v* = (–1)^(*S*)⋅2^(*E*–15)⋅(1 + *F*⋅2^(–10)) 来计算。'
- en: '![Diagram illustrating common number representations in machine learning, focusing
    on the bit structure of various floating-point and integer formats such as float32,
    float16, float8, and int8, with their respective ranges and components.](assets/hmls_ab01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![说明机器学习中常见数字表示的图表，重点关注各种浮点数和整数格式（如 float32、float16、float8 和 int8）的位结构，以及它们的相应范围和组成部分。](assets/hmls_ab01.png)'
- en: Figure B-1\. Common number representations in machine learning
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-1\. 机器学习中的常见数字表示
- en: The bfloat16 and bfloat8 formats were proposed by Google Brain (hence the *b*),
    and they offer a wider range for the values, at the cost of a significantly reduced
    precision. We will come back to that.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: bfloat16 和 bfloat8 格式由 Google Brain 提出（因此有 *b*），它们提供了更宽的数值范围，但代价是精度显著降低。我们稍后会回到这一点。
- en: Integers are often represented using 64 bits, with values ranging from 0 to
    2^(64) – 1 (about 1.8e^(19)) for unsigned integers, or –2^32 to 2^32 – 1 (about
    ±4.3e⁹) for signed integers. Integers are also frequently represented using 32
    bits, 16 bits, or 8 bits depending on the use case. In [Figure B-1](#number_representations_diagram),
    I only represented the integer types frequently used for quantization, such as
    8-bit integers (which can be unsigned or signed).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 整数通常使用 64 位表示，对于无符号整数，值范围从 0 到 2^(64) – 1（约 1.8e^(19)），对于有符号整数，范围从 –2^32 到 2^32
    – 1（约 ±4.3e⁹）。根据用例，整数也经常使用 32 位、16 位或 8 位表示。在 [图 B-1](#number_representations_diagram)
    中，我只表示了经常用于量化的整数类型，例如 8 位整数（可以是无符号或带符号）。
- en: 'When quantizing down to 4 bits, we usually pack 2 weights per byte, and when
    quantizing down to 2 bits, we pack 4 weights per byte. It’s even possible to quantize
    down to ternary values, where each weight can only be equal to –1, 0, or +1\.
    In this case, it’s common to store five weights per byte. For example, the byte
    178 can be written as 20121 in base 3 (since 178 = 2⋅3⁴ + 0⋅3³ + 1⋅3² + 2⋅3¹ +
    1⋅3⁰), and if we subtract 1 from each digit, we get 1, –1, 0, 1, 0: these are
    the 5 ternary weights stored in this single byte. Since 3⁵ = 243, which is less
    than 256, we can fit five ternary values into one byte. This format only uses
    1.6 bits per weight on average, which is 20 times less than using 32-bit floats!'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当量化到 4 位时，我们通常每字节打包 2 个权重，当量化到 2 位时，我们每字节打包 4 个权重。甚至可以将量化降低到三进制值，其中每个权重只能等于
    –1、0 或 +1。在这种情况下，通常每字节存储 5 个权重。例如，字节 178 可以写作 20121（三进制），因为 178 = 2⋅3⁴ + 0⋅3³
    + 1⋅3² + 2⋅3¹ + 1⋅3⁰），如果我们从每个数字中减去 1，我们得到 1、–1、0、1、0：这些是存储在这个单一字节中的 5 个三进制权重。由于
    3⁵ = 243，小于 256，我们可以将五个三进制值放入一个字节中。这种格式平均每个权重只使用 1.6 位，这比使用 32 位浮点数少 20 倍！
- en: 'It’s technically possible to quantize weights down to a single bit each, storing
    8 weights per byte: each bit represents a weight equal to either –1 or +1 (or
    sometimes 0 or 1). However, it’s very difficult to get reasonable accuracy using
    such severe quantization.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，可以将权重量化到每个位只有一位，每字节存储8个权重：每个位代表一个等于-1或+1（有时是0或1）的权重。然而，使用如此严重的量化很难获得合理的精度。
- en: 'As you can see, PyTorch’s default weight representation (32-bit floats) takes
    up a *lot* of space compared to other representations: there is room for us to
    shrink our models quite a bit! Let’s start by reducing the precision from 32 bits
    down to 16 bits.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，PyTorch的默认权重表示（32位浮点数）与其他表示相比占用*很多*空间：我们有相当大的空间来缩小我们的模型！让我们从将精度从32位降低到16位开始。
- en: Reduced Precision Models
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 精度降低的模型
- en: 'If you have a 32-bit PyTorch model, you can convert all of its parameters to
    16-bit floats—which is called *half-precision*—by calling the model’s `half()`
    method:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个32位的PyTorch模型，你可以通过调用模型的`half()`方法将所有参数转换为16位浮点数——这被称为*半精度*：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is a quick and easy way to halve the size of a trained model, usually without
    much impact on its quality. Moreover, since many GPUs have 16-bit float optimizations,
    and since there will be less data to transfer between the CPU and the GPU, the
    model will typically run almost twice as fast.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速简单的方法，通常不会对模型的质量产生太大影响，可以将训练模型的尺寸减半。此外，由于许多GPU具有16位浮点优化，并且CPU和GPU之间的数据传输量将减少，因此模型通常可以运行得快近两倍。
- en: Tip
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When downloading a pretrained model using the Transformers library’s `from_pretrained()`
    method, you can set `dtype="auto"` to let the library choose the optimal float
    representation for your hardware.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Transformers库的`from_pretrained()`方法下载预训练模型时，你可以设置`dtype="auto"`让库为你选择硬件的最佳浮点表示。
- en: 'To use the model, you now need to feed it 16-bit inputs, and it will output
    16-bit outputs as well:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用该模型，你现在需要给它提供16位输入，它也将输出16位输出：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'But what if you want to build and train a 16-bit model right from the start?
    In this case, you can set `dtype=torch.float16` whenever you create a tensor or
    a module with parameters, for example:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你想在一开始就构建和训练一个16位模型呢？在这种情况下，你可以在创建张量或具有参数的模块时设置`dtype=torch.float16`，例如：
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you prefer to avoid repeating `dtype=torch.float16` everywhere, then you
    can instead set the default data type to `torch.float16` using `torch.set_default_dtype(torch.float16)`.
    Be careful: this will apply to *all* tensors and modules created after that.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望避免在所有地方重复`dtype=torch.float16`，那么你可以通过使用`torch.set_default_dtype(torch.float16)`将默认数据类型设置为`torch.float16`。请注意：这将对之后创建的所有张量和模块生效。
- en: 'However, the reduced precision can cause some issues during training. Indeed,
    16-bit floats have a limited *dynamic range* (i.e., the ratio between the largest
    and smallest positive representable values): the smallest positive representable
    value is about 0.00000006 (i.e., 6.0e^(–8)), while the largest is 65,504 (i.e.,
    ~6.5e⁴). This implies that any gradient update smaller than ~6.0e^(–8) will *underflow*,
    meaning it will be rounded down to zero, and thus ignored. And conversely, any
    value larger than ~6.5e⁴ will *overflow*, meaning it will be rounded up to infinity,
    causing training to fail (once some weights are infinite, the loss will be infinite
    or NaN).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，降低精度可能会在训练过程中引起一些问题。确实，16位浮点数有一个有限的*动态范围*（即最大和最小可表示的正值之间的比率）：最小的可表示的正值大约是0.00000006（即6.0e^(–8)），而最大的是65,504（即~6.5e⁴）。这意味着任何小于~6.0e^(–8)的梯度更新都会*下溢*，意味着它将被舍入到零，因此会被忽略。相反，任何大于~6.5e⁴的值都会*上溢*，意味着它将被舍入到无穷大，导致训练失败（一旦某些权重变为无穷大，损失将变为无穷大或NaN）。
- en: 'To avoid underflows, one solution is to scale up the loss by a large factor
    (e.g., multiply it by 256): this will automatically scale up the gradients by
    the same factor during the backward pass, which will prevent them from being smaller
    than the smallest 16-bit representable value. However, you must scale the gradients
    back down before performing an optimizer step, and at this point you may get an
    underflow. Also, if you scale up the loss too much, you will run into overflows.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免下溢，一种解决方案是将损失乘以一个很大的因子（例如，乘以256）：这将自动在反向传播期间将梯度乘以相同的因子，从而防止它们小于最小的16位可表示值。然而，在执行优化器步骤之前，你必须将梯度缩小回原来的规模，这时你可能会遇到下溢。此外，如果你将损失放大得太多，你可能会遇到上溢。
- en: 'If you can’t find a good scaling factor that avoids both underflows and overflows,
    you can try to use `torch.bfloat16` rather than `torch.float16`, since bloat16
    has more bits for the exponent: the smallest value is ~9.2e^(–41), while the largest
    is ~3.4e^(38), so there’s less risk of any significant gradient updates being
    ignored, or reasonable values being rounded up to infinity.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你找不到一个好的缩放因子来避免下溢和上溢，你可以尝试使用`torch.bfloat16`而不是`torch.float16`，因为bfloat16有更多的指数位：最小值约为~9.2e^(–41)，最大值约为~3.4e^(38)，因此不太可能忽略任何显著的梯度更新，或者将合理的值四舍五入到无穷大。
- en: 'However, bfloat16 has historically had less hardware support (although this
    is improving), and it offers fewer bits for the fraction, which can cause some
    gradient updates to be ignored when the corresponding parameter values are much
    larger, causing training to stall. For example, if the gradient update is 4.5e^(–2)
    (i.e., 0.045) and the corresponding parameter value is equal to 1.23e² (i.e.,
    123), then the sum should be 1.23045e² (i.e., 123.045) but bfloat16 does not have
    enough fraction bits to store all these digits, so it must round the result to
    1.23e² (i.e., 123): as you can see, the gradient update is completely ignored.
    With regular 16-bit floats, the result would be 123.0625, which is not exactly
    right due to floating-point precision errors, but at least the parameter makes
    a step in the right direction. That said, if the gradient update was a bit smaller
    (e.g., 0.03), it would be ignored even in regular 16-bit float precision.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，bfloat16在历史上硬件支持较少（尽管这种情况正在改善），并且它提供的分数位较少，这可能导致当相应的参数值非常大时，一些梯度更新被忽略，导致训练停滞。例如，如果梯度更新是4.5e^(–2)（即0.045）且相应的参数值等于1.23e²（即123），那么总和应该是1.23045e²（即123.045），但bfloat16没有足够的分数位来存储所有这些数字，因此它必须将结果四舍五入到1.23e²（即123）：正如你所见，梯度更新被完全忽略。使用常规的16位浮点数，结果将是123.0625，这并不完全正确，因为浮点精度错误，但至少参数向正确的方向迈出了一步。话虽如此，如果梯度更新稍微小一点（例如，0.03），即使在常规16位浮点精度下也会被忽略。
- en: So if you try float16 and bfloat16 but you still encounter convergence issues
    during training, then you can try *mixed-precision training* instead.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你尝试了float16和bfloat16，但在训练过程中仍然遇到收敛问题，那么你可以尝试使用*混合精度训练*。
- en: Mixed-Precision Training
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混合精度训练
- en: '[*Mixed-precision training* (MPT)](https://homl.info/mpt) was proposed by Baidu
    and Nvidia researchers in 2017,⁠^([4](app02.html#id4388)) to address the issues
    often observed with 16-bit training. Here’s how it works:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[*混合精度训练*（MPT）](https://homl.info/mpt)是由百度和Nvidia研究人员在2017年提出的，⁠^([4](app02.html#id4388))，旨在解决16位训练中经常遇到的问题。以下是它的工作原理：'
- en: MPT stores a primary copy of the model parameters as 32-bit floats, and at each
    training iteration, it creates a 16-bit copy of these model parameters (see step
    1 in [Figure B-2](#mpt_diagram)), and uses them for the forward pass (step 2).
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MPT将模型参数的主要副本存储为32位浮点数，并在每次训练迭代中创建这些模型参数的16位副本（参见[图B-2](#mpt_diagram)中的步骤1），并使用它们进行正向传递（步骤2）。
- en: The loss is then scaled up by a large factor (step 3) to avoid underflows, as
    we discussed earlier.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，损失被放大一个很大的因子（步骤3），以避免下溢，正如我们之前讨论的那样。
- en: 'Lastly, we switch back to 32-bit precision to scale the gradients back down:
    this greater precision avoids the risk of underflow. Next we use the gradients
    to perform one optimizer step, improving the primary parameters (step 5). Performing
    the actual optimizer step in 32-bit precision ensures that small weight updates
    are not ignored when applied to much larger parameter values, since 32-bit floats
    have a very large fraction (23 bits).'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们切换回32位精度以将梯度缩放回原来的大小：这种更高的精度可以避免下溢的风险。接下来，我们使用梯度执行一个优化器步骤，改进主要参数（步骤5）。在32位精度下执行实际的优化器步骤确保了当应用于较大的参数值时，小的权重更新不会被忽略，因为32位浮点数有非常大的分数部分（23位）。
- en: '![Diagram illustrating mixed-precision training, showing the process of copying
    32-bit parameters to 16-bit, performing forward and backward passes, scaling losses,
    and completing with an optimizer step.](assets/hmls_ab02.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![说明混合精度训练的流程图，展示了将32位参数复制到16位，执行正向和反向传递，缩放损失，以及完成优化器步骤的过程。](assets/hmls_ab02.png)'
- en: Figure B-2\. Mixed-precision training
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图B-2\. 混合精度训练
- en: MPT offers almost all of the benefits of 16-bit training, without the instabilities.
    However, the model parameters take 50% more space than in 32-bit training because
    of the 16-bit copy at each training iteration, so how is this any better? Well,
    during training, most of the RAM is used to store the activations, not the model
    parameters, so in practice MPT requires just a bit more than half the RAM used
    by regular 32-bit training. And it typically runs twice as fast. Moreover, once
    training is finished, we no longer need 32-bit parameters, we can convert them
    to 16 bits, and we get a pure 16-bit model.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: MPT提供了16位训练几乎所有的优点，而没有不稳定性。然而，由于每次训练迭代中的16位复制，模型参数比32位训练多占用50%的空间，那么这有什么好处呢？嗯，在训练过程中，大部分RAM用于存储激活，而不是模型参数，所以实际上MPT只需要比常规32位训练多一点点RAM。而且它通常运行速度快一倍。此外，一旦训练完成，我们就不再需要32位参数，我们可以将它们转换为16位，从而得到一个纯16位模型。
- en: Warning
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'MPT does not always accelerate training: it depends on the model, the batch
    size, and the hardware. That said, most large transformers are trained using MPT.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: MPT并不总是加速训练：这取决于模型、批量大小和硬件。尽管如此，大多数大型Transformer都是使用MPT进行训练的。
- en: 'Rather than finding the best scaling factor by trial and error, you can run
    training in 32-bit precision for a little while (assuming you have enough RAM)
    and measure the gradient statistics to find the optimal scaling factor for your
    task: it should be large enough to avoid underflows, and small enough to avoid
    overflows.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通过试错找到最佳缩放因子，你可以先以32位精度运行训练一段时间（假设你有足够的RAM），并测量梯度统计信息以找到适合你任务的最佳缩放因子：它应该足够大以避免下溢，同时足够小以避免上溢。
- en: 'Alternatively, your training script can adapt the factor dynamically during
    training: if some gradients are infinite or NaN, this means that an overflow occurred
    so the factor must be reduced (e.g., halved) and the training step must be skipped,
    but if no overflow is detected then the scaling factor can be gradually increased
    (e.g., doubled every 2,000 training steps). PyTorch provides a `torch.amp.GradScaler`
    class that implements this approach, and also scales down the learning rate appropriately.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你的训练脚本可以在训练过程中动态调整因子：如果某些梯度是无穷大或NaN，这意味着发生了溢出，因此必须减少（例如减半）因子，并跳过训练步骤，但如果未检测到溢出，则缩放因子可以逐渐增加（例如每2,000个训练步骤加倍）。PyTorch提供了一个`torch.amp.GradScaler`类来实现这种方法，并且还会适当地调整学习率。
- en: PyTorch also provides a `torch.autocast()` function that returns a context within
    which many operations will automatically run in 16-bit precision. This includes
    operations that typically benefit the most from 16-bit precision, such as matrix
    multiplication and convolutions, but it does not include operations like reductions
    (e.g., `torch.sum()`) since running these in half precision offers no significant
    benefit and can damage precision.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还提供了一个`torch.autocast()`函数，它返回一个上下文，在该上下文中许多操作将自动以16位精度运行。这包括通常从16位精度中受益最大的操作，如矩阵乘法和卷积，但不包括像
    reductions（例如`torch.sum()`）这样的操作，因为这些操作以半精度运行不会带来显著的好处，并且可能会损害精度。
- en: 'Let’s update our training function to run the forward pass within an autocast
    context and use a `GradScaler` to dynamically scale the loss:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新我们的训练函数，在autocast上下文中运行前向传递，并使用`GradScaler`动态缩放损失：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When fine-tuning a transformer using the Hugging Face Transformers library,
    you can set `fp16=True` or `bf16=True` in the `TrainingArguments` to activate
    mixed-precision training.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用Hugging Face Transformers库微调Transformer时，你可以在`TrainingArguments`中将`fp16=True`或`bf16=True`设置为激活混合精度训练。
- en: Reducing precision down to 16-bits often works great, but can we shrink our
    models even further? Yes, we can, using quantization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 将精度降低到16位通常效果很好，但我们能否进一步缩小我们的模型？是的，我们可以，使用量化技术。
- en: Quantization
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 量化
- en: 'Quantization means mapping continuous values to discrete ones. In deep learning,
    this typically involves converting parameters, and often activations as well,
    from floats to integers—usually 32-bit floats to 8-bit integers. More generally,
    the goal is to shrink and speed up our model by reducing the number of bits used
    in parameters, and often in activations as well. Moreover, some embedded devices
    (e.g., ARM Cortex-M0) do not support floating-point operations at all (in part
    to reduce their cost and energy consumption), so models have to be quantized entirely
    (both weights and activations) before you can use them on the device. Modern smartphones
    do support floating point operations but still benefit significantly from quantization:
    int8 operations are 2 to 4 times faster and use 5 to 10 times less energy than
    FP32.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 量化意味着将连续值映射到离散值。在深度学习中，这通常涉及将参数，以及通常激活，从浮点数转换为整数——通常是32位浮点数到8位整数。更普遍地，目标是通过减少参数（以及通常激活）中使用的位数来缩小和加速我们的模型。此外，一些嵌入式设备（例如，ARM
    Cortex-M0）根本不支持浮点运算（部分是为了降低成本和能耗），因此模型在可以在设备上使用之前必须完全量化（包括权重和激活）。现代智能手机支持浮点运算，但仍然可以从量化中受益很大：int8运算比FP32快2到4倍，并且能耗低5到10倍。
- en: The simplest approach is *linear quantization*, so we’ll discuss it now. We
    will discuss a few nonlinear quantization methods later in this appendix.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是 *线性量化*，因此我们现在将讨论它。我们将在本附录的后面讨论一些非线性量化方法。
- en: Linear Quantization
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性量化
- en: 'Linear quantization dates back to digital signal processing in the 1950s, but
    it has become particularly important in machine learning over the past decade
    since models have become gigantic, and yet we wish to run them on mobile phones
    and other limited devices. It has two variants: asymmetric and symmetric. In *asymmetric
    linear quantization*, float values are simply mapped linearly to unsigned bytes
    with values ranging from 0 to 255 (or more generally from 0 to 2^(*n*) – 1 when
    quantizing to *n*-bit integers). For example, if the weights range between *a*
    = –0.1 and *b* = 0.6, then the float –0.1 will be mapped to the byte 0, the float
    0.0 to integer 36, 0.1 to 72, …​, 0.6 to 255, and more generally, the float tensor
    **w** will be mapped to the integer tensor **q** using [Equation B-1](#asymmetric_linear_quantization_equation).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 线性量化可以追溯到20世纪50年代的数字信号处理，但自从过去十年模型变得巨大，而我们又希望在手机和其他有限设备上运行它们，它变得尤为重要。它有两种变体：非对称和对称。在
    *非对称线性量化* 中，浮点值被简单地线性映射到无符号字节，其值范围从0到255（或者更普遍地，当量化到 *n*-位整数时，从0到2^(*n*) – 1）。例如，如果权重范围在
    *a* = –0.1 和 *b* = 0.6 之间，那么浮点数 –0.1 将被映射到字节 0，浮点数 0.0 将被映射到整数 36，0.1 到 72，…，0.6
    到 255，并且更普遍地，浮点张量 **w** 将被映射到整数张量 **q**，使用 [方程 B-1](#asymmetric_linear_quantization_equation)。
- en: Equation B-1\. Asymmetric linear quantization
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 B-1\. 非对称线性量化
- en: $StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis
    StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus
    z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript
    n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction
    a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals
    min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript
    i Endscripts w Subscript i Baseline EndLayout$
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: $StartLayout 1st Row 1st Column q Subscript i 2nd Column equals round left-parenthesis
    StartFraction w Subscript i Baseline Over s EndFraction right-parenthesis plus
    z 2nd Row 1st Column with s 2nd Column equals StartFraction b minus a Over 2 Superscript
    n Baseline minus 1 EndFraction and z equals minus round left-parenthesis StartFraction
    a Over s EndFraction right-parenthesis 3rd Row 1st Column where a 2nd Column equals
    min Underscript i Endscripts w Subscript i Baseline and b equals max Underscript
    i Endscripts w Subscript i Baseline EndLayout$
- en: 'In this equation:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*w*[*i*] is the *i*^(th) float in the original tensor **w**.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*i*] 是原始张量 **w** 中的第 *i* 个浮点数。'
- en: '*q*[i] is the *i*^(th) integer in the quantized tensor **q**. It is clamped
    between 0 and 2^(*n*) – 1 (e.g., 255 for 8-bit quantization).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*q*[i] 是量化张量 **q** 中的第 *i* 个整数。它被限制在 0 和 2^(*n*) – 1 之间（例如，对于8位量化，为255）。'
- en: '*s* is the *quantization scale*. Note that some authors define it as 1 / *s*
    and adapt the equation accordingly (i.e., they multiply rather than divide).'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* 是 *量化比例*. 注意，一些作者将其定义为 1 / *s* 并相应地调整方程（即，他们乘以而不是除以）。'
- en: '*z* is the *quantization bias* or *zero point*.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z* 是 *量化偏置* 或 *零点*。'
- en: '*a* is the minimum value of **w**, and *b* is the maximum value of **w**.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是 **w** 的最小值，而 *b* 是 **w** 的最大值。'
- en: The range [*a*, *b*] is known for weights, since their values do not change
    after training. However, the range of activation values depends on the inputs
    we feed to the model. As a result, for each activation that we want to quantize
    (e.g., the inputs of each layer), we will either have to compute *a* and *b* on
    the fly for each new input batch (this is called *dynamic quantization*) or run
    a calibration dataset once through the model to determine the typical range of
    activation values, then use this range to quantize the activations of all subsequent
    batches (this is called *static quantization*). Static quantization is a faster
    but less precise.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于权重，[*a*，*b*] 的范围是已知的，因为它们的值在训练后不会改变。然而，激活值的范围取决于我们输入到模型中的输入。因此，对于我们要量化的每个激活（例如，每层的输入），我们可能必须为每个新的输入批次动态计算
    *a* 和 *b*（这称为 *动态量化*）或者运行一个校准数据集一次通过模型来确定激活值的典型范围，然后使用这个范围来量化所有后续批次的激活（这称为 *静态量化*）。静态量化更快但精度较低。
- en: 'To approximately recover the original value *w*[*i*] from a quantized value
    *q*[*i*], we can compute *w*[*i*] ≈ s × (*q*[*i*] – *z*). This is called *dequantization*.
    For example, if *q*[*i*] = 72, then we get *w*[*i*] ≈ 0.0988, which is indeed
    close to 0.1\. The difference between the dequantized value (0.0988) and the original
    value (0.1) is called the *quantization noise*: with 8-bit quantization, the quantization
    noise usually leads to a slightly degraded accuracy. With 6-bit, 4-bit, or less,
    the quantization noise can hurt even more, especially since it has a cumulative
    effect: the deeper the network, the stronger the impact.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 要从量化值 *q*[*i*] 近似恢复原始值 *w*[*i*]，我们可以计算 *w*[*i*] ≈ s × (*q*[*i*] – *z*)。这被称为
    *反量化*。例如，如果 *q*[*i*] = 72，那么我们得到 *w*[*i*] ≈ 0.0988，这确实接近 0.1。反量化值（0.0988）与原始值（0.1）之间的差异称为
    *量化噪声*：在 8 位量化中，量化噪声通常会导致略微降低的精度。在 6 位、4 位或更少的情况下，量化噪声可能会造成更大的伤害，尤其是因为它具有累积效应：网络越深，影响越强。
- en: Note
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Equation B-1](#asymmetric_linear_quantization_equation) guarantees that any
    float equal to 0.0 can be quantized and dequantized back to 0.0 exactly: indeed,
    if *w*[*i*] = 0.0 then *q*[*i*] = *z*, and dequantizing *q*[*i*] gives back *w*[*i*]
    = *s* × (*z* – *z*) = 0.0\. This is particularly useful for sparse models where
    many weights are equal to zero. It is also important when using activations like
    ReLU which produce many zero activations.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程 B-1](#asymmetric_linear_quantization_equation) 保证任何等于 0.0 的浮点数都可以精确地量化并反量化回
    0.0：确实，如果 *w*[*i*] = 0.0，则 *q*[*i*] = *z*，反量化 *q*[*i*] 会得到 *w*[*i*] = *s* × (*z*
    – *z*) = 0.0。这对于许多权重为零的稀疏模型特别有用。当使用 ReLU 等产生许多零激活的激活函数时，这也非常重要。'
- en: 'In PyTorch, the `torch.quantize_per_tensor()` function lets you create a quantized
    tensor: this is a special kind of tensor that contains the quantized values (i.e.,
    integers), as well as the *quantization parameters* (i.e., the scale and zero
    point). Let’s use this function to quantize a tensor, then dequantize it. In this
    example we will use the data type `torch.quint8`, which uses 8-bit unsigned integers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，`torch.quantize_per_tensor()` 函数允许你创建一个量化张量：这是一种特殊的张量，它包含量化值（即整数）以及量化参数（即缩放和零点）。让我们使用这个函数来量化一个张量，然后对其进行反量化。在这个例子中，我们将使用数据类型
    `torch.quint8`，它使用 8 位无符号整数：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Quantizing a model to 8-bits divides its size by almost 4\. For example, suppose
    we have a convolutional layer with 64 kernels, 3 × 3 each, and it has 32 input
    channels. This layer requires 64 × 32 × 3 × 3 = 18,432 parameters (ignoring the
    bias terms). That’s 18,432 × 4 = 73,728 bytes before quantization, and just 18,432
    bytes after quantization, plus 2 × 4 = 8 bytes to store *s* and *z* (indeed, they
    are both stored as 32-bit floats, so 4 bytes each).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将模型量化为 8 位将几乎将其大小缩小 4 倍。例如，假设我们有一个具有 64 个核的卷积层，每个核大小为 3 × 3，并且它有 32 个输入通道。这个层需要
    64 × 32 × 3 × 3 = 18,432 个参数（忽略偏置项）。在量化之前，这是 18,432 × 4 = 73,728 字节，量化后仅为 18,432
    字节，加上 2 × 4 = 8 字节来存储 *s* 和 *z*（实际上，它们都存储为 32 位浮点数，所以每个 4 字节）。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'PyTorch also has a `torch.quantize_per_channel()` function which quantizes
    each channel separately: this offers better precision but requires a bit more
    space for the additional quantization parameters.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 还有一个 `torch.quantize_per_channel()` 函数，它可以将每个通道分别量化：这提供了更好的精度，但需要更多的空间来存储额外的量化参数。
- en: When the float values are approximately symmetric around zero, we can use *symmetric
    linear quantization*, where the values are mapped between –127 and +127, or more
    generally between –*r* and +*r* with *r* = 2^(*n*–1) – 1, using [Equation B-2](#symmetric_linear_quantization_equation).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当浮点值在零点附近大致对称时，我们可以使用*对称线性量化*，其中值映射到 –127 和 +127 之间，或者更一般地，映射到 –*r* 和 +*r* 之间，其中
    *r* = 2^(*n*–1) – 1，使用 [方程 B-2](#symmetric_linear_quantization_equation)。
- en: Equation B-2\. Symmetric linear quantization
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 B-2\. 对称线性量化
- en: $q Subscript i Baseline equals round left-parenthesis StartFraction w Subscript
    i Baseline Over s EndFraction right-parenthesis with s equals StartFraction max
    Underscript i Endscripts StartAbsoluteValue w Subscript i Baseline EndAbsoluteValue
    Over 2 Superscript n minus 1 Baseline minus 1 EndFraction$
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: $q Subscript i Baseline equals round left-parenthesis StartFraction w Subscript
    i Baseline Over s EndFraction right-parenthesis with s equals StartFraction max
    Underscript i Endscripts StartAbsoluteValue w Subscript i Baseline EndAbsoluteValue
    Over 2 Superscript n minus 1 Baseline minus 1 EndFraction$
- en: 'To implement symmetric linear quantization in PyTorch, we can use the `torch.quantize_per_tensor()`
    function again, but using a zero point equal to 0, and data type `qint8` (quantized
    signed 8-bit integer):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 PyTorch 中实现对称线性量化，我们可以再次使用 `torch.quantize_per_tensor()` 函数，但使用零点等于 0，数据类型
    `qint8`（量化有符号 8 位整数）：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Figure B-3](#symmetric_quantization_diagram) shows some floats ranging between
    –0.94 and +0.93, quantized to signed bytes (i.e., 8-bits) ranging between –127
    and +127,⁠^([5](app02.html#id4409)) using symmetric linear quantization. Notice
    that float 0.0 is always mapped to integer 0.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 B-3](#symmetric_quantization_diagram) 显示了一些介于 –0.94 和 +0.93 之间的浮点数，使用对称线性量化量化为有符号字节（即
    8 位），范围在 –127 到 +127 之间，⁠^([5](app02.html#id4409))。注意，浮点数 0.0 总是映射到整数 0。'
- en: '![Diagram showing the mapping of weights from floats between -0.94 and 0.93
    to quantized bytes ranging from -127 to 127 using symmetric linear quantization.](assets/hmls_ab03.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![显示使用对称线性量化将浮点值从 -0.94 到 0.93 映射到从 -127 到 127 的量化字节的图。](assets/hmls_ab03.png)'
- en: Figure B-3\. Symmetric linear quantization
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 B-3\. 对称线性量化
- en: Symmetric mode is often a bit faster than asymmetric mode, because there’s no
    zero point *z* to worry about. However, if the values are not symmetric, part
    of the integer range will be wasted. For example, if all the weights are positive,
    then symmetric mode will only use bytes 0 to 127 (rather than –127 to 127). As
    a result, symmetric mode can be a bit less precise than asymmetric mode. In practice,
    symmetric mode is generally preferred for weights (which are often fairly symmetric),
    and asymmetric mode for activations (especially when using ReLU, since it outputs
    only nonnegative values).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对称模式通常比非对称模式快一点，因为没有需要担心的零点 *z*。然而，如果值不是对称的，整数范围的一部分将被浪费。例如，如果所有权重都是正的，那么对称模式将只使用字节
    0 到 127（而不是 –127 到 127）。因此，对称模式可能比非对称模式精度略低。在实践中，对称模式通常更适用于权重（权重通常相当对称），而非对称模式适用于激活（特别是当使用
    ReLU 时，因为它只输出非负值）。
- en: 'Let’s now see how to quantize your models in practice using PyTorch’s `torch.​ao.quantization`
    package. The first approach is to quantize a trained model, which is called *post-training
    quantization* (PTQ). The second is to train (or fine-tune) your model with some
    fake quantization to get it used to the noise: this is called *quantization-aware
    training* (QAT). Let’s start with PTQ.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看如何使用 PyTorch 的 `torch.ao.quantization` 包在实际中对你的模型进行量化。第一种方法是量化一个训练好的模型，这被称为*后训练量化*（PTQ）。第二种是使用一些模拟量化来训练（或微调）你的模型，使其习惯噪声：这被称为*量化感知训练*（QAT）。让我们从
    PTQ 开始。
- en: Post-Training Quantization Using torch.ao.quantization
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 torch.ao.quantization 进行后训练量化
- en: 'The `torch.ao` package contains tools for architecture optimization (hence
    the name), including pruning, sparsity, and quantization. The `torch.ao.quantization`
    package offers two solutions to quantize trained models: dynamic quantization
    and static quantization. Let’s see how to implement both.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.ao` 包包含用于架构优化的工具（因此得名），包括剪枝、稀疏化和量化。`torch.ao.quantization` 包提供了两种量化训练模型的方法：动态量化和静态量化。让我们看看如何实现这两种方法。'
- en: Dynamic quantization
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态量化
- en: 'Dynamic quantization is best for MLPs, RNNs, and transformers. To implement
    it using PyTorch’s `torch.ao.quantization` package, you must first choose a quantization
    engine: PyTorch currently supports the *Facebook General Matrix Multiplication*
    (FBGEMM) engine for x86 CPUs, plus a newer x86 engine that supports recent x86
    CPUs but is less battle-tested, and finally the *Quantized Neural Networks Package*
    (QNNPACK) engine for ARM/mobile. This code will pick the appropriate engine depending
    on the platform:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 动态量化最适合MLPs、RNNs和transformers。要使用PyTorch的`torch.ao.quantization`包实现它，您必须首先选择一个量化引擎：PyTorch目前支持用于x86
    CPU的*Facebook通用矩阵乘法*（FBGEMM）引擎，以及一个支持最新x86 CPU但经验较少的新x86引擎，最后是用于ARM/移动的*量化神经网络包*（QNNPACK）引擎。此代码将根据平台选择适当的引擎：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: PyTorch does not offer an engine for CUDA or other hardware accelerators, but
    other libraries do, such as the bitsandbytes library (as we will see shortly).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch不提供CUDA或其他硬件加速器的引擎，但其他库提供了，例如bitsandbytes库（我们很快就会看到）。
- en: 'Once you have selected an engine, you can use the `quantize_dynamic()` function
    from the `torch.ao.quantization` package; just pass it your trained model, tell
    it the types of layers to quantize (typically just the `Linear` and RNN layers),
    specify the quantized data type, and boom, you have a ready-to-use quantized model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您选择了引擎，您就可以使用`torch.ao.quantization`包中的`quantize_dynamic()`函数；只需传递您的训练模型，告诉它要量化的层类型（通常是`Linear`和RNN层），指定量化数据类型，然后您就有一个现成的量化模型了：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `quantize_dynamic()` function replaces each `Linear` layer with a `DynamicQuantizedLinear`
    layer, with int8 weights. This layer behaves just like a regular linear layer,
    with float inputs and outputs, but it quantizes its inputs on the fly (recomputing
    the zero points and scales for each batch), performs matrix multiplication using
    integers only (with 32-bit integer accumulators), and dequantizes the result so
    the next layer gets float inputs. Now let’s look at static quantization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`quantize_dynamic()`函数将每个`Linear`层替换为具有int8权重的`DynamicQuantizedLinear`层。这个层的行为就像一个常规线性层一样，具有浮点输入和输出，但它会动态量化其输入（为每个批次重新计算零点和比例），仅使用整数执行矩阵乘法（使用32位整数累加器），并将结果反量化，以便下一层获得浮点输入。现在让我们看看静态量化。'
- en: Static quantization
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 静态量化
- en: 'This option is best for CNNs, and max inference speed. It’s also compulsory
    for edge devices without a *floating-point unit* (FPU), as they don’t support
    floats at all. Both the weights and activations are prepared for quantization
    ahead of time, for all layers. As we discussed earlier, weights are constant so
    they can be quantized once, while activations require a calibration step to determine
    their typical range. The model is then converted to a fully quantized model. Here
    is how to implement it:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项最适合CNNs，并且具有最快的推理速度。它对于没有*浮点单元*（FPU）的边缘设备也是强制性的，因为它们根本不支持浮点数。权重和激活都在提前为所有层准备好的，以进行量化。正如我们之前讨论的，权重是常数，因此可以一次性量化，而激活需要校准步骤来确定它们的典型范围。然后，模型被转换为完全量化的模型。以下是实现方法：
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s go through this code step by step:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分析这段代码：
- en: After the imports, we create our 32-bit model, but this time we add a `QuantStub`
    layer as the first layer, and a `DeQuantStub` layer as the last. Both layers are
    just passthrough for now.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在导入之后，我们创建了一个32位模型，但这次我们在第一个层添加了一个`QuantStub`层，在最后一个层添加了一个`DeQuantStub`层。这两个层目前只是简单的透传层。
- en: Next, the model can be trained normally (another option would be to take a pretrained
    model and place it between a `QuantStub` layer and a `DeQuantStub` layer).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，模型可以正常训练（另一种选择是使用预训练模型，并将其放置在`QuantStub`层和`DeQuantStub`层之间）。
- en: 'Next, we set the model’s `qconfig` to the output of the `get_default_qconfig()`
    function: this function takes the name of the desired quantization engine and
    returns a `QConfig` object containing a default quantization configuration for
    this engine. It specifies the quantization data type (e.g., `torch.qint8`), the
    quantization scheme (e.g., symmetric linear quantization per tensor), and two
    functions that will observe the weights and activations to determine their ranges.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们将模型的`qconfig`设置为`get_default_qconfig()`函数的输出：这个函数接受所需量化引擎的名称，并返回一个包含该引擎默认量化配置的`QConfig`对象。它指定了量化数据类型（例如，`torch.qint8`）、量化方案（例如，每个张量的对称线性量化）以及两个函数，这些函数将观察权重和激活以确定它们的范围。
- en: 'Next we call the `torch.ao.quantization.prepare()` function: it uses the weight
    observer specified in the configuration to determine the weights range, which
    it immediately uses to compute the zero points and scales for the weights. Since
    we don’t know what the input data looks like at this point, the function cannot
    compute the quantization parameters for the activations yet, so it inserts activation
    observers in the model itself: these are attached to the outputs of the `QuantStub`
    and `Linear` layers. The observer appended to the `QuantStub` layer is responsible
    for tracking the input range.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们调用 `torch.ao.quantization.prepare()` 函数：它使用配置中指定的权重观察者来确定权重范围，然后立即使用该范围来计算权重的零点和比例。由于我们此时不知道输入数据的形状，该函数还不能计算激活的量化参数，因此它在模型本身中插入激活观察者：这些观察者附加到
    `QuantStub` 和 `Linear` 层的输出。附加到 `QuantStub` 层的观察者负责跟踪输入范围。
- en: 'Next, we take a representative sample of input batches (i.e., the kind the
    model will get in production), and we pass these batches through the model: this
    allows the activation observers to track the activations.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们取输入批次的一个代表性样本（即模型在生产中将获得的类型），并将这些批次通过模型传递：这允许激活观察者跟踪激活。
- en: Once we have given the model enough data, we finally call the `torch.ao.​quanti⁠zation.convert()`
    function, which removes the observers from the model and replaces the layers with
    quantized versions. The `QuantStub` layer is replaced with a `Quantize` layer
    which will quantize the inputs. The `Linear` layers are replaced with `QuantizedLinear`
    layers. And the `DeQuantStub` layer is replaced with a `DeQuantize` layer which
    will dequantize the outputs.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们给模型提供了足够的数据，我们最终调用 `torch.ao.quantization.convert()` 函数，该函数从模型中移除观察者，并用量化版本替换层。`QuantStub`
    层被替换为 `Quantize` 层，该层将量化输入。`Linear` 层被替换为 `QuantizedLinear` 层。`DeQuantStub` 层被替换为
    `DeQuantize` 层，该层将反量化输出。
- en: Note
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are a few observers to choose from: they can just keep track of the minimum
    and maximum values for each tensor (`MinMaxObserver`), or for each channel (`PerChannelMinMaxObserver`),
    or they can compute an exponential moving average of the min/max values, which
    reduces the impact of a few outliers. Finally, they can even record a histogram
    of the observed values (`HistogramObserver`), making it possible to find an optimal
    quantization range that minimizes the quantization error. That said, the default
    observers are usually fine.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个观察者可供选择：它们可以仅跟踪每个张量（`MinMaxObserver`）的最小和最大值，或者每个通道（`PerChannelMinMaxObserver`）的最小和最大值，或者它们可以计算最小/最大值的指数移动平均值，这可以减少少数异常值的影响。最后，它们甚至可以记录观察值的直方图（`HistogramObserver`），这使得找到最优的量化范围成为可能，从而最小化量化误差。尽管如此，默认的观察者通常就足够好了。
- en: 'We now have a model that we can use normally, with float inputs and outputs,
    but which works entirely with integers internally, making it lightweight and fast.
    To deploy it to mobile or embedded devices, there are many options to choose from
    (which are beyond the scope of this book), including:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个可以正常使用的模型，它具有浮点输入和输出，但完全使用整数内部工作，使其轻量级且快速。要将它部署到移动或嵌入式设备，有许多选择（这些内容超出了本书的范围），包括：
- en: Use ExecuTorch, which is PyTorch’s lightweight edge runtime
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ExecuTorch，这是 PyTorch 的轻量级边缘运行时
- en: Export the model to ONNX and run it with ONNX Runtime (cross-platform)
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型导出为 ONNX 并使用 ONNX Runtime 运行（跨平台）
- en: Convert it to TFLite or TFLite Micro
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其转换为 TFLite 或 TFLite Micro
- en: Compile it for the target device using TVM or microTVM
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TVM 或 microTVM 为目标设备编译
- en: Moreover, the PyTorch team has released a separate library named [*PyTorch-native
    Architecture Optimization* (TorchAO)](https://homl.info/torchao), designed to
    be a robust and extensible model optimization framework. Over time, many features
    in PyTorch’s `torch.ao` package are expected to be migrated to—or superseded by—TorchAO.
    The library already includes advanced features such as 4-bit weight support and
    *per-block quantization*, in which each tensor is split into small blocks and
    each block is quantized independently, trading space for improved precision.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，PyTorch 团队发布了一个名为 [*PyTorch-native Architecture Optimization* (TorchAO)](https://homl.info/torchao)
    的独立库，旨在成为一个健壮且可扩展的模型优化框架。随着时间的推移，PyTorch 的 `torch.ao` 包中的许多功能预计将被迁移到或由 TorchAO
    取代。该库已经包括高级功能，如 4 位权重支持和 *按块量化*，其中每个张量被分成小块，每个块独立量化，以空间换取精度提升。
- en: Post-training quantization (either dynamic or static) can shrink and speed up
    your models significantly, but it will also degrade their accuracy. This is particularly
    the case when quantizing down to 4 bits or less, and it’s worse for static quantization
    than for dynamic quantization (which can at least adapt to each input batch independently).
    When the accuracy drop is unacceptable, you can try quantization-aware training,
    as we will discuss now.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后量化（无论是动态还是静态）可以显著缩小和加速你的模型，但也会降低它们的精度。这尤其适用于量化到4位或更少的情况，并且静态量化比动态量化更糟糕（动态量化至少可以独立地适应每个输入批次）。当精度下降无法接受时，你可以尝试量化感知训练，正如我们现在将要讨论的。
- en: Quantization-Aware Training (QAT)
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 量化感知训练（Quantization-Aware Training, QAT）
- en: 'QAT was introduced in a [2017 paper](https://homl.info/qat) by Google researchers.⁠^([6](app02.html#id4433))
    It rests upon a simple idea: why not introduce some fake quantization noise during
    training so the model can learn to cope with it? After training, we can then quantize
    the model for real, and it should remain fairly accurate. QAT also makes it possible
    to quantize more aggressively without losing too much accuracy, down to 4 bits,
    or even less. Sound promising? Let’s see how it can be done.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: QAT是由谷歌研究人员在2017年的一篇[论文](https://homl.info/qat)中引入的。⁠^([6](app02.html#id4433))
    它基于一个简单的想法：为什么不在训练期间引入一些伪量化噪声，让模型学会应对它？训练完成后，我们可以对模型进行真正的量化，并且它应该保持相当准确。QAT还使得可以更激进地进行量化而不会损失太多精度，低至4位，甚至更少。听起来很有希望？让我们看看它是如何实现的。
- en: 'To add fake quantization noise to weights, we can simply quantize them and
    immediately dequantize them. For example, a weight equal to 0.42 might be quantized
    to the 4-bit integer 7, and immediately dequantized back to 0.39: we’ve successfully
    introduced quantization noise, and it’s precisely the quantization noise that
    we would get if the model were really quantized. This fake quantization operation
    can be executed at each training step, and it can also be applied to some of the
    activations (e.g., to each layer output).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要向权重添加伪量化噪声，我们可以简单地量化它们，然后立即反量化。例如，一个等于0.42的权重可能被量化为4位整数7，然后立即反量化回0.39：我们已经成功引入了量化噪声，这正是如果模型真正量化时我们会得到的量化噪声。这种伪量化操作可以在每个训练步骤执行，也可以应用于一些激活（例如，每个层的输出）。
- en: 'However, there is one little problem: quantization involves rounding to the
    nearest integer, and the rounding operation has gradients equal to zero (or undefined
    at integer boundaries), so gradient descent cannot make any progress. Luckily,
    we can sidestep this issue by using the *straight-through estimator* (STE) trick:
    during the backward phase, we pretend that the fake quantization operation was
    just the identity function, so the gradients flow straight through it untouched.
    This works because the loss landscape is generally fairly smooth locally, so gradients
    are likely to be similar within a small region around the quantized value, including
    at the original value.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有一个小问题：量化涉及四舍五入到最接近的整数，而四舍五入操作的梯度为零（或在整数边界上未定义），因此梯度下降无法取得任何进展。幸运的是，我们可以通过使用*直通估计器*（STE）技巧来规避这个问题：在反向传播阶段，我们假装伪量化操作只是恒等函数，所以梯度直接通过它而不受影响。这之所以有效，是因为损失地形在局部通常是相当平滑的，因此梯度在量化值周围的小区域内很可能是相似的，包括原始值。
- en: 'Implementing QAT in PyTorch is fairly straightforward:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现QAT相当直接：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: After the import, we create our model, set its `qconfig` attribute to the default
    QAT configuration object for the chosen quantization engine, then we call the
    `prepare_qat()` function to add fake quantization operations to the model. This
    step also adds observers to determine the usual range of activation values. Next,
    we can train the model normally. Lastly, we switch the model to eval mode, and
    we call the `convert()` function to truly quantize it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 导入之后，我们创建我们的模型，将其`qconfig`属性设置为所选量化引擎的默认QAT配置对象，然后我们调用`prepare_qat()`函数向模型添加伪量化操作。这一步还会添加观察者以确定激活值的正常范围。接下来，我们可以正常训练模型。最后，我们将模型切换到评估模式，并调用`convert()`函数来真正量化它。
- en: Tip
  id: totrans-125
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'QAT doesn’t have to be used during all of training: you can take a pretrained
    model and just fine-tune it for a few epochs using QAT, typically using a lower
    learning rate to avoid damaging the pretrained weights.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: QAT不需要在整个训练过程中使用：你可以使用预训练模型，仅使用QAT对其进行几轮微调，通常使用较低的学习率以避免损坏预训练权重。
- en: 'We’ve seen how to implement PTQ and QAT using PyTorch’s `torch.ao` package.
    However, it’s primarily designed for CPUs. What if you want to run an LLM on a
    GPU that doesn’t quite have enough RAM? One option is to use the TorchAO library,
    which has growing GPU support. Another is to use the bitsandbytes library: let’s
    discuss it now.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何使用PyTorch的`torch.ao`包实现PTQ和QAT。然而，它主要是为CPU设计的。如果你想在内存不太够的GPU上运行LLM怎么办？一个选项是使用具有日益增长的GPU支持的TorchAO库。另一个选项是使用bitsandbytes库：让我们现在讨论它。
- en: Quantizing LLMs Using the bitsandbytes Library
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用bitsandbytes库对LLMs进行量化
- en: 'The bitsandbytes library (bnb), created by Tim Dettmers, is designed to make
    it easier to train and run large models on GPUs with limited VRAM. For this, it
    offers:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由Tim Dettmers创建的bitsandbytes库（bnb）旨在使在有限VRAM的GPU上训练和运行大型模型变得更加容易。为此，它提供了以下功能：
- en: Quantization tools, including 4-bit quantization, block-wise quantization, and
    more
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 量化工具，包括4位量化、块量化等
- en: Memory-efficient versions of popular optimizers such as Adam or AdamW, that
    operate on 8-bit tensors
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存高效的流行优化器版本，如Adam或AdamW，它们在8位张量上操作
- en: Custom CUDA kernels written specifically for 8-bit or 4-bit quantized models,
    for maximum speed
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为8位或4位量化模型专门编写的自定义CUDA内核，以实现最大速度
- en: Warning
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The bitsandbytes library is designed for Nvidia GPUs. It also has some limited
    support for CPUs and AMD GPUs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: bitsandbytes库是为Nvidia GPU设计的。它还对CPU和AMD GPU提供了一些有限的支持。
- en: 'For example, let’s see how to implement post-training static quantization down
    to 4 bits. If you are using Colab, you must first install the bitsandbytes library
    using `%pip install bitsandbytes`, then run this code:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看如何实现将训练后的静态量化降低到4位。如果你使用Colab，你必须首先使用`%pip install bitsandbytes`安装bitsandbytes库，然后运行以下代码：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This code starts by importing the necessary classes from the Transformers library
    (introduced in [Chapter 14](ch14.html#nlp_chapter)), then it creates a `BitsAndBytesConfig`
    object, which I will explain shortly. Lastly, it downloads a pretrained model
    (in this case a 1.1 billion parameter version of Llama named TinyLlama, fine-tuned
    for chat), specifying the desired quantization configuration.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码首先从Transformers库（在第14章中介绍）导入必要的类，然后创建一个`BitsAndBytesConfig`对象，我将在稍后解释。最后，它下载了一个预训练模型（在这种情况下是一个名为TinyLlama的11亿参数版本的Llama，用于聊天微调），并指定了所需的量化配置。
- en: 'Under the hood, the Transformers library uses the bitsandbytes library to quantize
    the model weights down to 4 bits just as they are loaded into the GPU: no extra
    step is required. You can now use this model normally to generate text (see [Chapter 15](ch15.html#transformer_chapter)).
    During inference, whenever some weights are needed, they are dequantized on the
    fly to the type specified by the `bnb_4bit_compute_dtype` argument (`bfloat16`
    in this case), and the computations are performed in this higher precision. As
    soon as the dequantized weights are no longer needed, they are dropped, so memory
    usage remains low.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Transformers库使用bitsandbytes库将模型权重量化到4位，就像它们被加载到GPU上一样：不需要额外的步骤。你现在可以使用这个模型正常生成文本（见第15章）。在推理过程中，每当需要某些权重时，它们会即时反量化到由`bnb_4bit_compute_dtype`参数指定的类型（在这种情况下为`bfloat16`），并且计算以更高的精度执行。一旦不再需要反量化后的权重，它们就会被丢弃，因此内存使用保持较低。
- en: 'In this example, the `BitsAndBytesConfig` object specifies *4-bit Normal Float*
    (NF4) quantization using `bfloat16` for computations. NF4 is a nonlinear 4-bit
    scheme where each of the 16 possible integer values represents a specific float
    value between –1 and +1\. Instead of being equally spaced (as in linear quantization),
    these values correspond to the quantiles of the normal distribution centered on
    zero: this means that they are closer together near zero. This improves accuracy
    because model weights tend to follow a normal distribution centered on zero, so
    having more precision near zero is helpful.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`BitsAndBytesConfig`对象指定使用`bfloat16`进行计算的*4位正常浮点数*（NF4）量化。NF4是一种非线性4位方案，其中16个可能的整数值中的每一个代表-1和+1之间的特定浮点值。与线性量化中均匀分布不同，这些值对应于以零为中心的正态分布的分位数：这意味着它们在零附近更接近。这提高了准确性，因为模型权重往往遵循以零为中心的正态分布，因此在零附近有更多的精度是有帮助的。
- en: 'NF4 was introduced as part of [QLoRA](https://homl.info/qlora),⁠^([7](app02.html#id4445))
    a technique that quantizes a frozen pretrained model with NF4, then uses LoRA
    adapters (see [Chapter 17](ch17.html#speedup_chapter)) for fine-tuning, along
    with activation checkpointing (see [Chapter 12](ch12.html#cnn_chapter)). This
    approach drastically reduces VRAM usage and compute: the authors managed to fine-tune
    a 65-billion parameter model using a single GPU with 48 GB of RAM, with only a
    small accuracy drop. Although activation checkpointing reduces VRAM usage overall,
    it can lead to memory spikes when processing batches with long sequences. To deal
    with such spikes, the QLoRA authors also introduced *paged optimizers* which take
    advantage of Nvidia unified memory: the CUDA driver automatically moves pages
    of data from GPU VRAM to CPU RAM whenever needed. Lastly, the authors also used
    *double quantization*, meaning that the quantization parameters themselves were
    quantized to save a bit more VRAM.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: NF4作为[QLoRA](https://homl.info/qlora)⁠^([7](app02.html#id4445))的一部分被引入，这是一种量化冻结预训练模型的方法，然后使用LoRA适配器（见[第17章](ch17.html#speedup_chapter)）进行微调，以及激活检查点（见[第12章](ch12.html#cnn_chapter)）。这种方法大大减少了VRAM使用量和计算量：作者成功使用单个具有48GB
    RAM的GPU微调了一个65亿参数的模型，只略有精度下降。尽管激活检查点总体上减少了VRAM使用量，但它会导致处理长序列批次时的内存峰值。为了处理这种峰值，QLoRA作者还引入了*分页优化器*，它利用Nvidia统一内存：CUDA驱动程序在需要时自动将数据页从GPU
    VRAM移动到CPU RAM。最后，作者还使用了*双重量化*，这意味着量化参数本身也被量化，以节省更多VRAM。
- en: For more details on 4-bit quantization in the Hugging Face ecosystem, check
    out this [great post by the QLoRA authors and other contributors](https://huggingface.co/blog/4bit-transformers-bitsandbytes).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Hugging Face生态系统中4位量化更详细的介绍，请查看QLoRA作者和其他贡献者写的这篇[优秀文章](https://huggingface.co/blog/4bit-transformers-bitsandbytes)。
- en: Using Pre-Quantized Models
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预量化模型
- en: 'Many popular pretrained models have already been quantized and published online,
    in particular on the Hugging Face Hub. For example, Tom Jobbins, better known
    by his Hugging Face username TheBloke, has published thousands of quantized models
    available at [*https://huggingface.co/TheBloke*](https://huggingface.co/TheBloke).
    Many of these models were quantized using one of the following modern methods:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 许多流行的预训练模型已经量化并发布在网上，特别是在Hugging Face Hub上。例如，Tom Jobbins，更广为人知的是他的Hugging Face用户名TheBloke，已经发布了数千个可用的量化模型，见[*https://huggingface.co/TheBloke*](https://huggingface.co/TheBloke)。许多这些模型都是使用以下现代方法之一进行量化的：
- en: '*Generative pre-training quantization* (GPTQ)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成式预训练量化* (GPTQ)'
- en: '[GPTQ](https://homl.info/gptq)⁠^([8](app02.html#id4454)) is a post-training
    quantization method, usually down to 4 bits, that treats quantization as an optimization
    problem. GPTQ goes through each layer, one by one, and optimizes the 4-bit weights
    to minimize the MSE between the layer’s original outputs (i.e., using the full
    precision weights) and the approximate outputs (i.e., using the 4-bit weights).
    Once the optimal 4-bit weights are found, the approximate outputs are passed to
    the next layer, and the process is repeated all the way to the output layer. During
    inference, the weights are dequantized whenever they are needed. GPTQ only quantizes
    the weights, not the activations: this is called *weight-only quantization*, which
    is great for inference, not for training. You can use the [Hugging Face Optimum
    library](https://huggingface.co/docs/optimum) or the [GPTQModel library](https://github.com/ModelCloud/GPTQModel)
    to quantize your models with GPTQ.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '[GPTQ](https://homl.info/gptq)⁠^([8](app02.html#id4454))是一种后训练量化方法，通常降低到4位，将量化视为一个优化问题。GPTQ逐层进行，一次一层，优化4位权重以最小化层原始输出（即使用全精度权重）和近似输出（即使用4位权重）之间的均方误差。一旦找到最优的4位权重，近似输出就传递到下一层，整个过程一直重复到输出层。在推理过程中，每当需要时，权重就会被反量化。GPTQ只量化权重，不量化激活：这被称为*仅权重量化*，这对于推理来说非常好，但不适用于训练。您可以使用[Hugging
    Face Optimum库](https://huggingface.co/docs/optimum)或[GPTQModel库](https://github.com/ModelCloud/GPTQModel)使用GPTQ量化您的模型。'
- en: '*Activation-aware Weight Quantization* (AWQ)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '*激活感知权重量化* (AWQ)'
- en: '[AWQ](https://homl.info/awq)⁠^([9](app02.html#id4458)) aims to improve the
    accuracy of block-wise weight-only quantization (typically 4-bit quantization).
    The idea is to preserve the precision of the most important weights. To identify
    these so-called *salient weights*, the algorithm runs a calibration dataset through
    the model and finds the largest activations for each quantization group (e.g.,
    the largest 0.1% to 1% activations), and the corresponding weights are considered
    salient. The authors observed that storing the salient weights using float16 greatly
    reduces the model’s *perplexity* (a common metric equal to the exponential of
    the cross-entropy). However, mixing 4-bit and 16-bit weights is not hardware-friendly,
    so AWQ uses another method to preserve the salient weight’s precision: they simply
    scale them up by some factor and add an operation in the model to scale down the
    corresponding activations (but this operation can generally be fused into the
    previous operation). Rather than using a fixed scaling factor, AWQ performs a
    search for the optimal factor, leading to the lowest quantization error. To implement
    AWQ, you can use the Hugging Face Optimum library.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWQ](https://homl.info/awq)⁠^([9](app02.html#id4458))旨在提高块状权重仅量化（通常是4位量化）的准确性。其思路是保留最重要的权重的精度。为了识别这些所谓的*显著权重*，算法将校准数据集通过模型运行，并找到每个量化组的最大激活（例如，最大的0.1%到1%的激活），相应的权重被认为是显著的。作者观察到，使用float16存储显著权重可以显著降低模型的总困惑度（一个常见的指标，等于交叉熵的指数）。然而，混合4位和16位权重对硬件不友好，因此AWQ使用另一种方法来保留显著权重的精度：他们简单地通过某个因子放大它们，并在模型中添加一个操作来缩小相应的激活（但这个操作通常可以融合到前面的操作中）。AWQ而不是使用固定的缩放因子，执行对最优因子的搜索，从而实现最低的量化误差。要实现AWQ，你可以使用Hugging
    Face Optimum库。'
- en: Llama.cpp quantization using the *GPT-Generated Unified Format* (GGUF)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*GPT-Generated Unified Format*（GGUF）的Llama.cpp量化
- en: '[GGUF](https://homl.info/gguf) is a binary file format designed to store LLMs
    efficiently. It was introduced by Georgi Gerganov, the creator of llama.cpp, and
    it supersedes previous file formats such as GGML, GGMF, and GGJT. A GGUF file
    includes the weights, the tokenizer, special tokens, the model architecture, the
    vocabulary size, and other metadata. Llama.cpp offers quantizers (e.g., using
    the `quantize` tool) to convert the model weights to one of GGUF’s supported quantized
    formats, such as Q4_K_M. Q4 stands for 4-bit quantization, K stands for per-block
    quantization (typically 32 or 64 weights per block depending on the chosen format),
    and M means medium size and precision for this quantization level (other options
    are S = Small and L = Large). There are also more recent and efficient quantization
    options such as Importance-aware Quantization (IQ), which uses various techniques
    to improve accuracy (e.g., nonlinear quantization), and Ternary Quantization (TQ).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[GGUF](https://homl.info/gguf)是一种二进制文件格式，旨在高效地存储LLMs。它由llama.cpp的创造者Georgi
    Gerganov引入，并取代了之前的文件格式，如GGML、GGMF和GGJT。一个GGUF文件包括权重、分词器、特殊标记、模型架构、词汇量大小和其他元数据。Llama.cpp提供了量化器（例如，使用`quantize`工具）将模型权重转换为GGUF支持的量化格式之一，例如Q4_K_M。Q4代表4位量化，K代表每块量化（通常取决于所选格式，每块32或64个权重），M表示此量化级别的中等大小和精度（其他选项是S
    = 小型和L = 大型）。还有更近期的更高效的量化选项，如重要性感知量化（IQ），它使用各种技术来提高准确性（例如，非线性量化），以及三值量化（TQ）。'
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: On the Hugging Face Hub, every repository is backed by Git, so it has branches
    and commits. When you call `from_pretrained()`, the model is fetched from the
    default branch, which is almost always `main`. But quantized models are often
    placed in a different branch. When calling `from_pretrained()`, you can choose
    a branch, a tag, or even a commit hash, by using the `revision` argument. Check
    the model card for the list of available files and versions. For GGUF models,
    you must specify the filename using the `gguf_file` argument.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hugging Face Hub上，每个存储库都由Git支持，因此它有分支和提交。当你调用`from_pretrained()`时，模型是从默认分支获取的，这几乎总是`main`。但是量化模型通常放在不同的分支中。当调用`from_pretrained()`时，你可以通过使用`revision`参数来选择分支、标签，甚至提交哈希。检查模型卡片以获取可用文件和版本的列表。对于GGUF模型，你必须使用`gguf_file`参数指定文件名。
- en: 'In conclusion, reduced precision, mixed-precision training, and quantization
    are arguably the most important tools to allow large models to run on limited
    hardware. But there are many more, including the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，降低精度、混合精度训练和量化可能是允许大型模型在有限硬件上运行的最重要工具。但还有更多，包括以下内容：
- en: You could tweak the model’s architecture before training, by reducing the number
    of layers, or the number of neurons per layer, or by sharing weights across layers
    (e.g., as in the ALBERT model, introduced in [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在训练之前调整模型的架构，例如通过减少层数、每层的神经元数量，或者通过在层之间共享权重（例如，在[第15章](ch15.html#transformer_chapter)中介绍的ALBERT模型中就是这样做的）。
- en: If you have a large trained model, you can shrink it by removing some of its
    weights, for example the ones with the smallest magnitude, or the ones with the
    smallest effect on the loss. You can also remove whole channels, layers, or attention
    heads. This is called *model pruning*, and you can implement it using the `torch.nn.utils.prune`
    module, or the Hugging Face Optimum library.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你有一个大型训练好的模型，你可以通过移除其中的一些权重来缩小它，例如那些幅度最小的权重，或者那些对损失影响最小的权重。你也可以移除整个通道、层或注意力头。这被称为*模型剪枝*，你可以使用`torch.nn.utils.prune`模块或Hugging
    Face Optimum库来实现它。
- en: 'As we saw in [Chapter 15](ch15.html#transformer_chapter), you can also use
    a large trained model as a teacher to train a smaller model: this is called distillation.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如我们在[第15章](ch15.html#transformer_chapter)中看到的，你也可以使用一个大型训练好的模型作为教师来训练一个较小的模型：这被称为蒸馏。
- en: 'A trained model can also be shrunk by fusing some of its layers, removing redundancy.
    For example, a batch-norm layer (introduced in [Chapter 11](ch11.html#deep_chapter))
    performs a linear operation, so if it comes immediately after a linear layer,
    you can fuse both layers into a single linear layer. Similarly, you can fuse a
    convolutional layer followed by a batch-norm layer into a single convolutional
    layer. This only works after training, since the batch-norm layer must compute
    running averages during training. You can implement layer fusion with the `torch.quantization.fuse_modules()`
    function, or with the Hugging Face Optimum library. In any case, make sure to
    fuse layers *before* quantizing your model: less layers means less quantization
    noise.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练好的模型也可以通过融合其部分层来缩小，移除冗余。例如，一个批归一化层（在第11章[deep_chapter]中介绍）执行线性操作，所以如果它紧接在一个线性层之后，你可以将这两个层融合成一个单一的线性层。同样，你也可以将一个卷积层和一个批归一化层融合成一个单一的卷积层。这只能在训练后进行，因为批归一化层必须在训练期间计算运行平均值。你可以使用`torch.quantization.fuse_modules()`函数或Hugging
    Face Optimum库来实现层融合。无论如何，确保在量化你的模型之前融合层：层越少，量化噪声就越少。
- en: You can use low-rank approximations, where a large matrix is replaced by the
    product of two smaller ones. For example, replace a large linear layer such as
    `Linear(10_000, 20_000)` with two linear layers `Linear(10_000, 100)` and `Linear(100,
    20_000)`. This reduces the number of parameters from about 200 million down to
    just three million, and also drastically reduces computations. The intermediate
    dimensionality (100 in this example) is a hyperparameter you can tune to balance
    accuracy and model size. This technique can be performed after training by factorizing
    the weight matrix using SVD (see the notebook for an example).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用低秩近似，其中一个大矩阵被两个较小矩阵的乘积所替代。例如，将一个大的线性层，如`Linear(10_000, 20_000)`，替换为两个线性层`Linear(10_000,
    100)`和`Linear(100, 20_000)`。这将从大约两亿个参数减少到仅仅三百万个，同时也极大地减少了计算。中间维度（本例中的100）是一个可以调整的超参数，以平衡准确性和模型大小。这种技术可以通过使用SVD（参见笔记本中的示例）对权重矩阵进行分解来在训练后执行。
- en: 'Give these techniques a try: shrink the models!'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试这些技术：缩小模型！
- en: Note
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Chapter 17 and Appendices C, D, and E are available online at [*https://homl.info*](https://homl.info).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第17章和附录C、D和E可在网上找到[*https://homl.info*](https://homl.info)。
- en: ^([1](app02.html#id4346-marker)) In general, –0 and +0 are considered equal,
    but some operations give different results, for example 1 / –0 = –infinity, while
    1 / +0 = +infinity.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](app02.html#id4346-marker)) 通常，-0和+0被认为是相等的，但某些操作会给出不同的结果，例如1 / -0 = -infinity，而1
    / +0 = +infinity。
- en: ^([2](app02.html#id4353-marker)) Some high-performance computing applications
    deactivate subnormal numbers because they slow down computations, and normalized
    numbers are generally sufficient (e.g., normalized fp32 can represent numbers
    as small as ±1.2e^(–38)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](app02.html#id4353-marker)) 一些高性能计算应用会禁用非正常数值，因为它们会减慢计算速度，而归一化数值通常足够（例如，归一化的fp32可以表示±1.2e^(–38)这样小的数值）。
- en: ^([3](app02.html#id4362-marker)) The *M* stands for *mantissa*, which is a term
    often used as a synonym for fraction. Unfortunately, it’s also used as a synonym
    for significand, leading to some confusion. This is why IEEE 754 no longer uses
    the term mantissa.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](app02.html#id4362-marker)) “*M*”代表**尾数**，这是一个常被用作分数同义词的术语。不幸的是，它也被用作有效数字的同义词，导致了一些混淆。这就是为什么
    IEEE 754 标准不再使用“尾数”这个术语。
- en: ^([4](app02.html#id4388-marker)) P. Micikevicius et al., “Mixed Precision Training”,
    arXiv preprint 2017, ICLR (2018).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](app02.html#id4388-marker)) P. Micikevicius 等人，“混合精度训练”，arXiv 预印本 2017，ICLR
    (2018)。
- en: '^([5](app02.html#id4409-marker)) PyTorch implements *restricted symmetric quantization*,
    meaning that it excludes the lowest possible signed integer (e.g., –128 for 8-bit
    integers) to ensure that the range is symmetric (e.g., –127 to +127). Some other
    implementations allow the full signed byte range (from –128 to +127): this is
    called *unrestricted symmetric quantization*. These implementations also subtract
    0.5 instead of 1 in the denominator of [Equation B-2](#symmetric_linear_quantization_equation).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](app02.html#id4409-marker)) PyTorch 实现了**限制对称量化**，这意味着它排除了可能的最小有符号整数（例如，8
    位整数的 -128）以确保范围是对称的（例如，-127 到 +127）。一些其他实现允许完整的有符号字节范围（从 -128 到 +127）：这被称为**非限制对称量化**。这些实现也在
    [方程 B-2](#symmetric_linear_quantization_equation) 的分母中减去 0.5 而不是 1。
- en: ^([6](app02.html#id4433-marker)) Benoit Jacob et al., “Quantization and Training
    of Neural Networks for Efficient Integer-Arithmetic-Only Inference”, arXiv preprint
    arXiv:1712.05877 (2017)”.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](app02.html#id4433-marker)) Benoit Jacob 等人，“用于高效整数算术推理的神经网络量化与训练”，arXiv
    预印本 arXiv:1712.05877 (2017)”。
- en: '^([7](app02.html#id4445-marker)) Tim Dettmers et al., “QLORA: Efficient Finetuning
    of Quantized LLMs”, arXiv preprint arXiv:2305.14314 (2023).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](app02.html#id4445-marker)) Tim Dettmers 等人，“QLORA：量化 LLM 的有效微调”，arXiv
    预印本 arXiv:2305.14314 (2023)。
- en: '^([8](app02.html#id4454-marker)) Elias Frantar et al., “GPTQ: Accurate Post-Training
    Quantization for Generative Pre-trained Transformers”, arXiv preprint arXiv:2210.17323
    (2022).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](app02.html#id4454-marker)) Elias Frantar 等人，“GPTQ：生成预训练变换器的准确后训练量化”，arXiv
    预印本 arXiv:2210.17323 (2022)。
- en: '^([9](app02.html#id4458-marker)) Ji Lin et al., “AWQ: Activation-aware Weight
    Quantization for LLM Compression and Acceleration”, arXiv preprint arXiv:2306.00978
    (2023).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](app02.html#id4458-marker)) Ji Lin 等人，“AWQ：用于 LLM 压缩和加速的激活感知权重量化”，arXiv
    预印本 arXiv:2306.00978 (2023)。
