- en: Chapter 9\. Advanced Techniques for Image Generation with Stable Diffusion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：使用Stable Diffusion的高级图像生成技术
- en: Most work with AI images only requires simple prompt engineering techniques,
    but there are more powerful tools available when you need more creative control
    over your output, or want to train custom models for specific tasks. These more
    complex abilities often requires more technical ability and structured thinking
    as part of the workflow of creating the final image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数与AI图像相关的工作只需要简单的提示工程技巧，但在你需要更多创意控制输出或想要为特定任务训练自定义模型时，有更多强大的工具可用。这些更复杂的能力通常需要更多的技术能力和结构化思维，作为创建最终图像的工作流程的一部分。
- en: All images in this chapter are generated by Stable Diffusion XL unless otherwise
    noted, as in the sections relying on extensions such as ControlNet, where more
    methods are supported with the older v1.5 model. The techniques discussed were
    devised to be transferrable to any future or alternative model. We make extensive
    use of AUTOMATIC1111’s Stable Diffusion WebUI and have provided detailed setup
    instructions that were current as of the time of writing, but please consult the
    [official repository](https://oreil.ly/hs_fS) for up-to-date instructions, and
    to diagnose any issues you encounter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有图像均由Stable Diffusion XL生成，除非另有说明，例如在依赖于ControlNet等扩展的章节中，那里支持更多的旧版v1.5模型的方法。讨论的技术旨在可迁移到任何未来的或替代模型。我们广泛使用了AUTOMATIC1111的Stable
    Diffusion WebUI，并提供了详细的设置说明，这些说明在撰写时是有效的，但请查阅[官方仓库](https://oreil.ly/hs_fS)以获取最新的说明，并诊断你遇到的问题。
- en: Running Stable Diffusion
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Stable Diffusion
- en: Stable Diffusion is an open source image generation model, so you can run it
    locally on your computer for free, if you have an NVIDIA or AMD GPU, or Apple
    Silicon, as powers the M1, M2, or M3 Macs. It was common to run the first popular
    version (1.4) of [Stable Diffusion in a Google Colab notebook](https://oreil.ly/OmBuR),
    which provides access to a free GPU in the cloud (though you may need to upgrade
    to a paid account if Google limits the free tier).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Stable Diffusion是一个开源的图像生成模型，因此如果你有NVIDIA或AMD GPU，或者苹果的M1、M2或M3芯片，你可以在自己的电脑上免费运行它。最初流行的版本（1.4）通常在[Google
    Colab笔记本](https://oreil.ly/OmBuR)中运行，它提供了访问云中免费GPU的途径（尽管如果谷歌限制了免费层，你可能需要升级到付费账户）。
- en: Visit the [Google Colab website](https://oreil.ly/2WGxQ) if you haven’t used
    it before or to find the latest information on limits. A copy of this Python notebook
    is saved in the [GitHub repository](https://oreil.ly/uauNn) for this book, but
    you should upload it to Google Drive and run it in Google Colab to avoid setup
    issues.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前没有使用过，或者想要找到有关限制的最新信息，请访问[Google Colab网站](https://oreil.ly/2WGxQ)。这本书的Python笔记本副本已保存在[GitHub仓库](https://oreil.ly/uauNn)中，但你应该将其上传到Google
    Drive，并在Google Colab中运行，以避免设置问题。
- en: 'Installing Stable Diffusion can be done via the Hugging Face diffusers libary,
    alongside a handful of dependencies. In the Google Colab the following code installs
    the necessary dependencies (you would drop the exclamation marks (!) if installing
    locally rather than in a Jupyter Notebook or Google Colab):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 安装Stable Diffusion可以通过Hugging Face diffusers库以及一些依赖项来完成。在Google Colab中，以下代码安装了必要的依赖项（如果你在本地安装而不是在Jupyter
    Notebook或Google Colab中，则应删除感叹号(!)）：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To download and use the model, you first build an inference pipeline (what
    runs when we use the model):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载和使用模型，你首先构建一个推理管道（当我们使用模型时运行的内容）：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s break down the script line by line:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析脚本：
- en: '`import torch`'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '`import torch`'
- en: This line is importing the torch library, also known as [PyTorch](https://pytorch.org).
    PyTorch is an open source machine learning library, used for applications such
    as computer vision and natural language processing.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码正在导入torch库，也称为[PyTorch](https://pytorch.org)。PyTorch是一个开源的机器学习库，用于计算机视觉和自然语言处理等应用。
- en: '`from diffusers import StableDiffusionPipeline`'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`from diffusers import StableDiffusionPipeline`'
- en: Here the script is importing the `StableDiffusionPipeline` class from the `diffusers`
    library. This specific class is probably a pipeline for using diffusion models,
    of which Stable Diffusion is the most popular example.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，脚本正在从`diffusers`库中导入`StableDiffusionPipeline`类。这个特定的类可能是一个用于使用扩散模型的管道，其中Stable
    Diffusion是最受欢迎的例子。
- en: '`pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16)`'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipe = StableDiffusionPipeline.from_pretrained("CompVis/stable-diffusion-v1-4",
    torch_dtype=torch.float16)`'
- en: This is creating an instance of the `StableDiffusionPipeline` class with pretrained
    weights. The method `from_pretrained` loads the weights of a pretrained model—in
    this case, the model is `CompVis/stable-diffusion-v1-4`.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在创建一个具有预训练权重的 `StableDiffusionPipeline` 类的实例。`from_pretrained` 方法加载预训练模型的权重——在这种情况下，模型是
    `CompVis/stable-diffusion-v1-4`。
- en: The `torch_dtype=torch.float16` argument specifies that the data type used in
    the model should be float16, which is a half-precision floating-point format.
    Using float16 can speed up model computation and reduce memory usage (necessary
    to stay within the Google Colab free tier limits).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch_dtype=torch.float16` 参数指定模型中使用的应该是 float16，这是一种半精度浮点格式。使用 float16 可以加快模型计算速度并减少内存使用（这对于保持在
    Google Colab 免费层限制内是必要的）。'
- en: '`pipe = pipe.to("cuda")`'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '`pipe = pipe.to("cuda")`'
- en: This line moves the pipe model to the GPU. The string `"cuda"` refers to CUDA,
    a parallel computing platform and application programming interface (API) model
    created by Nvidia. By doing this, all computations performed by the pipe model
    will be executed on the GPU, which can be significantly faster than running them
    on a CPU for large-scale models and data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将管道模型移动到 GPU 上。字符串 `"cuda"` 指的是 CUDA，这是由 Nvidia 创建的并行计算平台和应用编程接口 (API) 模型。通过这样做，管道模型执行的所有计算都将由
    GPU 执行，这比在 CPU 上运行大型模型和数据要快得多。
- en: 'Now that we have our pipe, we can pass in a prompt and other parameters for
    the model, like a random seed (change this to get a different image each time),
    the number of inference steps (more steps takes time but results in a higher-quality
    image), and the guidance scale (how closely the image matches the prompt):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的管道，我们可以传递一个提示和其他参数给模型，如随机种子（更改此值以每次得到不同的图像）、推理步骤的数量（更多的步骤需要时间但会产生更高质量的图像）和引导比例（图像与提示匹配的紧密程度）：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Figure 9-1](#figure-9-1) shows the output.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-1](#figure-9-1) 展示了输出。'
- en: 'Let’s walk through this script to explain what it does:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这个脚本，解释它做了什么：
- en: '`prompt = "a photograph of an astronaut riding a horse"`'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`prompt = "a photograph of an astronaut riding a horse"`'
- en: This is the prompt that will be passed into the model to guide the generation
    of an image.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将被传递到模型中以引导图像生成的提示。
- en: '`generator = torch.Generator("cuda").manual_seed(1024)`'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`generator = torch.Generator("cuda").manual_seed(1024)`'
- en: In this line, a PyTorch generator is created and assigned to the generator variable.
    The generator is initialized with `"cuda"`, which means that it will be using
    a GPU for computations. The `manual_seed(1024)` function is used to set the random
    seed for generating random numbers, ensuring that the results are reproducible.
    If you run this code with the same model, you should get the exact same image.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一行中，创建了一个 PyTorch 生成器并将其分配给生成器变量。生成器使用 `"cuda"` 初始化，这意味着它将使用 GPU 进行计算。`manual_seed(1024)`
    函数用于设置生成随机数的随机种子，以确保结果可重复。如果你用相同的模型运行此代码，你应该得到完全相同的图像。
- en: '`image = pipe(prompt, num_inference_steps=50, guidance_scale=7, generator=generator).images[0]`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`image = pipe(prompt, num_inference_steps=50, guidance_scale=7, generator=generator).images[0]`'
- en: This line runs the pipe model on the prompt to generate an image. The `num_inference_steps`
    argument is set to 50, meaning that the model will perform 50 steps of inference.
    The `guidance_scale` argument is set to 7, which adjusts how strongly the prompt
    guides the generated image (higher values tend to get grainy and less diverse).
    The generator argument passes in the random number generator created earlier.
    The result is an array of generated images, and `images[0]` selects the first
    image from this array.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码在提示上运行管道模型以生成图像。`num_inference_steps` 参数设置为 50，意味着模型将执行 50 步推理。`guidance_scale`
    参数设置为 7，这调整了提示引导生成的图像的强度（较高的值往往会导致图像颗粒化且多样性降低）。生成器参数传递了之前创建的随机数生成器。结果是生成图像的数组，`images[0]`
    从这个数组中选择第一张图像。
- en: '`image.save(f"astronaut_rides_horse.png")`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`image.save(f"astronaut_rides_horse.png")`'
- en: This line saves the generated image to a file.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将生成的图像保存到文件中。
- en: '`image`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`image`'
- en: This line of code will display the image if the code is running in an environment
    like a Jupyter Notebook or Google Colab. This happens because these environments
    automatically display the result of the last line of code in a code cell if it
    is not assigned to a variable.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将在代码运行在类似 Jupyter Notebook 或 Google Colab 的环境中时显示图像。这是因为这些环境会自动显示代码单元格中最后一行的结果，如果它没有被分配给变量。
- en: '![pega 0901](assets/pega_0901.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0901](assets/pega_0901.png)'
- en: Figure 9-1\. Photograph of an astronaut riding a horse
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1. 骑马的宇航员照片
- en: It’s powerful to be able to run an open source model locally or in the cloud
    and customize it to meet your needs. However, custom coding your own inference
    pipelines and building a user interface on top is likely overkill unless you are
    an extremely advanced user with deep machine learning knowledge or your intention
    is to build your own AI image generation product. Stablity AI, the company funding
    development of Stable Diffusion, has a hosted web interface called Dream Studio
    ([Figure 9-2](#figure-9-2)), which is similar to the DALL-E playground, also operating
    on a credit system and offering advanced functionality such as inpainting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 能够在本地或云中运行开源模型并对其进行定制以满足您的需求是非常强大的。然而，除非您是一个具有深厚机器学习知识的极高级用户，或者您的目的是构建自己的AI图像生成产品，否则自己编写推理管道和构建用户界面可能是过度杀鸡用牛刀。Stability
    AI公司，它资助了Stable Diffusion的开发，有一个名为Dream Studio的托管Web界面([图9-2](#figure-9-2))，它与DALL-E游乐场类似，也采用信用系统，并提供高级功能，如修复。
- en: '![pega 0902](assets/pega_0902.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0902](assets/pega_0902.png)'
- en: Figure 9-2\. Stability AI Dream-Studio
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2. Stability AI Dream-Studio
- en: 'Like DALL-E, Dream-Studio offers access via API, which can be convenient for
    building AI image applications or running programmatic scripts for generating
    lots of images, without the encumberance of hosting and running your own Stable
    Diffusion model. Visit [*https://oreil.ly/X3Ilb*](https://oreil.ly/X3Ilb) once
    you have created an account to get your API key, and top up with credits (at time
    of writing, 1,000 credits cost $10 and can generate approximately 5,000 images).
    The following code is included in the [GitHub repository](https://oreil.ly/aGLeX)
    for this book:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与DALL-E类似，Dream-Studio通过API提供访问权限，这对于构建AI图像应用程序或运行生成大量图像的程序脚本来说非常方便，无需自己托管和运行Stable
    Diffusion模型。在创建账户后，请访问[*https://oreil.ly/X3Ilb*](https://oreil.ly/X3Ilb)以获取您的API密钥，并充值（截至撰写本文时，1,000个积分花费10美元，可以生成大约5,000张图像）。以下代码包含在此书的[GitHub仓库](https://oreil.ly/aGLeX)中：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 9-3](#figure-9-3) shows the output.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-3](#figure-9-3)显示了输出结果。'
- en: '![pega 0903](assets/pega_0903.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0903](assets/pega_0903.png)'
- en: Figure 9-3\. Corporate Memphis illustration from the Dream-Studio API
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3. Dream-Studio API的Memphis公司插图
- en: 'Let’s break down this code step-by-step:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这段代码：
- en: 'First, set up the required environment variables:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，设置所需的环境变量：
- en: '`engine_id`: This refers to a specific model version at `stability.ai`.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`engine_id`：这指的是`stability.ai`上的特定模型版本。'
- en: '`api_host`: This retrieves the API host URL from environment variables. If
    not set, it defaults to `''https://api.stability.ai''`.'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`api_host`：这从环境变量中检索API主机URL。如果没有设置，则默认为`''https://api.stability.ai''`。'
- en: '`api_key`: This retrieves the API key from environment variables.'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`api_key`：这从环境变量中检索API密钥。'
- en: 'The `prompt`: This defines how the image should look, including the style and
    colors.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`prompt`：这定义了图像的外观，包括风格和颜色。'
- en: A `POST` request is made to the URL derived from `api_host` and `engine_id`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向由`api_host`和`engine_id`派生的URL发出`POST`请求。
- en: The headers for the request are set to accept and send JSON data and include
    an authorization header with the `api_key`.
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求的标头设置为接受和发送JSON数据，并包含带有`api_key`的授权标头。
- en: The JSON body of the request specifies the prompt (description of the image),
    the desired scale of the image, its dimensions, the number of samples, and the
    number of steps.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求的JSON主体指定了提示（图像描述）、图像的期望比例、其尺寸、样本数量和步骤数量。
- en: If the status code of the response is not 200 (indicating a successful request),
    an exception is raised with the response text to indicate something went wrong.
    Otherwise, the response is parsed into JSON format.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果响应的状态码不是200（表示请求成功），则会抛出一个异常，并带有响应文本以指示出了问题。否则，响应将被解析为JSON格式。
- en: 'If there isn’t a directory named *out*, one is created. For each artifact (image)
    in the response, the code does the following:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果没有名为*out*的目录，则会创建一个。对于响应中的每个工件（图像），代码执行以下操作：
- en: Sets a filename path.
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置文件名路径。
- en: Decodes the base64-encoded image data from the response.
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码响应中的base64编码的图像数据。
- en: Writes the decoded image data to a file.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将解码的图像数据写入文件。
- en: Appends the file’s path to the `image_paths` list.
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件的路径追加到`image_paths`列表中。
- en: This is typically where you would save the image to [Google Cloud Storage](https://oreil.ly/YsuBw)
    or Amazon Simple Storage Service (S3) to display later in your application.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这通常是您将图像保存到[Google Cloud Storage](https://oreil.ly/YsuBw)或Amazon Simple Storage
    Service (S3)的位置，以便稍后在您的应用程序中显示。
- en: The first image from the `image_paths` list (the only one, in this case) is
    displayed (only in Jupyter Notebooks or Google Colab) using the `Image` class
    from `IPython.display`.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`image_paths`列表中的第一张图像（在这种情况下是唯一一张）使用`IPython.display`中的`Image`类显示（仅在Jupyter
    Notebooks或Google Colab中）。
- en: The downside of using Stability AI’s service is a lack of control over customization.
    One of the great benefits of Stable Diffusion being open source is the ability
    to modify almost any aspect of the model and make use of community-built advanced
    functionality. In addition, there is no guarantee that functions or features you
    rely on for your scripts today will still be there in the future, as Stability
    AI strives to live up to the expectations of their investors, legal team, and
    corporate customers. For example, the popular (and more permissive) version 1.5
    model has been deprecated in favor of the new Stable Diffusion 2.0 and XL models,
    causing problems for those who had finely tuned their workflows, parameters, and
    prompts to work with v1.5.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Stability AI服务的缺点是缺乏对定制的控制。稳定扩散作为开源项目的一个巨大好处是能够修改模型的几乎任何方面，并利用社区构建的高级功能。此外，不能保证您今天依赖的函数或特性在未来仍然存在，因为Stability
    AI努力满足投资者的期望、法律团队和公司客户的需求。例如，流行的（更宽容的）1.5版本模型已被弃用，转而使用新的稳定扩散2.0和XL模型，这给那些已经精心调整了他们的工作流程、参数和提示以与v1.5版本协同工作的人带来了问题。
- en: AUTOMATIC1111 Web User Interface
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AUTOMATIC1111 网页用户界面
- en: 'Heavy users of Stable Diffusion typically recommend the [AUTOMATIC1111](https://oreil.ly/r-2vm)
    (pronounced “automatic eleven eleven”) web user interface, because it is feature-rich
    and comes with multiple extensions built by Stable Diffusion power users. This
    project is the gateway to taking advantage of the best aspect of Stable Diffusion:
    the vibrant open source community that has dedicated countless hours to integrating
    advanced functionality to the tool. Advanced users may also want to explore [ComfyUI](https://oreil.ly/LWVvC),
    as it supports more advanced workflows and increased flexibility (including [image-to-video](https://oreil.ly/dh7jR)),
    but we deemed this too complex for the majority of use cases, which can easily
    be handled by AUTOMATIC1111.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 稳定扩散的重度用户通常推荐使用[AUTOMATIC1111](https://oreil.ly/r-2vm)（发音为“automatic eleven
    eleven”）网页用户界面，因为它功能丰富，并附带由稳定扩散高级用户构建的多个扩展。这个项目是利用稳定扩散最佳特性的门户：一个投入了无数小时来整合高级功能到工具中的充满活力的开源社区。高级用户还可能想要探索[ComfyUI](https://oreil.ly/LWVvC)，因为它支持更高级的工作流程和更高的灵活性（包括[图像到视频](https://oreil.ly/dh7jR)），但我们认为这对于大多数用例来说过于复杂，这些用例可以很容易地由AUTOMATIC1111处理。
- en: You can use the normal text-to-image Stable Diffusion model, but also run image-to-image
    (similar to the base image feature in Midjourney), as well as upscaling finished
    images for higher quality, and inpainting (as is offered by DALL-E). It’s even
    possible to train and run custom models within this interface, and there are thousands
    of models shared publicly in communities such as [Hugging Face](https://oreil.ly/t5T7p)
    and [Civitai](https://civitai.com).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用正常的文本到图像稳定扩散模型，也可以运行图像到图像（类似于Midjourney的基础图像功能），以及提升完成图像以获得更高的质量，以及修复（如DALL-E提供）。甚至可以在该界面中训练和运行自定义模型，社区如[Hugging
    Face](https://oreil.ly/t5T7p)和[Civitai](https://civitai.com)上共享了数千个模型。
- en: Warning
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Some custom open source models are NSFW (not safe for work), so be careful when
    browsing websites like Civitai.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些自定义开源模型可能不适合工作环境（NSFW），所以在浏览像Civitai这样的网站时要小心。
- en: 'Running Stable Diffusion locally with AUTOMATIC1111 requires some technical
    setup, and it’s best to look for an up-to-date guide in the AUTOMATIC1111 Wiki:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AUTOMATIC1111在本地运行稳定扩散需要一些技术设置，最好在AUTOMATIC1111 Wiki中查找最新的指南：
- en: '[Install and run on NVidia GPUs](https://oreil.ly/DsKyU)'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在NVidia GPU上安装和运行](https://oreil.ly/DsKyU)'
- en: '[Install and run on AMD GPUs](https://oreil.ly/Oc7ix)'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在AMD GPU上安装和运行](https://oreil.ly/Oc7ix)'
- en: '[Install and run on Apple Silicon](https://oreil.ly/Ob2VK)'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[在Apple Silicon上安装和运行](https://oreil.ly/Ob2VK)'
- en: 'Installation generally involves ensuring you have Git and Python installed
    (as well as any other [dependencies](https://oreil.ly/vBOVI)), and downloading
    Stable Diffusion, as well as the Automatic1111 [code](https://oreil.ly/x0BMn)
    to your local computer. The images in this chapter use [the XL 1.0 version](https://oreil.ly/DIvUz)
    of Stable Diffusion, though many still use the older [version 1.5](https://oreil.ly/FNxf9)
    as it is considered more permissive and has a wealth of custom community-trained
    models. The techniques work the same across models, though the results and quality
    will differ: it’s commonly believed that removing NSFW images from the training
    data for version 2.0 led to worse performance at generating (even nonexplicit)
    images of realistic human figures (though this seems largely corrected in the
    XL version).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 安装通常涉及确保您已安装 Git 和 Python（以及任何其他 [依赖项](https://oreil.ly/vBOVI)），并下载 Stable Diffusion，以及将
    Automatic1111 [代码](https://oreil.ly/x0BMn)下载到您的本地计算机。本章中的图像使用的是 Stable Diffusion
    的 [XL 1.0 版本](https://oreil.ly/DIvUz)，尽管许多人仍然使用较旧的 [版本 1.5](https://oreil.ly/FNxf9)，因为它被认为更宽容，并且拥有丰富的自定义社区训练模型。这些技术在各个模型中都是通用的，尽管结果和品质会有所不同：普遍认为，从版本
    2.0 的训练数据中移除 NSFW 图像导致了生成（即使是非显性）逼真人物图像的性能下降（尽管在 XL 版本中这似乎得到了很大程度的纠正）。
- en: 'As the model is open source, you can get SDXL v1.0 on your local computer by
    visiting the model page on Hugging Face for the base and refiner models, and downloading
    the *.safetensors* files from the “Files and Versions” tab. This format is safer
    than the previous *.ckpt* file format, as it does not execute code on your computer
    when running:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于该模型是开源的，您可以通过访问基础和精炼模型的 Hugging Face 模型页面，并从“文件和版本”标签下载 *.safetensors* 文件，在您的本地计算机上获取
    SDXL v1.0。这种格式比之前的 *.ckpt* 文件格式更安全，因为它在运行时不会在您的计算机上执行代码：
- en: '[Base model](https://oreil.ly/wtHRj): *sd_xl_base_1.0.safetensors*'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[基础模型](https://oreil.ly/wtHRj): *sd_xl_base_1.0.safetensors*'
- en: '[Refiner model](https://oreil.ly/0Dlbv): *sd_xl_refiner_1.0.safetensors*'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[精炼模型](https://oreil.ly/0Dlbv): *sd_xl_refiner_1.0.safetensors*'
- en: These models take time to download, so start downloading them now and later
    you will place them in your models/Stable-diffusion folder once you have installed
    the AUTOMATIC111 interface. If you want to use the older v1.5 Stable Diffusuion
    model, download the *v1-5-pruned-emaonly.ckpt* file from [Hugging Face](https://oreil.ly/hwblq),
    and move that into the models folder where you placed the base and refiner models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型需要时间下载，所以现在就开始下载它们，一旦安装了 AUTOMATIC111 接口，您就可以将它们放入 models/Stable-diffusion
    文件夹中。如果您想使用较旧的 v1.5 Stable Diffusion 模型，请从 [Hugging Face](https://oreil.ly/hwblq)
    下载 *v1-5-pruned-emaonly.ckpt* 文件，并将其移动到放置基础和精炼模型的文件夹中。
- en: 'Once you have everything installed, the web interface is accessed by running
    a script that launches the application locally, which will show up as a web address
    in your browser. As one example, here are the current instructions (at time of
    writing) for Windows, with a computer that has an Nvidia GPU:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装完毕，可以通过运行一个本地启动应用程序的脚本来访问 Web 界面，该应用程序将显示为浏览器中的 Web 地址。以下是一个示例，适用于具有 Nvidia
    GPU 的 Windows 计算机（在撰写本文时）：
- en: Install [Python 3.10.6](https://oreil.ly/kGiyi) (selecting Add to PATH) and
    [Git](https://oreil.ly/Pdzb0).
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 [Python 3.10.6](https://oreil.ly/kGiyi)（选择添加到 PATH）和 [Git](https://oreil.ly/Pdzb0)。
- en: Open the command prompt from search bar, and type `**git clone [*https://github.com/AUTOMATIC1111/stable-diffusion-webui*](https://github.com/AUTOMATIC1111/stable-diffusion-webui)**`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从搜索栏打开命令提示符，并输入 `**git clone [*https://github.com/AUTOMATIC1111/stable-diffusion-webui*](https://github.com/AUTOMATIC1111/stable-diffusion-webui)**`。
- en: Remember to move the sd_xl_base_1.0.safetensors and sd_xl_refiner_1.0.safetensors
    models into the stable-diffusion-webui/models/Stable-diffusion folder.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记得将 sd_xl_base_1.0.safetensors 和 sd_xl_refiner_1.0.safetensors 模型移动到 stable-diffusion-webui/models/Stable-diffusion
    文件夹。
- en: Double-click the *webui-user.bat* file and visit the address the interface is
    running on (usually *[*http://127.0.0.1:7860*](http://127.0.0.1:7860)*). For Mac
    or Linux, you would run `bash webui.sh` in the terminal.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双击 *webui-user.bat* 文件，访问界面正在运行的地址（通常是 *[*http://127.0.0.1:7860*](http://127.0.0.1:7860)*）。对于
    Mac 或 Linux，您需要在终端中运行 `bash webui.sh`。
- en: From this interface, shown in [Figure 9-4](#figure-9-4) (taken from the official
    [GitHub repository](https://oreil.ly/OOpas)), you can enter your prompt (top left,
    under the “txt2img” tab) and click Generate to get your image.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个界面，如图 [图9-4](#figure-9-4)（来自官方 [GitHub 仓库](https://oreil.ly/OOpas)），您可以输入您的提示（在“txt2img”标签下，左上角）并点击生成以获取您的图像。
- en: If you run into an error or if you downloaded AUTOMATIC1111 web UI a while ago
    and need to update it, you can enter the stable-diffusion-webui folder in your
    terminal and run `git pull`. If you are running into errors, you may reset your
    implementation (move any files you want to save first) by running `git checkout
    -f master` in the stable-diffusion-webui folder.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您遇到错误，或者您之前下载了 AUTOMATIC1111 网页 UI 并需要更新它，您可以在终端中进入稳定扩散网页 UI 文件夹并运行 `git pull`。如果您遇到错误，您可以通过在稳定扩散网页
    UI 文件夹中运行 `git checkout -f master` 来重置您的实现（首先移动您想要保存的任何文件）。
- en: Warning
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Resetting AUTOMATIC1111 this way will delete any files in the folder, along
    with any customizations. We recommend you make a local copy in a different folder
    for recovery.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式重置 AUTOMATIC1111 将会删除文件夹中的所有文件，包括任何自定义设置。我们建议您在不同的文件夹中创建本地副本以备恢复。
- en: '![pega 0904](assets/pega_0904.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0904](assets/pega_0904.png)'
- en: Figure 9-4\. Stable Diffusion web UI
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-4\. 稳定扩散网页 UI
- en: 'The box immediately below the prompt input is where you can add negative prompts
    to remove concepts from an image and ensure they don’t show up (see [Chapter 8](ch08.html#standard_image_08)
    for more on negative prompts). Underneath, you’ll find a number of settings including
    the Seed (set to –1 for a new image each time), number of Sampling (inference)
    Steps, Batch Count (Number of generations to run one after another), and Batch
    Size (number of images processed in each batch at the cost of higher VRAM needed).
    When images are generated, you can download them from the interface directly,
    send them to various tabs with the buttons below, or visit the stable-diffusion-webui/outputs
    folder where they are organized by method (`text2img`, `img2img`) and date:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 提示输入框正下方的框中，您可以添加负面提示以从图像中移除概念并确保它们不会显示（有关负面提示的更多信息，请参阅[第 8 章](ch08.html#standard_image_08)）。下面，您会发现一些设置，包括种子（设置为
    -1 以每次生成新图像），采样（推理）步骤数，批次计数（依次运行的生成数量），以及批次大小（每个批次中处理的图像数量，这需要更高的 VRAM）。当图像生成时，您可以直接从界面下载它们，使用下面的按钮将它们发送到各种标签页，或者访问稳定扩散网页
    UI 的输出文件夹，其中它们按方法（`text2img`，`img2img`）和日期组织：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When you run the AUTOMATIC1111 web UI, any models you downloaded will appear
    in the Stable Diffusion Checkpoint drop-down menu at the top. Select the base
    model and enter your prompt as well as adjusting your settings as normal. Make
    sure you set the image size to 1024x1024\. For now, set the “Switch at” parameter
    under Refiner to `1` to run only the base model, as in [Figure 9-5](#figure-9-5).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行 AUTOMATIC1111 网页 UI 时，您下载的任何模型都将出现在顶部稳定扩散检查点下拉菜单中。选择基础模型，并像平常一样输入您的提示以及调整您的设置。确保您将图像大小设置为
    1024x1024。目前，将细化选项下的“切换至”参数设置为 `1` 以仅运行基础模型，如[图 9-5](#figure-9-5) 所示。
- en: '![pega 0905](assets/pega_0905.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0905](assets/pega_0905.png)'
- en: Figure 9-5\. Standard settings for SDXL
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-5\. SDXL 的标准设置
- en: The sampling methods available are relatively complex and technical to explain,
    but the trade-offs are generally between speed, quality, and randomness. `Euler`
    is the simplest sampler, and `DDIM` was the first designed specifically for Diffusion
    models. The sampling methods that have an *a* in the name, for example `Euler
    a`, are ancestral samplers, which inject noise into the image as part of the process.
    This gives less reproducible results as the image does not converge (there is
    some randomness to the image each time you run the model). The `DPM++ 2M Karras`
    and `UniPC` sampler running for 20–30 steps are excellent choices for robust,
    stable, and reproducible images. For higher-quality but slower and more random
    images, try the `DPM++ SDE Karras` or `DDIM` samplers with 10–15 steps.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的采样方法相对复杂且难以解释，但权衡通常在速度、质量和随机性之间。`Euler` 是最简单的采样器，`DDIM` 是第一个专门为扩散模型设计的。名称中带有
    *a* 的采样方法，例如 `Euler a`，是祖先采样器，它们在过程中将噪声注入图像。这会导致结果的可重复性较低，因为图像不会收敛（每次运行模型时图像都有一些随机性）。运行
    20-30 步的 `DPM++ 2M Karras` 和 `UniPC` 采样器是稳健、稳定和可重复图像的优秀选择。对于更高质量但速度较慢且更随机的图像，尝试使用
    10-15 步的 `DPM++ SDE Karras` 或 `DDIM` 采样器。
- en: 'Another important parameter is the CFG Scale (Classifier Free Guidance—the
    same as the `guidance_scale` introduced in the Stable Diffusion Inference Google
    Colab example). As a rule of thumb, here are common values for CFG Scale and what
    they equate to:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的参数是 CFG Scale（分类器自由指导——与在稳定扩散推理 Google Colab 示例中引入的 `guidance_scale` 相同）。作为一个经验法则，以下是
    CFG Scale 的常见值及其对应的含义：
- en: '*1*: Mostly ignore the prompt.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1*: 主要忽略提示。'
- en: '*3*: Feel free to be creative.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*3*: 随意发挥创意。'
- en: '*7*: A good balance between the prompt and creativity.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*7*：提示和创造力之间的良好平衡。'
- en: '*15*: Adhere to the prompt.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*15*：遵守提示。'
- en: '*30*: Strictly follow the prompt.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*30*：严格遵循提示。'
- en: You can change the size of the image generated with Height and Width, as well
    as the number of images using Batch Count. The checkbox Highres fix uses an upscaler
    to generate a larger high-resolution image (more on this later), the Restore faces
    checkbox uses a face restoration model (by default `Codeformer`) to fix the defects
    in human faces that often occur with Stable Diffusion, and the Tiling checkbox
    creates an image that can be tiled in a repeating pattern. There’s also the ability
    to save and insert styles that are just prompts you want to reuse regularly. There
    are many [powerful features](https://oreil.ly/MiSt1) in the different tabs, as
    well as community-built extensions you can add, with more added as they become
    available.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过高度和宽度来更改生成的图像大小，以及使用批量计数来设置图像数量。高分辨率修复复选框使用上采样器生成更大的高分辨率图像（关于这一点稍后会有更多介绍），修复面部缺陷复选框使用面部修复模型（默认为`Codeformer`）来修复Stable
    Diffusion经常出现的人脸缺陷，而平铺复选框创建一个可以重复平铺的图像。还有保存和插入您希望定期重用的提示样式的功能。在不同的标签页中有很多[强大功能](https://oreil.ly/MiSt1)，以及您可以添加的社区构建扩展，随着可用性的增加，还会添加更多。
- en: 'AUTOMATIC1111 supports prompt weights, or weighted terms, much like Midjourney
    (covered in [Chapter 8](ch08.html#standard_image_08)). The way you access them
    is slightly different, as instead of separating by double colons like in Midjourney,
    you use parentheses. For example, `(pirate)` would emphasize pirate features by
    10% or 1.1, and double parentheses `((pirate))` would multiply it again, so the
    weight would be 1.1 x 1.1 = 1.21\. You can also control the weights precisely
    by inputting your own number in the form of (keyword: factor), for example `(pirate:
    1.5)`, for the model to pay 50% more attention to those tokens.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'AUTOMATIC1111 支持提示权重，或加权术语，类似于 Midjourney（在第8章中介绍）。访问它们的方式略有不同，因为在 Midjourney
    中不是通过双冒号分隔，而是使用括号。例如，`(pirate)` 会通过10%或1.1的权重强调海盗特征，而双括号 `((pirate))` 会再次乘以它，因此权重将是
    1.1 x 1.1 = 1.21。您还可以通过输入自己的数字（形式为（keyword: factor））来精确控制权重，例如 `(pirate: 1.5)`，让模型对那些标记给予50%更多的关注。'
- en: 'Input:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Negative:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 负面：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 9-6](#figure-9-6) shows the output.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-6](#figure-9-6) 展示了输出结果。'
- en: '![pega 0906](assets/pega_0906.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0906](assets/pega_0906.png)'
- en: Figure 9-6\. Marilyn Monroe pirate
  id: totrans-107
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-6\. 玛丽莲·梦露海盗
- en: Square brackets `[pirate]` work the same way but in reverse, de-emphasising
    a term in a prompt by 10%. So for example, `[hat]` would be the same as a weight
    of 0.9, or `(hat:0.9)`. Note this is not the same as a negative prompt, because
    the term will still be present in the generation of the image, just dialed down.
    Prompt weights work in the negative prompt box as well, acting to more aggressively
    remove that concept from the image or reduce their effects. This can be used to
    ensure unwanted elements or styles don’t appear when a negative prompt isn’t enough.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号 `[pirate]` 以相反的方式工作，通过减少10%的权重来降低提示中一个术语的强调。例如，`[hat]` 就相当于权重为0.9，或者`(hat:0.9)`。注意，这与负面提示不同，因为该术语仍然会出现在图像生成中，只是被降低强度。提示权重在负面提示框中也同样有效，作用是更积极地从图像中移除该概念或减少其影响。这可以用来确保当负面提示不足以排除时，不希望出现的元素或风格不会出现。
- en: Give Direction
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 给出方向
- en: Providing more or less emphasis on specific words or sections of a prompt can
    give you more fine-grained control over what the model pays attention to.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中给予特定词语或部分更多或更少的强调，可以使你对模型关注的细节有更精细的控制。
- en: 'A more advanced technique used by power users of AUTOMATIC1111 is *prompt editing*,
    also known as *prompt switching*. During the diffusion process the early steps
    move from random noise to a fuzzy outline of the general shapes expected to be
    in the image, before the final details are filled in. Prompt editing allows you
    to pass a different prompt to the early or later steps in the diffusion process,
    giving you more creative control. The syntax is `[from:to:when]`, where `from`
    is your starting prompt, `to` is your finishing prompt, and `when` is when to
    make the switch, denoted in number of steps or a decimal representing a percentage.
    The prompt `[Emma Watson: Amber Heard: 0.5]` would start generating an image of
    Emma Watson, before switching halfway to generating an image of Amber Heard on
    top of the last frame, finishing with a mixture of the two actresses. This is
    a useful trick for creating images of people that look attractive and vaguely
    familiar, without being recongizeable as any specific celebrity, and therefore
    may be seen as more ethical and legally sound than simply copying a celebrity’s
    likeness (seek your own legal counsel):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'AUTOMATIC1111的高级用户使用的一种更高级的技术是*提示编辑*，也称为*提示切换*。在扩散过程中，早期步骤从随机噪声移动到模糊的轮廓，这是预期图像中可能出现的形状，然后在填充最终细节之前。提示编辑允许你在扩散过程的早期或后期步骤传递不同的提示，给你更多的创意控制。语法是
    `[from:to:when]`，其中`from`是你的起始提示，`to`是你的结束提示，`when`是切换的时间，用步数或表示百分比的十进制数表示。提示
    `[艾玛·沃森: 安伯·赫德: 0.5]` 将开始生成艾玛·沃森的图像，然后在中间切换到在最后一帧上生成安伯·赫德的图像，最后以两位女演员的混合结束。这是一个有用的技巧，可以创建看起来吸引人且模糊熟悉的人物图像，而不被识别为任何特定的名人，因此可能比简单地复制名人的形象更道德、更合法（寻求你自己的法律咨询）：'
- en: 'Input:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 9-7](#figure-9-7) shows the output.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-7](#figure-9-7) 显示了输出。'
- en: '![pega 0907](assets/pega_0907.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0907](assets/pega_0907.png)'
- en: Figure 9-7\. Emma Watson and Amber Heard mixed
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-7\. 艾玛·沃森和安伯·赫德混合
- en: Providing Direction
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供方向
- en: Prompt editing is an advanced technique that gets deep into the actual workings
    of the diffusion model. Interfering with what layers respond to what concepts
    can lead to very creative results if you know what you’re doing and are willing
    to undergo enough trial and error.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 提示编辑是一种深入扩散模型实际工作原理的高级技术。如果你知道自己在做什么，并且愿意经历足够的试错，干扰哪些层对哪些概念做出响应可以导致非常富有创意的结果。
- en: If you want the model to alternate between two concepts, the syntax is `[Emma
    Watson | Amber Heard]`, which will make the switch at every step, ending with
    a more blended mixture. There are many advanced uses of prompt editing, though
    it is seen as something of a dark art. In some cases experts report being able
    to get around difficult generations, for example starting by generating something
    easy for the model to generate, before switching to what is really needed in the
    final details phase. In practice we have found limited use out of this technique,
    but you should experiment and see what you can discover.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让模型在两个概念之间交替，语法是 `[艾玛·沃森 | 安伯·赫德]`，这将使每次切换，最终得到更混合的混合。尽管它被视为一种黑暗的艺术，但提示编辑有许多高级用途。在某些情况下，专家报告说能够绕过困难的生成，例如先生成模型容易生成的简单内容，然后再切换到最终细节阶段真正需要的内容。在实践中，我们发现这种技术用途有限，但你应该进行实验，看看你能发现什么。
- en: Img2Img
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Img2Img
- en: The AUTOMATIC1111 web UI supports `Img2Img` ([Figure 9-8](#figure-9-8)), which
    is the functional equivalent to Midjourney’s ability to submit an image along
    with the prompt. It grants you more control over the style and composition of
    your resulting image, by uploading an image for the model to use as guidance.
    To get good results with `Img2Img`, try using `Euler` sampling, 50 sampling steps,
    and a higher than usual CFG scale of 20 to 30.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: AUTOMATIC1111的Web界面支持`Img2Img`([图9-8](#figure-9-8))，这是Midjourney提交图像与提示的功能等效。通过上传图像作为模型使用的指导，它赋予你更多控制结果图像的风格和构图。为了使用`Img2Img`获得良好的结果，尝试使用`Euler`采样，50个采样步骤，以及高于常规的CFG比例20到30。
- en: '![pega 0908](assets/pega_0908.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0908](assets/pega_0908.png)'
- en: Figure 9-8\. Img2Img
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-8\. Img2Img
- en: 'The parameters are the same as the normal `Text2Image` mode with the addition
    of *denoising strength*, which controls how much random noise is added to your
    base image before running the generation process. A value of 0 will add zero noise,
    so your output will look exactly like your input, and a value of 1 will completely
    replace your input with noise (functionally the same as using `Text2Image`). Often
    you need to experiment with different combinations of values for Denoising Strength,
    CFG scale, and Seed alongside the words in your prompt. The following example
    in [Figure 9-9](#figure-9-9) creates a character in Pixar style just for fun:
    we wouldn’t recommend using protected IP in your prompt for commercial use.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 参数与正常的`Text2Image`模式相同，增加了*去噪强度*，它控制生成过程之前添加到你的基础图像中的随机噪声量。0的值将添加零噪声，所以你的输出将看起来与你的输入完全一样，而1的值将完全用噪声替换你的输入（功能上与使用`Text2Image`相同）。通常，你需要与提示中的单词一起实验不同的去噪强度、CFG比例和种子值的组合。以下图[图9-9](#figure-9-9)中的示例仅为了乐趣创建了一个皮克斯风格的字符：我们不建议在提示中使用受保护的知识产权进行商业用途。
- en: 'Input:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 9-9](#figure-9-9) shows the output.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-9](#figure-9-9)显示了输出。'
- en: '![pega 0909](assets/pega_0909.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0909](assets/pega_0909.png)'
- en: Figure 9-9\. The effect of different denoising strength values on an image
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-9. 不同去噪强度值对图像的影响
- en: If you want to test many different values for a parameter in AUTOMATIC1111 and
    generate a grid as is shown in [Figure 9-9](#figure-9-9), that is supported in
    the Script drop-down at the bottom, where you can select X/Y/Z Plot and choose
    up to three parameters to generate multiple values for. For example, you may try
    also adjusting the CFG scale to see how it interacts with Denoising. [Figure 9-10](#figure-9-10)
    shows how to select multiple values for the Denoising strength parameter. When
    you click the Generate button, a grid of images will be made, and you can find
    each individual image that populates the grid in your Output folder under the
    method (i.e., `Text2Image`, or `Img2Img`) and today’s date.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在AUTOMATIC1111中测试一个参数的许多不同值并生成如图[图9-9](#figure-9-9)所示的网格，这是在底部的脚本下拉菜单中支持的，你可以选择X/Y/Z图并选择最多三个参数来生成多个值。例如，你也可以尝试调整CFG比例来查看它与去噪的交互。图[图9-10](#figure-9-10)显示了如何选择去噪强度参数的多个值。当你点击生成按钮时，将生成一个图像网格，你可以在方法（即`Text2Image`或`Img2Img`）和今天的日期下的输出文件夹中找到填充网格的每个单独的图像。
- en: '![pega 0910](assets/pega_0910.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0910](assets/pega_0910.png)'
- en: Figure 9-10\. X/Y/Z plot of denoising parameter
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-10. 去噪参数的X/Y/Z图
- en: Evaluate Quality
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估质量
- en: Generating a grid of many different parameter combinations or values is one
    of the powerful advantages of running Stable Diffusion locally. Although it may
    take time to generate lots of images, there’s no better way to visually identify
    exactly what a parameter does and where the sweet spot is in terms of quality.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 生成许多不同参数组合或值的网格是本地运行Stable Diffusion的一个强大优势。尽管生成大量图像可能需要时间，但这是视觉上准确识别参数的作用以及质量最佳点的最佳方式。
- en: If you forgot what settings or prompt you used to generate an image, AUTOMATIC1111
    saves this as metadata on every image generated. You can visit the PNG Info tab
    ([Figure 9-11](#figure-9-11)) to read that metadata whenever needed. This also
    works with images you get from other users of the web interface, but only if they
    have posted the image on a website that doesn’t strip out this metadata.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忘记了生成图像时使用的设置或提示，AUTOMATIC1111会将这些信息作为每个生成的图像上的元数据保存。你可以访问PNG信息标签页([图9-11](#figure-9-11))来读取这些元数据，无论何时需要。这也适用于从网络界面的其他用户那里获取的图像，但前提是他们已经在一个不会删除此元数据的网站上发布了图像。
- en: The Resize Mode options are there to determine what happens when you upload
    an image that doesn’t match the dimensions of your base image, for example going
    from 1000 × 500 to 512 × 512, either stretching the aspect ratio to fit with Just
    Resize, cropping a part of the image in the right aspect ratio with Crop and Resize,
    adding noise to pad out the image with Resize and Fill, or generating an image
    in the new dimensions with Just Resize (latent upscale).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小模式选项用于确定当你上传一个与基础图像尺寸不匹配的图像时会发生什么，例如从1000 × 500到512 × 512，无论是通过仅调整大小来调整纵横比，还是通过裁剪和调整大小来裁剪图像的一部分以保持正确的纵横比，或者通过调整大小和填充来添加噪声以填充图像，或者通过仅调整大小（潜在上采样）来生成新尺寸的图像。
- en: '![pega 0911](assets/pega_0911.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0911](assets/pega_0911.png)'
- en: Figure 9-11\. PNG Info tab
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-11. PNG信息标签页
- en: Upscaling Images
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像上采样
- en: There’s also the ability to upscale images to higher resolution in AUTOMATIC1111’s
    Img2Img tab, just like you can in Midjourney but with more control. Upload your
    image and add a generic prompt like `highly detailed` in the prompt box. This
    is necessary because the upscaler works by breaking the image into tiles, expanding
    so there are gaps between the tiles, and then *filling in* the gaps using the
    prompt and context of the surrounding pixels. Go down to Scripts at the bottom
    and select the SD Upscale script, and then choose an upscaler ([Figure 9-12](#figure-9-12)).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在AUTOMATIC1111的Img2Img标签页中，也有将图像提升到更高分辨率的选项，就像你在Midjourney中可以做到的那样，但具有更多的控制。上传你的图像，并在提示框中添加一个通用的提示，例如`高度详细`。这是必要的，因为提升器通过将图像分割成瓦片，扩展以在瓦片之间产生间隙，然后使用提示和周围像素的上下文来*填充*这些间隙。向下滚动到底部的脚本，并选择SD
    Upscale脚本，然后选择一个提升器([图9-12](#figure-9-12))。
- en: '![pega 0912](assets/pega_0912.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0912](assets/pega_0912.png)'
- en: Figure 9-12\. SD Upscale interface
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-12. SD Upscale界面
- en: Typically we have found the R-ESRGAN 4x+ upscaler as a good default, but this
    can sometimes give a cartoonish quality, as shown in [Figure 9-12](#figure-9-12)
    with the grass. There are [more models](https://openmodeldb.info) available to
    test if you aren’t getting good results. When you download a new model (a *.pth*
    file), you just need to place it in the ESRGAN folder and restart the web interface
    for them to show up (in your terminal). You can also get good results with upscaling
    by modifying the prompt, particularly if you are losing some detail or the style
    is changing too much. However, it is not advised to use your original prompt,
    as that would have the strange effect of inpainting the same image in each tile.
    To show a wider quality difference, we have used the v1.5 model to generate the
    original image (SDXL creates images that are 4x larger, and at a higher quality,
    so upscaling is less needed).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我们发现R-ESRGAN 4x+提升器是一个好的默认选择，但有时这可能会给图像带来卡通般的质感，如图9-12中的草地所示。如果你没有得到好的结果，有[更多模型](https://openmodeldb.info)可供测试。当你下载一个新的模型（一个*.pth文件）时，你只需将其放置在ESRGAN文件夹中，然后重新启动Web界面，它们就会出现（在你的终端中）。你也可以通过修改提示来获得良好的提升效果，尤其是如果你丢失了一些细节或风格变化过多。然而，不建议使用你的原始提示，因为这会在每个瓦片中产生将相同图像修复的奇怪效果。为了展示更广泛的质量差异，我们使用了v1.5模型来生成原始图像（SDXL创建的图像是4倍更大，质量更高，因此提升的需求较少）。
- en: '![pega 0913](assets/pega_0913.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0913](assets/pega_0913.png)'
- en: Figure 9-13\. The impact of upscaling on sections of an image
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-13. 提升对图像部分的影响
- en: Specify Format
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定格式
- en: If you’re going to use the images you generate in the real world, often you
    can’t just use a square 512 x 512 image in low resolution. Using upscaling you
    can generate an image in any size and whatever the required resolution.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算在现实世界中使用你生成的图像，通常你无法仅使用低分辨率的512 x 512的正方形图像。使用提升技术，你可以生成任何尺寸和所需分辨率的图像。
- en: As with all things Stable Diffusion, it helps to experiment, but for good results
    we recommend a high number of steps (150–200+), a CFG scale of 8–15, and a Denoising
    strength of 0.1–0.2 to keep the base image intact. You can click Generate to get
    the resulting upscaled image (512 x 512 becomes 1024 x 1024), and then you can
    either download the higher resolution image or click Send to Img2Img and click
    Generate again to double the size of the image again. The process can take a significant
    amount of time due to the multiple tile generations and large number of sampling
    steps, approximately 10–30 minutes on a M2 MacBbook Air.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有Stable Diffusion的事物一样，实验是很有帮助的，但我们推荐使用高步骤数（150-200+），CFG比例8-15，去噪强度0.1-0.2以保持基础图像完整。你可以点击生成来获取生成的提升图像（512
    x 512变为1024 x 1024），然后你可以下载更高分辨率的图像，或者点击发送到Img2Img并再次点击生成来将图像大小加倍。由于多个瓦片生成和大量采样步骤，这个过程可能需要相当长的时间，大约在M2
    MacBbook Air上需要10-30分钟。
- en: Interrogate CLIP
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 询问CLIP
- en: In the Img2Img tab the CLIP embeddings model (which is also used by Stable Diffusion)
    is implemented in the Interrogate CLIP button (in some versions shown as a paperclip),
    which allows you to reverse engineer the prompt from an image, similar to Midjourney’s
    Describe feature, covered in [Chapter 8](ch08.html#standard_image_08). Once you
    click the button and the script has run, the prompt will appear in your prompt
    box ([Figure 9-14](#figure-9-14)).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在Img2Img标签页中，CLIP嵌入模型（也被Stable Diffusion使用）在Interrogate CLIP按钮（在某些版本中显示为回形针）中实现，这允许你从图像中逆向工程提示，类似于第8章中介绍的Midjourney的Describe功能，[第8章](ch08.html#standard_image_08)。一旦点击按钮并运行脚本，提示将出现在你的提示框中（[图9-14](#figure-9-14)）。
- en: '![pega 0914](assets/pega_0914.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0914](assets/pega_0914.png)'
- en: Figure 9-14\. Interrogate CLIP
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-14\. Interrogate CLIP
- en: 'Output:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: SD Inpainting and Outpainting
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SD修复和扩展
- en: Img2Img also supports inpainting and outpainting and provides a simple canvas
    tool for creating the mask. To use inpainting or outpainting, click the Inpaint
    subtab in the Img2Img tab and upload your image. It’s optionally recommended to
    use a specific inpainting model for better results, which you can install by [downloading](https://oreil.ly/s_trl)
    the *sd-v1-5-inpainting.ckpt* file and moving it into your Models > Stable-Diffusion
    folder. Restart the interface; the model should appear in the top left drop-down.
    The canvas allows you to use a brush to remove parts of the image just like in
    DALL-E (see [Chapter 8](ch08.html#standard_image_08)), which is adjustable in
    size for fine-grained control. In [Figure 9-15](#figure-9-15), the center of a
    stone circle in the middle of a castle courtyard has been removed.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Img2Img也支持修复和扩展，并提供了一个简单的画布工具来创建遮罩。要使用修复或扩展，请点击Img2Img标签页中的Inpaint子标签页并上传你的图像。建议使用特定的修复模型以获得更好的结果，你可以通过[下载](https://oreil.ly/s_trl)
    *sd-v1-5-inpainting.ckpt* 文件并将其移动到你的Models > Stable-Diffusion文件夹中来安装。重新启动界面；模型应该出现在左上角的下拉菜单中。画布允许你使用画笔移除图像的部分，就像在DALL-E中一样（参见[第8章](ch08.html#standard_image_08)），大小可调，以便进行精细控制。在[图9-15](#figure-9-15)中，城堡庭院中央的石头圆圈已被移除。
- en: '![pega 0915](assets/pega_0915.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0915](assets/pega_0915.png)'
- en: Figure 9-15\. Inpainting canvas in Img2Img
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-15\. Img2Img中的修复画布
- en: The advice typically given for DALL-E, which also supports inpainting, is to
    use your prompt to describe the entire image, not just the inpainted area. This
    is a good default and should be tried first. Make sure Inpaint area is set to
    *Whole picture* rather than *Only masked*, or it’ll try to fit the whole scene
    in the masked area (don’t worry, even if you select *Whole picture*, it will only
    paint in your masked area). It can also help to carry over your Seed from the
    original image if it was AI generated. However, adding to or changing the prompt
    to include specifics about the region you want modified or fixed tends to get
    better results in our experience. At the very least you should change the subject
    of the prompt; for example, in [Figure 9-15](#figure-9-15), the prompt changed
    from `castle` to `statue` because that’s what we wanted to appear in the courtyard.
    You can also try only prompting for the infilled region, though that risks getting
    an image that isn’t globally consistent in style.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DALL-E通常给出的建议，它也支持修复，是使用你的提示来描述整个图像，而不仅仅是修复区域。这是一个好的默认设置，应该首先尝试。确保将Inpaint
    area设置为*Whole picture*而不是*Only masked*，否则它将尝试将整个场景拟合到遮罩区域（不用担心，即使你选择*Whole picture*，它也只会在你遮罩的区域上绘画）。如果原始图像是AI生成的，还可以帮助将Seed从原始图像中携带过来。然而，根据我们经验，添加或更改提示以包括你想要修改或修复的区域的具体信息通常会得到更好的结果。至少，你应该更改提示的主题；例如，在[图9-15](#figure-9-15)中，提示从`castle`更改为`statue`，因为我们想要在庭院中出现雕像。你也可以尝试只提示填充区域，但这可能会得到一个在风格上不全局一致的图像。
- en: 'Input:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Figure 9-16](#figure-9-16) shows the output.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-16](#figure-9-16)显示了输出结果。'
- en: '![pega 0916](assets/pega_0916.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0916](assets/pega_0916.png)'
- en: Figure 9-16\. Inpainting to add a statue to an image
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-16\. 向图像中添加雕像的修复
- en: Providing Direction
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供方向
- en: Inpainting is so powerful because it gives you control. The ability to isolate
    an individual part of an image and give detailed directions on how to fix it gives
    you a more efficient workflow, without affecting the rest of the image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 修复功能之所以强大，是因为它赋予了你控制权。能够隔离图像的某个部分并给出详细的修复指示，使你拥有更高效的流程，而不会影响图像的其他部分。
- en: If it’s a small adjustment to the inpainted area, use Original as the masked
    content option and use a Denoising Strength of 0.2 to 0.4\. If you’re totally
    replacing an element of the image, you may need the Latent Noise option and as
    high as 0.8 for Denoising Strength, though any time you get above 0.4 you start
    to see globally inconsistent elements and hallucinations in the image, so it can
    take time to iterate toward something that works. The Fill option is also useful
    as it matches the colors of the surrounding area. If you’re getting ugly seams
    at the edge of the inpainting area, you can increase the Mask Blur, but typically
    the default of 4 works well. Inpainting is an iterative process. We recommend
    working on fixing one issue or artifact at a time, applying it as many times as
    you want, and experimenting with different parameters until you’re satisfied with
    the final image.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是对修复区域的微小调整，请使用“原始”作为遮罩内容选项，并使用0.2到0.4的降噪强度。如果您需要完全替换图像中的某个元素，可能需要使用潜在噪声选项，降噪强度可高达0.8，尽管任何超过0.4的情况都可能导致图像中出现全局不一致的元素和幻觉，因此可能需要花费时间迭代以达到满意的效果。填充选项也很有用，因为它匹配周围区域的颜色。如果您在修复区域的边缘看到难看的缝隙，可以增加遮罩模糊度，但通常默认的4已经足够好。修复是一个迭代的过程。我们建议一次修复一个问题或瑕疵，应用尽可能多次，并尝试不同的参数，直到对最终图像满意。
- en: Outpainting doesn’t work the same as in Midjourney (see [Chapter 8](ch08.html#standard_image_08)),
    which has the ability to specify 1.5x or 2x zoom, or a custom aspect ratio. Instead
    in AUTOMATIC1111, outpainting is implemented by scrolling down to the Script drop-down
    and selecting “Poor man’s outpainting.” You need to set the Resize mode to Resize
    and fill in the Img2Img Inpaint tab, and set a relatively high Denoising Strength
    to make this work. This extension allows you to expand the pixels on different
    sides of the image, while setting the Masked Content and Mask Blur parameters
    as usual for these gaps on the side to be inpainted.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Outpainting在AUTOMATIC1111中与Midjourney中的工作方式不同（见[第8章](ch08.html#standard_image_08)），Midjourney具有指定1.5倍或2倍缩放或自定义宽高比的能力。相反，在AUTOMATIC1111中，outpainting是通过向下滚动到脚本下拉菜单并选择“穷人的扩展”来实现的。您需要将调整大小模式设置为调整大小，并在Img2Img
    Inpaint标签中填写，并设置一个相对较高的降噪强度以使其工作。此扩展允许您扩展图像不同侧面的像素，同时像通常为要修复的这些侧面的缝隙设置遮罩内容和遮罩模糊参数一样设置。
- en: '[Figure 9-17](#figure-9-17) shows the output.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-17](#figure-9-17)展示了输出结果。'
- en: '![pega 0917](assets/pega_0917.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0917](assets/pega_0917.png)'
- en: Figure 9-17\. Outpainting in Img2Img
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-17\. Img2Img中的扩展
- en: As you can see in [Figure 9-17](#figure-9-17), with the extra castle being added
    to the sky, the potential for hallucination is high and the quality can be low.
    It often takes a lot of experimentation and iteration to get this process right.
    This is a similar technique to how early adopters of generative AI would add extra
    empty space on the sides of photos in Photoshop, before inpainting them to match
    the rest of the image in Stable Diffusion. This technique is essentially just
    inpainting with extra steps, so all of the same advice previously listed applies.
    This can be quicker than using the outpainting functionality in AUTOMATIC1111
    because of the poor quality and limitations of not having a proper canvas.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图9-17](#figure-9-17)所示，添加到天空中的额外城堡增加了幻觉的可能性，并且质量可能较低。通常需要大量的实验和迭代才能正确完成这个过程。这与早期采用生成式AI的用户在Photoshop中在照片两侧添加额外空白空间，然后在Stable
    Diffusion中修复以匹配图像其余部分的技术类似。这种技术本质上只是增加了额外步骤的修复，因此所有之前列出的建议都适用。这比使用AUTOMATIC1111的扩展功能更快，因为扩展功能的图像质量较差，并且没有合适的画布。
- en: ControlNet
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ControlNet
- en: Using prompting and Img2Img or base images, it’s possible to control the style
    of an image, but often the pose of people in the image, composition of the scene,
    or structure of the objects will differ greatly in the final image. ControlNet
    is an advanced way of conditioning input images for image generation models like
    Stable Diffusion.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提示和Img2Img或基础图像，可以控制图像的风格，但通常图像中人物的姿态、场景的构图或物体的结构在最终图像中会有很大的差异。ControlNet是一种为图像生成模型如Stable
    Diffusion进行输入图像条件化的高级方法。
- en: It allows you to gain more control over the final image generated through various
    techniques like edge detection, pose, depth, and many more. You upload an image
    you want to emulate and use one of the pretrained model options for processing
    the image to input alongside your prompt, resulting in a matching image composition
    with a different style ([Figure 9-18](#figure-9-18), from the [ControlNet paper](https://oreil.ly/suOJz)).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许你通过边缘检测、姿态、深度等多种技术获得对最终图像生成的更多控制。你上传一个想要模仿的图像，并使用预训练模型选项之一处理图像，以便与你的提示一起输入，从而得到不同风格的匹配图像组成（[图
    9-18](#figure-9-18)，来自 [ControlNet 论文](https://oreil.ly/suOJz)）。
- en: What’s referred to as ControlNet is really a series of [open source models](https://oreil.ly/E-bjw)
    released following the paper “Adding Conditional Control to Text-to-Image Diffusion
    Models” ([Zhang, Rao, and Agrawala, 2023](https://oreil.ly/ZH-Ow)). While it is
    possible to code this in Python and build your own user interface for it, the
    quickest and easiest way to to get up and running is via the [ControlNet](https://oreil.ly/Dw2rs)
    extension for AUTOMATIC1111\. As of the time of writing, not all ControlNet methods
    are available for SDXL, so we are using Stable Diffusion v1.5 (make sure you use
    a ControlNet model that matches the version of Stable Diffusion you’re using).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 所说的 ControlNet 实际上是一系列在论文“Adding Conditional Control to Text-to-Image Diffusion
    Models”([Zhang, Rao, and Agrawala, 2023](https://oreil.ly/ZH-Ow))之后发布的 [开源模型](https://oreil.ly/E-bjw)。虽然你可以用
    Python 编写代码并为其构建自己的用户界面，但最快、最简单的方法是通过 AUTOMATIC1111 的 [ControlNet](https://oreil.ly/Dw2rs)
    扩展程序来启动。截至写作时，并非所有 ControlNet 方法都适用于 SDXL，所以我们使用 Stable Diffusion v1.5（确保你使用与
    Stable Diffusion 版本匹配的 ControlNet 模型）。
- en: '![pega 0918](assets/pega_0918.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0918](assets/pega_0918.png)'
- en: Figure 9-18\. ControlNet Stable Diffusion with canny edge map
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-18\. ControlNet Stable Diffusion 与 canny 边缘图
- en: 'You can install the extension following these instructions:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下说明安装扩展程序：
- en: Navigate to the Extensions tab and click the subtab labeled Available.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“扩展”标签并点击标有“可用”的子标签。
- en: Click the Load from button.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“从URL加载”按钮。
- en: In the Search box type `**sd-webui-controlnet**` to find the Extension.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入 `**sd-webui-controlnet**` 以查找扩展程序。
- en: Click Install in the Action column to the far right.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最右侧的操作列中点击“安装”。
- en: Web UI will now download the necessary files and install ControlNet on your
    local version of Stable Diffusion.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Web UI 现在将下载必要的文件并在你的本地 Stable Diffusion 版本上安装 ControlNet。
- en: 'If you have trouble executing the preceding steps, you can try the following
    alternate method:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到执行前面步骤的问题，可以尝试以下替代方法：
- en: Navigate to the Extensions tab and click Install from URL subtab.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“扩展”标签并点击“从URL安装”子标签。
- en: 'In the URL field for the Git repository, paste the link to the extension: *[*https://github.com/Mikubill/sd-webui-controlnet*](https://github.com/Mikubill/sd-webui-controlnet)*.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Git 仓库的 URL 字段中，粘贴扩展程序的链接：[*https://github.com/Mikubill/sd-webui-controlnet*](https://github.com/Mikubill/sd-webui-controlnet)。
- en: Click Install.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“安装”。
- en: WebUI will download and install the necessary files for ControlNet.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WebUI 将下载并安装 ControlNet 所需的文件。
- en: Now that you have ControlNet installed, restart AUTOMATIC1111 from your terminal
    or command line, or visit Settings and click “Apply and restart UI.”
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了 ControlNet，从终端或命令行重新启动 AUTOMATIC1111，或者访问设置并点击“应用并重启 UI”。
- en: The extension will appear below the normal parameter options you get for Stable
    Diffusion, in an accordion tab ([Figure 9-19](#figure-9-19)). You first upload
    an image and then click Enable before selecting the ControlNet preprocessor and
    model you want to use. If your system has less than 6 GB of VRAM (Video Random
    Access Memory), you should check the Low VRAM box. Depending on the task at hand,
    you might want to experiment with a number of models and make adjustments to the
    parameters of those models in order to see which gets results.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展程序将出现在你为 Stable Diffusion 获得的正常参数选项下方，在一个手风琴标签中（[图 9-19](#figure-9-19)）。你首先上传一个图像，然后点击“启用”，在选择了你想要使用的
    ControlNet 预处理器和模型之后。如果你的系统 VRAM（视频随机访问内存）少于 6 GB，你应该勾选“低 VRAM”框。根据任务的不同，你可能想要尝试多种模型并调整这些模型的参数，以查看哪种能得到结果。
- en: 'Control Weight is analogous to prompt weight or influence, similar to putting
    words in brackets with a weighting `(prompt words: 1.2)`, but for the ControlNet
    input. The Starting Control Steps and Ending Control Steps are when in the diffusion
    process the ControlNet applies, by default from start to finish (0 to 1), akin
    to prompt editing/shifting such as `[prompt words::0.8]` (apply this part of the
    prompt from the beginning until 80% of the total steps are complete). Because
    the image diffuses from larger elements down to finer details, you can achieve
    different results by controlling where in that process the ControlNet applies;
    for example; removing the last 20% of steps (Ending Control Step = 0.8) may allow
    the model more creativity when filling in finer detail. The Preprocessor Resolution
    also helps maintain control here, determining how much fine detail there is in
    the intermediate image processing step. Some models have their own unique parameters,
    such as the Canny Low and High Thresholds, which determine what pixels constitute
    an *edge*. Finally, the Control Mode determines how much the model follows the
    ControlNet input relative to your prompt.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '控制权重类似于提示权重或影响，类似于在括号中放置带权重的词语 `(提示词语: 1.2)`，但对于 ControlNet 输入。起始控制步骤和结束控制步骤是
    ControlNet 在扩散过程中应用的时间，默认情况下从开始到结束（0 到 1），类似于 `[提示词语::0.8]`（从开始应用此部分提示，直到完成总步骤的
    80%）。由于图像从较大的元素扩散到更精细的细节，通过控制 ControlNet 在该过程中的应用位置，你可以实现不同的结果；例如，移除最后 20% 的步骤（结束控制步骤
    = 0.8）可能会让模型在填充更精细的细节时拥有更多的创造力。预处理分辨率也有助于在此处保持控制，确定中间图像处理步骤中有多少精细细节。一些模型有自己的独特参数，例如
    Canny 低阈值和高阈值，它们决定了哪些像素构成 *边缘*。最后，控制模式决定了模型相对于你的提示跟随 ControlNet 输入的程度。'
- en: '![pega 0919](assets/pega_0919.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0919](assets/pega_0919.png)'
- en: Figure 9-19\. ControlNet extension interface in AUTOMATIC1111
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-19\. AUTOMATIC1111 中的 ControlNet 扩展界面
- en: When you first install ControlNet, you won’t have any models downloaded. For
    them to populate in the drop-down, you should install them by downloading them
    from the [models page](https://oreil.ly/csYK_) and then dropping them in the Models
    > ControlNet folder. If you’re unsure of which model to try, start with [Canny
    edge detection](https://oreil.ly/z9XC6) as it is the most generally useful. Each
    model is relatively large (in the order of a few gigabytes), so only download
    the ones you plan to use. Following are examples from some of the more common
    models. All images in this section are generated with the `DPM++ SDE Karras` sampler,
    a CFG scale of 1.5, Control Mode set to Balanced, Resize Mode set to Crop and
    Resize (the uploaded image is cropped to match the dimensions of the generated
    image, 512 × 512), and 30 sampling steps, with the default settings for each ControlNet
    model. Version 1.5 of Stable Diffusion was used as not all of these ControlNet
    models are available for Stable Diffusion XL at the time of writing, but the techniques
    should be transferrable between models.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 当你首次安装 ControlNet 时，你将没有任何已下载的模型。为了使它们在下拉菜单中显示，你应该通过从 [模型页面](https://oreil.ly/csYK_)
    下载并然后将它们放入 Models > ControlNet 文件夹中来安装它们。如果你不确定要尝试哪个模型，可以从 [Canny 边缘检测](https://oreil.ly/z9XC6)
    开始，因为它是最通用的。每个模型相对较大（大约几吉字节），因此只下载你计划使用的模型。以下是一些常见模型的示例。本节中所有图像均使用 `DPM++ SDE
    Karras` 样本器生成，CFG 尺度为 1.5，控制模式设置为平衡，调整模式设置为裁剪和调整大小（上传的图像被裁剪以匹配生成图像的尺寸，512 × 512），以及
    30 个采样步骤，每个 ControlNet 模型的默认设置。由于并非所有这些 ControlNet 模型在撰写本文时都适用于 Stable Diffusion
    XL，因此使用了 Stable Diffusion 的 1.5 版本，但技术应可在模型之间迁移。
- en: Canny edge detection creates simple, sharp pixel outlines around areas of high
    contrast. It can be very detailed and give excellent results but can also pick
    up unwanted noise and give too much control of the image to ControlNet. In images
    where there is a high degree of detail that needs to be transferred to a new image
    with a different style, Canny excels and should be used as the default option.
    For example, redrawing a city skyline in a specific style works very well with
    the Canny model, as we did with an image of New York City (by [Robert Bye](https://oreil.ly/wEPLB)
    on [Unsplash](https://oreil.ly/_iyxU)) in [Figure 9-20](#figure-9-20).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Canny 边缘检测在对比度高的区域周围创建简单的、尖锐的像素轮廓。它可以非常详细，并给出优秀的结果，但也可以捕捉到不需要的噪声，并将过多的图像控制权交给
    ControlNet。在需要将高度详细的信息转移到具有不同风格的新的图像中时，Canny 表现优秀，应作为默认选项使用。例如，以特定风格重新绘制城市天际线与
    Canny 模型配合得非常好，就像我们在[图 9-20](#figure-9-20)中用纽约市（由[Robert Bye](https://oreil.ly/wEPLB)在[Unsplash](https://oreil.ly/_iyxU)上提供）的图片所做的那样。
- en: 'Input:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE11]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[Figure 9-20](#figure-9-20) shows the output.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-20](#figure-9-20) 展示了输出。'
- en: '![pega 0920](assets/pega_0920.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0920](assets/pega_0920.png)'
- en: Figure 9-20\. ControlNet Canny
  id: totrans-201
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-20\. ControlNet Canny
- en: Sometimes in traditional img2img prompting, some elements of an image get confused
    or merged, because Stable Diffusion doesn’t understand the depth of those objects
    in relation to each other. The Depth model creates a depth map estimation based
    on the image, which provides control over the composition and spatial position
    of image elements. If you’re not familiar with depth maps, whiter areas are closer
    to the viewer, and blacker are farther away. This can be seen in [Figure 9-21](#figure-9-21),
    where an image of a band (by [Hans Vivek](https://oreil.ly/tlCrf) on [Unsplash](https://oreil.ly/BOKJ7))
    is turned into an image of soldiers with the same positions and depth of field.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在传统的 img2img 提示中，图像的一些元素可能会混淆或合并，因为 Stable Diffusion 不理解这些物体之间的相对深度。深度模型基于图像创建一个深度图估计，这提供了对图像元素构图和空间位置的掌控。如果你不熟悉深度图，白色区域更靠近观察者，而黑色区域更远。这可以在[图
    9-21](#figure-9-21)中看到，其中一支乐队（由[Hans Vivek](https://oreil.ly/tlCrf)在[Unsplash](https://oreil.ly/BOKJ7)上提供）的图片被转换成具有相同位置和深度的士兵图像。
- en: 'Input:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE12]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[Figure 9-21](#figure-9-21) shows the output.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-21](#figure-9-21) 展示了输出。'
- en: '![pega 0921](assets/pega_0921.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0921](assets/pega_0921.png)'
- en: Figure 9-21\. ControlNet Depth
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-21\. ControlNet 深度
- en: The Normal model creates a mapping estimation that functions as a 3-D model
    of objects in the image. The colors red, green, and blue are used by 3-D programs
    to determine how smooth or bumpy an object is, with each color corresponding to
    a direction (left/right, up/down, close/far). This is just an estimation, however,
    so it can have unintended consequences in some cases. This method tends to excel
    if you need more textures and lighting to be taken into consideration but can
    sometimes offer too much detail in the case of faces, constraining the creativity
    of the output. In [Figure 9-22](#figure-9-22), a woman playing a keyboard (by
    [Soundtrap](https://oreil.ly/RP1Ei) on [Unsplash](https://oreil.ly/I3QGY)) is
    transported back in time to the *Great Gatsby* era.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 正常模型创建了一个映射估计，该估计作为一个图像中物体的 3D 模型。红色、绿色和蓝色被 3D 程序用来确定一个物体是平滑的还是凹凸不平的，每种颜色对应一个方向（左右、上下、近远）。然而，这只是一个估计，因此在某些情况下可能会产生意想不到的后果。这种方法在需要考虑更多纹理和光照时表现优秀，但在面部的情况下有时会提供过多的细节，从而限制了输出的创造性。在[图
    9-22](#figure-9-22)中，一位弹奏键盘的女士（由[Soundtrap](https://oreil.ly/RP1Ei)在[Unsplash](https://oreil.ly/I3QGY)上提供）被带回到*伟大的盖茨比*时代。
- en: 'Input:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE13]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Figure 9-22](#figure-9-22) shows the output.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-22](#figure-9-22) 展示了输出。'
- en: '![pega 0922](assets/pega_0922.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0922](assets/pega_0922.png)'
- en: Figure 9-22\. ControlNet Normal
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-22\. ControlNet 正常
- en: The OpenPose method creates a skeleton for a figure by determining its posture,
    hand placement, and facial expression. For this model to work you typically need
    to have a human subject with the full body visible, though there are portrait
    options. It is very common practice to use multiple OpenPose skeletons and compose
    them together into a single image, if multiple people are required in the scene.
    [Figure 9-23](#figure-9-23) transposes the [Mona Lisa’s pose](https://oreil.ly/7n02i)
    onto an image of Rachel Weisz.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: OpenPose 方法通过确定姿势、手部位置和面部表情为人物创建一个骨架。为了使此模型工作，通常需要一个全身可见的人类主体，尽管也有肖像选项。如果场景中需要多人，使用多个
    OpenPose 骨架并将它们组合成单个图像是非常常见的做法。[图 9-23](#figure-9-23) 将[蒙娜丽莎的姿势](https://oreil.ly/7n02i)转换到Rachel
    Weisz的图片上。
- en: 'Input:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE14]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[Figure 9-23](#figure-9-23) shows the output.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-23](#figure-9-23) 展示了输出。'
- en: '![pega 0923](assets/pega_0923.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0923](assets/pega_0923.png)'
- en: Figure 9-23\. ControlNet OpenPose
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-23\. ControlNet OpenPose
- en: The M-LSD (Mobile Line Segment Detection) technique is quite often used in architecture
    and interior design, as it’s well suited to tracing straight lines. Straight lines
    tend only to appear in man-made objects, so it isn’t well suited to nature scenes
    (though it might create an interesting effect). Man-made objects like houses are
    well suited to this approach, as shown in the image of a modern apartment (by
    [Collov Home Design](https://oreil.ly/OtV_O) on [Unsplash](https://oreil.ly/z38do))
    reimagined for the *Mad Men* era, in [Figure 9-24](#figure-9-24).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: M-LSD（移动线段检测）技术在建筑和室内设计中相当常用，因为它非常适合追踪直线。直线通常只出现在人造物体中，所以它不太适合自然场景（尽管它可能会产生有趣的效果）。像房屋这样的人造物体非常适合这种方法，如图中现代公寓的图像（由
    [Collov Home Design](https://oreil.ly/OtV_O) 在 [Unsplash](https://oreil.ly/z38do)
    上重新构想，以适应 *Mad Men* 时代，如图 9-24(#figure-9-24) 所示。
- en: 'Input:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE15]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Figure 9-24](#figure-9-24) shows the output.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-24](#figure-9-24) 展示了输出。'
- en: '![pega 0924](assets/pega_0924.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0924](assets/pega_0924.png)'
- en: Figure 9-24\. ControlNet M-LSD
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-24\. ControlNet M-LSD
- en: 'The SoftEdge technique, also known as HED (holistically-nested edge detection),
    is an alternative to Canny edge detection, creating smoother outlines around objects.
    It is very commonly used and provides good detail like Canny but can be less noisy
    and deliver more aesthetically pleasing results. This method is great for stylizing
    and recoloring images, and it tends to allow for better manipulation of faces
    compared to Canny. Thanks to ControlNet, you don’t need to enter too much of a
    detailed prompt of the overall image and can just prompt for the change you want
    to see. [Figure 9-25](#figure-9-25) shows a reimagining of [Vermeer’s *Girl with
    a Pearl Earring*](https://oreil.ly/RjUur), with Scarlett Johansson:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: SoftEdge 技术也称为 HED（整体嵌套边缘检测），是 Canny 边缘检测的替代方案，为物体周围创建更平滑的轮廓。它非常常用，提供与 Canny
    相当的细节，但可能更少噪声，并能提供更美观的结果。这种方法非常适合风格化和重新着色图像，并且与 Canny 相比，它通常允许更好地处理面部。多亏了 ControlNet，您不需要输入太多关于整体图像的详细提示，只需提示您想要看到的变化即可。[图
    9-25](#figure-9-25) 展示了对 [Vermeer 的 *戴珍珠耳环的少女*](https://oreil.ly/RjUur) 的重新构想，由
    Scarlett Johansson 扮演：
- en: 'Input:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE16]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Negative:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 负面：
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Figure 9-25](#figure-9-25) shows the output.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-25](#figure-9-25) 展示了输出。'
- en: '![pega 0925](assets/pega_0925.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0925](assets/pega_0925.png)'
- en: Figure 9-25\. ControlNet SoftEdge
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-25\. ControlNet SoftEdge
- en: Another popular technique for architecture is segmentation, which divides the
    image into related areas or segments that are somewhat related to one another.
    It is roughly analogous to using an image mask in Img2Img, except with better
    results. Segmentation can be used when you require greater command over various
    objects within an image. One powerful use case is on outdoor scenes, which can
    vary for the time of day and surroundings, or even the era. Take a look at [Figure 9-26](#figure-9-26),
    showing a modern-day photograph of a castle (by [Richard Clark](https://oreil.ly/SG9CT)
    on [Unsplash](https://oreil.ly/2FlyI)), turned into a fantasy-style castle illustration.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在建筑中流行的技术是分割，它将图像分割成相关区域或段，这些段之间多少有些关联。这大致相当于在 Img2Img 中使用图像蒙版，但结果更好。当您需要更精确地控制图像中的各种对象时，可以使用分割。一个强大的用例是户外场景，这些场景可能因一天中的时间、周围环境或甚至时代而异。看看
    [图 9-26](#figure-9-26)，展示了一座城堡的现代照片（由 [Richard Clark](https://oreil.ly/SG9CT)
    在 [Unsplash](https://oreil.ly/2FlyI) 上拍摄），变成了一个幻想风格的城堡插图。
- en: 'Input:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE18]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[Figure 9-26](#figure-9-26) shows the output.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-26](#figure-9-26) 展示了输出。'
- en: '![pega 0926](assets/pega_0926.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0926](assets/pega_0926.png)'
- en: Figure 9-26\. ControlNet segmentation
  id: totrans-239
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-26\. ControlNet 分割
- en: One powerful feature is the ability to draw on a canvas and use that in ControlNet.
    You can also draw offline and take a picture to upload your image, but it can
    be quicker for simple images to click the pencil emoji in the Stable Diffusion
    web UI, and draw with the provided brush. Even a simple scribble is often sufficient,
    and the edges don’t have to be perfect, as shown in [Figure 9-27](#figure-9-27).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强大的功能是能够在画布上绘制并使用它来控制 ControlNet。您也可以离线绘制并拍照上传图片，但对于简单图像，点击 Stable Diffusion
    网页 UI 中的铅笔表情符号并使用提供的画笔绘制可能更快。即使是简单的涂鸦也通常足够，边缘也不必完美，如图 9-27(#figure-9-27) 所示。
- en: 'Input:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[Figure 9-27](#figure-9-27) shows the output.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-27](#figure-9-27) 展示了输出。'
- en: '![pega 0927](assets/pega_0927.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0927](assets/pega_0927.png)'
- en: Figure 9-27\. ControlNet scribble
  id: totrans-245
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-27\. ControlNet 笔迹
- en: Provide Examples
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供示例
- en: ControlNet gives an AI artist the ability to make an image that *looks like*
    another image in terms of composition, simply by providing an example image to
    emulate. This allows more control over visual consistency and more flexibility
    in making more sophisticated images.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet赋予AI艺术家通过提供一个示例图像来模仿另一个图像的能力，从而在构图方面使图像看起来像另一个图像。这允许在视觉一致性方面有更多的控制，并在制作更复杂的图像方面有更多的灵活性。
- en: Each of these ControlNet methods has its own preprocessor, and they must match
    the model for the image to make sense. For example, if you’re using a Canny preprocessor,
    you should use a Canny model like control_v11p_sd15_canny. It’s also important
    to choose a model that gives enough freedom for the task you’re trying to accomplish;
    for example, an image of a cat with the SoftEdge model might perhaps have too
    much detail to be turned into a lion, and you might want to try something less
    fine-grained. As with all things Stable Diffusion, finding the exact combination
    of model and parameters requires experimentation, with new functionality and options
    proliferating all the time.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 每种ControlNet方法都有自己的预处理程序，并且它们必须与图像模型相匹配才能有意义。例如，如果你使用的是Canny预处理程序，你应该使用像control_v11p_sd15_canny这样的Canny模型。选择一个为你试图完成的任务提供足够自由度的模型也很重要；例如，使用SoftEdge模型的一张猫的图片可能细节过多，难以变成狮子，你可能想尝试更粗略的模型。与所有Stable
    Diffusion的事物一样，找到精确的模型和参数组合需要实验，因为新的功能和选项一直在不断涌现。
- en: ControlNet supports being run with a simple prompt or even without a prompt
    at all. It will match the existing image you submit and ensure a high level of
    consistency. You can run a generic prompt like `a professional, detailed, high-quality
    image` and get a good version of the existing image. Most often, however, you’ll
    be attempting to change certain aspects of the image and will want to input a
    full prompt, as in the previous examples. The resulting image will match both
    the prompt and the ControlNet output, and you can experiment with adjusting the
    parameters available to see what gets results.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ControlNet支持使用简单的提示或根本不使用提示来运行。它将匹配你提交的现有图像并确保高度的一致性。你可以运行一个通用的提示，如`a professional,
    detailed, high-quality image`，以获得现有图像的好版本。然而，大多数情况下，你将尝试改变图像的某些方面，并希望输入一个完整的提示，就像之前的例子一样。生成的图像将匹配提示和ControlNet输出，你可以通过调整可用的参数来实验，看看能得到什么结果。
- en: Segment Anything Model (SAM)
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Segment Anything Model (SAM)
- en: When working on an AI-generated image, it is often beneficial to be able to
    separate out a *mask* representing a specific person, object, or element. For
    example, dividing an image of a person from the background of the image would
    allow you to inpaint a new background behind that person. This can take a long
    time and lead to mistakes when using a brush tool, so it can be helpful to be
    able to automatically segment the image based on an AI model’s interpretation
    of where the lines are.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理由AI生成的图像时，能够分离出代表特定人物、物体或元素的*遮罩*通常很有益。例如，将人物图像从背景中分离出来，可以让你在人物后面修复新的背景。这可能会花费很长时间并导致使用画笔工具时的错误，因此能够根据AI模型对线条的解释自动分割图像可能会有所帮助。
- en: The most popular and powerful model for doing this is *SAM*, which stands for
    Segment Anything Model, [released open source on GitHub](https://oreil.ly/BuunX)
    by Meta. The model is trained on a dataset of 11 million images and 1.1 billion
    masks, and is able to infer where the image mask should be based on user input
    (clicking to add one to three dots to the image where masks should be), or it
    can automatically mask all the elements individually in an image. These masks
    can then be exported for use in inpainting,in ControlNet, or as base images.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 做这件事最受欢迎且功能强大的模型是*SAM*，即Segment Anything Model，由Meta在GitHub上[开源发布](https://oreil.ly/BuunX)。该模型在包含1100万张图像和11亿个遮罩的数据集上进行了训练，能够根据用户输入（在图像上点击添加一到三个点以确定遮罩位置）推断出图像遮罩的位置，或者它可以自动为图像中的所有元素单独生成遮罩。这些遮罩可以导出用于修复、ControlNet或作为基础图像。
- en: 'You can use SAM in the AUTOMATIC1111 interface using the [*sd-webui-segment-anything*](https://oreil.ly/rFMJN)
    extension. Once AUTOMATIC1111 is installed and running, you can install the SAM
    extension following these instructions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用[*sd-webui-segment-anything*](https://oreil.ly/rFMJN)扩展在AUTOMATIC1111界面中使用SAM。一旦AUTOMATIC1111安装并运行，你可以按照以下说明安装SAM扩展：
- en: Navigate to the Extensions tab and click the subtab labeled “Available.”
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“扩展”标签，并点击名为“可用”的子标签。
- en: Click the “Load from” button.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“从...加载”按钮。
- en: 'In the Search box type in: `**sd-webui-segment-anything**` to find the extension.'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在搜索框中输入：`**sd-webui-segment-anything**`以查找扩展。
- en: Click Install in the Action column to the far right.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最右侧的操作列中点击“安装”。
- en: WebUI will now download the necessary files and install SAM on your local version
    of Stable Diffusion.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WebUI现在将下载必要的文件并在本地Stable Diffusion版本上安装SAM。
- en: 'If you have trouble executing the preceding steps, you can try the following
    alternate method:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在执行前面的步骤时遇到困难，你可以尝试以下替代方法：
- en: Navigate to the “Extensions” tab and click the “Install from URL” subtab.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“扩展”标签并点击“从URL安装”子标签。
- en: 'In the URL field for the Git repository, paste the link to the extension: *[*https://github.com/continue-revolution/sd-webui-segment-anything*](https://github.com/continue-revolution/sd-webui-segment-anything)*.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Git仓库的URL字段中粘贴扩展的链接：[*https://github.com/continue-revolution/sd-webui-segment-anything*](https://github.com/continue-revolution/sd-webui-segment-anything)。
- en: Click Install.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“安装”。
- en: WebUI will download and install the necessary files for SAM on your local version
    of Stable Diffusion.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: WebUI将下载并安装SAM在本地Stable Diffusion版本上所需的文件。
- en: You also need to download the actual SAM model weights, linked to [from the
    repository](https://oreil.ly/IqrbI). The 1.25 GB *sam_vit_l_0b3195.pth* is what’s
    being used in this chapter. If you encounter issues with low VRAM (your computer
    freezes or lags), you should switch to smaller models. Move the model you downloaded
    into the *stable-diffusion-webui/sd-webui-segment-anything/models/sam* folder.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要下载实际的SAM模型权重，链接到[仓库](https://oreil.ly/IqrbI)。本章节使用的是1.25 GB的*sam_vit_l_0b3195.pth*。如果你遇到低VRAM（你的电脑冻结或延迟）的问题，你应该切换到较小的模型。将你下载的模型移动到*stable-diffusion-webui/sd-webui-segment-anything/models/sam*文件夹。
- en: Now that you have SAM fully installed, restart AUTOMATIC1111 from your terminal
    or command line, or visit Settings and click “Apply and restart UI.”
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完全安装了SAM，从你的终端或命令行重新启动AUTOMATIC1111，或者访问设置并点击“应用并重启UI”。
- en: You should see the extension in the Img2Img tab, by scrolling down past the
    canvas and Seed parameter, in an accordion component alongside the ControlNet
    extension. Upload an image here (we used the photo for [Figure 9-28](#figure-9-28)
    by [Luca Baini](https://oreil.ly/Lb3xE) on [Unsplash](https://oreil.ly/jvCjz))
    and click the image to select individual prompt points. These prompt points go
    along to SAM as user input to help the model determine what should be segmented
    out from the image. You can click Preview to see what mask will be created, and
    iteratively add or remove plot points until the mask is correct. There is a checkbox
    labeled “Preview automatically when add/remove points,” which updates the mask
    with each click. Often SAM gets it right with a single plot point, but if you
    are struggling, you can also add negative plot points to parts of the image you
    don’t want to mask by right-clicking. Select the mask you want ([Figure 9-28](#figure-9-28))
    from the three options provided (counting from 0 to 2).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在Img2Img标签中看到扩展，通过滚动过画布和Seed参数，在ControlNet扩展旁边的折叠组件中。在此处上传一张图片（我们使用了[图9-28](#figure-9-28)的图片，由[Luca
    Baini](https://oreil.ly/Lb3xE)在[Unsplash](https://oreil.ly/jvCjz)提供）并点击图片以选择单个提示点。这些提示点将作为用户输入传递给SAM，以帮助模型确定从图像中应该分割出什么。你可以点击“预览”来查看将创建什么蒙版，并迭代地添加或删除绘图点，直到蒙版正确为止。有一个标记为“添加/删除点时自动预览”的复选框，它会在每次点击时更新蒙版。通常SAM只需要一个绘图点就能正确分割，但如果你遇到困难，你也可以通过右键点击添加负绘图点到你不希望蒙版的图像部分。从提供的三个选项中选择你想要的蒙版（从0到2计数）。
- en: '![pega 0928](assets/pega_0928.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0928](assets/pega_0928.png)'
- en: Figure 9-28\. Adding plot points
  id: totrans-268
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-28\. 添加绘图点
- en: When your mask is ready, make sure the box Copy to Inpaint Upload & img2img
    ControlNet Inpainting is checked, and click the Switch to Inpaint Upload button.
    You won’t see anything happen visually, but when you switch to the Inpainting
    tab, you should be able to generate your prompt with the mask generated by SAM.
    There is no need to upload the picture or mask to the Inpainting tab. You can
    also download your mask for later upload in the “Inpaint upload” tab. This method
    was unreliable during our testing, and there may be a better supported method
    for inpainting with SAM and Stable Diffusion made available.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的蒙版准备好后，确保勾选“复制到Inpaint上传 & img2img ControlNet Inpainting”的框，然后点击“切换到Inpaint上传”按钮。你不会在视觉上看到任何变化，但当你切换到Inpainting标签时，你应该能够使用SAM生成的蒙版生成你的提示。不需要将图片或蒙版上传到Inpainting标签。你还可以在“上传蒙版”标签中下载你的蒙版以供稍后上传。这种方法在我们的测试中不可靠，可能还有更好的支持SAM和Stable
    Diffusion的修复蒙版方法。
- en: Divide Labor
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分工
- en: Generative models like Midjourney and Stable Diffusion are powerful, but they
    can’t do everything. In training a separate image segmentation model, Meta has
    made it possible to generate more complex images by splitting out the elements
    of an image into different masks, which can be worked on separately before being
    aggregated together for the final product.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型如Midjourney和Stable Diffusion非常强大，但它们不能做所有事情。在训练一个单独的图像分割模型时，Meta公司通过将图像的元素分割成不同的掩码，使得生成更复杂的图像成为可能，这些掩码可以单独处理，然后再汇总成最终产品。
- en: DreamBooth Fine-Tuning
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DreamBooth微调
- en: 'The original Stable Diffusion model cost a reported [$600,000 to train](https://oreil.ly/s739b)
    using a total of 150,000 GPU hours, so training your own foundational model is
    likely out of the question for most organizations. However, it is possible to
    build on top of Stable Diffusion, using the Dreambooth technique, which was introduced
    in the paper “DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven
    Generation” ([Ruiz et al., 2022](https://oreil.ly/ZqdjB)). DreamBooth allows you
    to fine-tune or train the model to understand a new concept it hasn’t encountered
    yet in its training data. Not having to start from scratch to build a new model
    means significantly less time and resources: about 45 minutes to an hour on 1
    GPU. DreamBooth actually updates the weights of the new model, which gives you
    a new 2 GB model file to use in AUTOMATIC1111 instead of the base Stable Diffusion
    model.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '据报道，原始的Stable Diffusion模型训练成本为[600,000美元](https://oreil.ly/s739b)，总共使用了150,000个GPU小时，因此对于大多数组织来说，训练自己的基础模型可能是不切实际的。然而，可以在Stable
    Diffusion的基础上构建，使用在论文“DreamBooth: Fine Tuning Text-to-Image Diffusion Models for
    Subject-Driven Generation”([Ruiz et al., 2022](https://oreil.ly/ZqdjB))中引入的DreamBooth技术。DreamBooth允许您微调或训练模型，使其能够理解其训练数据中尚未遇到的新概念。不必从头开始构建新模型意味着显著减少时间和资源：在1个GPU上大约需要45分钟到1小时。实际上，DreamBooth会更新新模型的权重，这为您提供了一个新的2
    GB模型文件，可以在AUTOMATIC1111中使用，而不是使用基础Stable Diffusion模型。'
- en: 'There are many DreamBooth-based models available on websites like [Hugging
    Face](https://oreil.ly/2efOO) and [Civitai](https://civitai.com). To use these
    models in AUTOMATIC1111, you simply download them and move them into the stable-diffusion-webui/models/Stable-diffusion/
    folder. Dreambooth models often have a specific word or token needed for triggering
    the style or subject, which must be included in the prompt. For example, the [Inkpunk
    Diffusion](https://oreil.ly/spsy3) model requires the word *nvinkpunk*. Note:
    the underlying base model here is v1.5 of Stable Diffusion, so reset your image
    size to 512 × 512.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在[Hugging Face](https://oreil.ly/2efOO)和[Civitai](https://civitai.com)等网站上有很多基于DreamBooth的模型可供使用。要在AUTOMATIC1111中使用这些模型，您只需下载它们并将它们移动到`stable-diffusion-webui/models/Stable-diffusion/`文件夹中。Dreambooth模型通常需要一个特定的词或标记来触发风格或主题，这个标记必须包含在提示中。例如，[Inkpunk
    Diffusion](https://oreil.ly/spsy3)模型需要单词*nvinkpunk*。注意：这里的底层基础模型是Stable Diffusion的v1.5版本，因此请将图像大小重置为512
    × 512。
- en: 'Input:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE20]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Figure 9-29](#figure-9-29) shows the output.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-29](#figure-9-29)展示了输出结果。'
- en: '![pega 0929](assets/pega_0929.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0929](assets/pega_0929.png)'
- en: Figure 9-29\. InkPunk skateboarder
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-29\. InkPunk滑板运动员
- en: Divide Labor
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分工
- en: The mistake many people make with AI is assuming there’s one model to rule them
    all. In reality there are many creative models out there, and often training on
    a specific task yields better results than the general foundational models. While
    the foundation models like Stable Diffusion XL are what most practicioners start
    with, commonly they begin to experiment with fine-tuning their own models on specific
    tasks, often based on smaller, more efficient models like v1.5.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人使用AI时犯的一个错误是认为有一个模型可以统治一切。实际上，有许多创造性的模型存在，通常在特定任务上的训练比通用的基础模型能产生更好的结果。虽然像Stable
    Diffusion XL这样的基础模型是大多数实践者开始的地方，但通常他们开始尝试在特定任务上微调自己的模型，这些模型通常是更小、更高效的，如v1.5版本。
- en: The preferred method for training a DreamBooth model is [Shivam Shrirao’s repository](https://oreil.ly/AJnnL),
    which uses HuggingFace’s `diffusers` library. What follows is an explanation of
    the code in [Google Colab](https://oreil.ly/790FZ). Version 1.5 is used in this
    notebook, as it is a smaller model, and is able to be trained in a few hours in
    the Google Colab environment for free. A copy of this Python notebook is saved
    in the [GitHub repository](https://oreil.ly/NzzGm) for this book for posterity,
    but it should be noted that it will only run on an Nvidia GPU, not on a MacBook.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 训练DreamBooth模型的推荐方法是[Shivam Shrirao的仓库](https://oreil.ly/AJnnL)，它使用HuggingFace的`diffusers`库。以下是对[Google
    Colab](https://oreil.ly/790FZ)中代码的解释。在这个笔记本中使用的是版本1.5，因为它是一个较小的模型，并且能够在谷歌Colab环境中免费训练几小时。这个Python笔记本的副本被保存在[GitHub仓库](https://oreil.ly/NzzGm)中，以供后人参考，但应注意的是，它只能在Nvidia
    GPU上运行，不能在MacBook上运行。
- en: 'First the Colab checks whether there is access to an Nvidia GPU. This is one
    good reason to run Dreambooth on Google Colab, because you are given access to
    the right resource to run the code without any configuration needed:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Colab会检查是否有访问Nvidia GPU的权限。这是在谷歌Colab上运行Dreambooth的一个很好的理由，因为您将获得运行代码所需的正确资源，而无需任何配置：
- en: '[PRE21]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next the necessary libraries are installed, including the `diffusers` library
    from Hugging Face:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，安装必要的库，包括来自Hugging Face的`diffusers`库：
- en: '[PRE22]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Run the next cell to set the output directory of the model when it is finished
    running. It’s recommended to save the model to Google Drive (even if temporarily)
    because you can more reliably download large files (4–5 GB) from there than you
    can from the Google Colab filesystem. Ensure that you have selected the right
    base model from the Hugging Face hub `runwayml/stable-diffusion-v1-5` and choose
    a name for your token for the output directory (usually *ukj* or *zwx*; more on
    this later):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 运行下一个单元格以设置模型完成运行后的输出目录。建议将模型保存到谷歌驱动（即使暂时保存），因为您可以从那里比从谷歌Colab文件系统更可靠地下载大文件（4-5
    GB）。请确保您已从Hugging Face hub中选择正确的基模型`runwayml/stable-diffusion-v1-5`，并为输出目录选择一个token名称（通常是*ukj*或*zwx*；关于这一点稍后会有更多说明）：
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Before training, you need to add the concepts you want to train on. In our
    experience, training on multiple concepts tends to harm performance, so typically
    we would train on only one subject or style. You can merge models later in the
    Checkpoint Merger tab of AUTOMATIC1111, although this gets into more advanced
    territory not covered in this book. The instance prompt includes the token you’ll
    use in your prompt to trigger the model, and ideally it’s a word that doesn’t
    have any other meaning, like *zwx* or *ukj*. The class prompt is a starting point
    for the training, so if you’re training a model of a specific person, you start
    from `photo of a person` to make the training more effective:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，您需要添加您想要训练的概念。根据我们的经验，在多个概念上进行训练往往会损害性能，所以我们通常会只在一个主题或风格上进行训练。您可以在AUTOMATIC1111的检查点合并标签页中稍后合并模型，尽管这涉及到本书未涵盖的更高级领域。实例提示包括您将在提示中使用的token，理想情况下是一个没有其他含义的词，如*zwx*或*ukj*。类别提示是训练的起点，因此如果您正在训练一个特定人物的模型，您应从`照片中的人物`开始以使训练更有效：
- en: '[PRE24]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we upload the images through Google Colab. Dreambooth can work with as
    few as 5 images, but typically it’s recommended you use about 20–30 images, although
    some train with hundreds of images. One creative use case is to use the Consistent
    Characters method discussed in [Chapter 8](ch08.html#standard_image_08) to generate
    20 different images of the same AI-generated character and use them to train a
    Dreambooth model on. Alternatively, you could upload 20 pictures of yourself to
    create an AI profile photo, or 20 pictures of a product your company sells to
    generate AI product photography. You can upload the files locally to the *instance_data_dir*
    in the Google Colab filesystem (which can be faster) or run the next cell to get
    an upload button:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过谷歌Colab上传图片。Dreambooth可以使用尽可能少的5张图片，但通常建议您使用大约20-30张图片，尽管有些人使用数百张图片进行训练。一个创造性的用例是使用第8章中讨论的**一致性角色方法**来生成20个相同的AI生成角色的不同图像，并使用它们来训练Dreambooth模型。或者，您可以上传20张自己的照片来创建AI个人资料照片，或者上传20张您公司销售的产品照片来生成AI产品摄影。您可以将文件本地上传到谷歌Colab文件系统中的*instance_data_dir*（这可能更快）或运行下一个单元格以获取上传按钮：
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now the actual training begins! This code runs on the GPU and outputs the final
    weights when finished. Make sure to change `save_sample_prompt` before running
    to use the token you assigned, in this case `photo of ukj person`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在真正的训练开始了！这段代码在GPU上运行，并在完成后输出最终的权重。确保在运行之前更改`save_sample_prompt`，以便使用你分配的令牌，在这种情况下是`photo
    of ukj person`：
- en: '[PRE26]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now that the training is complete, the next two cells of code define the directory
    and then display a grid of images so you can see visually whether the model correctly
    understood your concept and is now capable of generating useful images of your
    style of subject:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在训练已经完成，接下来的两个代码单元格定义了目录，然后显示一个图像网格，这样你可以直观地看到模型是否正确理解了你的概念，并且现在能够生成你风格的主题的有用图像：
- en: '[PRE27]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, you want to run the conversion process to get a *.ckpt* file, which
    is what you will use in AUTOMATIC1111:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要运行转换过程以获取*.ckpt*文件，这是你将在AUTOMATIC1111中使用的文件：
- en: '[PRE28]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: You can then visit the weights directory *stable_diffusion_weights/zwx* to find
    the model and download it. If you are having issues downloading such a large file
    from the Google Colab filesystem, try checking the option to save to Google Drive
    before running the model, and download from there. We recommend renaming the model
    before dropping it into your *stable-diffusion-webui/models/Stable-diffusion/*
    folder so you can tell what model it is when using it later.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以访问权重目录*stable_diffusion_weights/zwx*来找到模型并下载它。如果你在从Google Colab文件系统下载如此大的文件时遇到问题，请在运行模型之前尝试检查保存到Google
    Drive的选项，并从那里下载。我们建议在将模型放入你的*stable-diffusion-webui/models/Stable-diffusion/*文件夹之前重命名模型，这样你以后在使用时可以知道它是哪个模型。
- en: 'Input:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE29]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Figure 9-30](#figure-9-30) shows the output.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9-30](#figure-9-30)显示了输出。'
- en: '![pega 0930](assets/pega_0930.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0930](assets/pega_0930.png)'
- en: Figure 9-30\. A Dreambooth model image of one of the authors
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-30\. 作者之一的一个Dreambooth模型图像
- en: There is also [an extension](https://oreil.ly/xbt2d) for training Dreambooth
    models via Automatic1111, based on Shivam Shrirao’s method. This extension can
    be installed in the same way as you installed ControlNet and Segment Anything
    in previous sections of this chapter. This tool is for advanced users as it exposes
    a significant number of features and settings for experimentation, many of which
    you need to be a machine learning expert to understand. To start learning what
    these parameters and settings mean so you can experiment with different options,
    check out the [beginner’s guide to training](https://oreil.ly/gfdY3) in the extension
    wiki. The benefit of using this method instead of Google Colab is that it runs
    locally on your computer, so you can leave it running without worrying it will
    time out and lose progress.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个[扩展](https://oreil.ly/xbt2d)，用于通过Automatic1111训练Dreambooth模型，基于Shivam
    Shrirao的方法。这个扩展可以像安装ControlNet和Segment Anything一样安装。这个工具是针对高级用户的，因为它暴露了大量的功能和设置供实验，其中许多需要你是机器学习专家才能理解。要开始学习这些参数和设置的含义，以便你可以尝试不同的选项，请查看扩展wiki中的[训练入门指南](https://oreil.ly/gfdY3)。使用这种方法而不是Google
    Colab的好处是它在你的计算机上本地运行，所以你可以放心地让它运行，不用担心它会超时并丢失进度。
- en: Provide Examples
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供示例
- en: Dreambooth helps you personalize your experience with generative AI. You just
    need to supply 5–30 images that serve as examples of a concept, and in less than
    an hour of training time, you can have a fully personalized custom model.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Dreambooth帮助你通过生成式AI个性化你的体验。你只需要提供5-30张作为概念示例的图像，在不到一小时的训练时间内，你就可以拥有一个完全个性化的定制模型。
- en: 'There are other training and fine-tuning methods available besides Dreambooth,
    but this technique is currently the most commonly used. An older technique is
    [Textual Inversion](https://oreil.ly/GgnJV), which doesn’t update the model weights
    but instead approximates the right location for a token to represent your concept,
    though this tends to perform far worse than Dreambooth. One promising new technique
    is LoRA, from the paper “LoRA: Low-Rank Adaptation of Large Language Models” ([Hu
    et al., 2021](https://oreil.ly/NtoiB)), also prevalent in the text-generation
    space with LLMs. This technique adds new layers into the model and trains just
    those new layers to build a custom model without expending too many resources.
    There are also Hypernetworks, which train parameters that can then generate these
    new layers, as [introduced by Kurumuz](https://oreil.ly/zFH0-) in the Medium article
    “NovelAI Improvements on Stable Diffusion.” Both of these methods are experimental
    and only make up a small number of the models on Civitai at the time of writing
    (less than 10%), as well as having in general lower user ratings in terms of quality.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Dreambooth 之外，还有其他训练和微调方法可用，但这项技术目前是最常用的。一种较老的技术是 [文本反转](https://oreil.ly/GgnJV)，它不会更新模型权重，而是近似地确定一个标记表示您概念的正确位置，尽管这种方法的性能往往远低于
    Dreambooth。一种有希望的新技术是 LoRA，来自论文“LoRA：大型语言模型的低秩自适应”（[Hu 等人，2021](https://oreil.ly/NtoiB)），在文本生成空间中与
    LLMs 频繁出现。这项技术向模型中添加新的层，并仅训练这些新层来构建自定义模型，而不会消耗太多资源。还有超网络，它训练可以生成这些新层的参数，正如 [Kurumuz](https://oreil.ly/zFH0-)
    在 Medium 文章“NovelAI 对 Stable Diffusion 的改进”中所述。这两种方法都是实验性的，并且在撰写本文时在 Civitai 上的模型中只占一小部分（不到
    10%），并且在总体上用户评分较低。
- en: Stable Diffusion XL Refiner
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Stable Diffusion XL 精炼器
- en: The SDXL v1.0 model has 6.6 billion parameters, compared to 0.98 billion for
    the v1.5 model ([Rombach et al., 2023](https://oreil.ly/vc1zS)). The increased
    firepower yields impressive results, and as such the model is starting to win
    over die-hard 1.5 enthusiasts. Part of the power of SDXL comes from the division
    of labor between the base model, which sets the global composition, and a refiner
    model ([Figure 9-31](#figure-9-31)), which adds finer details (optional).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL v1.0 模型有 66 亿个参数，而 v1.5 模型有 9.8 亿个参数（[Rombach 等人，2023](https://oreil.ly/vc1zS)）。增加的计算能力带来了令人印象深刻的成果，因此该模型开始赢得坚定的
    1.5 粉丝。SDXL 的部分力量来自基础模型和精炼器模型之间的劳动分工（[图 9-31](#figure-9-31)），基础模型设置全局构图，而精炼器模型添加更精细的细节（可选）。
- en: '![pega 0931](assets/pega_0931.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0931](assets/pega_0931.png)'
- en: Figure 9-31\. Stable Diffusion XL base and refiner model
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-31\. Stable Diffusion XL 基础和精炼器模型
- en: 'The underlying language model that infers meaning from your prompts is a combination
    of OpenClip (ViT-G/14) and OpenAI’s CLIP ViT-L. Stable Diffusion v2 used OpenClip
    alone and therefore prompts that worked on v1.5 were not as transferable: that
    problem has been largely solved with SDXL. Additionally, the SDXL model has been
    trained with a more diverse set of image sizes, leading to better results when
    you need an image that isn’t the standard square aspect ratio. [Stablity AI’s
    research](https://oreil.ly/_b7xX) indicates that users overwhelmingly prefer the
    XL model over v1.5 ([Figure 9-32](#figure-9-32)).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 从您的提示中推断意义的底层语言模型是 OpenClip (ViT-G/14) 和 OpenAI 的 CLIP ViT-L 的组合。Stable Diffusion
    v2 仅使用 OpenClip，因此 v1.5 上有效的提示在 v2 中并不容易迁移：这个问题在 SDXL 中得到了很大程度的解决。此外，SDXL 模型使用更多样化的图像尺寸进行训练，当您需要非标准方形比例的图像时，这会导致更好的结果。[Stability
    AI 的研究](https://oreil.ly/_b7xX)表明，用户普遍更喜欢 XL 模型而不是 v1.5（[图 9-32](#figure-9-32)）。
- en: '![pega 0932](assets/pega_0932.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0932](assets/pega_0932.png)'
- en: Figure 9-32\. Relative performance preference
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-32\. 相对性能偏好
- en: To make use of the refiner model, you must utilize the “Switch at” functionality
    in the AUTOMATIC1111 interface. This value controls at which step the pipeline
    switches to the refiner model. For example, switching at 0.6 with 30 steps means
    the base model will be used for the first 18 steps, and then it will switch to
    the refiner model for the final 12 steps ([Figure 9-33](#figure-9-33)).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用精炼器模型，您必须在 AUTOMATIC1111 界面中利用“切换至”功能。此值控制管道在哪个步骤切换到精炼器模型。例如，在 0.6 步骤切换，使用
    30 步，意味着基础模型将用于前 18 步，然后切换到精炼器模型进行最后的 12 步（[图 9-33](#figure-9-33)）。
- en: '![pega 0933](assets/pega_0933.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0933](assets/pega_0933.png)'
- en: Figure 9-33\. Refiner—Switch at parameter
  id: totrans-318
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-33\. 精炼器—切换至参数
- en: Common advice is to switch between 0.4 and 1.0 (a value of 1.0 will not switch
    and only uses the base model), with 20–50 sampling steps for the best results.
    In our experience, switching at 0.6 with 30 sampling steps produces the highest-quality
    image, but like all things Stable Diffusion, you must experiment to discover what
    gets the best results for your image. Setting the refiner to switch at 0.6 gives
    the output shown in [Figure 9-35](#figure-9-35).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 常见建议是在 0.4 和 1.0 之间切换（1.0 的值不会切换，仅使用基础模型），使用 20-50 个采样步骤以获得最佳结果。根据我们的经验，在 0.6
    处切换并使用 30 个采样步骤可以产生最高质量的图像，但像所有 Stable Diffusion 一样，你必须进行实验以发现最适合你图像的最佳结果。将细化器设置为在
    0.6 处切换会得到 [图 9-35](#figure-9-35) 中所示的输出。
- en: 'Input:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE30]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Negative:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 负面：
- en: '[PRE31]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[Figure 9-34](#figure-9-34) shows the output.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9-34](#figure-9-34) 展示了输出。'
- en: '![pega 0934](assets/pega_0934.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0934](assets/pega_0934.png)'
- en: Figure 9-34\. Anime cat girl with SDXL base model versus refiner at 0.6
  id: totrans-326
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-34\. 使用 SDXL 基础模型与细化器在 0.6 的动漫猫娘
- en: Divide Labor
  id: totrans-327
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分工合作
- en: The architecture of SDXL is a perfect example of splitting a task into multiple
    jobs, and using the right model for the job. The base model sets the scene and
    guides the composition of the image, while the refiner increases fine detail.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: SDXL 的架构是分割任务为多个工作并使用适合该工作的正确模型的完美示例。基础模型设置场景并引导图像的构图，而细化器则增加精细细节。
- en: One quality-of-life modification you can make is to install the aspect ratio
    selector extension, which can be loaded with image sizes or aspect ratios you
    use regularly, allowing one-click setting of the correct size and aspect ratio
    for either model.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做出的一个生活质量改进是安装长宽比选择器扩展，它可以加载你经常使用的图像大小或长宽比，允许一键设置模型正确的尺寸和长宽比。
- en: 'To install the extension, browse to the Extensions tab, go to Install from
    URL, paste in *[*https://github.com/alemelis/sd-webui-ar*](https://github.com/alemelis/sd-webui-ar)*,
    and click Install. Go to the extension folder stable-diffusion-webui/extensions/sd-webui-ar
    and add the following to the *resolutions.txt* file (or replace what’s there for
    cleanliness):'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装扩展，浏览到扩展选项卡，转到从 URL 安装，粘贴 *[*https://github.com/alemelis/sd-webui-ar*](https://github.com/alemelis/sd-webui-ar)*，然后点击安装。转到扩展文件夹
    stable-diffusion-webui/extensions/sd-webui-ar 并将以下内容添加到 *resolutions.txt* 文件中（或替换现有内容以保持整洁）：
- en: '[PRE32]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Clicking one of these preset buttons will automatically adjust the width and
    height accordingly. You may also replace *aspect ratios.txt* with the following,
    allowing you to automatically calculate the aspect ratio based on the height value
    you have set in the web UI, and they’ll show in the web UI interface ([Figure 9-35](#figure-9-35)):'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 点击这些预设按钮之一将自动相应地调整宽度和高度。你也可以用以下内容替换 *aspect ratios.txt*，允许你根据在 Web UI 中设置的身高值自动计算长宽比，它们将在
    Web UI 界面中显示 ([图 9-35](#figure-9-35))：
- en: '[PRE33]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![pega 0935](assets/pega_0935.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0935](assets/pega_0935.png)'
- en: Figure 9-35\. Aspect ratios
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-35\. 长宽比
- en: Summary
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned advanced techniques for image generation using
    Stable Diffusion, an open source model. If you followed along, you successfully
    installed Stable Diffusion and built an inference pipeline using the HuggingFace
    *diffusers* library. You hopefully generated images based on prompts using the
    Stable Diffusion inference model in Google Colab. Additionally, this chapter recommended
    exploring the open source community and user interfaces like AUTOMATIC1111 for
    running Stable Diffusion with advanced features.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了使用 Stable Diffusion 生成图像的高级技术，Stable Diffusion 是一个开源模型。如果你跟随着步骤，你成功安装了
    Stable Diffusion 并使用 HuggingFace *diffusers* 库构建了一个推理管道。你希望使用 Stable Diffusion
    推理模型在 Google Colab 中根据提示生成图像。此外，本章建议探索开源社区和用户界面，如 AUTOMATIC1111，以运行具有高级功能的 Stable
    Diffusion。
- en: The chapter also introduced the concept of ControlNet, which allows for controlling
    the style of an image using prompting and base images, and Segment Anything, a
    model for masking specific parts of an image. By applying these techniques, you
    are now able to customize generated images to meet your specific needs. You also
    learned about techniques for personalization, specifically DreamBooth fine-tuning,
    allowing you to train a model to understand new concepts not encountered in its
    training data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还介绍了 ControlNet 的概念，它允许通过提示和基础图像来控制图像的风格，以及 Segment Anything，这是一个用于遮罩图像特定部分的模型。通过应用这些技术，你现在能够根据特定需求定制生成的图像。你还了解到了个性化技术，特别是
    DreamBooth 微调，它允许你训练一个模型来理解其训练数据中未遇到的新概念。
- en: In the next chapter, you’ll get the chance to put everything you’ve learned
    throughout this book into action. We’ll be exploring how to build an AI blog post
    generator that produces both the blog text and an accompanying image. That final
    exciting chapter will take you through the process of creating an end-to-end system
    that generates high-quality blog posts based on user input, complete with custom
    illustrations in a consistent visual style. You’ll learn how to optimize prompts,
    generate engaging titles, and create AI-generated images that match your desired
    style!
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将有机会将本书中学到的所有知识付诸实践。我们将探讨如何构建一个既能生成博客文本又能生成配套图片的人工智能博客文章生成器。那最后一章将令人兴奋，它将带你通过创建一个端到端系统的过程，该系统能根据用户输入生成高质量的博客文章，并附带风格一致的定制插图。你将学习如何优化提示词、生成吸引人的标题，以及创建符合你期望风格的AI生成图像！
