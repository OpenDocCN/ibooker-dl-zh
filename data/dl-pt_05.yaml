- en: 5 The mechanics of learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 学习的机制
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Understanding how algorithms can learn from data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解算法如何从数据中学习
- en: Reframing learning as parameter estimation, using differentiation and gradient
    descent
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将学习重新定义为参数估计，使用微分和梯度下降
- en: Walking through a simple learning algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 走进一个简单的学习算法
- en: How PyTorch supports learning with autograd
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch如何通过自动求导支持学习
- en: 'With the blooming of machine learning that has occurred over the last decade,
    the notion of machines that learn from experience has become a mainstream theme
    in both technical and journalistic circles. Now, how is it exactly that a machine
    learns? What are the mechanics of this process--or, in words, what is the *algorithm*
    behind it? From the point of view of an observer, a learning algorithm is presented
    with input data that is paired with desired outputs. Once learning has occurred,
    that algorithm will be capable of producing correct outputs when it is fed new
    data that is *similar enough* to the input data it was trained on. With deep learning,
    this process works even when the input data and the desired output are *far* from
    each other: when they come from different domains, like an image and a sentence
    describing it, as we saw in chapter 2.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着过去十年中机器学习的蓬勃发展，从经验中学习的机器的概念已经成为技术和新闻界的主题。那么，机器是如何学习的呢？这个过程的机制是什么--或者说，背后的*算法*是什么？从观察者的角度来看，一个学习算法被提供了与期望输出配对的输入数据。一旦学习发生，当它被喂入与其训练时的输入数据*足够相似*的新数据时，该算法将能够产生正确的输出。通过深度学习，即使输入数据和期望输出相距*很远*，这个过程也能够工作：当它们来自不同的领域时，比如一幅图像和描述它的句子，正如我们在第2章中看到的那样。
- en: 5.1 A timeless lesson in modeling
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 建模中的永恒教训
- en: Building models that allow us to explain input/output relationships dates back
    centuries at least. When Johannes Kepler, a German mathematical astronomer (1571-1630),
    figured out his three laws of planetary motion in the early 1600s, he based them
    on data collected by his mentor Tycho Brahe during naked-eye observations (yep,
    seen with the naked eye and written on a piece of paper). Not having Newton’s
    law of gravitation at his disposal (actually, Newton used Kepler’s work to figure
    things out), Kepler extrapolated the simplest possible geometric model that could
    fit the data. And, by the way, it took him six years of staring at data that didn’t
    make sense to him, together with incremental realizations, to finally formulate
    these laws.[¹](#pgfId-1011855) We can see this process in figure 5.1.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 允许我们解释输入/输出关系的建模模型至少可以追溯到几个世纪前。当德国数学天文学家约翰内斯·开普勒（1571-1630）在17世纪初发现他的三大行星运动定律时，他是基于他的导师第谷·布拉赫在裸眼观测（是的，用肉眼看到并写在一张纸上）中收集的数据。没有牛顿的万有引力定律（实际上，牛顿使用了开普勒的工作来解决问题），开普勒推断出了可能适合数据的最简单几何模型。顺便说一句，他花了六年时间盯着他看不懂的数据，连续的领悟，最终制定了这些定律。[¹](#pgfId-1011855)
    我们可以在图5.1中看到这个过程。
- en: '![](../Images/CH05_F01_Stevens2_GS.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F01_Stevens2_GS.png)'
- en: Figure 5.1 Johannes Kepler considers multiple candidate models that might fit
    the data at hand, settling on an ellipse.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 约翰内斯·开普勒考虑了多个可能符合手头数据的模型，最终选择了一个椭圆。
- en: 'Kepler’s first law reads: “The orbit of every planet is an ellipse with the
    Sun at one of the two *foci*.” He didn’t know what caused orbits to be ellipses,
    but given a set of observations for a planet (or a moon of a large planet, like
    Jupiter), he could estimate the shape (the *eccentricity*) and size (the *semi-latus
    rectum*) of the ellipse. With those two parameters computed from the data, he
    could tell where the planet might be during its journey in the sky. Once he figured
    out the second law--“A line joining a planet and the Sun sweeps out equal areas
    during equal intervals of time”--he could also tell *when* a planet would be at
    a particular point in space, given observations in time.[²](#pgfId-1011938)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 开普勒的第一定律是：“每颗行星的轨道都是一个椭圆，太阳位于两个*焦点*之一。”他不知道是什么导致轨道是椭圆的，但是在给定一个行星（或大行星的卫星，比如木星）的一组观测数据后，他可以估计椭圆的形状（*离心率*）和大小（*半通径矢量*）。通过从数据中计算出这两个参数，他可以预测行星在天空中的运行轨迹。一旦他弄清楚了第二定律--“连接行星和太阳的一条线在相等的时间间隔内扫过相等的面积”--他也可以根据时间观测推断出行星何时会在空间中的特定位置。[²](#pgfId-1011938)
- en: 'So, how did Kepler estimate the eccentricity and size of the ellipse without
    computers, pocket calculators, or even calculus, none of which had been invented
    yet? We can learn how from Kepler’s own recollection, in his book *New Astronomy*,
    or from how J. V. Field put it in his series of articles, “The origins of proof,”
    ([http://mng.bz/9007](http://mng.bz/9007)):'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，开普勒如何在没有计算机、口袋计算器甚至微积分的情况下估计椭圆的离心率和大小呢？我们可以从开普勒自己在他的书《新天文学》中的回忆中学到，或者从J.V.菲尔德在他的一系列文章“证明的起源”中的描述中了解（[http://mng.bz/9007](http://mng.bz/9007)）：
- en: Essentially, Kepler had to try different shapes, using a certain number of observations
    to find the curve, then use the curve to find some more positions, for times when
    he had observations available, and then check whether these calculated positions
    agreed with the observed ones.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，开普勒不得不尝试不同的形状，使用一定数量的观测结果找到曲线，然后使用曲线找到更多位置，用于他有观测结果可用的时间，然后检查这些计算出的位置是否与观测到的位置一致。
- en: --J. V. Field
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: --J.V.菲尔德
- en: So let’s sum things up. Over six years, Kepler
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么让我们总结一下。在六年的时间里，开普勒
- en: Got lots of good data from his friend Brahe (not without some struggle)
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从他的朋友布拉赫那里得到了大量的好数据（不是没有一点挣扎）
- en: Tried to visualize the heck out of it, because he felt there was something fishy
    going on
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试尽可能将其可视化，因为他觉得有些不对劲
- en: Chose the simplest possible model that had a chance to fit the data (an ellipse)
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择可能适合数据的最简单模型（椭圆）
- en: Split the data so that he could work on part of it and keep an independent set
    for validation
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割，以便他可以处理其中一部分，并保留一个独立的集合用于验证
- en: Started with a tentative eccentricity and size for the ellipse and iterated
    until the model fit the observations
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个椭圆的初步离心率和大小开始，并迭代直到模型符合观测结果
- en: Validated his model on the independent observations
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在独立观测上验证了他的模型
- en: Looked back in disbelief
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 惊讶地回顾过去
- en: There’s a data science handbook for you, all the way from 1609\. The history
    of science is literally constructed on these seven steps. And we have learned
    over the centuries that deviating from them is a recipe for disaster.[³](#pgfId-1017676)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为你准备了一本数据科学手册，一直延续至1609年。科学的历史实际上是建立在这七个步骤上的。几个世纪以来，我们已经学会了偏离这些步骤是灾难的前兆。
- en: 'This is exactly what we will set out to do in order to *learn* something from
    data. In fact, in this book there is virtually no difference between saying that
    we’ll *fit* the data or that we’ll make an algorithm *learn* from data. The process
    always involves a function with a number of unknown parameters whose values are
    estimated from data: in short, a *model*.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是我们将要从数据中*学习*的内容。事实上，在这本书中，几乎没有区别是说我们将*拟合*数据还是让算法从数据中*学习*。这个过程总是涉及一个具有许多未知参数的函数，其值是从数据中估计的：简而言之，一个*模型*。
- en: We can argue that *learning from data* presumes the underlying model is not
    engineered to solve a specific problem (as was the ellipse in Kepler’s work) and
    is instead capable of approximating a much wider family of functions. A neural
    network would have predicted Tycho Brahe’s trajectories really well without requiring
    Kepler’s flash of insight to try fitting the data to an ellipse. However, Sir
    Isaac Newton would have had a much harder time deriving his laws of gravitation
    from a generic model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以认为*从数据中学习*意味着底层模型并非是为解决特定问题而设计的（就像开普勒的椭圆一样），而是能够逼近更广泛函数族的模型。一个神经网络可以非常好地预测第谷·布拉赫的轨迹，而无需开普勒的灵感来尝试将数据拟合成椭圆。然而，艾萨克·牛顿要从一个通用模型中推导出他的引力定律就要困难得多。
- en: In this book, we’re interested in models that are not engineered for solving
    a specific narrow task, but that can be automatically adapted to specialize themselves
    for any one of many similar tasks using input and output pairs--in other words,
    general models trained on data relevant to the specific task at hand. In particular,
    PyTorch is designed to make it easy to create models for which the derivatives
    of the fitting error, with respect to the parameters, can be expressed analytically.
    No worries if this last sentence didn’t make any sense at all; coming next, we
    have a full section that hopefully clears it up for you.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们对不是为解决特定狭窄任务而设计的模型感兴趣，而是可以自动调整以专门为任何一个类似任务进行自我特化的模型--换句话说，根据与手头特定任务相关的数据训练的通用模型。特别是，PyTorch旨在使创建模型变得容��，使拟合误差对参数的导数能够被解析地表达。如果最后一句话让你完全不明白，别担心；接下来，我们有一个完整的章节希望为你澄清这一点。
- en: This chapter is about how to automate generic function-fitting. After all, this
    is what we do with deep learning--deep neural networks being the generic functions
    we’re talking about--and PyTorch makes this process as simple and transparent
    as possible. In order to make sure we get the key concepts right, we’ll start
    with a model that is a lot simpler than a deep neural network. This will allow
    us to understand the mechanics of learning algorithms from first principles in
    this chapter, so we can move to more complicated models in chapter 6\.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论如何自动化通用函数拟合。毕竟，这就是我们用深度学习做的事情--深度神经网络就是我们谈论的通用函数--而PyTorch使这个过程尽可能简单透明。为了确保我们理解关键概念正确，我们将从比深度神经网络简单得多的模型开始。这将使我们能够从本章的第一原则理解学习算法的机制，以便我们可以在第6章转向更复杂的模型。
- en: 5.2 Learning is just parameter estimation
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 学习只是参数估计
- en: 'In this section, we’ll learn how we can take data, choose a model, and estimate
    the parameters of the model so that it will give good predictions on new data.
    To do so, we’ll leave the intricacies of planetary motion and divert our attention
    to the second- hardest problem in physics: calibrating instruments.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何利用数据，选择一个模型，并估计模型的参数，以便在新数据上进行良好的预测。为此，我们将把注意力从行星运动的复杂性转移到物理学中第二难的问题：校准仪器。
- en: Figure 5.2 shows the high-level overview of what we’ll implement by the end
    of the chapter. Given input data and the corresponding desired outputs (ground
    truth), as well as initial values for the weights, the model is fed input data
    (forward pass), and a measure of the error is evaluated by comparing the resulting
    outputs to the ground truth. In order to optimize the parameter of the model--its
    *weights*--the change in the error following a unit change in weights (that is,
    the *gradient* of the error with respect to the parameters) is computed using
    the chain rule for the derivative of a composite function (backward pass). The
    value of the weights is then updated in the direction that leads to a decrease
    in the error. The procedure is repeated until the error, evaluated on unseen data,
    falls below an acceptable level. If what we just said sounds obscure, we’ve got
    a whole chapter to clear things up. By the time we’re done, all the pieces will
    fall into place, and this paragraph will make perfect sense.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2展示了本章末尾我们将要实现的高层概述。给定输入数据和相应的期望输出（标准答案），以及权重的初始值，模型接收输入数据（前向传播），通过将生成的输出与标准答案进行比较来评估误差的度量。为了优化模型的参数--其*权重*，使用复合函数的导数链式法则（反向传播）计算单位权重变化后误差的变化（即误差关于参数的*梯度*）。然后根据导致误差减少的方向更新权重的值。该过程重复进行，直到在未见数据上评估的误差降至可接受水平以下。如果我们刚才说的听起来晦涩难懂，我们有整整一章来澄清事情。到我们完成时，所有的部分都会成为一体，这段文字将变得非常清晰。
- en: We’re now going to take a problem with a noisy dataset, build a model, and implement
    a learning algorithm for it. When we start, we’ll be doing everything by hand,
    but by the end of the chapter we’ll be letting PyTorch do all the heavy lifting
    for us. When we finish the chapter, we will have covered many of the essential
    concepts that underlie training deep neural networks, even if our motivating example
    is very simple and our model isn’t actually a neural network (yet!).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将处理一个带有嘈杂数据集的问题，构建一个模型，并为其实现一个��习算法。当我们开始时，我们将手工完成所有工作，但在本章结束时，我们将让PyTorch为我们完成所有繁重的工作。当我们完成本章时，我们将涵盖训练深度神经网络的许多基本概念，即使我们的激励示例非常简单，我们的模型实际上并不是一个神经网络（但！）。
- en: '![](../Images/CH05_F02_Stevens2_GS.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F02_Stevens2_GS.png)'
- en: Figure 5.2 Our mental model of the learning process
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 我们对学习过程的心理模型
- en: 5.2.1 A hot problem
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 一个热门问题
- en: 'We just got back from a trip to some obscure location, and we brought back
    a fancy, wall-mounted analog thermometer. It looks great, and it’s a perfect fit
    for our living room. Its only flaw is that it doesn’t show units. Not to worry,
    we’ve got a plan: we’ll build a dataset of readings and corresponding temperature
    values in our favorite units, choose a model, adjust its weights iteratively until
    a measure of the error is low enough, and finally be able to interpret the new
    readings in units we understand.[⁴](#pgfId-1017897)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚从某个偏僻的地方旅行回来，带回了一个时髦的壁挂式模拟温度计。它看起来很棒，完全适合我们的客厅。它唯一的缺点是它不显示单位。不用担心，我们有一个计划：我们将建立一个读数和相应温度值的数据集，选择一个模型，迭代调整其权重直到误差的度量足够低，最终能够以我们理解的单位解释新的读数。[⁴](#pgfId-1017897)
- en: 'Let’s try following the same process Kepler used. Along the way, we’ll use
    a tool he never had available: PyTorch!'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试按照开普勒使用的相同过程进行。在这个过程中，我们将使用一个他从未拥有过的工具：PyTorch！
- en: 5.2.2 Gathering some data
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 收集一些数据
- en: 'We’ll start by making a note of temperature data in good old Celsius[⁵](#pgfId-1017936)
    and measurements from our new thermometer, and figure things out. After a couple
    of weeks, here’s the data (code/p1ch5/1_parameter_estimation.ipynb):'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先记录摄氏度的温度数据和我们新温度计的测量值，并弄清楚事情。几周后，这是数据（code/p1ch5/1_parameter_estimation.ipynb）：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the `t_c` values are temperatures in Celsius, and the `t_u` values are
    our unknown units. We can expect noise in both measurements, coming from the devices
    themselves and from our approximate readings. For convenience, we’ve already put
    the data into tensors; we’ll use it in a minute.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`t_c`值是摄氏度温度，`t_u`值是我们未知的单位。我们可以预期两个测量中都会有噪音，来自设备本身和我们的近似读数。为了方便起见，我们已经将数据放入张量中；我们将在一分钟内使用它。
- en: 5.2.3 Visualizing the data
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 可视化数据
- en: A quick plot of our data in figure 5.3 tells us that it’s noisy, but we think
    there’s a pattern here.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 中我们数据的快速绘图告诉我们它很嘈杂，但我们认为这里有一个模式。
- en: '![](../Images/CH05_F03_Stevens2_GS.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F03_Stevens2_GS.png)'
- en: Figure 5.3 Our unknown data just might follow a linear model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 我们的未知数据可能遵循一个线性模型。
- en: '*Note* Spoiler alert: we know a linear model is correct because the problem
    and data have been fabricated, but please bear with us. It’s a useful motivating
    example to build our understanding of what PyTorch is doing under the hood.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 剧透警告：我们知道线性模型是正确的，因为问题和数据都是虚构的，但请耐心等待。这是一个有用的激励性例子，可以帮助我们理解PyTorch在幕后做了什么。'
- en: 5.2.4 Choosing a linear model as a first try
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 选择线性模型作为第一次尝试
- en: 'In the absence of further knowledge, we assume the simplest possible model
    for converting between the two sets of measurements, just like Kepler might have
    done. The two may be linearly related--that is, multiplying `t_u` by a factor
    and adding a constant, we may get the temperature in Celsius (up to an error that
    we omit):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有进一步的知识的情况下，我们假设将两组测量值之间转换的最简单模型，就像开普勒可能会做的那样。这两者可能是线性相关的--也就是说，通过乘以一个因子并添加一个常数，我们可以得到摄氏度的温度（我们忽略的误差）：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Is this a reasonable assumption? Probably; we’ll see how well the final model
    performs. We chose to name `w` and `b` after *weight* and *bias*, two very common
    terms for linear scaling and the additive constant--we’ll bump into those all
    the time.[⁶](#pgfId-1018187)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这个假设合理吗？可能；我们将看到最终模型的表现如何。我们选择将`w`和`b`命名为*权重*和*偏差*，这是线性缩放和加法常数的两个非常常见的术语--我们将一直遇到这些术语。[⁶](#pgfId-1018187)
- en: OK, now we need to estimate `w` and `b`, the parameters in our model, based
    on the data we have. We must do it so that temperatures we obtain from running
    the unknown temperatures `t_u` through the model are close to temperatures we
    actually measured in Celsius. If that sounds like fitting a line through a set
    of measurements, well, yes, because that’s exactly what we’re doing. We’ll go
    through this simple example using PyTorch and realize that training a neural network
    will essentially involve changing the model for a slightly more elaborate one,
    with a few (or a metric ton) more parameters.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要根据我们拥有的数据来估计`w`和`b`，即我们模型中的参数。我们必须这样做，以便通过将未知温度`t_u`输入模型后得到的温度接近我们实际测量的摄氏度温度。如果这听起来像是在一组测量值中拟合一条直线，那么是的，因为这正是我们正在做的。我们将使用PyTorch进行这个简单的例子，并意识到训练神经网络实质上涉及将模型更改为稍微更复杂的模型，其中有一些（或者是一吨）更多的参数。
- en: 'Let’s flesh it out again: we have a model with some unknown parameters, and
    we need to estimate those parameters so that the error between predicted outputs
    and measured values is as low as possible. We notice that we still need to exactly
    define a measure of the error. Such a measure, which we refer to as the *loss
    function*, should be high if the error is high and should ideally be as low as
    possible for a perfect match. Our optimization process should therefore aim at
    finding `w` and `b` so that the loss function is at a minimum.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次详细说明一下：我们有一个具有一些未知参数的模型，我们需要估计这些参数，以便预测输出和测量值之间的误差尽可能低。我们注意到我们仍然需要准确定义一个误差度量。这样一个度量，我们称之为*损失函数*，如果误差很大，应该很高，并且应该在完美匹配时尽可能低。因此，我们的优化过程应该旨在找到`w`和`b`，使得损失函数最小化。
- en: 5.3 Less loss is what we want
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 较少的损失是我们想要的
- en: 'A *loss function* (or *cost function*) is a function that computes a single
    numerical value that the learning process will attempt to minimize. The calculation
    of loss typically involves taking the difference between the desired outputs for
    some training samples and the outputs actually produced by the model when fed
    those samples. In our case, that would be the difference between the predicted
    temperatures `t_p` output by our model and the actual measurements: `t_p - t_c`.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*损失函数*（或*成本函数*）是一个计算单个数值的函数，学习过程将尝试最小化该数值。损失的计算通常涉及取一些训练样本的期望输出与模型在馈送这些样本时实际产生的输出之间的差异。在我们的情况下，这将是我们的模型输出的预测温度`t_p`与实际测量值之间的差异：`t_p
    - t_c`。'
- en: We need to make sure the loss function makes the loss positive both when `t_p`
    is greater than and when it is less than the true `t_c`, since the goal is for
    `t_p` to match `t_c`. We have a few choices, the most straightforward being `|t_p
    - t_c|` and `(t_p - t_c)^2`. Based on the mathematical expression we choose, we
    can emphasize or discount certain errors. Conceptually, a loss function is a way
    of prioritizing which errors to fix from our training samples, so that our parameter
    updates result in adjustments to the outputs for the highly weighted samples instead
    of changes to some other samples’ output that had a smaller loss.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要确保损失函数在`t_p`大于真实`t_c`和小于真实`t_c`时都是正的，因为目标是让`t_p`匹配`t_c`。我们有几种选择，最直接的是`|t_p
    - t_c|`和`(t_p - t_c)^2`。根据我们选择的数学表达式，我们可以强调或折扣某些错误。概念上，损失函数是一种优先考虑从我们的训练样本中修复哪些错误的方法，以便我们的参数更新导致对高权重样本的输出进行调整，而不是对一些其他样本的输出进行更改，这些样本的损失较小。
- en: Both of the example loss functions have a clear minimum at zero and grow monotonically
    as the predicted value moves further from the true value in either direction.
    Because the steepness of the growth also monotonically increases away from the
    minimum, both of them are said to be *convex*. Since our model is linear, the
    loss as a function of `w` and `b` is also convex.[⁷](#pgfId-1018463) Cases where
    the loss is a convex function of the model parameters are usually great to deal
    with because we can find a minimum very efficiently through specialized algorithms.
    However, we will instead use less powerful but more generally applicable methods
    in this chapter. We do so because for the deep neural networks we are ultimately
    interested in, the loss is not a convex function of the inputs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个示例损失函数在零处有明显的最小值，并且随着预测值向任一方向偏离真值，它们都会单调增加。由于增长的陡峭度也随着远离最小值而单调增加，它们都被称为*凸函数*。由于我们的模型是线性的，所以损失作为`w`和`b`的函数也是凸的。[⁷](#pgfId-1018463)
    损失作为模型参数的凸函数的情况通常很容易处理，因为我们可以通过专门的算法非常有效地找到最小值。然而，在本章中，我们将使用功能更弱但更普遍适用的方法。我们这样做是因为对于我们最终感兴趣的深度神经网络，损失不是输入的凸函数。
- en: 'For our two loss functions `|t_p - t_c|` and `(t_p - t_c)^2`, as shown in figure
    5.4, we notice that the square of the differences behaves more nicely around the
    minimum: the derivative of the error-squared loss with respect to `t_p` is zero
    when `t_p` equals `t_c`. The absolute value, on the other hand, has an undefined
    derivative right where we’d like to converge. This is less of an issue in practice
    than it looks like, but we’ll stick to the square of differences for the time
    being.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的两个损失函数`|t_p - t_c|`和`(t_p - t_c)^2`，如图5.4所示，我们注意到差的平方在最小值附近的行为更好：对于`t_p`，误差平方损失的导数在`t_p`等于`t_c`时为零。另一方面，绝对值在我们希望收敛的地方具有未定义的导数。实际上，这在实践中并不是问题，但我们暂时将坚持使用差的平方。
- en: '![](../Images/CH05_F04_Stevens2_GS.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F04_Stevens2_GS.png)'
- en: Figure 5.4 Absolute difference versus difference squared
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 绝对差与差的平方
- en: It’s worth noting that the square difference also penalizes wildly wrong results
    more than the absolute difference does. Often, having more slightly wrong results
    is better than having a few wildly wrong ones, and the squared difference helps
    prioritize those as desired.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，平方差也比绝对差惩罚更严重的错误。通常，有更多略微错误的结果比有几个极端错误的结果更好，而平方差有助于按预期优先考虑这些结果。
- en: 5.3.1 From problem back to PyTorch
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 从问题返回到PyTorch
- en: We’ve figured out the model and the loss function--we’ve already got a good
    part of the high-level picture in figure 5.2 figured out. Now we need to set the
    learning process in motion and feed it actual data. Also, enough with math notation;
    let’s switch to PyTorch--after all, we came here for the *fun*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了模型和损失函数--我们已经在图5.2的高层图片中找到了一个很好的部分。现在我们需要启动学习过程并提供实际数据。另外，数学符号够了；让我们切换到PyTorch--毕竟，我们来这里是为了*乐趣*。
- en: 'We’ve already created our data tensors, so now let’s write out the model as
    a Python function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了我们的数据张量，现在让我们将模型写成一个Python函数：
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We’re expecting `t_u`, `w`, and `b` to be the input tensor, weight parameter,
    and bias parameter, respectively. In our model, the parameters will be PyTorch
    scalars (aka zero-dimensional tensors), and the product operation will use broadcasting
    to yield the returned tensors. Anyway, time to define our loss:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望`t_u`，`w`和`b`分别是输入张量，权重参数和偏置参数。在我们的模型中，参数将是PyTorch标量（也称为零维张量），并且乘法操作将使用广播产生返回的张量。无论如何，是时候定义我们的损失了：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that we are building a tensor of differences, taking their square element-wise,
    and finally producing a scalar loss function by averaging all of the elements
    in the resulting tensor. It is a *mean square loss*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们正在构建一个差异张量，逐元素取平方，最终通过平均所有结果张量中的元素产生一个标量损失函数。这是一个*均方损失*。
- en: We can now initialize the parameters, invoke the model,
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以初始化参数，调用模型，
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'and check the value of the loss:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 并检查损失的值：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We implemented the model and the loss in this section. We’ve finally reached
    the meat of the example: how do we estimate `w` and `b` such that the loss reaches
    a minimum? We’ll first work things out by hand and then learn how to use PyTorch’s
    superpowers to solve the same problem in a more general, off-the-shelf way.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中实现了模型和损失。我们终于到达了示例的核心：我们如何估计`w`和`b`，使损失达到最小？我们首先手动解决问题，然后学习如何使用PyTorch的超能力以更通用、现成的方式解决相同的问题。
- en: Broadcasting
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 广播
- en: We mentioned broadcasting in chapter 3, and we promised to look at it more carefully
    when we need it. In our example, we have two scalars (zero-dimensional tensors)
    `w` and `b`, and we multiply them with and add them to vectors (one-dimensional
    tensors) of length b.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第3章提到了广播，并承诺在需要时更仔细地研究它。在我们的例子中，我们有两个标量（零维张量）`w`和`b`，我们将它们与长度为b的向量（一维张量）相乘并相加。
- en: Usually--and in early versions of PyTorch, too--we can only use element-wise
    binary operations such as addition, subtraction, multiplication, and division
    for arguments of the same shape. The entries in matching positions in each of
    the tensors will be used to calculate the corresponding entry in the result tensor.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通常——在PyTorch的早期版本中也是如此——我们只能对形状相同的参数使用逐元素二元操作，如加法、减法、乘法和除法。在每个张量中的匹配位置的条目将用于计算结果张量中相应条目。
- en: 'Broadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this
    assumption for most binary operations. It uses the following rules to match tensor
    elements:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 广播，在NumPy中很受欢迎，并被PyTorch采用，放宽了大多数二元操作的这一假设。它使用以下规则来匹配张量元素：
- en: For each index dimension, counted from the back, if one of the operands is size
    1 in that dimension, PyTorch will use the single entry along this dimension with
    each of the entries in the other tensor along this dimension.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个索引维度，从后往前计算，如果其中一个操作数在该维度上的大小为1，则PyTorch将使用该维度上的单个条目与另一个张量沿着该维度的每个条目。
- en: If both sizes are greater than 1, they must be the same, and natural matching
    is used.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个大小都大于1，则它们必须相同，并且使用自然匹配。
- en: If one of the tensors has more index dimensions than the other, the entirety
    of the other tensor will be used for each entry along these dimensions.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个张量中一个的索引维度比另一个多，则另一个张量的整体将用于沿着这些维度的每个条目。
- en: This sounds complicated (and it can be error-prone if we don’t pay close attention,
    which is why we have named the tensor dimensions as shown in section 3.4), but
    usually, we can either write down the tensor dimensions to see what happens or
    picture what happens by using space dimensions to show the broadcasting, as in
    the following figure.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很复杂（如果我们不仔细注意，可能会出错，这就是为什么我们在第3.4节中将张量维度命名的原因），但通常，我们可以写下张量维度来看看会发生什么，或者通过使用空间维度来展示广播的方式来想象会发生什么，就像下图所示。
- en: 'Of course, this would all be theory if we didn’t have some code examples:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果没有一些代码示例，这一切都只是理论：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/CH05_F05_Stevens2_GS_B.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F05_Stevens2_GS_B.png)'
- en: '[PRE7]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 5.4 Down along the gradient
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 沿着梯度下降
- en: We’ll optimize the loss function with respect to the parameters using the *gradient
    descent* algorithm. In this section, we’ll build our intuition for how gradient
    descent works from first principles, which will help us a lot in the future. As
    we mentioned, there are ways to solve our example problem more efficiently, but
    those approaches aren’t applicable to most deep learning tasks. Gradient descent
    is actually a very simple idea, and it scales up surprisingly well to large neural
    network models with millions of parameters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用*梯度下降*算法优化参数的损失函数。在本节中，我们将从第一原理建立对梯度下降如何工作的直觉，这将在未来对我们非常有帮助。正如我们提到的，有更有效地解决我们示例问题的方法，但这些方法并不适用于大多数深度学习任务。梯度下降实际上是一个非常简单的想法，并且在具有数百万参数的大型神经网络模型中表现出色。
- en: '![](../Images/CH05_F06_Stevens2_GS.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F06_Stevens2_GS.png)'
- en: Figure 5.5 A cartoon depiction of the optimization process, where a person with
    knobs for w and b searches for the direction to turn the knobs that makes the
    loss decrease
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 优化过程的卡通描绘，一个人带有w和b旋钮，寻找使损失减少的旋钮转动方向
- en: 'Let’s start with a mental image, which we conveniently sketched out in figure
    5.5\. Suppose we are in front of a machine sporting two knobs, labeled `w` and
    `b`. We are allowed to see the value of the loss on a screen, and we are told
    to minimize that value. Not knowing the effect of the knobs on the loss, we start
    fiddling with them and decide for each knob which direction makes the loss decrease.
    We decide to rotate both knobs in their direction of decreasing loss. Suppose
    we’re far from the optimal value: we’d likely see the loss decrease quickly and
    then slow down as it gets closer to the minimum. We notice that at some point,
    the loss climbs back up again, so we invert the direction of rotation for one
    or both knobs. We also learn that when the loss changes slowly, it’s a good idea
    to adjust the knobs more finely, to avoid reaching the point where the loss goes
    back up. After a while, eventually, we converge to a minimum.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个心理形象开始，我们方便地在图5.5中勾画出来。假设我们站在一台带有标有`w`和`b`的两个旋钮的机器前。我们可以在屏幕上看到损失值，并被告知要将该值最小化。不知道旋钮对损失的影响，我们开始摆弄它们，并为每个旋钮决定哪个方向使损失减少。我们决定将两个旋钮都旋转到损失减少的方向。假设我们离最佳值很远：我们可能会看到损失迅速减少，然后随着接近最小值而减慢。我们注意到在某个时刻，损失再次上升，因此我们反转一个或两个旋钮的旋转方向。我们还了解到当损失变化缓慢时，调整旋钮更精细是个好主意，以避免达到损失再次上升的点。过一段时间，最终，我们收敛到一个最小值。
- en: 5.4.1 Decreasing loss
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 减小损失
- en: 'Gradient descent is not that different from the scenario we just described.
    The idea is to compute the rate of change of the loss with respect to each parameter,
    and modify each parameter in the direction of decreasing loss. Just like when
    we were fiddling with the knobs, we can estimate the rate of change by adding
    a small number to `w` and `b` and seeing how much the loss changes in that neighborhood:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降与我们刚刚描述的情景并没有太大不同。其思想是计算损失相对于每个参数的变化率，并将每个参数修改为减小损失的方向。就像我们在调节旋钮时一样，我们可以通过向`w`和`b`添加一个小数并观察在该邻域内损失的变化来估计变化率：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This is saying that in the neighborhood of the current values of `w` and `b`,
    a unit increase in `w` leads to some change in the loss. If the change is negative,
    then we need to increase `w` to minimize the loss, whereas if the change is positive,
    we need to decrease `w`. By how much? Applying a change to `w` that is proportional
    to the rate of change of the loss is a good idea, especially when the loss has
    several parameters: we apply a change to those that exert a significant change
    on the loss. It is also wise to change the parameters slowly in general, because
    the rate of change could be dramatically different at a distance from the neighborhood
    of the current `w` value. Therefore, we typically should scale the rate of change
    by a small factor. This scaling factor has many names; the one we use in machine
    learning is `learning_rate`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在当前`w`和`b`的值的邻域内，增加`w`会导致损失发生一些变化。如果变化是负的，那么我们需要增加`w`以最小化损失，而如果变化是正的，我们需要减少`w`。增加多少？根据损失的变化率对`w`应用变化是个好主意，特��是当损失有几个参数时：我们对那些对损失产生显著变化的参数应用变化。通常，总体上缓慢地改变参数是明智的，因为在当前`w`值的邻域之外，变化率可能截然不同。因此，我们通常应该通过一个小因子来缩放变化率。这个缩放因子有许多名称；我们在机器学习中使用的是`learning_rate`：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can do the same with `b`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用`b`做同样的事情：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This represents the basic parameter-update step for gradient descent. By reiterating
    these evaluations (and provided we choose a small enough learning rate), we will
    converge to an optimal value of the parameters for which the loss computed on
    the given data is minimal. We’ll show the complete iterative process soon, but
    the way we just computed our rates of change is rather crude and needs an upgrade
    before we move on. Let’s see why and how.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这代表了梯度下降的基本参数更新步骤。通过重复这些评估（并且只要我们选择足够小的学习率），我们将收敛到使给定数据上计算的损失最小的参数的最佳值。我们很快将展示完整的迭代过程，但我们刚刚计算变化率的方式相当粗糙，在继续之前需要进行升级。让我们看看为什么以及如何。
- en: 5.4.2 Getting analytical
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 进行分析
- en: Computing the rate of change by using repeated evaluations of the model and
    loss in order to probe the behavior of the loss function in the neighborhood of
    `w` and `b` doesn’t scale well to models with many parameters. Also, it is not
    always clear how large the neighborhood should be. We chose `delta` equal to 0.1
    in the previous section, but it all depends on the shape of the loss as a function
    of `w` and `b`. If the loss changes too quickly compared to `delta`, we won’t
    have a very good idea of in which direction the loss is decreasing the most.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重复评估模型和损失来计算变化率，以探究在`w`和`b`邻域内损失函数的行为的方法在具有许多参数的模型中不具有良好的可扩展性。此外，并不总是清楚邻域应该有多大。我们在前一节中选择了`delta`等于0.1，但这完全取决于损失作为`w`和`b`函数的形状。如果损失相对于`delta`变化太快，我们将无法很好地了解损失减少最多的方向。
- en: 'What if we could make the neighborhood infinitesimally small, as in figure
    5.6? That’s exactly what happens when we analytically take the derivative of the
    loss with respect to a parameter. In a model with two or more parameters like
    the one we’re dealing with, we compute the individual derivatives of the loss
    with respect to each parameter and put them in a vector of derivatives: the *gradient*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以使邻域无限小，就像图5.6中那样，会发生什么？这正是当我们对参数的损失进行导数分析时发生的情况。在我们处理的具有两个或更多参数的模型中，我们计算损失相对于每个参数的各个导数，并将它们放入导数向量中：*梯度*。
- en: '![](../Images/CH05_F07_Stevens2_GS.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F07_Stevens2_GS.png)'
- en: Figure 5.6 Differences in the estimated directions for descent when evaluating
    them at discrete locations versus analytically
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 在离散位置评估时下降方向的估计差异与分析方法
- en: Computing the derivatives
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 计算导数
- en: 'In order to compute the derivative of the loss with respect to a parameter,
    we can apply the chain rule and compute the derivative of the loss with respect
    to its input (which is the output of the model), times the derivative of the model
    with respect to the parameter:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算损失相对于参数的导数，我们可以应用链式法则，并计算损失相对于其输入（即模型的输出）的导数，乘以模型相对于参数的导数：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Recall that our model is a linear function, and our loss is a sum of squares.
    Let’s figure out the expressions for the derivatives. Recalling the expression
    for the loss:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们的模型是一个线性函数，我们的损失是平方和。让我们找出导数的表达式。回想一下损失的表达式：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Remembering that `d x^2 / d x = 2 x`, we get
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 记住`d x^2 / d x = 2 x`，我们得到
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The division is from the derivative of mean.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分割是来自均值的导数。
- en: Applying the derivatives to the model
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将导数应用于模型
- en: For the model, recalling that our model is
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型，回想一下我们的模型是
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'we get these derivatives:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到这些导数：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Defining the gradient function
  id: totrans-115
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 定义梯度函数
- en: Putting all of this together, the function returning the gradient of the loss
    with respect to `w` and `b` is
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，返回损失相对于`w`和`b`的梯度的函数是
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The summation is the reverse of the broadcasting we implicitly do when applying
    the parameters to an entire vector of inputs in the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 求和是我们��模型中将参数应用于整个输入向量时隐式执行的广播的反向。
- en: The same idea expressed in mathematical notation is shown in figure 5.7\. Again,
    we’re averaging (that is, summing and dividing by a constant) over all the data
    points to get a single scalar quantity for each partial derivative of the loss.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 用数学符号表示相同的想法如图5.7所示。再次，我们对所有数据点进行平均（即，求和并除以一个常数），以获得每个损失的偏导数的单个标量量。
- en: '![](../Images/CH05_F07_Stevens2_GSII.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F07_Stevens2_GSII.png)'
- en: Figure 5.7 The derivative of the loss function with respect to the weights
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 损失函数相对于权重的导数
- en: 5.4.3 Iterating to fit the model
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 迭代拟合模型
- en: We now have everything in place to optimize our parameters. Starting from a
    tentative value for a parameter, we can iteratively apply updates to it for a
    fixed number of iterations, or until `w` and `b` stop changing. There are several
    stopping criteria; for now, we’ll stick to a fixed number of iterations.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好优化我们的参数了。从参数的一个暂定值开始，我们可以迭代地对其应用更新，进行固定次数的迭代，或直到`w`和`b`停止改变。有几个停止标准；现在，我们将坚持固定次数的迭代。
- en: The training loop
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练循环
- en: Since we’re at it, let’s introduce another piece of terminology. We call a training
    iteration during which we update the parameters for all of our training samples
    an *epoch*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们在这里，让我们介绍另一个术语。我们称之为训练迭代，我们在其中为所有训练样本更新参数一个*时代*。
- en: 'The complete training loop looks like this (code/p1ch5/1_parameter_estimation
    .ipynb):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的训练循环如下（code/p1ch5/1_parameter_estimation .ipynb）：
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Forward pass
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 正向传播
- en: ❷ Backward pass
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 反向传播
- en: ❸ This logging line can be very verbose.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这个记录行可能非常冗长。
- en: 'The actual logging logic used for the output in this text is more complicated
    (see cell 15 in the same notebook: [http://mng.bz/pBB8](http://mng.bz/pBB8)),
    but the differences are unimportant for understanding the core concepts in this
    chapter.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 用于文本输出的实际记录逻辑更复杂（请参见同一笔记本中的第15单元：[http://mng.bz/pBB8](http://mng.bz/pBB8)），但这些差异对于理解本章的核心概念并不重要。
- en: 'Now, let’s invoke our training loop:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们调用我们的训练循环：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Overtraining
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过度训练
- en: 'Wait, what happened? Our training process literally blew up, leading to losses
    becoming `inf`. This is a clear sign that `params` is receiving updates that are
    too large, and their values start oscillating back and forth as each update overshoots
    and the next overcorrects even more. The optimization process is unstable: it
    *diverges* instead of converging to a minimum. We want to see smaller and smaller
    updates to `params`, not larger, as shown in figure 5.8.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，发生了什么？我们的训练过程实际上爆炸了，导致损失变为`inf`。这清楚地表明`params`正在接收太大的更新，它们的值开始来回振荡，因为每次更新都超过了，下一个更正得更多。优化过程不稳定：它*发散*而不是收敛到最小值。我们希望看到对`params`的更新越来越小，而不是越来越大，如图5.8所示。
- en: '![](../Images/CH05_F08_Stevens2_GS.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F08_Stevens2_GS.png)'
- en: 'Figure 5.8 Top: Diverging optimization on a convex function (parabola-like)
    due to large steps. Bottom: Converging optimization with small steps.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 顶部：由于步长过大，在凸函数（类似抛物线）上发散的优化。底部：通过小步骤收敛的优化。
- en: 'How can we limit the magnitude of `learning_rate * grad`? Well, that looks
    easy. We could simply choose a smaller `learning_rate`, and indeed, the learning
    rate is one of the things we typically change when training does not go as well
    as we would like.[⁸](#pgfId-1020620) We usually change learning rates by orders
    of magnitude, so we might try with `1e-3` or `1e-4`, which would decrease the
    magnitude of the updates by orders of magnitude. Let’s go with `1e-4` and see
    how it works out:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何限制`learning_rate * grad`的幅度？嗯，这看起来很容易。我们可以简单地选择一个较小的`learning_rate`，实际上，当训练不如我们希望的那样顺利时，学习率是我们通常更改的事物之一。我们通常按数量级更改学习率，因此我们可以尝试使用`1e-3`或`1e-4`，这将使更新的幅度减少数量级。让我们选择`1e-4`，看看效果如何：
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Nice--the behavior is now stable. But there’s another problem: the updates
    to parameters are very small, so the loss decreases very slowly and eventually
    stalls. We could obviate this issue by making `learning_rate` adaptive: that is,
    change according to the magnitude of updates. There are optimization schemes that
    do that, and we’ll see one toward the end of this chapter, in section 5.5.2.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 不错--行为现在稳定了。但还有另一个问题：参数的更新非常小，因此损失下降非常缓慢，最终停滞。我们可以通过使`learning_rate`自适应来避免这个问题：即根据更新的幅度进行更改。有一些优化方案可以做到这一点，我们将在本章末尾的第5.5.2节中看到其中一个。
- en: 'However, there’s another potential troublemaker in the update term: the gradient
    itself. Let’s go back and look at `grad` at epoch 1 during optimization.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在更新项中还有另一个潜在的麻烦制造者：梯度本身。让我们回过头看看在优化期间第1个时期的`grad`。
- en: 5.4.4 Normalizing inputs
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 标准化输入
- en: We can see that the first-epoch gradient for the weight is about 50 times larger
    than the gradient for the bias. This means the weight and bias live in differently
    scaled spaces. If this is the case, a learning rate that’s large enough to meaningfully
    update one will be so large as to be unstable for the other; and a rate that’s
    appropriate for the other won’t be large enough to meaningfully change the first.
    That means we’re not going to be able to update our parameters unless we change
    something about our formulation of the problem. We could have individual learning
    rates for each parameter, but for models with many parameters, this would be too
    much to bother with; it’s babysitting of the kind we don’t like.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，权重的第一轮梯度大约比偏置的梯度大50倍。这意味着权重和偏置存在于不同比例的空间中。如果是这种情况，一个足够大以便有意义地更新一个参数的学习率对于另一个参数来说会太大而不稳定；而对于另一个参数来说合适的速率将不足以有意义地改变第一个参数。这意味着除非改变问题的表述，否则我们将无法更新我们的参数。我们可以为每个参数设置单独的学习率，但对于具有许多参数的模型来说，这将是太麻烦的事情；这是我们不喜欢的照看的一种方式。
- en: 'There’s a simpler way to keep things in check: changing the inputs so that
    the gradients aren’t quite so different. We can make sure the range of the input
    doesn’t get too far from the range of `-`1.0 to 1.0, roughly speaking. In our
    case, we can achieve something close enough to that by simply multiplying `t_u`
    by 0.1:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个更简单的方法来控制事物：改变输入，使得梯度不那么不同。我们可以确保输入的范围不会远离`-1.0`到`1.0`的范围��粗略地说。在我们的情况下，我们可以通过简单地将`t_u`乘以0.1来实现接近这个范围：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here, we denote the normalized version of `t_u` by appending an `n` to the
    variable name. At this point, we can run the training loop on our normalized input:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过在变量名后附加一个`n`来表示`t_u`的归一化版本。此时，我们可以在我们的归一化输入上运行训练循环：
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ We’ve updated t_u to our new, rescaled t_un.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们已经将`t_u`更新为我们的新的、重新缩放的`t_un`。
- en: 'Even though we set our learning rate back to `1e-2`, parameters don’t blow
    up during iterative updates. Let’s take a look at the gradients: they’re of similar
    magnitude, so using a single `learning_rate` for both parameters works just fine.
    We could probably do a better job of normalization than a simple rescaling by
    a factor of 10, but since doing so is good enough for our needs, we’re going to
    stick with that for now.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们将学习率设置回`1e-2`，参数在迭代更新过程中不会爆炸。让我们看一下梯度：它们的数量级相似，因此对两个参数使用相同的`learning_rate`效果很好。我们可能可以比简单地乘以10进行更好的归一化，但由于这种方法对我们的需求已经足够好，我们暂时将坚持使用这种方法。
- en: '*Note* The normalization here absolutely helps get the network trained, but
    you could make an argument that it’s not strictly needed to optimize the parameters
    for this particular problem. That’s absolutely true! This problem is small enough
    that there are numerous ways to beat the parameters into submission. However,
    for larger, more sophisticated problems, normalization is an easy and effective
    (if not crucial!) tool to use to improve model convergence.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 这里的归一化绝对有助于训练网络，但你可以提出一个论点，即对于这个特定问题，严格来说并不需要优化参数。这绝对正确！这个问题足够小，有很多方法可以击败参数。然而，对于更大、更复杂的问题，归一化是一个简单而有效（如果不是至关重要！）的工具，用来改善模型的收敛性。'
- en: 'Let’s run the loop for enough iterations to see the changes in `params` get
    small. We’ll change `n_epochs` to 5,000:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行足够的迭代次数来看到`params`的变化变得很小。我们将`n_epochs`更改为5,000：
- en: '[PRE22]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Good: our loss decreases while we change parameters along the direction of
    gradient descent. It doesn’t go exactly to zero; this could mean there aren’t
    enough iterations to converge to zero, or that the data points don’t sit exactly
    on a line. As we anticipated, our measurements were not perfectly accurate, or
    there was noise involved in the reading.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 很好：我们的损失在我们沿着梯度下降方向改变参数时减少。它并没有完全降到零；这可能意味着没有足够的迭代次数收敛到零，或者数据点并不完全位于一条直线上。正如我们预料的那样，我们的测量并不完全准确，或者在读数中存在噪音。
- en: 'But look: the values for `w` and `b` look an awful lot like the numbers we
    need to use to convert Celsius to Fahrenheit (after accounting for our earlier
    normalization when we multiplied our inputs by 0.1). The exact values would be
    `w=5.5556` and `b=-17.7778`. Our fancy thermometer was showing temperatures in
    Fahrenheit the whole time. No big discovery, except that our gradient descent
    optimization process works!'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但是看：`w`和`b`的值看起来非常像我们需要用来将摄氏度转换为华氏度的数字（在我们将输入乘以0.1进行归一化之后）。确切的值将是`w=5.5556`和`b=-17.7778`。我们时髦的温度计一直显示的是华氏温度。没有什么大的发现，除了我们的梯度下降优化过程有效！
- en: 5.4.5 Visualizing (again)
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.5 再次可视化
- en: 'Let’s revisit something we did right at the start: plotting our data. Seriously,
    this is the first thing anyone doing data science should do. Always plot the heck
    out of the data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视一下我们一开始做的事情：绘制我们的数据。说真的，这是任何从事数据科学的人都应该做的第一件事。始终大量绘制数据：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Remember that we’re training on the normalized unknown units. We also use
    argument unpacking.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记住我们是在归一化的未知单位上进行训练。我们还使用参数解包。
- en: ❷ But we’re plotting the raw unknown values.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 但我们正在绘制原始的未知值。
- en: 'We are using a Python trick called argument unpacking here: `*params` means
    to pass the elements of `params` as individual arguments. In Python, this is usually
    done with lists or tuples, but we can also use argument unpacking with PyTorch
    tensors, which are split along the leading dimension. So here, `model(t_un, *params)`
    is equivalent to `model(t_un, params[0], params[1])`.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个名为参数解包的Python技巧：`*params`意味着将`params`的元素作为单独的参数传递。在Python中，这通常是用于列表或元组的，但我们也可以在PyTorch张量中使用参数解包，这些张量沿着主导维度分割。因此，在这里，`model(t_un,
    *params)`等同于`model(t_un, params[0], params[1])`。
- en: This code produces figure 5.9\. Our linear model is a good model for the data,
    it seems. It also seems our measurements are somewhat erratic. We should either
    call our optometrist for a new pair of glasses or think about returning our fancy
    thermometer.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成图5.9。我们的线性模型似乎是数据的一个很好的模型。看起来我们的测量有些不稳定。我们应该给我们的验光师打电话换一副新眼镜，或者考虑退还我们的高级温度计。
- en: '![](../Images/CH05_F10_Stevens2_GS.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F10_Stevens2_GS.png)'
- en: Figure 5.9 The plot of our linear-fit model (solid line) versus our input data
    (circles)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 我们的线性拟合模型（实线）与输入数据（圆圈）的绘图
- en: '5.5 PyTorch’s autograd: Backpropagating all things'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 PyTorch 的 autograd：反向传播一切
- en: 'In our little adventure, we just saw a simple example of backpropagation: we
    computed the gradient of a composition of functions--the model and the loss--with
    respect to their innermost parameters (`w` and `b`) by propagating derivatives
    backward using the *chain rule*. The basic requirement here is that all functions
    we’re dealing with can be differentiated analytically. If this is the case, we
    can compute the gradient--what we earlier called “the rate of change of the loss”--with
    respect to the parameters in one sweep.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的小冒险中，我们刚刚看到了反向传播的一个简单示例：我们使用*链式法则*向后传播导数，计算了函数组合（模型和损失）相对于它们最内部参数（`w` 和
    `b`）的梯度。这里的基本要求是，我们处理的所有函数都可以在解析上进行微分。如果是这种情况，我们可以一次性计算出相对于参数的梯度--我们之前称之为“损失变化率”。
- en: Even if we have a complicated model with millions of parameters, as long as
    our model is differentiable, computing the gradient of the loss with respect to
    the parameters amounts to writing the analytical expression for the derivatives
    and evaluating them *once*. Granted, writing the analytical expression for the
    derivatives of a very deep composition of linear and nonlinear functions is not
    a lot of fun.[⁹](#pgfId-1021810) It isn’t particularly quick, either.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们有一个包含数百万参数的复杂模型，只要我们的模型是可微的，计算相对于参数的损失梯度就相当于编写导数的解析表达式并评估它们*一次*。当然，编写一个非常深层次的线性和非线性函数组合的导数的解析表达式并不是一件有趣的事情。这也不是特别快的过程。
- en: 5.5.1 Computing the gradient automatically
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 自动计算梯度
- en: 'This is when PyTorch tensors come to the rescue, with a PyTorch component called
    *autograd*. Chapter 3 presented a comprehensive overview of what tensors are and
    what functions we can call on them. We left out one very interesting aspect, however:
    PyTorch tensors can remember where they come from, in terms of the operations
    and parent tensors that originated them, and they can automatically provide the
    chain of derivatives of such operations with respect to their inputs. This means
    we won’t need to derive our model by hand;[^(10)](#pgfId-1021855) given a forward
    expression, no matter how nested, PyTorch will automatically provide the gradient
    of that expression with respect to its input parameters.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是当 PyTorch 张量发挥作用时的时候，PyTorch 组件 *autograd* 就派上用场了。第3章介绍了张量是什么以及我们可以在它们上调用什么函数的全面概述。然而，我们遗漏了一个非常有趣的方面：PyTorch
    张量可以记住它们的来源，即生成它们的操作和父张量，并且可以自动提供这些操作相对于它们的输入的导数链。这意味着我们不需要手动推导我们的模型；给定一个前向表达式，无论多么嵌套，PyTorch
    都会自动提供该表达式相对于其输入参数的梯度。
- en: Applying autograd
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用 autograd
- en: At this point, the best way to proceed is to rewrite our thermometer calibration
    code, this time using autograd, and see what happens. First, we recall our model
    and loss function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，继续前进的最佳方式是重新编写我们的温度计校准代码，这次使用 autograd，并看看会发生什么。首先，我们回顾一下我们的模型和损失函数。
- en: code/p1ch5/2_autograd.ipynb
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: code/p1ch5/2_autograd.ipynb
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s again initialize a parameters tensor:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次初始化一个参��张量：
- en: '[PRE25]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Using the grad attribute
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 grad 属性
- en: Notice the `requires_grad=True` argument to the tensor constructor? That argument
    is telling PyTorch to track the entire family tree of tensors resulting from operations
    on `params`. In other words, any tensor that will have `params` as an ancestor
    will have access to the chain of functions that were called to get from `params`
    to that tensor. In case these functions are differentiable (and most PyTorch tensor
    operations will be), the value of the derivative will be automatically populated
    as a `grad` attribute of the `params` tensor.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意张量构造函数中的 `requires_grad=True` 参数？该参数告诉 PyTorch 跟踪由于对 `params` 进行操作而产生的张量的整个家族树。换句话说，任何将
    `params` 作为祖先的张量都将访问从 `params` 到该张量的链式函数。如果这些函数是可微的（大多数 PyTorch 张量操作都是可微的），导数的值将自动填充为
    `params` 张量的 `grad` 属性。
- en: 'In general, all PyTorch tensors have an attribute named `grad`. Normally, it’s
    `None`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，所有 PyTorch 张量都有一个名为 `grad` 的属性。通常，它是 `None`：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'All we have to do to populate it is to start with a tensor with `requires_grad`
    set to `True`, then call the model and compute the loss, and then call `backward`
    on the `loss` tensor:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需开始一个 `requires_grad` 设置为 `True` 的张量，然后调用模型并计算损失，然后在 `loss` 张量上调用 `backward`：
- en: '[PRE27]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: At this point, the `grad` attribute of `params` contains the derivatives of
    the loss with respect to each element of params.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`params` 的 `grad` 属性包含了相对于每个元素的 params 的损失的导数。
- en: When we compute our `loss` while the parameters `w` and `b` require gradients,
    in addition to performing the actual computation, PyTorch creates the autograd
    graph with the operations (in black circles) as nodes, as shown in the top row
    of fig-ure 5.10\. When we call `loss.backward()`, PyTorch traverses this graph
    in the reverse direction to compute the gradients, as shown by the arrows in the
    bottom row of the figure.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在参数 `w` 和 `b` 需要梯度时计算我们的 `loss` 时，除了执行实际计算外，PyTorch 还会创建带有操作（黑色圆圈）的 autograd
    图，如图5.10顶部行所示。当我们调用 `loss.backward()` 时，PyTorch 沿着这个图的反向方向遍历以计算梯度，如图的底部行所示的箭头所示。
- en: '![](../Images/CH05_F11_Stevens2_GS.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F11_Stevens2_GS.png)'
- en: Figure 5.10 The forward graph and backward graph of the model as computed with
    autograd
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 模型的前向图和后向图，使用 autograd 计算
- en: Accumulating grad functions
  id: totrans-185
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 累积 grad 函数
- en: We could have any number of tensors with `requires_grad` set to `True` and any
    composition of functions. In this case, PyTorch would compute the derivatives
    of the loss throughout the chain of functions (the computation graph) and accumulate
    their values in the `grad` attribute of those tensors (the leaf nodes of the graph).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以有任意数量的张量，其`requires_grad`设置为`True`，以及任意组合的函数。在这种情况下，PyTorch会计算整个函数链（计算图）中损失的导数，并将其值累积在这些张量的`grad`属性中（图的叶节点）。
- en: Alert! *Big gotcha ahead*. This is something PyTorch newcomers--and a lot of
    more experienced folks, too--trip up on regularly. We just wrote *accumulate*,
    not *store*.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 警告！*大坑在前方*。这是PyTorch新手——以及许多更有经验的人——经常会遇到的问题。我们刚刚写的是*累积*，而不是*存储*。
- en: '*Warning* Calling `backward` will lead derivatives to *accumulate* at leaf
    nodes. We need to *zero the gradient explicitly* after using it for parameter
    updates.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*警告* 调用`backward`会导致导数在叶节点*累积*。在使用参数更新后，我们需要*显式地将梯度清零*。'
- en: 'Let’s repeat together: calling `backward` will lead derivatives to *accumulate*
    at leaf nodes. So if `backward` was called earlier, the loss is evaluated again,
    `backward` is called again (as in any training loop), and the gradient at each
    leaf is accumulated (that is, summed) on top of the one computed at the previous
    iteration, which leads to an incorrect value for the gradient.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一起重复：调用`backward`会导致导数在叶节点*累积*。因此，如果`backward`在之前被调用，损失会再次被评估，`backward`会再次被调用（就像在任何训练循环中一样），并且每个叶节点的梯度会累积（即求和）在上一次迭代计算的梯度之上，这会导致梯度的值不正确。
- en: 'In order to prevent this from occurring, we need to *zero the gradient explicitly*
    at each iteration. We can do this easily using the in-place `zero_` method:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止这种情况发生，我们需要在每次迭代时*显式地将梯度清零*。我们可以很容易地使用就地`zero_`方法来实现：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '*Note* You might be curious why zeroing the gradient is a required step instead
    of zeroing happening automatically whenever we call `backward`. Doing it this
    way provides more flexibility and control when working with gradients in complicated
    models.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*注意* 你可能会好奇为什么清零梯度是一个必需的步骤，而不是在每次调用`backward`时自动清零。这样做提供了更多在处理复杂模型中梯度时的灵活性和控制。'
- en: 'Having this reminder drilled into our heads, let’s see what our autograd-enabled
    training code looks like, start to finish:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 将这个提醒铭记在心，让我们看看我们启用自动求导的训练代码是什么样子，从头到尾：
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ This could be done at any point in the loop prior to calling loss.backward().
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这可以在调用loss.backward()之前的循环中的任何时候完成。
- en: ❷ This is a somewhat cumbersome bit of code, but as we’ll see in the next section,
    it’s not an issue in practice.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是一段有些繁琐的代码，但正如我们将在下一节看到的，实际上并不是问题。
- en: Note that our code updating `params` is not quite as straightforward as we might
    have expected. There are two particularities. First, we are encapsulating the
    update in a `no_grad` context using the Python `with` statement. This means within
    the `with` block, the PyTorch autograd mechanism should *look away*:[^(11)](#pgfId-1022903)
    that is, not add edges to the forward graph. In fact, when we are executing this
    bit of code, the forward graph that PyTorch records is consumed when we call `backward`,
    leaving us with the `params` leaf node. But now we want to change this leaf node
    before we start building a fresh forward graph on top of it. While this use case
    is usually wrapped inside the optimizers we discuss in section 5.5.2, we will
    take a closer look when we see another common use of `no_grad` in section 5.5.4.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们更新`params`的代码并不像我们可能期望的那样直截了当。有两个特殊之处。首先，我们使用Python的`with`语句在`no_grad`上下文中封装更新。这意味着在`with`块内，PyTorch自动求导机制应该*不要关注*：即，在前向图中不添加边。实际上，当我们执行这段代码时，PyTorch记录的前向图在我们调用`backward`时被消耗掉，留下`params`叶节点。但现在我们想要在开始构建新的前向图之前更改这个叶节点。虽然这种用例通常包含在我们在第5.5.2节中讨论的优化器中，但当我们在第5.5.4节看到`no_grad`的另一个常见用法时，我们将更仔细地看一下。
- en: Second, we update `params` in place. This means we keep the same `params` tensor
    around but subtract our update from it. When using autograd, we usually avoid
    in-place updates because PyTorch’s autograd engine might need the values we would
    be modifying for the backward pass. Here, however, we are operating without autograd,
    and it is beneficial to keep the `params` tensor. Not replacing the parameters
    by assigning new tensors to their variable name will become crucial when we register
    our parameters with the optimizer in section 5.5.2.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们就地更新`params`。这意味着我们保留相同的`params`张量，但从中减去我们的更新。在使用自动求导时，我们通常避免就地更新，因为PyTorch的自动求导引擎可能需要我们将要修改的值用于反向传播。然而，在这里，我们在没有自动求导的情况下操作，保留`params`张量是有益的。在第5.5.2节中向优化器注册参数时，不通过将新张量分配给其变量名来替换参数将变得至关重要。
- en: 'Let’s see if it works:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是否有效：
- en: '[PRE30]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Adding requires_grad=True is key
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加requires_grad=True至关重要
- en: ❷ Again, we’re using the normalized t_un instead of t_u.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 再次，我们使用了标准化的t_un而不是t_u。
- en: The result is the same as we got previously. Good for us! It means that while
    we are *capable* of computing derivatives by hand, we no longer need to.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与我们之前得到的相同。对我们来说很好！这意味着虽然我们*能够*手动计算导数，但我们不再需要这样做。
- en: 5.5.2 Optimizers a la carte
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 自选优化器
- en: In the example code, we used *vanilla* gradient descent for optimization, which
    worked fine for our simple case. Needless to say, there are several optimization
    strategies and tricks that can assist convergence, especially when models get
    complicated.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例代码中，我们使用了*普通*梯度下降进行优化，这对我们简单的情况效果很好。不用说，有几种优化策略和技巧可以帮助收敛，特别是在模型变得复杂时。
- en: 'We’ll dive deeper into this topic in later chapters, but now is the right time
    to introduce the way PyTorch abstracts the optimization strategy away from user
    code: that is, the training loop we’ve examined. This saves us from the boilerplate
    busywork of having to update each and every parameter to our model ourselves.
    The `torch` module has an `optim` submodule where we can find classes implementing
    different optimization algorithms. Here’s an abridged list (code/p1ch5/3_optimizers.ipynb):'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节深入探讨这个主题，但现在是介绍PyTorch如何将优化策略从用户代码中抽象出来的正确时机：也就是我们已经检查过的训练循环。这样可以避免我们不得不手动更新模型的每个参数的样板繁琐工作。`torch`模块有一个`optim`子模块，我们可以在其中找到实现不同优化算法的类。这里是一个简略列表（code/p1ch5/3_optimizers.ipynb）：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Every optimizer constructor takes a list of parameters (aka PyTorch tensors,
    typically with `requires_grad` set to `True`) as the first input. All parameters
    passed to the optimizer are retained inside the optimizer object so the optimizer
    can update their values and access their `grad` attribute, as represented in figure
    5.11.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 每个优化器构造函数的第一个输入都是参数列表（也称为PyTorch张量，通常将`requires_grad`设置为`True`）。所有传递给优化器的参数都会被保留在优化器对象内部，因此优化器可以更新它们的值并访问它们的`grad`属性，如图5.11所示。
- en: '![](../Images/CH05_F12_Stevens2_GS.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F12_Stevens2_GS.png)'
- en: Figure 5.11 (A) Conceptual representation of how an optimizer holds a reference
    to parameters. (B) After a loss is computed from inputs, (C) a call to `.backward`
    leads to `.grad` being populated on parameters. (D) At that point, the optimizer
    can access `.grad` and compute the parameter updates.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11（A）优化器如何保存参数的概念表示。（B）从输入计算损失后，（C）调用`.backward`会使参数上的`.grad`被填充。（D）此时，优化器可以访问`.grad`并计算参数更新。
- en: 'Each optimizer exposes two methods: `zero_grad` and `step`. `zero_grad` zeroes
    the `grad` attribute of all the parameters passed to the optimizer upon construction.
    `step` updates the value of those parameters according to the optimization strategy
    implemented by the specific optimizer.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每个优化器都暴露两个方法：`zero_grad`和`step`。`zero_grad`将在构造时将所有传递给优化器的参数的`grad`属性清零。`step`根据特定优化器实现的优化策略更新这些参数的值。
- en: Using a gradient descent optimizer
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用梯度下降优化器
- en: 'Let’s create `params` and instantiate a gradient descent optimizer:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建`params`并实例化一个梯度下降优化器：
- en: '[PRE32]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here SGD stands for *stochastic gradient descent*. Actually, the optimizer itself
    is exactly a vanilla gradient descent (as long as the `momentum` argument is set
    to `0.0`, which is the default). The term *stochastic* comes from the fact that
    the gradient is typically obtained by averaging over a random subset of all input
    samples, called a *minibatch*. However, the optimizer does not know if the loss
    was evaluated on all the samples (vanilla) or a random subset of them (stochastic),
    so the algorithm is literally the same in the two cases.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这里SGD代表*随机梯度下降*。实际上，优化器本身就是一个标准的梯度下降（只要`momentum`参数设置为`0.0`，这是默认值）。术语*随机*来自于梯度通常是通过对所有输入样本的随机子集进行平均得到的，称为*小批量*。然而，优化器不知道损失是在所有样本（标准）上评估的还是在它们的随机子集（随机）上评估的，所以在这两种情况下算法实际上是相同的。
- en: 'Anyway, let’s take our fancy new optimizer for a spin:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，让我们尝试一下我们新的优化器：
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The value of `params` is updated upon calling `step` without us having to touch
    it ourselves! What happens is that the optimizer looks into `params.grad` and
    updates `params`, subtracting `learning_rate` times `grad` from it, exactly as
    in our former hand-rolled code.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用`step`时，`params`的值会被更新，而无需我们自己操作！发生的情况是，优化器查看`params.grad`并更新`params`，从中减去`learning_rate`乘以`grad`，与我们以前手动编写的代码完全相同。
- en: 'Ready to stick this code in a training loop? Nope! The big gotcha almost got
    us--we forgot to zero out the gradients. Had we called the previous code in a
    loop, gradients would have accumulated in the leaves at every call to `backward`,
    and our gradient descent would have been all over the place! Here’s the loop-ready
    code, with the extra `zero_grad` at the correct spot (right before the call to
    `backward`):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 准备将这段代码放入训练循环中？不！几乎让我们犯了大错--我们忘记了将梯度清零。如果我们在循环中调用之前的代码，梯度会在每次调用`backward`时在叶子节点中累积，我们的梯度下降会一团糟！这是循环准备就绪的代码，正确位置是在`backward`调用之前额外加上`zero_grad`：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ As before, the exact placement of this call is somewhat arbitrary. It could
    be earlier in the loop as well.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 与以前一样，这个调用的确切位置有些随意。它也可以在循环中较早的位置。
- en: Perfect! See how the `optim` module helps us abstract away the specific optimization
    scheme? All we have to do is provide a list of params to it (that list can be
    extremely long, as is needed for very deep neural network models), and we can
    forget about the details.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！看看`optim`模块如何帮助我们将特定的优化方案抽象出来？我们所要做的就是向其提供一个参数列表（该列表可能非常长，对于非常深的神经网络模型是必需的），然后我们��以忘记细节。
- en: 'Let’s update our training loop accordingly:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们相应地更新我们的训练循环：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ It’s important that both params are the same object; otherwise the optimizer
    won’t know what parameters were used by the model.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 很重要的一点是两个参数必须是同一个对象；否则优化器将不知道模型使用了哪些参数。
- en: 'Again, we get the same result as before. Great: this is further confirmation
    that we know how to descend a gradient by hand!'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 再次得到与以前相同的结果。太好了：这进一步证实了我们知道如何手动下降梯度！
- en: Testing other optimizers
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试其他优化器
- en: In order to test more optimizers, all we have to do is instantiate a different
    optimizer, say `Adam`, instead of `SGD`. The rest of the code stays as it is.
    Pretty handy stuff.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试更多的优化器，我们只需实例化一个不同的优化器，比如`Adam`，而不是`SGD`。其余代码保持不变。非常方便。
- en: 'We won’t go into much detail about Adam; suffice to say that it is a more sophisticated
    optimizer in which the learning rate is set adaptively. In addition, it is a lot
    less sensitive to the scaling of the parameters--so insensitive that we can go
    back to using the original (non-normalized) input `t_u`, and even increase the
    learning rate to `1e-1`, and Adam won’t even blink:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细讨论Adam；可以说，它是一种更复杂的优化器，其中学习率是自适应设置的。此外，它对参数的缩放不太敏感--如此不敏感，以至于我们可以回到使用原始（非归一化）输入`t_u`，甚至将学习率增加到`1e-1`，Adam也不会有任何反应：
- en: '[PRE36]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ New optimizer class
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新的优化器类
- en: ❷ We’re back to the original t_u as our input.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们又回到了将`t_u`作为我们的输入。
- en: The optimizer is not the only flexible part of our training loop. Let’s turn
    our attention to the model. In order to train a neural network on the same data
    and the same loss, all we would need to change is the `model` function. It wouldn’t
    make particular sense in this case, since we know that converting Celsius to Fahrenheit
    amounts to a linear transformation, but we’ll do it anyway in chapter 6\. We’ll
    see quite soon that neural networks allow us to remove our arbitrary assumptions
    about the shape of the function we should be approximating. Even so, we’ll see
    how neural networks manage to be trained even when the underlying processes are
    highly nonlinear (such in the case of describing an image with a sentence, as
    we saw in chapter 2).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器并不是我们训练循环中唯一灵活的部分。让我们将注意力转向模型。为了在相同数据和相同损失上训练神经网络，我们需要改变的只是`model`函数。在这种情况下并没有特别意义，因为我们知道将摄氏度转换为华氏度相当于进行线性变换，但我们还是会在第6章中这样做。我们很快就会看到，神经网络允许我们消除对我们应该逼近的函数形状的任意假设。即使如此，我们将看到神经网络如何在基础过程高度非线性时进行训练（例如在描述图像与句子之间的情况，正如我们在第2章中看到的）。
- en: 'We have touched on a lot of the essential concepts that will enable us to train
    complicated deep learning models while knowing what’s going on under the hood:
    backpropagation to estimate gradients, autograd, and optimizing weights of models
    using gradient descent or other optimizers. Really, there isn’t a lot more. The
    rest is mostly filling in the blanks, however extensive they are.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涉及了许多基本概念，这些概念将使我们能够在了解内部运作的情况下训练复杂的深度学习模型：反向传播来估计梯度，自动微分，以及使用梯度下降或其他优化器来优化模型的权重。实际上，并没有太多内容。其余的大部分内容都是填空，无论填空有多广泛。
- en: Next up, we’re going to offer an aside on how to split our samples, because
    that sets up a perfect use case for learning how to better control autograd.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提供一个关于如何分割样本的插曲，因为这为学习如何更好地控制自动微分提供了一个完美的用例。
- en: 5.5.3 Training, validation, and overfitting
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 训练、验证和过拟合
- en: 'Johannes Kepler taught us one last thing that we didn’t discuss so far, remember?
    He kept part of the data on the side so that he could validate his models on independent
    observations. This is a vital thing to do, especially when the model we adopt
    could potentially approximate functions of any shape, as in the case of neural
    networks. In other words, a highly adaptable model will tend to use its many parameters
    to make sure the loss is minimal *at* the data points, but we’ll have no guarantee
    that the model behaves well *away from* or *in between* the data points. After
    all, that’s what we’re asking the optimizer to do: minimize the loss *at* the
    data points. Sure enough, if we had independent data points that we didn’t use
    to evaluate our loss or descend along its negative gradient, we would soon find
    out that evaluating the loss at those independent data points would yield higher-than-expected
    loss. We have already mentioned this phenomenon, called *overfitting*.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰内斯·开普勒教给我们一个迄今为止我们没有讨论过的最后一件事，记得吗？他将部分数据保留在一边，以便可以在独立观测上验证他的模型。这是一件至关重要的事情，特别是当我们采用的模型可能近似于任何形状的函数时，就像神经网络的情况一样。换句话说，一个高度适应的模型将倾向于使用其许多参数来确保损失在数据点处最小化，但我们无法保证模型在数据点之外或之间的表现。毕竟，这就是我们要求优化器做的事情：在数据点处最小化损失。毫无疑问，如果我们有一些独立的数据点，我们没有用来评估损失或沿着其负梯度下降，我们很快就会发现，在这些独立数据点上评估损失会产生比预期更高的损失。我们已经提到了这种现象，称为*过拟合*。
- en: The first action we can take to combat overfitting is recognizing that it might
    happen. In order to do so, as Kepler figured out in 1600, we must take a few data
    points out of our dataset (the *validation set*) and only fit our model on the
    remaining data points (the *training set*), as shown in figure 5.12\. Then, while
    we’re fitting the model, we can evaluate the loss once on the training set and
    once on the validation set. When we’re trying to decide if we’ve done a good job
    of fitting our model to the data, we must look at both!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取的第一步对抗过拟合的行动是意识到它可能发生。为了做到这一点，正如开普勒在1600年发现的那样，我们必须从数据集中取出一些数据点���*验证集*），并仅在剩余数据点上拟合我们的模型（*训练集*），如图5.12所示。然后，在拟合模型时，我们可以在训练集上评估损失一次，在验证集上评估损失一次。当我们试图决定我们是否已经很好地将模型拟合到数据时，我们必须同时看两者！
- en: '![](../Images/CH05_F13_Stevens2_GS.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F13_Stevens2_GS.png)'
- en: Figure 5.12 Conceptual representation of a dataproducing process and the collection
    and use of training data and independent validation data
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 数据生成过程的概念表示以及训练数据和独立验证数据的收集和使用。
- en: Evaluating the training loss
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估训练损失
- en: The training loss will tell us if our model can fit the training set at all--in
    other words, if our model has enough *capacity* to process the relevant information
    in the data. If our mysterious thermometer somehow managed to measure temperatures
    using a logarithmic scale, our poor linear model would not have had a chance to
    fit those measurements and provide us with a sensible conversion to Celsius. In
    that case, our training loss (the loss we were printing in the training loop)
    would stop decreasing well before approaching zero.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 训练损失将告诉我们，我们的模型是否能够完全拟合训练集——换句话说，我们的模型是否具有足够的*容量*来处理数据中的相关信息。如果我们神秘的温度计以对数刻度测量温度，我们可怜的线性模型将无法拟合这些测量值，并为我们提供一个合理的摄氏度转换。在这种情况下，我们的训练损失（在训练循环中打印的损失）会在接近零之前停止下降。
- en: 'A deep neural network can potentially approximate complicated functions, provided
    that the number of neurons, and therefore parameters, is high enough. The fewer
    the number of parameters, the simpler the shape of the function our network will
    be able to approximate. So, rule 1: if the training loss is not decreasing, chances
    are the model is too simple for the data. The other possibility is that our data
    just doesn’t contain meaningful information that lets it explain the output: if
    the nice folks at the shop sell us a barometer instead of a thermometer, we will
    have little chance of predicting temperature in Celsius from just pressure, even
    if we use the latest neural network architecture from Quebec ([www.umontreal.ca/en/artificialintelligence](https://www.umontreal.ca/en/artificialintelligence/)).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络可以潜在地逼近复杂的函数，只要神经元的数量，因此参数的数量足够多。参数数量越少，我们的网络将能够逼近的函数形状就越简单。所以，规则1：如果训练损失不降低，那么模型对数据来说可能太简单了。另一种可能性是我们的数据只包含让其解释输出的有意义信息：如果商店里的好人卖给我们一个气压计而不是温度计，我们将很难仅凭压力来预测摄氏度，即使我们使用魁北克最新的神经网络架构（[www.umontreal.ca/en/artificialintelligence](https://www.umontreal.ca/en/artificialintelligence/)）。
- en: Generalizing to the validation set
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 泛化到验证集
- en: 'What about the validation set? Well, if the loss evaluated in the validation
    set doesn’t decrease along with the training set, it means our model is improving
    its fit of the samples it is seeing during training, but it is not *generalizing*
    to samples outside this precise set. As soon as we evaluate the model at new,
    previously unseen points, the values of the loss function are poor. So, rule 2:
    if the training loss and the validation loss diverge, we’re overfitting.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 那验证集呢？如果在验证集中评估的损失不随着训练集一起减少，这意味着我们的模型正在改善对训练期间看到的样本的拟合，但没有*泛化*到这个精确集之外的样本。一旦我们在新的、以前未见过的点上评估模型，损失函数的值就会很差。所以，规则2：如果训练损失和验证损失发散，我们就过拟合了。
- en: Let’s delve into this phenomenon a little, going back to our thermometer example.
    We could have decided to fit the data with a more complicated function, like a
    piecewise polynomial or a really large neural network. It could generate a model
    meandering its way through the data points, as in figure 5.13, just because it
    pushes the loss very close to zero. Since the behavior of the function away from
    the data points does not increase the loss, there’s nothing to keep the model
    in check for inputs away from the training data points.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这种现象，回到我们的温度计示例。我们可以决定用更复杂的函数来拟合数据，比如分段多项式或非常大的神经网络。它可能会生成一个模型，沿着数据点蜿蜒前进，就像图5.13中所示，只是因为它将损失推得非常接近零。由于函数远离数据点的行为不会增加损失，因此没有任何东西可以限制模型对训练数据点之外的输入。
- en: '![](../Images/CH05_F14_Stevens2_GS.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F14_Stevens2_GS.png)'
- en: Figure 5.13 Rather extreme example of overfitting
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 过拟合的极端示例
- en: What’s the cure, though? Good question. From what we just said, overfitting
    really looks like a problem of making sure the behavior of the model *in between*
    data points is sensible for the process we’re trying to approximate. First of
    all, we should make sure we get enough data for the process. If we collected data
    from a sinusoidal process by sampling it regularly at a low frequency, we would
    have a hard time fitting a model to it.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，治疗方法呢？好问题。从我们刚才说的来看，过拟合看起来确实是确保模型在数据点之间的行为对我们试图逼近的过程是合理的问题。首先，我们应该确保我们为该过程收集足够的数据。如果我们通过以低频率定期对正弦过程进行采样来收集数据，我们将很难将模型拟合到它。
- en: Assuming we have enough data points, we should make sure the model that is capable
    of fitting the training data is as regular as possible in between them. There
    are several ways to achieve this. One is adding *penalization terms* to the loss
    function, to make it cheaper for the model to behave more smoothly and change
    more slowly (up to a point). Another is to add noise to the input samples, to
    artificially create new data points in between training data samples and force
    the model to try to fit those, too. There are several other ways, all of them
    somewhat related to these. But the best favor we can do to ourselves, at least
    as a first move, is to make our model simpler. From an intuitive standpoint, a
    simpler model may not fit the training data as perfectly as a more complicated
    model would, but it will likely behave more regularly in between data points.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有足够的数据点，我们应该确保能够拟合训练数据的模型在它们之间尽可能地规则。有几种方法可以实现这一点。一种方法是向损失函数添加*惩罚项*，使模型更平滑、变化更慢（在一定程度上）更便宜。另一种方法是向输入样本添加噪声，人为地在训练数据样本之间创建新的数据点，并迫使模型尝试拟合这些数据点。还有其他几种方法，所有这些方法都与这些方法有些相关。但我们可以为自己做的最好的事情，至少作为第一步，是使我们的模型更简单。从直觉上讲，一个简单的模型可能不会像一个更复杂的模型那样完美地拟合训练数据，但它可能在数据点之间的行为更加规则。
- en: 'We’ve got some nice trade-offs here. On the one hand, we need the model to
    have enough capacity for it to fit the training set. On the other, we need the
    model to avoid overfitting. Therefore, in order to choose the right size for a
    neural network model in terms of parameters, the process is based on two steps:
    increase the size until it fits, and then scale it down until it stops overfitting.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里有一些不错的权衡。一方面，我们需要模型具有足够的容量来适应训练集。另一方面，我们需要模型避免过拟合。因此，为了选择神经网络模型的正确参数大小，该过程基于两个步骤：增加大小直到适应，然后缩小直到停止过拟合。
- en: We’ll see more about this in chapter 12--we’ll discover that our life will be
    a balancing act between fitting and overfitting. For now, let’s get back to our
    example and see how we can split the data into a training set and a validation
    set. We’ll do it by shuffling `t_u` and `t_c` the same way and then splitting
    the resulting shuffled tensors into two parts.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第12章中更多地了解这一点--我们将发现我们的生活将是在拟合和过拟合之间的平衡。现在，让我们回到我们的例子，看看我们如何将数据分成训练集和验证集。我们将通过相同的方式对`t_u`和`t_c`进行洗牌，然后将结果洗牌后的张量分成两部分。
- en: Splitting a dataset
  id: totrans-253
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分割数据集
- en: 'Shuffling the elements of a tensor amounts to finding a permutation of its
    indices. The `randperm` function does exactly this:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对张量的元素进行洗牌相当于找到其索引的排列。`randperm`函数正是这样做的：
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Since these are random, don’t be surprised if your values end up different
    from here on out.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于这些是随机的，如果你的数值与这里的不同，不要感到惊讶。
- en: 'We just got index tensors that we can use to build training and validation
    sets starting from the data tensors:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚得到了索引张量，我们可以使用它们从数据张量开始构建训练和验证集：
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Our training loop doesn’t really change. We just want to additionally evaluate
    the validation loss at every epoch, to have a chance to recognize whether we’re
    overfitting:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练循环并没有真正改变。我们只是想在每个时代额外评估验证损失，以便有机会识别我们是否过拟合：
- en: '[PRE39]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ These two pairs of lines are the same except for the train_* vs. val_* inputs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这两对行是相同的，除了train_* vs. val_*输入。
- en: ❷ Note that there is no val_loss.backward() here, since we don’t want to train
    the model on the validation data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 注意这里没有`val_loss.backward()`，因为我们不想在验证数据上训练模型。
- en: ❸ Since we’re using SGD again, we’re back to using normalized inputs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于我们再次使用SGD，我们又回到了使用归一化的输入。
- en: Here we are not being entirely fair to our model. The validation set is really
    small, so the validation loss will only be meaningful up to a point. In any case,
    we note that the validation loss is higher than our training loss, although not
    by an order of magnitude. We expect a model to perform better on the training
    set, since the model parameters are being shaped by the training set. Our main
    goal is to also see both the training loss *and* the validation loss decreasing.
    While ideally both losses would be roughly the same value, as long as the validation
    loss stays reasonably close to the training loss, we know that our model is continuing
    to learn generalized things about our data. In figure 5.14, case C is ideal, while
    D is acceptable. In case A, the model isn’t learning at all; and in case B, we
    see overfitting. We’ll see more meaningful examples of overfitting in chapter
    12\.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们对我们的模型并不完全公平。验证集真的很小，因此验证损失只有到一定程度才有意义。无论如何，我们注意到验证损失高于我们的训练损失，尽管不是数量级。我们期望模型在训练集上表现更好，因为模型参数是由训练集塑造的。我们的主要目标是看到训练损失和验证损失都在减小。虽然理想情况下，两个损失值应该大致相同，但只要验证损失保持与训练损失相当接近，我们就知道我们的模型继续学习关于我们数据的泛化内容。在图5.14中，情况C是理想的，而D是可以接受的。在情况A中，模型根本没有学习；在情况B中，我们看到过拟合。我们将在第12章看到更有意义的过拟合示例。
- en: '![](../Images/CH05_F16_Stevens2_GS.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F16_Stevens2_GS.png)'
- en: 'Figure 5.14 Overfitting scenarios when looking at the training (solid line)
    and validation (dotted line) losses. (A) Training and validation losses do not
    decrease; the model is not learning due to no information in the data or insufficient
    capacity of the model. (B) Training loss decreases while validation loss increases:
    overfitting. (C) Training and validation losses decrease exactly in tandem. Performance
    may be improved further as the model is not at the limit of overfitting. (D) Training
    and validation losses have different absolute values but similar trends: overfitting
    is under control.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 当查看训练（实线）和验证（虚线）损失时的过拟合情况。 (A) 训练和验证损失不减少；模型由于数据中没有信息或模型容量不足而无法学习。 (B)
    训练损失减少，而验证损失增加：过拟合。 (C) 训练和验证损失完全同步减少。性能可能进一步提高，因为模型尚未达到过拟合的极限。 (D) 训练和验证损失具有不同的绝对值，但趋势相似：过拟合得到控制。
- en: 5.5.4 Autograd nits and switching it off
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.4 自动微分细节和关闭它
- en: From the previous training loop, we can appreciate that we only ever call `backward`
    on `train_loss`. Therefore, errors will only ever backpropagate based on the training
    set--the validation set is used to provide an independent evaluation of the accuracy
    of the model’s output on data that wasn’t used for training.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的训练循环中，我们可以看到我们只在`train_loss`上调用`backward`。因此，错误只会基于训练集反向传播--验证集用于提供对模型在未用于训练的数据上输出准确性的独立评估。
- en: The curious reader will have an embryo of a question at this point. The model
    is evaluated twice--once on `train_t_u` and once on `val_t_u`--and then `backward`
    is called. Won’t this confuse autograd? Won’t `backward` be influenced by the
    values generated during the pass on the validation set?
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，好奇的读者可能会有一个问题的雏形。模型被评估两次--一次在`train_t_u`上，一次在`val_t_u`上--然后调用`backward`。这不会让自动微分混乱吗？`backward`会受到在验证集上传递期间生成的值的影响吗？
- en: Luckily for us, this isn’t the case. The first line in the training loop evaluates
    `model` on `train_t_u` to produce `train_t_p`. Then `train_loss` is evaluated
    from `train_t_p`. This creates a computation graph that links `train_t_u` to `train_t_p`
    to `train_loss`. When `model` is evaluated again on `val_t_u`, it produces `val_t_p`
    and `val_loss`. In this case, a separate computation graph will be created that
    links `val_t_u` to `val_t_p` to `val_loss`. Separate tensors have been run through
    the same functions, `model` and `loss_fn`, generating separate computation graphs,
    as shown in figure 5.15.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这种情况并不会发生。训练循环中的第一行评估`model`在`train_t_u`上产生`train_t_p`。然后从`train_t_p`评估`train_loss`。这创建了一个计算图，将`train_t_u`链接到`train_t_p`到`train_loss`。当再次在`val_t_u`上评估`model`时，它会产生`val_t_p`和`val_loss`。在这种情况下，将创建一个将`val_t_u`链接到`val_t_p`到`val_loss`的单独计算图。相同的张量已经通过相同的函数`model`和`loss_fn`运行，生成了不同的计算图，如图5.15所示。
- en: '![](../Images/CH05_F15_Stevens2_GS.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH05_F15_Stevens2_GS.png)'
- en: Figure 5.15 Diagram showing how gradients propagate through a graph with two
    losses when .backward is called on one of them
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 显示当在其中一个上调用.backward时，梯度如何通过具有两个损失的图传播
- en: The only tensors these two graphs have in common are the parameters. When we
    call `backward` on `train_loss`, we run `backward` on the first graph. In other
    words, we accumulate the derivatives of `train_loss` with respect to the parameters
    based on the computation generated from `train_t_u`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个图唯一共同拥有的张量是参数。当我们在`train_loss`上调用`backward`时，我们在第一个图上运行`backward`。换句话说，我们根据从`train_t_u`生成的计算累积`train_loss`相对于参数的导数。
- en: 'If we (incorrectly) called `backward` on `val_loss` as well, we would accumulate
    the derivatives of `val_loss` with respect to the parameters *on the same leaf
    nodes*. Remember the `zero_grad` thing, whereby gradients are accumulated on top
    of each other every time we call `backward` unless we zero out the gradients explicitly?
    Well, here something very similar would happen: calling `backward` on `val_loss`
    would lead to gradients accumulating in the `params` tensor, on top of those generated
    during the `train_loss.backward()` call. In this case, we would effectively train
    our model on the whole dataset (both training and validation), since the gradient
    would depend on both. Pretty interesting.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们（错误地）在`val_loss`上也调用了`backward`，那么我们将在*相同的叶节点*上累积`val_loss`相对于参数的导数。还记得`zero_grad`的事情吗？每次我们调用`backward`时，梯度都会累积在一起，除非我们明确地将梯度清零？嗯，在这里会发生类似的事情：在`val_loss`上调用`backward`会导致梯度在`params`张量中累积，这些梯度是在`train_loss.backward()`调用期间生成的。在这种情况下，我们实际上会在整个数据集上训练我们的模型（包括训练和验证），因为梯度会依赖于两者。非常有趣。
- en: There’s another element for discussion here. Since we’re not ever calling `backward`
    on `val_loss`, why are we building the graph in the first place? We could in fact
    just call `model` and `loss_fn` as plain functions, without tracking the computation.
    However optimized, building the autograd graph comes with additional costs that
    we could totally forgo during the validation pass, especially when the model has
    millions of parameters.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有另一个讨论的要素。由于我们从未在`val_loss`上调用`backward`，那么我们为什么要首先构建计算图呢？实际上，我们可以只调用`model`和`loss_fn`作为普通函数，而不跟踪计算。然而，构建自动求导图虽然经过了优化，但会带来额外的成本，在验证过程中我们完全可以放弃这些成本，特别是当模型有数百万个参数时。
- en: 'In order to address this, PyTorch allows us to switch off autograd when we
    don’t need it, using the `torch.no_grad` context manager.[^(12)](#pgfId-1026114)
    We won’t see any meaningful advantage in terms of speed or memory consumption
    on our small problem. However, for larger models, the differences can add up.
    We can make sure this works by checking the value of the `requires_grad` attribute
    on the `val_loss` tensor:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，PyTorch允许我们在不需要时关闭自动求导，使用`torch.no_grad`上下文管理器。在我们的小问题上，我们不会看到任何关于速度或内存消耗方面的有意义的优势。然而，对于更大的模型，差异可能会累积。我们可以通过检查`val_loss`张量的`requires_grad`属性的值来确保这一点：
- en: '[PRE40]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Context manager here
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里是上下文管理器
- en: ❷ Checks that our output requires_grad args are forced to False inside this
    block
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查我们的输出`requires_grad`参数在此块内被强制为False
- en: 'Using the related `set_grad_enabled` context, we can also condition the code
    to run with `autograd` enabled or disabled, according to a Boolean expression--typically
    indicating whether we are running in training or inference mode. We could, for
    instance, define a `calc_forward` function that takes data as input and runs `model`
    and `loss_fn` with or without autograd according to a Boolean `train_is` argument:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相关的`set_grad_enabled`上下文，我们还可以根据布尔表达式（通常表示我们是在训练还是推理模式下运行）来条件运行代码，启用或禁用`autograd`。例如，我们可以定义一个`calc_forward`函数，根据布尔`train_is`参数，以有或无自动求导的方式运行`model`和`loss_fn`：
- en: '[PRE41]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 5.6 Conclusion
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 结论
- en: 'We started this chapter with a big question: how is it that a machine can learn
    from examples? We spent the rest of the chapter describing the mechanism with
    which a model can be optimized to fit data. We chose to stick with a simple model
    in order to see all the moving parts without unneeded complications.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个大问题开始了这一章：机器如何能够从示例中学习？我们在本章的其余部分描述了优化模型以拟合数据的机制。我们选择坚持使用简单模型，以便在不需要的复杂性的情况下看到所有移动部件。
- en: 'Now that we’ve had our fill of appetizers, in chapter 6 we’ll finally get to
    the main course: using a neural network to fit our data. We’ll work on solving
    the same thermometer problem, but with the more powerful tools provided by the
    `torch.nn` module. We’ll adopt the same spirit of using this small problem to
    illustrate the larger uses of PyTorch. The problem doesn’t need a neural network
    to reach a solution, but it will allow us to develop a simpler understanding of
    what’s required to train a neural network.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经品尝了开胃菜，在第6章中我们终于要进入主菜了：使用神经网络来拟合我们的数据。我们将继续解决相同的温度计问题，但使用`torch.nn`模块提供的更强大工具。我们将采用相同的精神，使用这个小问题来说明PyTorch的更大用途。这个问题不需要神经网络来找到解决方案，但它将让我们更简单地了解训练神经网络所需的内容。
- en: 5.7 Exercise
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 练习
- en: Redefine the model to be `w2 * t_u ** 2 + w1 * t_u + b`.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义模型为 `w2 * t_u ** 2 + w1 * t_u + b`。
- en: What parts of the training loop, and so on, need to change to accommodate this
    redefinition?
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些部分的训练循环等需要更改以适应这个重新定义？
- en: What parts are agnostic to swapping out the model?
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪些部分对于更换模型是不可知的？
- en: Is the resulting loss higher or lower after training?
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练后损失是更高还是更低？
- en: Is the actual result better or worse?
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实际结果是更好还是更差？
- en: 5.8 Summary
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 总结
- en: Linear models are the simplest reasonable model to use to fit data.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型是用来拟合数据的最简单合理的模型。
- en: Convex optimization techniques can be used for linear models, but they do not
    generalize to neural networks, so we focus on stochastic gradient descent for
    parameter estimation.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凸优化技术可以用于线性模型，但不适用于神经网络，因此我们专注于随机梯度下降进行参数估计。
- en: Deep learning can be used for generic models that are not engineered for solving
    a specific task, but instead can be automatically adapted to specialize themselves
    on the problem at hand.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习可以用于通用模型，这些模型并非专门用于解决特定任务，而是可以自动适应并专门化解决手头的问题。
- en: Learning algorithms amount to optimizing parameters of models based on observations.
    A loss function is a measure of the error in carrying out a task, such as the
    error between predicted outputs and measured values. The goal is to get the loss
    function as low as possible.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习算法涉及根据观察结果优化模型参数。损失函数是执行任务时的错误度量，例如预测输出与测量值之间的误差。目标是尽可能降低损失函数。
- en: The rate of change of the loss function with respect to the model parameters
    can be used to update the same parameters in the direction of decreasing loss.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数相对于模型参数的变化率可用于更新相同参数以减少损失。
- en: The `optim` module in PyTorch provides a collection of ready-to-use optimizers
    for updating parameters and minimizing loss functions.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch 中的 `optim` 模块提供了一系列用于更新参数和最小化损失函数的现成优化器。
- en: Optimizers use the autograd feature of PyTorch to compute the gradient for each
    parameter, depending on how that parameter contributes to the final output. This
    allows users to rely on the dynamic computation graph during complex forward passes.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器使用 PyTorch 的 autograd 功能来计算每个参数的梯度，具体取决于该参数对最终输出的贡献。这使用户在复杂的前向传递过程中依赖于动态计算图。
- en: Context managers like `with torch.no_grad():` can be used to control autograd’s
    behavior.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像 `with torch.no_grad():` 这样的上下文管理器可用于控制 autograd 的行为。
- en: Data is often split into separate sets of training samples and validation samples.
    This lets us evaluate a model on data it was not trained on.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据通常被分成独立的训练样本集和验证样本集。这使我们能够在未经训练的数据上评估模型。
- en: Overfitting a model happens when the model’s performance continues to improve
    on the training set but degrades on the validation set. This is usually due to
    the model not generalizing, and instead memorizing the desired outputs for the
    training set.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合模型发生在模型在训练集上的表现继续改善但在验证集上下降的情况下。这通常是由于模型没有泛化，而是记忆了训练集的期望输出。
- en: '* * *'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.)As recounted by physicist Michael Fowler: [http://mng.bz/K2Ej](http://mng.bz/K2Ej).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)据物理学家迈克尔·福勒回忆：[http://mng.bz/K2Ej](http://mng.bz/K2Ej)。
- en: ^(2.)Understanding the details of Kepler’s laws is not needed to understand
    this chapter, but you can find more information at [https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)理解开普勒定律的细节并不是理解本章所需的，但你可以在[https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion](https://en.wikipedia.org/wiki/Kepler%27s_laws_of_planetary_motion)找到更多信息。
- en: ^(3.)Unless you’re a theoretical physicist ;).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)除非你是理论物理学家 ;).
- en: ^(4.)This task--fitting model outputs to continuous values in terms of the types
    discussed in chapter 4--is called a *regression* problem. In chapter 7 and part
    2, we will be concerned with *classification* problems.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)这个任务——将模型输出拟合为第4章讨论的类型的连续值——被称为*回归*问题。在第7章和第2部分中，我们将关注*分类*问题。
- en: ^(5.)The author of this chapter is Italian, so please forgive him for using
    sensible units.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^(5.)本章的作者是意大利人，所以请原谅他使用合理的单位。
- en: ^(6.)The weight tells us how much a given input influences the output. The bias
    is what the output would be if all inputs were zero.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^(6.)权重告诉我们给定输入对输出的影响程度。偏差是如果所有输入都为零时的输出。
- en: ^(7.)Contrast that with the function shown in figure 5.6, which is not convex.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^(7.)与图5.6中显示的函数形成对比，该函数不是凸的。
- en: ^(8.)The fancy name for this is *hyperparameter tuning*. *Hyperparameter* refers
    to the fact that we are training the model’s parameters, but the hyperparameters
    control how this training goes. Typically these are more or less set manually.
    In particular, they cannot be part of the same optimization.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ^(8.)这个的花哨名称是*超参数调整*。*超参数*指的是我们正在训练模型的参数，但超参数控制着这个训练的进行方式。通常这些是手动设置的。特别是，它们不能成为同一优化的一部分。
- en: ^(9.)Or maybe it is; we won’t judge how you spend your weekend!
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^(9.)或许是吧；我们不会判断你周末怎么过！
- en: ^(10.)Bummer! What are we going to do on Saturdays, now?
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^(10.)糟糕！现在周六我们要做什么？
- en: ^(11.)In reality, it will track that something changed params using an in-place
    operation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^(11.)实际上，它将跟踪使用原地操作更改参数的情况。
- en: ^(12.)We should not think that using `torch.no_grad` necessarily implies that
    the outputs do not require gradients. There are particular circumstances (involving
    views, as discussed in section 3.8.1) in which `requires_grad` is not set to `False`
    even when created in a `no_grad` context. It is best to use the `detach` function
    if we need to be sure.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^(12.)我们不应认为使用 `torch.no_grad` 必然意味着输出不需要梯度。在特定情况下（涉及视图，如第3.8.1节所讨论的），即使在 `no_grad`
    上下文中创建时，`requires_grad` 也不会设置为 `False`。如果需要确保，最好使用 `detach` 函数。
