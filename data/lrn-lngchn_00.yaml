- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: On November 30, 2022, San Francisco–based firm OpenAI [publicly released ChatGPT](https://oreil.ly/uAnsr)—the
    viral AI chatbot that can generate content, answer questions, and solve problems
    like a human. Within two months of its launch, ChatGPT attracted over [100 million
    monthly active users](https://oreil.ly/ATsLe), the fastest adoption rate of a
    new consumer technology application (so far). ChatGPT is a chatbot experience
    powered by an instruction and dialogue-tuned version of OpenAI’s GPT-3.5 family
    of large language models (LLMs). We’ll get to definitions of these concepts very
    shortly.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月30日，总部位于旧金山的公司OpenAI [公开发布了ChatGPT](https://oreil.ly/uAnsr)——这个病毒式的AI聊天机器人可以像人类一样生成内容、回答问题和解决问题。在其发布后的两个月内，ChatGPT吸引了超过[1亿每月活跃用户](https://oreil.ly/ATsLe)，这是新消费技术应用的最高采用率（到目前为止）。ChatGPT是一个由OpenAI的GPT-3.5系列大型语言模型（LLMs）的指令和对话调整版本驱动的聊天机器人体验。我们很快就会定义这些概念。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Building LLM applications with or without LangChain requires the use of an LLM.
    In this book we will be making use of the OpenAI API as the LLM provider we use
    in the code examples (pricing is listed on its [platform](https://oreil.ly/-YYoR)).
    One of the benefits of working with LangChain is that you can follow along with
    all of these examples using either OpenAI or alternative commercial or open source
    LLM providers.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 使用或不用LangChain构建LLM应用都需要使用LLM。在这本书中，我们将使用OpenAI API作为我们在代码示例中使用的LLM提供商（价格列表可在其[平台](https://oreil.ly/-YYoR)上找到）。与LangChain合作的一个好处是，你可以使用OpenAI或替代的商业或开源LLM提供商来跟随所有这些示例。
- en: Three months later, OpenAI [released the ChatGPT API](https://oreil.ly/DwU7R),
    giving developers access to the chat and speech-to-text capabilities. This kickstarted
    an uncountable number of new applications and technical developments under the
    loose umbrella term of *generative AI*.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 三个月后，OpenAI [发布了ChatGPT API](https://oreil.ly/DwU7R)，使开发者能够访问聊天和语音转文本功能。这启动了无数新的应用和技术发展，这些都在“生成式AI”这个宽泛的术语下进行。
- en: Before we define generative AI and LLMs, let’s touch on the concept of *machine
    learning* (ML). Some computer *algorithms* (imagine a repeatable recipe for achievement
    of some predefined task, such as sorting a deck of cards) are directly written
    by a software engineer. Other computer algorithms are instead *learned* from vast
    amounts of training examples—the job of the software engineer shifts from writing
    the algorithm itself to writing the training logic that creates the algorithm.
    A lot of attention in the ML field went into developing algorithms for predicting
    any number of things, from tomorrow’s weather to the most efficient delivery route
    for an Amazon driver.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定义生成式AI和LLMs之前，让我们先谈谈“机器学习”（ML）的概念。一些计算机算法（想象一下实现某些预定义任务的可重复的食谱，例如排序一副扑克牌）是由软件工程师直接编写的。其他计算机算法则是从大量的训练示例中“学习”的——软件工程师的工作从编写算法本身转变为编写创建算法的训练逻辑。在机器学习领域，大量的注意力都集中在开发用于预测各种事物的算法上，从明天的天气到亚马逊司机的最有效配送路线。
- en: With the advent of LLMs and other generative models (such as diffusion models
    for generating images, which we don’t cover in this book), those same ML techniques
    are now applied to the problem of generating new content, such as a new paragraph
    of text or drawing, that is at the same time unique and informed by examples in
    the training data. LLMs in particular are generative models dedicated to generating
    text.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs和其他生成模型（例如用于生成图像的扩散模型，我们在这本书中不涉及）的出现，相同的ML技术现在被应用于生成新内容的问题，例如一段新的文本或绘画，这些内容既独特又受训练数据中示例的启发。特别是LLMs是专门用于生成文本的生成模型。
- en: 'LLMs have two other differences from previous ML algorithms:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs与之前的ML算法有两个其他的不同之处：
- en: They are trained on much larger amounts of data; training one of these models
    from scratch would be very costly.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在大量数据上进行训练；从头开始训练这些模型将非常昂贵。
- en: They are more versatile.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们更加通用。
- en: The same text generation model can be used for summarization, translation, classification,
    and so forth, whereas previous ML models were usually trained and used for a specific
    task.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的文本生成模型可以用于摘要、翻译、分类等等，而之前的机器学习模型通常只针对特定任务进行训练和使用。
- en: These two differences conspire to make the job of the software engineer shift
    once more, with increasing amounts of time dedicated to working out how to get
    an LLM to work for their use case. And that’s what LangChain is all about.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个差异共同导致软件工程师的工作再次发生转变，他们投入越来越多的时间来弄清楚如何让LLM为他们的用例工作。这正是LangChain的宗旨。
- en: By the end of 2023, competing LLMs emerged, including Anthropic’s Claude and
    Google’s Bard (later renamed Gemini), providing even wider access to these new
    capabilities. And subsequently, thousands of successful startups and major enterprises
    have incorporated generative AI APIs to build applications for various use cases,
    ranging from customer support chatbots to writing and debugging code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 到2023年底，出现了竞争性的LLMs，包括Anthropic的Claude和Google的Bard（后来更名为Gemini），这为这些新功能提供了更广泛的访问。随后，数千家成功的初创公司和大型企业将生成式AI
    API集成到他们的应用中，用于各种用例，从客户支持聊天机器人到编写和调试代码。
- en: 'On October 22, 2022, Harrison Chase [published the first commit](https://oreil.ly/mCdYZ)
    on GitHub for the LangChain open source library. LangChain started from the realization
    that the most interesting LLM applications needed to use LLMs together with [“other
    sources of computation or knowledge”](https://oreil.ly/uXiPi). For instance, you
    can try to get an LLM to generate the answer to this question:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年10月22日，哈里森·查斯在GitHub上[发布了LangChain开源库的第一个提交](https://oreil.ly/mCdYZ)。LangChain始于这样一个认识：最有趣的LLM应用需要将LLM与“其他计算或知识来源”一起使用。[“other
    sources of computation or knowledge”](https://oreil.ly/uXiPi)。例如，你可以尝试让LLM生成这个问题的答案：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You’ll likely be disappointed by its math prowess. However, if you pair it
    up with a calculator function, you can instead instruct the LLM to reword the
    question into an input that a calculator could handle:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会对其数学能力感到失望。然而，如果你将其与计算器功能配对，你就可以指示LLM将问题重新措辞为一个计算器可以处理的数据输入：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then you can pass that to a calculator function and get an accurate answer to
    your original question. LangChain was the first (and, at the time of writing,
    the largest) library to provide such building blocks and the tooling to reliably
    combine them into larger applications. Before discussing what it takes to build
    compelling applications with these new tools, let’s get more familiar with LLMs
    and LangChain.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将这个结果传递给计算器函数，并得到你原始问题的准确答案。LangChain是第一个（也是截至写作时最大的）提供这些构建块和工具的库，可以可靠地将它们组合成更大的应用。在讨论如何使用这些新工具构建引人入胜的应用之前，让我们更熟悉LLMs和LangChain。
- en: Brief Primer on LLMs
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简要介绍LLMs
- en: In layman’s terms, LLMs are trained algorithms that receive text input and predict
    and generate humanlike text output. Essentially, they behave like the familiar
    autocomplete feature found on many smartphones, but taken to an extreme.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 用通俗易懂的话来说，LLMs是一种经过训练的算法，它们接收文本输入并预测和生成类似人类的文本输出。本质上，它们就像许多智能手机上熟悉的自动完成功能，但被推向了极致。
- en: 'Let’s break down the term *large language model*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分解一下“大型语言模型”这个术语：
- en: '*Large* refers to the size of these models in terms of training data and parameters
    used during the learning process. For example, OpenAI’s GPT-3 model contains 175
    billion *parameters*, which were learned from training on 45 terabytes of text
    data.^([1](preface01.html#id259)) *Parameters* in a neural network model are made
    up of the numbers that control the output of each *neuron* and the relative weight
    of its connections with its neighboring neurons. (Exactly which neurons are connected
    to which other neurons varies for each neural network architecture and is beyond
    the scope of this book.)'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “大型”指的是这些模型在训练数据和学习过程中使用的参数大小。例如，OpenAI的GPT-3模型包含1750亿个参数，这些参数是通过在4500太字节文本数据上训练获得的。[1](preface01.html#id259)
    神经网络模型中的“参数”由控制每个“神经元”输出的数字以及与其相邻神经元连接的相对权重组成。（连接到哪些其他神经元的神经元对于每个神经网络架构都是不同的，这超出了本书的范围。）
- en: '*Language model* refers to a computer algorithm trained to receive written
    text (in English or other languages) and produce output also as written text (in
    the same language or a different one). These are *neural networks*, a type of
    ML model which resembles a stylized conception of the human brain, with the final
    output resulting from the combination of the individual outputs of many simple
    mathematical functions, called *neurons*, and their interconnections. If many
    of these neurons are organized in specific ways, with the right training process
    and the right training data, this produces a model that is capable of interpreting
    the meaning of individual words and sentences, which makes it possible to use
    them for generating plausible, readable, written text.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语言模型*指的是一种计算机算法，经过训练可以接收书面文本（英语或其他语言）并产生书面文本输出（同一语言或不同语言）。这些是*神经网络*，一种类似于人类大脑风格化概念的机器学习模型，最终输出是由许多简单数学函数（称为*神经元*）的个别输出及其相互连接的组合产生的。如果许多神经元以特定方式组织，并且经过适当的训练过程和训练数据，这将产生一种能够解释单个单词和句子含义的模型，这使得它们可以用于生成合理、可读的书面文本。'
- en: Because of the prevalence of English in the training data, most models are better
    at English than they are at other languages with a smaller number of speakers.
    By “better” we mean it is easier to get them to produce desired outputs in English.
    There are LLMs designed for multilingual output, such as [BLOOM](https://oreil.ly/Nq7w0),
    that use a larger proportion of training data in other languages. Curiously, the
    difference in performance between languages isn’t as large as might be expected,
    even in LLMs trained on a predominantly English training corpus. Researchers have
    found that LLMs are able to transfer some of their semantic understanding to other
    languages.^([2](preface01.html#id267))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据中英语的普遍性，大多数模型在英语上的表现优于其他使用人数较少的语言。我们所说的“更好”是指更容易让它们在英语中产生期望的输出。有一些LLM是为多语言输出设计的，例如[BLOOM](https://oreil.ly/Nq7w0)，它使用了更大比例的其他语言训练数据。有趣的是，即使在以英语为主的训练语料库上训练的LLM中，不同语言之间的性能差异也没有预期的那么大。研究人员发现，LLM能够将其部分语义理解转移到其他语言上.^([2](preface01.html#id267))
- en: Put together, *large language models* are instances of big, general-purpose
    language models that are trained on vast amounts of text. In other words, these
    models have learned from patterns in large datasets of text—books, articles, forums,
    and other publicly available sources—to perform general text-related tasks. These
    tasks include text generation, summarization, translation, classification, and
    more.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，*大型语言模型*是大型通用语言模型的实例，这些模型在大量文本上进行训练。换句话说，这些模型已经从大量文本数据集的规律中学习——书籍、文章、论坛和其他公开可用的来源，以执行各种文本相关任务。这些任务包括文本生成、摘要、翻译、分类等等。
- en: 'Let’s say we instruct an LLM to complete the following sentence:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们指示一个大型语言模型（LLM）完成以下句子：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The LLM will take that input text and predict the correct output answer as `London`.
    This looks like magic, but it’s not. Under the hood, the LLM estimates the probability
    of a sequence of word(s) given a previous sequence of words.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LLM将接受该输入文本并预测正确的输出答案为`London`。这看起来像是魔术，但并非如此。在底层，LLM根据之前的单词序列估计给定序列中单词的概率。
- en: Tip
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'Technically speaking, the model makes predictions based on tokens, not words.
    A *token* represents an atomic unit of text. Tokens can represent individual characters,
    words, subwords, or even larger linguistic units, depending on the specific tokenization
    approach used. For example, using GPT-3.5’s tokenizer (called `cl100k`), the phrase
    *good morning dearest friend* would consist of [five tokens](https://oreil.ly/dU83b)
    (using `_` to show the space character):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度讲，模型基于令牌进行预测，而不是单词。*令牌*代表文本的原子单元。令牌可以代表单个字符、单词、子词，甚至更大的语言单元，具体取决于所使用的特定分词方法。例如，使用GPT-3.5的分词器（称为`cl100k`），短语*good
    morning dearest friend*将包含[五个令牌](https://oreil.ly/dU83b)（使用`_`表示空格字符）：
- en: '`Good`'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`Good`'
- en: With token ID `19045`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌ID `19045`
- en: '`_morning`'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`_morning`'
- en: With token ID `6693`
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌ID `6693`
- en: '`_de`'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`_de`'
- en: With token ID `409`
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌ID `409`
- en: '`arest`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`arest`'
- en: With token ID `15795`
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌ID `15795`
- en: '`_friend`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`_friend`'
- en: With token ID `4333`
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用令牌ID `4333`
- en: Usually tokenizers are trained with the objective of having the most common
    words encoded into a single token, for example, the word *morning* is encoded
    as the token `6693`. Less common words, or words in other languages (usually tokenizers
    are trained on English text), require several tokens to encode them. For example,
    the word *dearest* is encoded as tokens `409, 15795`. One token spans on average
    four characters of text for common English text, or roughly three quarters of
    a word.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，标记化器被训练的目标是将最常见的单词编码为单个标记，例如，单词*morning*被编码为标记`6693`。不常见的单词或其他语言的单词（通常标记化器是在英语文本上训练的）需要多个标记来编码它们。例如，单词*dearest*被编码为标记`409,
    15795`。一个标记平均跨越四个字符的文本，对于常见的英语文本来说，大约是四分之三的单词。
- en: The driving engine behind LLMs’ predictive power is known as the *transformer
    neural network architecture*.^([3](preface01.html#id271)) The transformer architecture
    enables models to handle sequences of data, such as sentences or lines of code,
    and make predictions about the likeliest next word(s) in the sequence. Transformers
    are designed to understand the context of each word in a sentence by considering
    it in relation to every other word. This allows the model to build a comprehensive
    understanding of the meaning of a sentence, paragraph, and so on (in other words,
    a sequence of words) as the joint meaning of its parts in relation to each other.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动LLM预测能力的引擎被称为*Transformer神经网络架构*。[3](preface01.html#id271) Transformer架构使模型能够处理数据序列，如句子或代码行，并预测序列中最可能出现的下一个单词。Transformer的设计是通过考虑每个单词与句子中其他单词的关系来理解每个单词的上下文。这使得模型能够构建对句子、段落等（换句话说，单词序列）的综合理解，即其部分之间的联合意义。
- en: So, when the model sees the sequence of words *the capital of England is*, it
    makes a prediction based on similar examples it saw during its training. In the
    model’s training corpus the word *England* (or the token(s) that represent it)
    would have often shown up in sentences in similar places to words like *France*,
    *United States*, *China*. The word *capital* would figure in the training data
    in many sentences also containing words like *England*, *France*, and *US*, and
    words like *London*, *Paris*, *Washington*. This repetition during the model’s
    training resulted in the capacity to correctly predict that the next word in the
    sequence should be *London*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当模型看到单词序列*英格兰的首都是*时，它会根据其在训练期间看到的类似示例进行预测。在模型的训练语料库中，单词*英格兰*（或代表它的标记）通常会出现在与*法国*、*美国*、*中国*等单词相似的句子中的相似位置。单词*首都*会在包含*英格兰*、*法国*和*US*等单词以及*伦敦*、*巴黎*、*华盛顿*等单词的许多句子中的训练数据中出现。这种在模型训练过程中的重复导致模型能够正确预测序列中的下一个单词应该是*伦敦*。
- en: The instructions and input text you provide to the model is called a *prompt*.
    Prompting can have a significant impact on the quality of output from the LLM.
    There are several best practices for *prompt design* or *prompt engineering*,
    including providing clear and concise instructions with contextual examples, which
    we discuss later in this book. Before we go further into prompting, let’s look
    at some different types of LLMs available for you to use.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你提供给模型的指示和输入文本被称为*提示*。提示可以显著影响LLM输出的质量。在*提示设计*或*提示工程*方面，有一些最佳实践，包括提供清晰简洁的指示，并附上上下文示例，我们将在本书后面讨论。在我们进一步探讨提示之前，让我们看看一些可供你使用的不同类型的LLM。
- en: 'The base type, from which all the others derive, is commonly known as a *pretrained
    LLM*: it has been trained on very large amounts of text (found on the internet
    and in books, newspapers, code, video transcripts, and so forth) in a self-supervised
    fashion. This means that—unlike in supervised ML, where prior to training the
    researcher needs to assemble a dataset of pairs of *input* to *expected output*—for
    LLMs those pairs are inferred from the training data. In fact, the only feasible
    way to use datasets that are so large is to assemble those pairs from the training
    data automatically. Two techniques to do this involve having the model do the
    following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 所有其他类型都从中派生出来的基本类型通常被称为*预训练LLM*：它已经在大量文本上进行了训练（这些文本可以在互联网和书籍、报纸、代码、视频脚本等地方找到），并且是以自监督的方式进行训练。这意味着——与监督ML不同，在训练之前，研究人员需要组装一个包含*输入*到*预期输出*对的数据库——对于LLM，这些对是从训练数据中推断出来的。实际上，使用如此庞大的数据集的唯一可行方法是从训练数据中自动组装这些对。完成此操作的两个技术涉及模型执行以下操作：
- en: Predict the next word
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 预测下一个单词
- en: Remove the last word from each sentence in the training data, and that yields
    a pair of *input* and *expected output*, such as *The capital of England is ___*
    and *London*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练数据中的每一句话中删除最后一个词，这样就得到了一对 *输入* 和 *预期输出*，例如 *The capital of England is ___*
    和 *London*。
- en: Predict a missing word
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 预测缺失的单词
- en: Similarly, if you take each sentence and omit a word from the middle, you now
    have other pairs of input and expected output, such as *The ___ of England is
    London* and *capital*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果你从每一句话中删除中间的一个词，你现在就有了其他输入和预期输出的配对，例如 *The ___ of England is London* 和
    *capital*。
- en: These models are quite difficult to use as is, they require you to prime the
    response with a suitable prefix. For instance, if you want to know the capital
    of England, you might get a response by prompting the model with *The capital
    of England is*, but not with the more natural *What is the capital of England?*
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型本身使用起来相当困难，它们需要你用一个合适的词缀来预热响应。例如，如果你想了解英格兰的首都，你可能需要通过提示模型 *The capital of
    England is* 来获得响应，而不是更自然的 *What is the capital of England?*
- en: Instruction-Tuned LLMs
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 指令微调LLM
- en: '[Researchers](https://oreil.ly/lP6hr) have made pretrained LLMs easier to use
    by further training (additional training applied on top of the long and costly
    training described in the previous section), also known as *fine-tuning* them
    on the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[研究人员](https://oreil.ly/lP6hr) 通过进一步训练（在之前章节中描述的漫长且昂贵的训练基础上进行的额外训练），也称为在以下方面对预训练的LLM进行*微调*，使得它们更容易使用：'
- en: Task-specific datasets
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 任务特定数据集
- en: 'These are datasets of pairs of questions/answers manually assembled by researchers,
    providing examples of desirable responses to common questions that end users might
    prompt the model with. For example, the dataset might contain the following pair:
    *Q: What is the capital of England? A: The capital of England is London.* Unlike
    the pretraining datasets, these are manually assembled, so they are by necessity
    much smaller:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '这些是由研究人员手动组装的问答对数据集，提供了用户可能提示模型的常见问题的期望响应示例。例如，数据集可能包含以下配对：*Q: What is the
    capital of England? A: The capital of England is London.* 与预训练数据集不同，这些是手动组装的，因此它们必然要小得多：'
- en: Reinforcement learning from human feedback (RLHF)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 来自人类反馈的强化学习（RLHF）
- en: Through the use of [RLHF methods](https://oreil.ly/lrlAK), those manually assembled
    datasets are augmented with user feedback received on output produced by the model.
    For example, user A preferred *The capital of England is London* to *London is
    the capital of England* as an answer to the earlier question.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 [RLHF方法](https://oreil.ly/lrlAK)，那些手动组装的数据集通过模型生成的输出所收到的用户反馈得到了增强。例如，用户A更喜欢
    *The capital of England is London* 作为对之前问题的回答，而不是 *London is the capital of England*。
- en: Instruction-tuning has been key to broadening the number of people who can build
    applications with LLMs, as they can now be prompted with *instructions*, often
    in the form of questions such as, *What is the capital of England?*, as opposed
    to *The capital of England is*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调对于扩大能够使用LLM构建应用程序的人数至关重要，因为它们现在可以用*指令*来提示，通常是问题的形式，例如，*What is the capital
    of England?*，而不是 *The capital of England is*。
- en: Dialogue-Tuned LLMs
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对话微调LLM
- en: 'Models tailored for dialogue or chat purposes are a [further enhancement](https://oreil.ly/1DxW6)
    of instruction-tuned LLMs. Different providers of LLMs use different techniques,
    so this is not necessarily true of all *chat models*, but usually this is done
    via the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 专为对话或聊天目的定制的模型是对指令微调LLM的[进一步改进](https://oreil.ly/1DxW6)。LLM的不同提供商使用不同的技术，因此这并不一定适用于所有
    *聊天模型*，但通常这是通过以下方式实现的：
- en: Dialogue datasets
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对话数据集
- en: The manually assembled *fine-tuning* datasets are extended to include more examples
    of multiturn dialogue interactions, that is, sequences of prompt-reply pairs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 手动组装的 *微调* 数据集被扩展，包括更多多轮对话交互的示例，即提示-回复对的序列。
- en: Chat format
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天格式
- en: The input and output formats of the model are given a layer of structure over
    freeform text, which divides text into parts associated with a role (and optionally
    other metadata like a name). Usually, the roles available are *system* (for instructions
    and framing of the task), *user* (the actual task or question), and *assistant*
    (for the outputs of the model). This method evolved from early [prompt engineering
    techniques](https://oreil.ly/dINx0) and makes it easier to tailor the model’s
    output while making it harder for models to confuse user input with instructions.
    Confusing user input with prior instructions is also known as *jailbreaking*,
    which can, for instance, lead to carefully crafted prompts, possibly including
    trade secrets, being exposed to end users.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输入和输出格式在自由文本之上增加了一层结构，将文本划分为与角色（以及可选的其他元数据，如名称）相关的部分。通常，可用的角色是*系统*（用于指令和任务的框架）、*用户*（实际的任务或问题）和*助手*（用于模型的输出）。这种方法是从早期的[提示工程技术](https://oreil.ly/dINx0)演变而来的，它使得调整模型输出变得更加容易，同时使得模型更难将用户输入与指令混淆。将用户输入与先前指令混淆也称为*越狱*，例如，可能导致精心设计的提示被暴露给最终用户，这些提示可能包括商业机密。
- en: Fine-Tuned LLMs
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精调大型语言模型
- en: Fine-tuned LLMs are created by taking base LLMs and further training them on
    a proprietary dataset for a specific task. Technically, instruction-tuned and
    dialogue-tuned LLMs are fine-tuned LLMs, but the term “fine-tuned LLM” is usually
    used to describe LLMs that are tuned by the developer for their specific task.
    For example, a model can be fine-tuned to accurately extract the sentiment, risk
    factors, and key financial figures from a public company’s annual report. Usually,
    fine-tuned models have improved performance on the chosen task at the expense
    of a loss of generality. That is, they become less capable of answering queries
    on unrelated tasks.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 精调大型语言模型是通过在特定任务上对基础大型语言模型进行进一步训练，并在专有数据集上进行训练创建的。技术上讲，指令调优和对话调优的大型语言模型都是精调大型语言模型，但“精调大型语言模型”这个术语通常用来描述由开发者针对特定任务进行调优的大型语言模型。例如，可以将模型调优以准确从上市公司的年度报告中提取情感、风险因素和关键财务数据。通常，精调模型在所选任务上的性能有所提高，但以泛化能力的损失为代价。也就是说，它们在回答与无关任务相关的查询时变得不那么有能力。
- en: Throughout the rest of this book, when we use the term *LLM*, we mean instruction-tuned
    LLMs, and for *chat model* we mean dialogue-instructed LLMs, as defined earlier
    in this section. These should be your workhorses when using LLMs—the first tools
    you reach for when starting a new LLM application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，当我们使用术语*LLM*时，我们指的是指令调优的 LLM，而对于*聊天模型*，我们指的是对话指令的 LLM，如本节之前定义的。这些应该是您使用
    LLM 时的得力助手——在开始新的 LLM 应用程序时首先会使用的工具。
- en: Now let’s quickly discuss some common LLM prompting techniques before diving
    into LangChain.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在深入探讨 LangChain 之前，让我们快速讨论一些常见的 LLM 提示技术。
- en: Brief Primer on Prompting
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示工程简明指南
- en: As we touched on earlier, the main task of the software engineer working with
    LLMs is not to train an LLM, or even to fine-tune one (usually), but rather to
    take an existing LLM and work out how to get it to accomplish the task you need
    for your application. There are commercial providers of LLMs, like OpenAI, Anthropic,
    and Google, as well as open source LLMs ([Llama](https://oreil.ly/ld3Fu), [Gemma](https://oreil.ly/RGKfi),
    and others), released free-of-charge for others to build upon. Adapting an existing
    LLM for your task is called *prompt engineering*.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，与 LLM 一起工作的软件工程师的主要任务不是训练一个 LLM，甚至通常也不是对其进行微调，而是要找到一个现有的 LLM，并找出如何让它完成您应用程序所需的任务。有商业
    LLM 提供商，如 OpenAI、Anthropic 和 Google，以及开源 LLM（[Llama](https://oreil.ly/ld3Fu)、[Gemma](https://oreil.ly/RGKfi)
    等），免费提供给他人构建。为您的任务调整现有 LLM 被称为*提示工程*。
- en: Many prompting techniques have been developed in the past two years, and in
    a broad sense, this is a book about how to do prompt engineering with LangChain—how
    to use LangChain to get LLMs to do what you have in mind. But before we get into
    LangChain proper, it helps to go over some of these techniques first (and we apologize
    in advance if your favorite [prompting technique](https://oreil.ly/8uGK_) isn’t
    listed here; there are too many to cover).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去两年中，已经开发了许多提示技术，从广义上讲，这本书是关于如何使用 LangChain 进行提示工程——如何使用 LangChain 让大型语言模型完成您心中的任务。但在我们深入探讨
    LangChain 之前，先回顾一下这些技术是有帮助的（如果您的最喜欢的[提示技术](https://oreil.ly/8uGK_)没有列在这里，我们提前表示歉意；要涵盖的内容太多）。
- en: 'To follow along with this section we recommend copying these prompts to the
    OpenAI Playground to try them yourself:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随本节内容，我们建议您将这些提示复制到 OpenAI 操场进行尝试：
- en: Create an account for the OpenAI API at [*http://platform.openai.com*](http://platform.openai.com),
    which will let you use OpenAI LLMs programmatically, that is, using the API from
    your Python or JavaScript code. It will also give you access to the OpenAI Playground,
    where you can experiment with prompts from your web browser.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在[*http://platform.openai.com*](http://platform.openai.com)创建OpenAI API的账户，这将让你能够通过Python或JavaScript代码编程地使用OpenAI
    LLM，也就是说，使用API。它还将为你提供访问OpenAI游乐场的权限，在那里你可以通过网页浏览器进行提示实验。
- en: If necessary, add payment details for your new OpenAI account. OpenAI is a commercial
    provider of LLMs and charges a fee for each time you use their models through
    OpenAI’s API or through Playground. You can find the latest pricing on their [website](https://oreil.ly/MiKRD).
    Over the past two years, the price for using OpenAI’s models has come down significantly
    as new capabilities and optimizations are introduced.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要，请为你的新OpenAI账户添加支付详情。OpenAI是LLM的商业提供商，每次你通过OpenAI的API或通过游乐场使用他们的模型时都会收费。你可以在他们的[网站](https://oreil.ly/MiKRD)上找到最新的定价。在过去两年中，随着新功能和优化的引入，使用OpenAI模型的价格已经大幅下降。
- en: Head on over to the [OpenAI Playground](https://oreil.ly/rxiAG) and you’re ready
    to try out the following prompts for yourself. We’ll make use of the OpenAI API
    throughout this book.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 直接前往[OpenAI游乐场](https://oreil.ly/rxiAG)，你就可以尝试以下提示了。我们将在整本书中使用OpenAI API。
- en: Once you’ve navigated to the Playground, you will see a panel of presets on
    the right side of the screen, including your model of choice. If you look further
    down the panel, you will see Temperature under the “Model configuration” title.
    Move the Temperature toggle from middle to left until the number shows 0.00\.
    Essentially, temperature controls the randomness of LLM output. The lower the
    temperature, the more deterministic the model output.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你导航到游乐场，你将在屏幕右侧看到一个预设的面板，包括你选择的项目。如果你继续向下滚动面板，你将在“模型配置”标题下看到“温度”。将温度开关从中间移动到左侧，直到数字显示为0.00。本质上，温度控制LLM输出的随机性。温度越低，模型输出越确定。
- en: Now on to the prompts!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是提示的使用方法！
- en: Zero-Shot Prompting
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本提示
- en: 'The first and most straightforward prompting technique consists of simply instructing
    the LLM to perform the desired task:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种也是最直接的提示技术，就是简单地指示LLM执行所需的任务：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is typically what you should try first, and it will usually work for simple
    questions, especially when the answer to it was likely present in some of the
    training data. If we prompt OpenAI’s `gpt-3.5-turbo` with the preceding prompt,
    the following is returned:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常是你应该尝试的第一件事，它通常适用于简单的问题，特别是当答案很可能包含在部分训练数据中时。如果我们用前面的提示提示OpenAI的`gpt-3.5-turbo`，则会返回以下内容：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may get a different result from what we get. There is an element of randomness
    to how LLMs generate responses, and OpenAI may have updated the model by the time
    you try it.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会得到与我们不同的结果。LLM生成响应的方式有一定的随机性，OpenAI可能在尝试时已经更新了模型。
- en: While the model did correctly identify the 30th president, the answer isn’t
    quite right. Often you’ll have to iterate on prompts and responses to get a reliable
    system. The next sections expand on how.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型正确地识别了第30任总统，但答案并不完全正确。通常，你需要迭代提示和响应来获得可靠的系统。接下来的几节将扩展如何做到这一点。
- en: Chain-of-Thought
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 思维链
- en: 'A very useful iteration is to further instruct the model to [take the time
    to *think*](https://oreil.ly/8xWcs). This technique has been found to increase
    performance on a variety of tasks. Curiously, a more recent paper^([4](preface01.html#id304))
    suggests that this technique can also reduce LLM performance on tasks where thinking
    reduces human performance. Called chain-of-thought (CoT) prompting, this is usually
    done by prepending the prompt with instructions for the LLM to describe how it
    could arrive at the answer:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有用的迭代是进一步指示模型[花时间*思考*](https://oreil.ly/8xWcs)。这项技术已被发现可以提高各种任务的表现。有趣的是，一篇更近期的论文^([4](preface01.html#id304))表明，这项技术还可以降低在思考会降低人类表现的任务上LLM的表现。称为思维链（CoT）提示，这通常是通过在提示前添加指令来完成的，指示LLM如何得出答案：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And now let’s see what the same model produces for this new prompt:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看同一模型对这个新提示会产生什么结果：
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice how the only change in the prompt was the addition of `Think step by
    step`, and how much of a difference that made in the model’s output. Now the model
    outputs a sequence of steps you could go through when trying to answer this question.
    Unfortunately, while the steps are very reasonable, the final output is still
    incorrect, given it got some facts wrong, namely it listed the wrong birth and
    death years for President Coolidge’s wife’s mother. Let’s see how to improve on
    this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到提示的唯一变化是添加了`Think step by step`，以及这对模型输出的影响有多大。现在模型输出了一系列你可以尝试的步骤来回答这个问题。不幸的是，尽管步骤非常合理，但最终输出仍然不正确，因为它在事实上有误，具体来说，它列出了柯立芝总统妻子的母亲错误的出生和去世年份。让我们看看如何改进这一点。
- en: Retrieval-Augmented Generation
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索增强生成
- en: '*Retrieval-augmented generation* (RAG) consists of finding relevant pieces
    of text, also known as *context*, such as facts you’d find in an encyclopedia
    and including that context in the prompt. The RAG technique can (and in real applications
    should) be combined with CoT, but for simplicity we’ll use these techniques one
    at a time here. Here’s the prompt including RAG:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*检索增强生成*（RAG）包括找到相关的文本片段，也称为*上下文*，例如你可以在百科全书找到的事实，并将这些上下文包含在提示中。RAG技术可以（并且在实际应用中应该）与CoT结合使用，但为了简单起见，我们在这里一次使用这些技术中的一个。以下是包含RAG的提示：'
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And the output from the model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出如下：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now we’re a lot closer to the correct answer, but as we touched on earlier,
    LLMs aren’t great at out-of-the-box math. In this case, the final result of 54
    years old is off by 3\. Let’s see how we can improve on this.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们离正确答案更近了，但正如我们之前提到的，大型语言模型在处理离谱的数学问题时并不擅长。在这种情况下，最终结果54岁是错误的，偏差了3岁。让我们看看我们如何改进这一点。
- en: Tool Calling
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具调用
- en: 'The *tool calling* technique consists of prepending the prompt with a list
    of external functions the LLM can make use of, along with descriptions of what
    each is good for and instructions on how to signal in the output that it *wants*
    to use one (or more) of these functions. Finally, you—the developer of the application—should
    parse the output and call the appropriate functions. Here’s one way to do this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*工具调用*技术包括在提示前加上一个LLM可以使用的函数列表，以及每个函数的用途描述和如何在输出中指示它*想要*使用（或使用多个）这些函数的说明。最后，你——应用程序的开发者——应该解析输出并调用适当的函数。以下是这样做的一种方法：'
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And this is the output you might get:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可能会得到的输出：
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'While the LLM correctly followed the output format instructions, the tools
    and inputs selected aren’t the most appropriate for this question. This gets at
    one of the most important things to keep in mind when prompting LLMs: *each prompting
    technique is most useful when used in combination with (some of) the others*.
    For instance, here we could improve on this by combining tool calling, chain-of-thought,
    and RAG into a prompt that uses all three. Let’s see what that looks like:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然LLM正确地遵循了输出格式说明，但选择的工具和输入并不是最合适的。这涉及到在提示LLM时需要记住的最重要的事情之一：*每种提示技术在使用时与（一些）其他技术结合最为有用*。例如，在这里，我们可以通过将工具调用、思维链和RAG结合到一个提示中，使用所有三种技术来改进这一点。让我们看看这会是什么样子：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And with this prompt, maybe after a few tries, we might get this output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 而且有了这个提示，经过几次尝试，我们可能会得到这个输出：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we parse that CSV output, and have a calculator function execute the operation
    `1929 - 1827`, we finally get the right answer: 57 years.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们解析那个CSV输出，并让计算器函数执行操作`1929 - 1827`，我们最终得到正确答案：57岁。
- en: As per the previous example, by combining RAG with chain-of-thought and tool
    calling, you can retrieve the most relevant data to ground your model’s output,
    then guide it step by step to ensure it uses that context effectively.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的例子，通过将RAG与思维链和工具调用相结合，你可以检索到最相关的数据来确保你的模型输出有据可依，然后逐步引导它，确保它有效地使用该上下文。
- en: Few-Shot Prompting
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 少样本提示
- en: 'Finally, we come to another very useful prompting technique: *few-shot prompting*.
    This consists of providing the LLM with examples of other questions and the correct
    answers, which enables the LLM to *learn* how to perform a new task without going
    through additional training or fine-tuning. When compared to fine-tuning, few-shot
    prompting is more flexible—you can do it on the fly at query time—but less powerful,
    and you might achieve better performance with fine-tuning. That said, you should
    usually always try few-shot prompting before fine-tuning:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们来到另一个非常有用的提示技术：*少样本提示*。这包括向LLM提供其他问题的示例和正确答案，这使得LLM能够*学习*如何执行新任务，而无需进行额外的训练或微调。与微调相比，少样本提示更加灵活——你可以在查询时即时进行——但功能较弱，你可能会通过微调获得更好的性能。尽管如此，你应该通常总是先尝试少样本提示，然后再进行微调：
- en: Static few-shot prompting
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 静态少样本提示
- en: The most basic version of few-shot prompting is to assemble a predetermined
    list of a small number of examples that you include in the prompt.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本提示的最基本版本是组装一个预定的少量示例列表，并将其包含在提示中。
- en: Dynamic few-shot prompting
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 动态少样本提示
- en: If you assemble a dataset of many examples, you can instead pick the most relevant
    examples for each new query.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你组装了一个包含许多示例的数据集，你可以选择每个新查询中最相关的示例。
- en: The next section covers using LangChain to build applications using LLMs and
    these prompting techniques.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将介绍如何使用LangChain构建使用LLM和这些提示技术的应用程序。
- en: LangChain and Why It’s Important
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LangChain及其重要性
- en: LangChain was one of the earliest open source libraries to provide LLM and prompting
    building blocks and the tooling to reliably combine them into larger applications.
    As of writing, LangChain has amassed over [28 million monthly downloads](https://oreil.ly/8OKbf),
    [99,000 GitHub stars](https://oreil.ly/bF5pc), and the largest developer community
    in generative AI ([72,000+ strong](https://oreil.ly/PNWL3)). It has enabled software
    engineers who don’t have an ML background to utilize the power of LLMs to build
    a variety of apps, ranging from AI chatbots to AI agents that can reason and take
    action responsibly.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain是早期开源库之一，提供LLM和提示构建块以及将它们可靠地组合成更大应用程序的工具。截至编写本文时，LangChain已经积累了超过[2800万月度下载量](https://oreil.ly/8OKbf)，[99,000
    GitHub星标](https://oreil.ly/bF5pc)，以及生成AI领域最大的开发者社区（[72,000+](https://oreil.ly/PNWL3)）。它使得没有机器学习背景的软件工程师能够利用LLM的力量来构建各种应用程序，从AI聊天机器人到能够进行推理并负责任采取行动的AI代理。
- en: 'LangChain builds on the idea stressed in the preceding section: that prompting
    techniques are most useful when used together. To make that easier, LangChain
    provides simple *abstractions* for each major prompting technique. By abstraction
    we mean Python and JavaScript functions and classes that encapsulate the ideas
    of those techniques into easy-to-use wrappers. These abstractions are designed
    to play well together and to be combined into a larger LLM application.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain建立在上一节强调的思想之上：提示技术在使用时结合最为有用。为了使这更加容易，LangChain为每种主要的提示技术提供了简单的*抽象*。通过抽象，我们指的是将那些技术的思想封装到易于使用的Python和JavaScript函数和类中的封装器。这些抽象被设计成能够很好地协同工作，并可以组合成更大的LLM应用程序。
- en: First of all, LangChain provides integrations with the major LLM providers,
    both commercial ([OpenAI](https://oreil.ly/TTLXA), [Anthropic](https://oreil.ly/O4UXw),
    [Google](https://oreil.ly/12g3Z), and more) and open source ([Llama](https://oreil.ly/5WAVi),
    [Gemma](https://oreil.ly/-40Ne), and others). These integrations share a common
    interface, making it very easy to try out new LLMs as they’re announced and letting
    you avoid being locked-in to a single provider. We’ll use these in [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LangChain提供了与主要LLM提供商的集成，包括商业的（[OpenAI](https://oreil.ly/TTLXA)，[Anthropic](https://oreil.ly/O4UXw)，[Google](https://oreil.ly/12g3Z)等）和开源的（[Llama](https://oreil.ly/5WAVi)，[Gemma](https://oreil.ly/-40Ne)等）。这些集成共享一个共同的接口，使得尝试新宣布的LLM变得非常容易，并让你避免被锁定在单一提供商。我们将在[第1章](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)中使用这些。
- en: LangChain also provides *prompt template* abstractions, which enable you to
    reuse prompts more than once, separating static text in the prompt from placeholders
    that will be different for each time you send it to the LLM to get a completion
    generated. We’ll talk more about these also in [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004).
    LangChain prompts can also be stored in the LangChain Hub for sharing with teammates.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain还提供了*提示模板*抽象，这使得你可以多次重用提示，将提示中的静态文本与每次发送到LLM以生成完成时将不同的占位符分开。我们将在[第1章](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)中更多地讨论这些内容。LangChain提示也可以存储在LangChain
    Hub中，以便与队友分享。
- en: LangChain contains many integrations with third-party services (such as Google
    Sheets, Wolfram Alpha, Zapier, just to name a few) exposed as *tools*, which is
    a standard interface for functions to be used in the tool-calling technique.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain包含许多与第三方服务的集成（例如Google Sheets、Wolfram Alpha、Zapier，仅举几例），这些集成以*工具*的形式暴露出来，这是工具调用技术中函数的标准接口。
- en: For RAG, LangChain provides integrations with the major *embedding models* (language
    models designed to output a numeric representation, the *embedding*, of the meaning
    of a sentence, paragraph, and so on), *vector stores* (databases dedicated to
    storing embeddings), and *vector indexes* (regular databases with vector-storing
    capabilities). You’ll learn a lot more about these in Chapters [2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    and [3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于RAG，LangChain提供了与主要*嵌入模型*（设计用于输出句子、段落等含义的数值表示，即*嵌入*的语言模型）、*向量存储*（专门用于存储嵌入的数据库）和*向量索引*（具有向量存储能力的常规数据库）的集成。你将在第[2章](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)和第[3章](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)中学到更多关于这些内容。
- en: 'For CoT, LangChain (through the LangGraph library) provides *agent* abstractions
    that combine chain-of-thought reasoning and tool calling, first popularized by
    the [ReAct paper](https://oreil.ly/27BIC). This enables building LLM applications
    that do the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CoT，LangChain（通过LangGraph库）提供了*代理*抽象，它结合了思维链推理和工具调用，首先由[ReAct论文](https://oreil.ly/27BIC)推广。这使得构建执行以下操作的LLM应用程序成为可能：
- en: Reason about the steps to take.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理出需要采取的步骤。
- en: Translate those steps into external tool calls.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些步骤转换为外部工具调用。
- en: Receive the output of those tool calls.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收这些工具调用的输出。
- en: Repeat until the task is accomplished.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行，直到任务完成。
- en: We cover these in Chapters [5](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)
    through [8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第[5章](ch05.html#ch05_cognitive_architectures_with_langgraph_1736545670030774)到第[8章](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)中介绍了这些内容。
- en: For chatbot use cases, it becomes useful to keep track of previous interactions
    and use them when generating the response to a future interaction. This is called
    *memory*, and [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)
    discusses using it in LangChain.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于聊天机器人的用例，跟踪之前的交互并在生成未来交互的响应时使用它们变得非常有用。这被称为*记忆*，[第4章](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)讨论了在LangChain中使用它。
- en: Finally, LangChain provides the tools to compose these building blocks into
    cohesive applications. Chapters [1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
    through [6](ch06.html#ch06_agent_architecture_1736545671750341) talk more about
    this.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，LangChain提供了将这些构建块组合成统一应用程序的工具。第[1章](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)到第[6章](ch06.html#ch06_agent_architecture_1736545671750341)将更详细地介绍这一点。
- en: In addition to this library, LangChain provides [LangSmith](https://oreil.ly/geRgx)—a
    platform to help debug, test, deploy, and monitor AI workflows—and LangGraph Platform—a
    platform for deploying and scaling LangGraph agents. We cover these in Chapters
    [9](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604)
    and [10](ch10.html#ch10_testing_evaluation_monitoring_and_continuous_im_1736545678108525).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这个库之外，LangChain还提供了[LangSmith](https://oreil.ly/geRgx)——一个帮助调试、测试、部署和监控AI工作流的平台，以及LangGraph平台——一个用于部署和扩展LangGraph代理的平台。我们将在第[9章](ch09.html#ch09_deployment_launching_your_ai_application_into_pro_1736545675509604)和第[10章](ch10.html#ch10_testing_evaluation_monitoring_and_continuous_im_1736545678108525)中介绍这些内容。
- en: What to Expect from This Book
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从这本书中你可以期待什么
- en: With this book, we hope to convey the excitement and possibility of adding LLMs
    to your software engineering toolbelt.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，我们希望传达将LLM添加到你的软件工程工具包中的兴奋和可能性。
- en: We got into programming because we like building things, getting to the end
    of a project, looking at the final product and realizing there’s something new
    out there, and we built it. Programming with LLMs is so exciting to us because
    it expands the set of things we can build, it makes previously hard things easy
    (for example, extracting relevant numbers from a long text) and previously impossible
    things possible—try building an automated assistant a year ago and you end up
    with the *phone tree hell* we all know and love from calling up customer support
    numbers.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以涉足编程，是因为我们喜欢构建事物，完成项目，看到最终产品，并意识到世界上有新事物出现，而且是我们自己构建的。使用LLMs进行编程对我们来说非常令人兴奋，因为它扩大了我们能构建的事物范围，使以前难以做到的事情变得容易（例如，从长文本中提取相关数字），以及使以前不可能的事情成为可能——试想一年前尝试构建一个自动助手，你最终会陷入我们所有人都熟悉和喜爱的“电话树地狱”，这是从拨打客户支持电话号码时遇到的。
- en: Now with LLMs and LangChain, you can actually build pleasant assistants (or
    myriad other applications) that chat with you and understand your intent to a
    very reasonable degree. The difference is night and day! If that sounds exciting
    to you (as it does to us) then you’ve come to the right place.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了LLMs和LangChain，你实际上可以构建令人愉悦的助手（或无数其他应用），它们可以与你聊天，并且可以非常合理地理解你的意图。这种差异是天上地下！如果你觉得这很令人兴奋（就像我们一样），那么你就来对地方了。
- en: In this Preface, we’ve given you a refresher on what makes LLMs tick and why
    exactly that gives you “thing-building” superpowers. Having these very large ML
    models that understand language and can output answers written in conversational
    English (or some other language) gives you a *programmable* (through prompt engineering),
    versatile language-generation tool. By the end of the book, we hope you’ll see
    just how powerful that can be.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇前言中，我们为你回顾了LLMs的工作原理以及为什么这会给你“构建事物”的超能力。拥有这些理解语言并能以对话式英语（或某种其他语言）输出答案的非常大的ML模型，通过提示工程使其可编程，提供了一种多才多艺的语言生成工具。到本书结束时，我们希望你能看到这有多么强大。
- en: 'We’ll begin with an AI chatbot customized by, for the most part, plain English
    instructions. That alone should be an eye-opener: you can now “program” part of
    the behavior of your application without code.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从大部分使用普通英语指令定制的AI聊天机器人开始。这本身就足以让人眼前一亮：你现在可以不写代码就“编程”你应用的部分行为。
- en: 'Then comes the next capability: giving your chatbot access to your own documents,
    which takes it from a generic assistant to one that’s knowledgeable about any
    area of human knowledge for which you can find a library of written text. This
    will allow you to have the chatbot answer questions or summarize documents you
    wrote, for instance.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然后是下一个能力：让你的聊天机器人访问你自己的文档，这使它从通用助手转变为对任何你能够找到文本库的人类知识领域的知识有所了解的助手。这将允许你让聊天机器人回答问题或总结你写的文档，例如。
- en: 'After that, we’ll make the chatbot remember your previous conversations. This
    will improve it in two ways: It will feel a lot more natural to have a conversation
    with a chatbot that remembers what you have previously chatted about, and over
    time the chatbot can be personalized to the preferences of each of its users individually.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将让聊天机器人记住你之前的对话。这将从两个方面提升它：与记得你之前聊过什么的聊天机器人交谈会感觉更加自然，而且随着时间的推移，聊天机器人可以根据每个用户的偏好进行个性化定制。
- en: Next, we’ll use chain-of-thought and tool-calling techniques to give the chatbot
    the ability to plan and act on those plans, iteratively. This will enable it to
    work toward more complicated requests, such as writing a research report about
    a subject of your choice.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用思维链和工具调用技术，让聊天机器人能够规划和执行这些计划，迭代地进行。这将使它能够朝着更复杂的请求工作，例如撰写关于你选择的主题的研究报告。
- en: As you use your chatbot for more complicated tasks, you’ll feel the need to
    give it the tools to collaborate with you. This encompasses both giving you the
    ability to interrupt or authorize actions before they are taken, as well as providing
    the chatbot with the ability to ask for more information or clarification before
    acting.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用你的聊天机器人进行更复杂的任务时，你会感觉到需要给它提供工具来与你协作。这包括在你采取行动之前打断或授权行动的能力，以及提供聊天机器人请求更多信息或澄清的能力。
- en: Finally, we’ll show you how to deploy your chatbot to production and discuss
    what you need to consider before and after taking that step, including latency,
    reliability, and security. Then we’ll show you how to monitor your chatbot in
    production and continue to improve it as it is used.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将向您展示如何将您的聊天机器人部署到生产环境中，并讨论在采取这一步骤之前和之后需要考虑的因素，包括延迟、可靠性和安全性。然后我们将向您展示如何监控生产环境中的聊天机器人，并在其使用过程中持续改进它。
- en: Along the way, we’ll teach you the ins and outs of each of these techniques,
    so that when you finish the book, you will have truly added a new tool (or two)
    to your software engineering toolbelt.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中，我们将向您传授这些技术的方方面面，以便您在完成本书后，真正地为您的软件工程工具箱增添了一个（或两个）新工具。
- en: Conventions Used in This Book
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用了以下排版约定：
- en: '*Italic*'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`常宽字体`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: Tip
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: This element signifies a tip or suggestion.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般性说明。
- en: Using Code Examples
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://oreil.ly/supp-LearningLangChain*](https://oreil.ly/supp-LearningLangChain).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://oreil.ly/supp-LearningLangChain*](https://oreil.ly/supp-LearningLangChain)下载。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*support@oreilly.com*](mailto:support@oreilly.com).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在使用代码示例时遇到技术问题或问题，请发送电子邮件至[*support@oreilly.com*](mailto:support@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您完成工作。一般来说，如果本书提供了示例代码，您可以在您的程序和文档中使用它。除非您正在复制代码的较大部分，否则您无需联系我们获取许可。例如，编写一个使用本书中多个代码片段的程序不需要许可。通过引用本书并引用示例代码来回答问题不需要许可。将本书的大量示例代码纳入您产品的文档中则需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Learning LangChain*
    by Mayo Oshin and Nuno Campos (O’Reilly). Copyright 2025 Olumayowa “Mayo” Olufemi
    Oshin, 978-1-098-16728-8.”'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但通常不需要归属。归属通常包括标题、作者、出版社和ISBN。例如：“*《Learning LangChain》由Mayo Oshin和Nuno
    Campos（O’Reilly）编写。版权所有2025 Olumayowa “Mayo” Olufemi Oshin，978-1-098-16728-8。””
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您对代码示例的使用超出了合理使用或上述许可的范围，请随时联系我们[*permissions@oreilly.com*](mailto:permissions@oreilly.com)。
- en: O’Reilly Online Learning
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: Note
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去40多年里，[*O’Reilly Media*](https://oreilly.com) 为公司提供技术培训和业务培训、知识和洞察力，以帮助公司成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和200多家其他出版商的大量文本和视频。更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969（美国或加拿大）
- en: 707-827-7019 (international or local)
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/learning-langchain*](https://oreil.ly/learning-langchain).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，其中列出了勘误表、示例以及任何其他附加信息。您可以通过[*https://oreil.ly/learning-langchain*](https://oreil.ly/learning-langchain)访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 了解我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media).'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。
- en: Acknowledgments
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to express our gratitude and appreciation to the reviewers—Rajat
    Kant Goel, Douglas Bailley, Tom Taulli, Gourav Bais, and Jacob Lee—for providing
    valuable technical feedback on improving this book.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想对审稿人——Rajat Kant Goel、Douglas Bailley、Tom Taulli、Gourav Bais和Jacob Lee——表示感谢，他们为改进本书提供了宝贵的技术反馈。
- en: ^([1](preface01.html#id259-marker)) Tom B. Brown et al., [“Language Models Are
    Few-Shot Learners”](https://oreil.ly/1qoM6), arXiv, July 22, 2020.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](preface01.html#id259-marker)) Tom B. Brown等人，[“Language Models Are Few-Shot
    Learners”](https://oreil.ly/1qoM6)，arXiv，2020年7月22日。
- en: '^([2](preface01.html#id267-marker)) Xiang Zhang et al., [“Don’t Trust ChatGPT
    When Your Question Is Not in English: A Study of Multilingual Abilities and Types
    of LLMs”](https://oreil.ly/u5Cy1), Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, December 6–10, 2023\.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](preface01.html#id267-marker)) 张翔等人，[“当你的问题不是英文时不要相信ChatGPT：关于多语言能力和LLMs类型的研究”](https://oreil.ly/u5Cy1)，2023年自然语言处理实证方法会议论文集，2023年12月6日至10日。
- en: ^([3](preface01.html#id271-marker)) For more information, see Ashish Vaswani
    et al., [“Attention Is All You Need "](https://oreil.ly/Frtul), arXiv, June 12,
    2017.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](preface01.html#id271-marker)) 更多信息，请参阅Ashish Vaswani等人，[“Attention Is
    All You Need"](https://oreil.ly/Frtul)，arXiv，2017年6月12日。
- en: '^([4](preface01.html#id304-marker)) Ryan Liu et al. [“Mind Your Step (by Step):
    Chain-of-Thought Can Reduce Performance on Tasks Where Thinking Makes Humans Worse”](https://oreil.ly/UHFp9),
    arXiv, November 8, 2024\.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](preface01.html#id304-marker)) Ryan Liu等人，[“Mind Your Step (by Step):
    Chain-of-Thought Can Reduce Performance on Tasks Where Thinking Makes Humans Worse”](https://oreil.ly/UHFp9)，arXiv，2024年11月8日。'
