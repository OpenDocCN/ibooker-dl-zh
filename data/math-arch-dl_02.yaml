- en: 3 Classifiers and vector calculus
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 分类器和向量微积分
- en: 'We took a first look at the core concept of machine learning in section [1.3](../Text/01.xhtml#sec-cat_brain).
    Then, in section [2.8.2](02.xhtml#subsec-hyper-planes-classifiers), we examined
    classifiers as a special case. But so far, we have skipped the topic of error
    minimization: given one or more training examples, how do we adjust the weights
    and biases to make the machine closer to the desired ideal? We will study this
    topic in this chapter by discussing the concept of gradients.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[1.3](../Text/01.xhtml#sec-cat_brain)节中首次了解了机器学习的核心概念。然后，在[2.8.2](02.xhtml#subsec-hyper-planes-classifiers)节中，我们考察了分类器作为一个特殊情况。但到目前为止，我们还没有涉及误差最小化的主题：给定一个或多个训练示例，我们如何调整权重和偏差，使机器更接近理想的期望？我们将通过讨论梯度的概念来研究这个主题。
- en: NOTE The complete PyTorch code for this chapter is available at [http://mng.bz](http://mng.bz/4Zya)
    [/4Zya](http://mng.bz/4Zya) in the form of fully functional and executable Jupyter
    notebooks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的完整PyTorch代码以完全功能性和可执行的Jupyter笔记本的形式，可在[http://mng.bz](http://mng.bz/4Zya)
    [/4Zya](http://mng.bz/4Zya)找到。
- en: 3.1 Geometrical view of image classification
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 图像分类的几何视图
- en: To fix our ideas, consider a machine that classifies whether an image contains
    a car or a giraffe. Such classifiers, with only two classes, are known as *binary
    classifiers*. The first question is how to represent the input.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了固定我们的想法，考虑一个机器，它能够分类图像是否包含汽车或长颈鹿。这样的分类器，只有两个类别，被称为*二元分类器*。第一个问题是如何表示输入。
- en: 3.1.1 Input representation
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 输入表示
- en: 'The car-versus-giraffe scenario belongs to a special class of problems where
    we are analyzing a visual scene. Here, the inputs are the brightness levels of
    various points in the 3D scene projected onto a 2D image plane. Each element of
    the image represents a point in the actual scene and is referred to as a *pixel*.
    The image is a two-dimensional array representing the collection of pixel values
    at a given instant in time. It is usually scaled to a fixed size, say 224 × 224.
    As such, the image can be viewed as a matrix:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车与长颈鹿的场景属于我们分析视觉场景的特殊问题类别。在这里，输入是3D场景中各个点的亮度级别，这些点被投影到二维图像平面上。图像的每个元素代表实际场景中的一个点，被称为*像素*。图像是表示在给定时间点像素值的二维数组。它通常被缩放到固定大小，例如224
    × 224。因此，图像可以被视为一个矩阵：
- en: '![](../../OEBPS/Images/eq_03-00-a.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-00-a.png)'
- en: Each element of the matrix, *X[i, j]*, is a pixel color value in the range [0,255].
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的每个元素，*X[i, j]*，是像素颜色值，范围在[0,255]之间。
- en: Image rasterization
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图像光栅化
- en: In the previous chapters, we have always seen a *vector* as the input to a machine
    learning system. The vector representation of the input allowed us to view it
    as a point in a high-dimensional space. This led to many geometric insights about
    classification. But here, our input is an image, which is akin to a *matrix* rather
    than a vector. Can we represent an image (matrix) as a vector?
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们总是将*向量*视为机器学习系统的输入。输入的向量表示使我们能够将其视为高维空间中的一个点。这导致了关于分类的许多几何洞察。但在这里，我们的输入是一个图像，它更像是*矩阵*而不是向量。我们能否将图像（矩阵）表示为向量？
- en: The answer is yes. A matrix can always be converted into a vector by a process
    called *rasterization*. During rasterization, we iterate over the elements of
    the matrix from left to right and top to bottom, storing successive encountered
    elements into a vector. The result is the rasterized vector. It has the same elements
    as the original matrix, but they are organized differently. The length of the
    rasterized vector is equal to the product of the row count and column count of
    the matrix. The rasterized vector for the earlier matrix *X* has 224 × 224 = 50176
    elements
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是肯定的。矩阵总可以通过称为*光栅化*的过程转换为向量。在光栅化过程中，我们从左到右和从上到下迭代矩阵的元素，将连续遇到的元素存储到一个向量中。结果是光栅化的向量。它具有与原始矩阵相同的元素，但它们的组织方式不同。光栅化向量的长度等于矩阵的行数和列数的乘积。早期矩阵*X*的光栅化向量有224
    × 224 = 50176个元素
- en: '![](../../OEBPS/Images/eq_03-00-b.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-00-b.png)'
- en: where *x[i]* ∈ [0,255] are values of the image pixels. Thus, a 224 × 224 input
    image can be viewed as a vector (equivalently, a point) in a 50, 176-dimensional
    space.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x[i]* ∈ [0,255] 是图像像素的值。因此，一个224 × 224的输入图像可以被视为一个50,176维空间中的向量（等价地，一个点）。
- en: 3.1.2 Classifiers as decision boundaries
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 分类器作为决策边界
- en: We see that input images can be converted to vectors via rasterization. Each
    vector can be viewed as a point in a high-dimensional space. But the points corresponding
    to any given object or class, say *giraffe* or *car*, are not distributed randomly
    all over the space. Rather, they occupy a small portion subspace) in the vast
    high-dimensional space of inputs. This is because there is always inherent commonality
    in members of a class. For instance, all giraffes are predominantly yellow with
    a bit of black, and cars have a somewhat fixed shape. This causes the pixel values
    in images containing the same object to have somewhat similar values. Overall,
    this means points belonging to a class loosely form a *cluster*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，输入图像可以通过光栅化转换为向量。每个向量可以看作是高维空间中的一个点。但是，对应于任何给定对象或类别的点，比如 *长颈鹿* 或 *汽车*，并不是在整个空间中随机分布的。相反，它们占据输入高维空间中一个小的子空间。这是因为类别的成员之间总是存在固有的共性。例如，所有的长颈鹿主要是黄色，带有一点黑色，而汽车有相对固定的形状。这导致包含相同对象的图像中的像素值有某种相似性。总的来说，这意味着属于一个类别的点松散地形成一个
    *簇*。
- en: NOTE Geometrically speaking, a classifier is a hypersurface that separates the
    point clusters for the classes we want to recognize. This surface forms a *decision
    boundary*—the decision about which class a specific input point belongs to is
    made by looking at which side of the surface the point belongs to.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：从几何学的角度来看，分类器是一个超曲面，它将我们想要识别的类别的点簇分开。这个表面形成了一个 *决策边界*——关于特定输入点属于哪个类别的决策是通过查看点属于表面的哪一侧来做出的。
- en: '![](../../OEBPS/Images/CH03_F01a_Chaudhury.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F01a_Chaudhury.png)'
- en: (a) Car vs. giraffe classifier
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 汽车与长颈鹿分类器
- en: '![](../../OEBPS/Images/CH03_F01b_Chaudhury.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F01b_Chaudhury.png)'
- en: (b) Horse vs. zebra classifier
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 马与斑马分类器
- en: 'Figure 3.1 Geometric depiction of a classification problem. In the multidimensional
    input space, each data instance corresponds to a point. In figure [3.1a](#fig-classifier_diagram),
    the points marked *c* denote cars, and points marked *g* denote giraffes. This
    is a simple case: the points form reasonably distinct clusters, so the classification
    can be done with a relatively simple surface, a hyperplane. The exact parameters
    of the hyperplane—orientation and position—are determined via training. In figure
    [3.1b](#fig-classifier_diagram), the points marked *h* denote horses, and those
    marked *z* denote zebras. This case is a bit more difficult: the classification
    has to be done with a curved (nonplanar) surface, a hypersphere. The parameters
    of the hypersphere—radius and center—are determined via training.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 分类问题的几何描述。在多维输入空间中，每个数据实例对应一个点。在图 [3.1a](#fig-classifier_diagram) 中，标记为
    *c* 的点表示汽车，标记为 *g* 的点表示长颈鹿。这是一个简单的情况：点形成合理的不同簇，因此可以使用相对简单的表面，即超平面进行分类。超平面的精确参数——方向和位置——是通过训练确定的。在图
    [3.1b](#fig-classifier_diagram) 中，标记为 *h* 的点表示马，标记为 *z* 的点表示斑马。这个情况稍微复杂一些：分类需要使用曲线（非平面）的表面，即超球面。超球面的参数——半径和中心——是通过训练确定的。
- en: Figure [3.1a](#fig-classifier_diagram) shows an example of a rasterized space
    for the giraffe and car classification problem. The points corresponding to a
    giraffe are marked *g*, and those corresponding to a car are marked *c*. This
    is a simple case. Here, the classifier surface (aka decision boundary) that separates
    the cluster of points corresponding to *car* from those corresponding to *giraffe*
    is a hyperplane, depicted in figure [3.1a](#fig-classifier_diagram).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3.1a](#fig-classifier_diagram) 展示了长颈鹿和汽车分类问题的光栅化空间示例。对应于长颈鹿的点被标记为 *g*，对应于汽车的点被标记为
    *c*。这是一个简单的情况。在这里，将对应于 *汽车* 的点簇与对应于 *长颈鹿* 的点簇分开的分类表面（也称为决策边界）是一个超平面，如图 [3.1a](#fig-classifier_diagram)
    所示。
- en: NOTE We often call surfaces *hypersurfaces* and planes *hyperplanes* in greater
    than three dimensions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：在超过三维的情况下，我们通常将表面称为 *超曲面*，将平面称为 *超平面*。
- en: 'Figure [3.1b](#fig-classifier_diagram) shows a more difficult example: horse
    and zebra classification in images. Here the points corresponding to horses are
    marked *h* and those corresponding to zebras are marked *z*. In this example,
    we need a nonlinear (curved) surface (such as a hypersphere) to separate the two
    classes.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3.1b](#fig-classifier_diagram) 展示了一个更复杂的例子：图像中的马和斑马分类。在这里，对应于马的点被标记为 *h*，而对应于斑马的点被标记为
    *z*。在这个例子中，我们需要一个非线性（曲线）的表面（例如超球面）来分离这两个类别。
- en: 3.1.3 Modeling in a nutshell
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 简要建模
- en: Unfortunately, in the typical scenario, we do not know the separating surface.
    In fact, we do not even know all the points belonging to a class of interest.
    All we know is a *sampled* set of inputs ![](../../OEBPS/Images/AR_x.png)*[i]*
    (training inputs) and corresponding classes *![](../../OEBPS/Images/AR_y2.png)[i]*
    (the ground truth). The complete set of training inputs plus ground truth—{*![](../../OEBPS/Images/AR_x.png)[i],
    ![](../../OEBPS/Images/AR_y2.png)[i]*} for a large set of *i* values—is called
    the *training data*. When we want to teach a baby to recognize a car, we show
    the baby several example cars and say “This is a car.” The training data plays
    the same role for a neural network.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，在典型场景中，我们并不知道分离表面。实际上，我们甚至不知道属于一个感兴趣类别的所有点。我们所知道的就是一组*采样*的输入集 ![图片](../../OEBPS/Images/AR_x.png)*[i]*（训练输入）以及相应的类别
    ![图片](../../OEBPS/Images/AR_y2.png)*[i]*（真实标签）。包含所有训练输入和真实标签的完整集合——{*![图片](../../OEBPS/Images/AR_x.png)[i],
    ![图片](../../OEBPS/Images/AR_y2.png)[i]*} 对于一个大的*i*值集合——被称为*训练数据*。当我们想要教一个婴儿识别汽车时，我们会给婴儿展示几辆汽车样品，并说“这是一辆汽车。”对于神经网络来说，训练数据扮演着同样的角色。
- en: From only this training dataset {*![](../../OEBPS/Images/AR_x.png)[i], ![](../../OEBPS/Images/AR_y2.png)[i]*}
    ∀*[i] ∈* [1, *n*], we have to identify a good enough approximation of the general
    classifying surface that when presented with a random scene, we can map it to
    an input point ![](../../OEBPS/Images/AR_x.png), check which side of the surface
    that point lies on, and identify the class (car or giraffe). This process of developing
    a best guess for a surface that forms a decision boundary between various classes
    of interest is called *modeling the classifier*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从仅有的这个训练数据集{*![图片](../../OEBPS/Images/AR_x.png)[i], ![图片](../../OEBPS/Images/AR_y2.png)[i]*}
    ∀*[i] ∈* [1, *n*]，我们必须找到一个足够好的分类表面的近似，当呈现一个随机场景时，我们可以将其映射到一个输入点 ![图片](../../OEBPS/Images/AR_x.png)，检查该点位于表面的哪一侧，并识别类别（汽车或长颈鹿）。这个过程是开发一个最佳猜测，以形成一个决策边界，该边界区分各种感兴趣类别，被称为*建模分类器*。
- en: NOTE The ground truth labels (*![](../../OEBPS/Images/AR_y2.png)[i]*) for the
    training images ![](../../OEBPS/Images/AR_x.png)*[i]* are often created manually.
    This process of manually generating labels for the training images is one of the
    most painful aspects of machine learning, and significant research effort is going
    on at the moment to mitigate it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：训练图像 ![图片](../../OEBPS/Images/AR_x.png)*[i]* 的真实标签 (*![图片](../../OEBPS/Images/AR_y2.png)[i]*)
    通常是通过人工创建的。为训练图像手动生成标签的过程是机器学习中最痛苦的部分之一，目前正在进行大量研究工作以减轻这一问题。
- en: 'As indicated in section [1.3](../Text/01.xhtml#sec-cat_brain), modeling has
    two steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如[1.3](../Text/01.xhtml#sec-cat_brain)节所述，建模有两个步骤：
- en: '*Model architecture selection*: Choose the parametric model function *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*). This function takes an input vector ![](../../OEBPS/Images/AR_x.png) and
    emits the class *y*. It has a set of parameters ![](../../OEBPS/Images/AR_w.png),
    *b*, which are unknown at first. This function is typically chosen from a bank
    of well-known functions that are tried and tested; for simple problems, we may
    choose a linear model, and for more complex problems, we choose nonlinear models.
    The model designer makes the choice based on their understanding of the problem.
    Remember, at this point the parameters are still unknown—we have only decided
    on the *function family* for the model.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型架构选择*：选择参数化模型函数 *ϕ*(![图片](../../OEBPS/Images/AR_x.png); ![图片](../../OEBPS/Images/AR_w.png),
    *b*)。这个函数接受一个输入向量 ![图片](../../OEBPS/Images/AR_x.png) 并输出类别 *y*。它有一组参数 ![图片](../../OEBPS/Images/AR_w.png),
    *b*，最初是未知的。这个函数通常从一系列经过验证和测试的已知函数中选择；对于简单问题，我们可能选择线性模型，而对于更复杂的问题，我们选择非线性模型。模型设计者根据他们对问题的理解做出选择。记住，在这个阶段，参数仍然是未知的——我们只是决定了模型的*函数族*。'
- en: '*Model training*: Estimate the parameters ![](../../OEBPS/Images/AR_w.png),
    *b* such that *ϕ* emits the known correct output (as closely as possible) on the
    training data inputs. This is typically done via an iterative process. For each
    training data instance ![](../../OEBPS/Images/AR_x.png)[i], we evaluate *y**[i]*
    = *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i ]*;![](../../OEBPS/Images/AR_w.png),
    *b*). This emitted output is compared with the corresponding known outputs *ȳ[i]*.
    Their difference, *e[i]* = ||*y**[i]* − *ȳ[i]*||, is called the *training error*.
    The sum of training errors over all training data is the aggregate training error.
    We iteratively adjust the parameters ![](../../OEBPS/Images/AR_w.png), *b* such
    that the aggregate training error keeps going down. This means at each iteration,
    we adjust the parameters so the model output *y**[i]* moves a little closer to
    the target output *ȳ[i]* for all *i*. Exactly how to adjust the parameters to
    reduce the error forms the bulk of this chapter and will be introduced in section
    [3.3](../Text/03.xhtml#sec-grad).'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*模型训练*：估计参数 ![](../../OEBPS/Images/AR_w.png), *b*，使得 *ϕ* 在训练数据输入上产生已知的正确输出（尽可能接近）。这通常通过迭代过程来完成。对于每个训练数据实例
    ![](../../OEBPS/Images/AR_x.png)[i]，我们评估 *y**[i]* = *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i ]*;![](../../OEBPS/Images/AR_w.png),
    *b*). 这个产生的输出与相应的已知输出 *ȳ[i]* 进行比较。它们的差值，*e[i]* = ||*y**[i]* − *ȳ[i]*||，被称为 *训练误差*。所有训练数据上的训练误差之和是总训练误差。我们迭代调整参数
    ![](../../OEBPS/Images/AR_w.png), *b*，使得总训练误差持续下降。这意味着在每次迭代中，我们调整参数，使得模型输出 *y**[i]*
    对于所有 *i* 都稍微接近目标输出 *ȳ[i]*。如何调整参数以减少误差构成了本章的主要内容，将在第 [3.3](../Text/03.xhtml#sec-grad)
    节中介绍。'
- en: The function *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*) represents the decision boundary hypersurface. For example, in the binary
    classification problem depicted in figure [3.1](#fig-classifier_diagram), *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*) may represent a plane (shown by the dashed line). Points on one side of the
    plane are classified as cars, while points on the other side are classified as
    giraffes. Here,
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png), *b*)
    代表决策边界超曲面。例如，在图 [3.1](#fig-classifier_diagram) 所示的二值分类问题中，*ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*) 可能代表一个平面（由虚线表示）。平面一侧的点被分类为汽车，而另一侧的点被分类为长颈鹿。在这里，
- en: '*ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png), *b*)
    = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png), *b*)
    = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) + *b*'
- en: From equation [2.14](02.xhtml#eq-plane-1) we know this equation represents a
    plane.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从方程 [2.14](02.xhtml#eq-plane-1) 我们知道这个方程代表一个平面。
- en: In figure [3.1b](#fig-classifier_diagram), a good planar separation does not
    exist—we need a nonlinear separator, such as the spherical separator shown with
    dashed lines. Here,
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3.1b](#fig-classifier_diagram) 中，不存在良好的平面分离——我们需要一个非线性分离器，如用虚线表示的球形分离器。在这里，
- en: '![](../../OEBPS/Images/eq_03-00-c.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-00-c.png)'
- en: This equation represents a sphere.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程代表一个球面。
- en: It should be noted that in typical real-life cases, the separating surface does
    not correspond to any known geometric surface (see figure [3.2](#fig-real_life_classifier_diagram)).
    But in this chapter, we will continue to use simple examples to bring out the
    underlying concepts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 应该注意的是，在典型的现实生活案例中，分离表面不对应于任何已知的几何表面（参见图 [3.2](#fig-real_life_classifier_diagram)）。但在本章中，我们将继续使用简单的例子来阐述基本概念。
- en: '![](../../OEBPS/Images/CH03_F02_Chaudhury.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F02_Chaudhury.png)'
- en: Figure 3.2 In real-life problems, the surface is often not a well-known surface
    like a plane or sphere. And often, the classification is not perfect—some points
    fall on the wrong side of the separator.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 在现实生活中的问题中，表面通常不是像平面或球面这样的已知表面。而且，分类通常并不完美——一些点落在分离器的错误一侧。
- en: 3.1.4 Sign of the surface function in binary classification
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 二元分类中超曲面函数的符号
- en: In the special case of binary classifiers, the *sign* of the expression *ϕ*(![](../../OEBPS/Images/AR_x.png);![](../../OEBPS/Images/AR_w.png),
    *b*) representing the decision boundary has a special significance. To see this,
    consider a line in a 2D plane corresponding to the equation
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类器的特殊情况中，表示决策边界的表达式 *ϕ*(![](../../OEBPS/Images/AR_x.png);![](../../OEBPS/Images/AR_w.png),
    *b*) 的 *符号* 具有特殊的意义。为了理解这一点，考虑一个对应于以下方程的二维平面上的直线
- en: '*y* + 2*x* + 1 = 0'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* + 2*x* + 1 = 0'
- en: All points *on* the line have *x*, *y* coordinate values satisfying this equation.
    The line divides the 2D plane into two half planes. All points on one half plane
    have *x*, *y* values such that *y* + 2*x* + 1 is negative. All points in the other
    half plane have *x*, *y* values such that *y* + 2*x* + 1 is positive. This is
    shown in figure [3.3](#fig-line_sign). This idea can be extended to other surfaces
    and higher dimensions. Thus, in binary classification, once we have estimated
    an optimal decision surface *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*), given any input vector ![](../../OEBPS/Images/AR_x.png), we can compute
    the sign of *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*) to predict the class.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 线上的所有点都满足这个方程的 *x*, *y* 坐标值。这条线将二维平面分为两个半平面。一个半平面上的所有点的 *x*, *y* 值使得 *y* + 2*x*
    + 1 为负。另一个半平面上的所有点的 *x*, *y* 值使得 *y* + 2*x* + 1 为正。这如图[3.3](#fig-line_sign)所示。这个想法可以扩展到其他表面和更高维度。因此，在二元分类中，一旦我们估计了一个最优决策表面
    *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png), *b*)，对于任何输入向量
    ![](../../OEBPS/Images/AR_x.png)，我们可以计算 *ϕ*(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_w.png),
    *b*) 的符号来预测类别。
- en: '![](../../OEBPS/Images/CH03_F03_Chaudhury.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![训练数据图](../../OEBPS/Images/CH03_F03_Chaudhury.jpg)'
- en: Figure 3.3 Given a point (*x*[0], *y*[0]) and a separator *y* + 2*x* + 1 = 0,
    we can tell which side of the separator the point lies on from the sign of *y*[0]
    + 2*x*[0] + 1.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 给定点 (*x*[0], *y*[0]) 和分隔线 *y* + 2*x* + 1 = 0，我们可以根据 *y*[0] + 2*x*[0] +
    1 的符号来判断点位于分隔线的哪一侧。
- en: 3.2 Error, aka loss function
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 错误，即损失函数
- en: As stated earlier, during training, we adjust the parameters ![](../../OEBPS/Images/AR_w.png),
    *b* so that the error keeps going down. Let’s derive a quantitative expression
    for this error aka loss function). Later, we will see how to minimize it.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在训练过程中，我们调整参数 ![](../../OEBPS/Images/AR_w.png), *b*，以便误差持续下降。让我们推导出这个误差，即损失函数的定量表达式。稍后，我们将看到如何最小化它。
- en: 'Overall, training data consists of a set of labeled inputs (training data instances
    paired with known ground truths):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，训练数据由一组标记的输入（训练数据实例与已知的真实值配对）组成：
- en: '![](../../OEBPS/Images/eq_03-00-d.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![分隔线方程图](../../OEBPS/Images/eq_03-00-d.png)'
- en: 'Now we define a *loss function*. On a specific training data instance, the
    loss function effectively measures the error made by the machine on that particular
    training data—input-target pair (![](../../OEBPS/Images/AR_x.png)^((*i*)), *y*^((*i*))).
    Although there are many sophisticated error functions more suitable for this problem,
    for now, let’s use a squared error function for the sake of simplicity (introduced
    in section [2.5.4](02.xhtml#subsection-vector_length)). The squared error on the
    *i*th training data element is the squared difference between the output yielded
    by the model and the expected or target output:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一个*损失函数*。在特定的训练数据实例上，损失函数实际上衡量了机器在该特定训练数据——输入-目标对 (![](../../OEBPS/Images/AR_x.png)^((*i*)),
    *y*^((*i*))) 上犯的错误。尽管有许多更适合此问题的复杂错误函数，但为了简单起见，我们现在使用平方误差函数（在[2.5.4](02.xhtml#subsection-vector_length)节中介绍过）。第
    *i* 个训练数据元素的平方误差是模型输出的平方与期望或目标输出的平方差：
- en: '![](../../OEBPS/Images/eq_03-01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![平方误差方程图](../../OEBPS/Images/eq_03-01.png)'
- en: Equation 3.1
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 方程3.1
- en: The total loss (aka squared error) during training is
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程中的总损失（即平方误差）为
- en: '![](../../OEBPS/Images/eq_03-02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![方程图](../../OEBPS/Images/eq_03-02.png)'
- en: Equation 3.2
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 方程3.2
- en: Note that this total error is not a function of any specific training data instance.
    Rather, it is the *overall error over the entire training data set*. This is what
    we minimize by adjusting ![](../../OEBPS/Images/AR_w.png) and *b*. To be precise,
    we estimate the ![](../../OEBPS/Images/AR_w.png) and *b* that will minimize *L*(![](../../OEBPS/Images/AR_w.png),
    *b*).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个总误差不是任何特定训练数据实例的函数。相反，它是整个训练数据集的*总体误差*。这是我们通过调整 ![](../../OEBPS/Images/AR_w.png)
    和 *b* 来最小化的。更准确地说，我们估计 ![](../../OEBPS/Images/AR_w.png) 和 *b*，以最小化 *L*(![](../../OEBPS/Images/AR_w.png),
    *b*)。
- en: '3.3 Minimizing loss functions: Gradient vectors'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 最小化损失函数：梯度向量
- en: 'The goal of training is to estimate the weights and bias parameter ![](../../OEBPS/Images/AR_w.png),
    *b* that will minimize *L*. This is usually done by an iterative process. We start
    with random values of ![](../../OEBPS/Images/AR_w.png), *b* and adjust these values
    so that the loss *L*(![](../../OEBPS/Images/AR_w.png), *b*) = *E*²(![](../../OEBPS/Images/AR_w.png),
    *b*) declines rapidly. Doing this many times is likely to take us close to the
    optimal values for ![](../../OEBPS/Images/AR_w.png), *b*. This is the essential
    idea behind the process of training a model. It is important to note that we are
    minimizing the total error: this prevents us from over-indexing on any particular
    training instance. If the training data is a well-sampled set, the parameters
    ![](../../OEBPS/Images/AR_w.png), *b* that minimize loss over the training dataset
    will also work well during inferencing.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的目标是估计权重和偏差参数 ![](../../OEBPS/Images/AR_w.png), *b*，这些参数将最小化 *L*。这通常通过迭代过程来完成。我们以
    ![](../../OEBPS/Images/AR_w.png), *b* 的随机值开始，并调整这些值，使得损失 *L*(![](../../OEBPS/Images/AR_w.png),
    *b*) = *E*²(![](../../OEBPS/Images/AR_w.png), *b*) 迅速下降。这样做多次可能使我们接近 ![](../../OEBPS/Images/AR_w.png),
    *b* 的最优值。这是训练模型过程背后的基本思想。重要的是要注意，我们是在最小化总误差：这防止我们对任何特定的训练实例过度依赖。如果训练数据是一个良好采样的集合，那么在训练数据集上最小化损失的参数
    ![](../../OEBPS/Images/AR_w.png), *b* 也会在推理过程中表现良好。
- en: How do we “adjust” ![](../../OEBPS/Images/AR_w.png), *b* so that the value of
    loss *L* = *E*² declines? This is where gradients come in. For any function *L*(![](../../OEBPS/Images/AR_w.png),
    *b*), the gradient with respect to ![](../../OEBPS/Images/AR_w.png), *b*—that
    is, ∇[![](../../OEBPS/Images/AR_w.png), *b*]*L*(![](../../OEBPS/Images/AR_w.png),
    *b*)—indicates the direction along which the maximum change in *L* occurs. The
    gradient is the analog of a derivative in 1D calculus. Intuitively, going down
    along the direction of the gradient of a function seems like the best strategy
    for minimizing the function value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何“调整” ![](../../OEBPS/Images/AR_w.png), *b* 以使损失 *L* = *E*² 的值下降？这正是梯度发挥作用的地方。对于任何函数
    *L*(![](../../OEBPS/Images/AR_w.png), *b*)，相对于 ![](../../OEBPS/Images/AR_w.png),
    *b* 的梯度——即 ∇[![](../../OEBPS/Images/AR_w.png), *b*]*L*(![](../../OEBPS/Images/AR_w.png),
    *b*)——指示了 *L* 发生最大变化的方向。梯度是一维微积分中导数的类似物。直观上，沿着函数梯度的方向下降似乎是使函数值最小化的最佳策略。
- en: Geometrically speaking, if we start at an arbitrary point on the surface corresponding
    to *L*(![](../../OEBPS/Images/AR_w.png), *b*) and move along the direction of
    the gradient ∇[![](../../OEBPS/Images/AR_w.png), *b*]*L*(![](../../OEBPS/Images/AR_w.png),
    *b*), we will go toward the minimum at the fastest rate (this is discussed in
    detail throughout the rest of this section). Hence, during training, we iteratively
    move toward the minimum by taking steps along ∇[![](../../OEBPS/Images/AR_w.png),
    *b*]*L*(![](../../OEBPS/Images/AR_w.png), *b*). Note that *the gradient is with
    respect to weights, not the input*. The overall algorithm is shown in algorithm
    [3.2](../Text/03.xhtml#alg-supervised_training_detailed).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何角度来说，如果我们从对应于 *L*(![](../../OEBPS/Images/AR_w.png), *b*) 的任意表面点开始，沿着梯度 ∇[![](../../OEBPS/Images/AR_w.png),
    *b*]*L*(![](../../OEBPS/Images/AR_w.png), *b*) 的方向移动，我们将以最快的速度走向最小值（这一点将在本节的其余部分详细讨论）。因此，在训练过程中，我们通过沿着
    ∇[![](../../OEBPS/Images/AR_w.png), *b*]*L*(![](../../OEBPS/Images/AR_w.png),
    *b*) 移动步子来迭代地走向最小值。请注意，*梯度是相对于权重，而不是输入的*。整体算法在算法 [3.2](../Text/03.xhtml#alg-supervised_training_detailed)
    中展示。
- en: Algorithm 3.2 Training a supervised model (overall idea)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3.2 训练监督模型（总体思路）
- en: Initialize ![](../../OEBPS/Images/AR_w.png), *b* with random values
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用随机值初始化 ![](../../OEBPS/Images/AR_w.png), *b*。
- en: '**while** *L*(![](../../OEBPS/Images/AR_w.png), *b*) > *threshold* **do**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**while** *L*(![](../../OEBPS/Images/AR_w.png), *b*) > *threshold* **do**'
- en: '![](../../OEBPS/Images/eq_03-02-a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-02-a.png)'
- en: Recompute *L* on new ![](../../OEBPS/Images/AR_w.png), *b*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的 ![](../../OEBPS/Images/AR_w.png), *b* 上重新计算 *L*。
- en: '**end** **while**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**end** **while**'
- en: '![](../../OEBPS/Images/AR_w.png)[*] ← ![](../../OEBPS/Images/AR_w.png), *b*[*]
    ← *b*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/AR_w.png)[*] ← ![](../../OEBPS/Images/AR_w.png), *b*[*]
    ← *b*'
- en: 'Note the following points:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下要点：
- en: In each iteration, we are adjusting ![](../../OEBPS/Images/AR_w.png), *b* along
    the gradient of the error function. We will see in section [3.3](../Text/03.xhtml#sec-grad)
    that this is the direction of maximum change for *L*. Thus, *L* is reduced at
    a maximal rate.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中，我们沿着误差函数的梯度调整 ![](../../OEBPS/Images/AR_w.png), *b*。我们将在第 [3.3](../Text/03.xhtml#sec-grad)
    节中看到，这是 *L* 的最大变化方向。因此，*L* 以最大速率减少。
- en: '*μ* is the learning rate: larger values imply longer steps, and smaller values
    imply shorter steps. The simplest approach, outlined in algorithm [3.2](../Text/03.xhtml#alg-supervised_training_detailed),
    takes equal-sized steps everywhere. In later chapters, we will study more sophisticated
    approaches where we try to sense how close to the minimum we are and vary the
    step size accordingly:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*μ* 是学习率：较大的值意味着更长的步长，而较小的值意味着更短的步长。在算法 [3.2](../Text/03.xhtml#alg-supervised_training_detailed)
    中概述的最简单方法，在所有地方都采取等大小的步长。在后面的章节中，我们将研究更复杂的方法，其中我们试图感知我们距离最小值有多近，并相应地调整步长：'
- en: We take longer steps when far from the minimum, to progress quickly.
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当远离最小值时，我们采取更长的步长，以快速进步。
- en: We take shorter steps when near the minimum, to avoid overshooting it.
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当接近最小值时，我们采取更短的步长，以避免超过最小值。
- en: Mathematically, we should keep iterating until the loss becomes minimal (that
    is, the gradient of the loss is zero). But in practice, we simply iterate until
    the accuracy is good enough for the purpose at hand.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数学上讲，我们应该持续迭代，直到损失最小化（即损失的梯度为零）。但在实践中，我们只需迭代到准确度足够好，以满足当前目的。
- en: '3.3.1 Gradients: A machine learning-centric introduction'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 梯度：以机器学习为中心的介绍
- en: In machine learning, we model the output as a parametric function of the inputs.
    We define a loss function that quantifies the difference between the model output
    and the known ideal output on the set of training inputs. Then we try to obtain
    the parameter values that will minimize this loss. This effectively identifies
    the parameters that will result in the model function emitting outputs as close
    as possible to the ideal on the set of training inputs.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们将输出建模为输入的参数函数。我们定义一个损失函数，该函数量化了模型输出与训练输入集上已知理想输出之间的差异。然后我们尝试获得最小化这个损失的参数值。这实际上确定了那些将在训练输入集上使模型函数输出尽可能接近理想的参数。
- en: The loss function depends on ![](../../OEBPS/Images/AR_x.png) (the model inputs),
    *ȳ* (the known ideal outputs on the training data—aka ground truth), and ![](../../OEBPS/Images/AR_w.png)
    (the parameters). Here only the behavior of the loss function with respect to
    the parameters is of interest to us, so we are ignoring everything else and denoting
    the loss function as a function of the parameters as *L*(![](../../OEBPS/Images/AR_w.png)).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数取决于 ![](../../OEBPS/Images/AR_x.png)（模型输入），*ȳ*（训练数据上的已知理想输出，即真实值），以及 ![](../../OEBPS/Images/AR_w.png)（参数）。在这里，我们只对损失函数相对于参数的行为感兴趣，所以我们忽略了其他所有内容，并将损失函数表示为参数的函数，即
    *L*(![](../../OEBPS/Images/AR_w.png))。
- en: NOTE For the sake of brevity, here we use the symbol *w* to denote all parameters—weight
    as well as bias.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了简洁起见，在这里我们使用符号 *w* 来表示所有参数——权重以及偏差。
- en: 'The core question we are trying to answer is this: given a loss *L*(![](../../OEBPS/Images/AR_w.png))
    and current parameter values ![](../../OEBPS/Images/AR_w.png), what is the optimal
    change in the parameters ![](../../OEBPS/Images/AR_delta.png) that maximally reduces
    the loss? Equivalently, we want to determine ![](../../OEBPS/Images/AR_delta.png)
    that will make *δL* = *L*(![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_delta.png))
    - *L*(![](../../OEBPS/Images/AR_w.png)) as negative as possible. Toward that goal,
    we will study the relationship between the loss function *L*(*w*) and change in
    parameter values ![](../../OEBPS/Images/AR_delta.png) in several scenarios of
    increasing complexity.[¹](#fn9)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图回答的核心问题是这样的：给定一个损失 *L*(![](../../OEBPS/Images/AR_w.png)) 和当前的参数值 ![](../../OEBPS/Images/AR_w.png)，参数
    ![](../../OEBPS/Images/AR_delta.png) 的最佳变化是什么，以最大限度地减少损失？等价地，我们希望确定 ![](../../OEBPS/Images/AR_delta.png)，使得
    *δL* = *L*(![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_delta.png))
    - *L*(![](../../OEBPS/Images/AR_w.png)) 尽可能地负。为了达到这个目标，我们将研究损失函数 *L*(*w*) 和参数值变化
    ![](../../OEBPS/Images/AR_delta.png) 在越来越复杂的几种场景中的关系。[¹](#fn9)
- en: '![](../../OEBPS/Images/CH03_F04a_Chaudhury.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F04a_Chaudhury.png)'
- en: '(a) Line: *L*(*w*) = 2*w* + 1, *dL*/*dw* = *m*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 直线：*L*(*w*) = 2*w* + 1, *dL*/*dw* = *m*
- en: '![](../../OEBPS/Images/CH03_F04b_Chaudhury.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F04b_Chaudhury.png)'
- en: '(b) Parabola: *L*(*w*) = *w*², *dL*/*dw* = 2*w*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 抛物线：*L*(*w*) = *w*², *dL*/*dw* = 2*w*
- en: 'Figure 3.4 *δL* in terms of *δw* in one dimension, illustrated with two example
    curves: a straight line and a parabola. In general, *δL* = (*dL*/*dw)* *δw*. To
    decrease loss, *δw* must have the opposite sign of the derivative *dL*/*dw*. In
    (a), this implies we always have to move left (decrease *w*) to decrease *L*.
    In (b), if we are in the left half (e.g., point Q), the derivative is negative,
    and we have to move to the right to decrease *L*. But if we are in the right half,
    the derivative is positive, and we have to move to the left to decrease *L*. Geometrically,
    this is equivalent to following the tangent “downward.”'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 一维中 *δL* 与 *δw* 的关系，用两条示例曲线表示：一条直线和一个抛物线。一般来说，*δL* = (*dL*/*dw)* *δw*。为了减少损失，*δw*
    必须与导数 *dL*/*dw* 具有相反的符号。在 (a) 中，这意味着我们总是必须向左移动（减少 *w*）以减少 *L*。在 (b) 中，如果我们处于左侧（例如，点
    Q），导数是负的，我们必须向右移动以减少 *L*。但如果我们处于右侧，导数是正的，我们必须向左移动以减少 *L*。从几何上看，这相当于沿着切线“向下”移动。
- en: One-dimensional loss functions
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一维损失函数
- en: 'For simplicity, we begin by examining this topic in one dimension—meaning there
    is a single parameter *w*. The first example we will study is the simplest possible
    case: a linear one-dimensional loss function, shown in figure [3.5](#fig-line_tangent).
    A linear loss function in one dimension can be written as *L*(*w*) = *mw* + *c*.
    If we change the parameter *w* by a small amount *δw*, what is the corresponding
    change in loss *δL*? We have *δL* = *L*(*w* + *δw*) − *L*(*w*) = (*m*(*w* + *δw*)+*c*)
    − (*m*(*w*)+*c*) = *m* *δw* which gives us *δL*/*δw* = *m*, a constant. By definition,
    the derivative *dL*/*dw* = lim[*δw*→0] *δL*/*δw*, which leads to *dL*/*dw* = *m*.
    Thus, for the straight line *L*(*w*) = *mw* + *c*, the rate of change of *L* with
    respect to *w* is constant everywhere and equals the slope *m*. Putting all this
    together, we get *δL* = *m δw* = *dL*/*dw* *δw*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们首先在一维中考察这个主题——这意味着有一个单个参数 *w*。我们将研究的第一个例子是最简单的情况：一个一维的线性损失函数，如图 [3.5](#fig-line_tangent)
    所示。一维的线性损失函数可以表示为 *L*(*w*) = *mw* + *c*。如果我们通过一个小的量 *δw* 改变参数 *w*，损失 *δL* 的相应变化是多少？我们有
    *δL* = *L*(*w* + *δw*) − *L*(*w*) = (*m*(*w* + *δw*)+*c*) − (*m*(*w*)+*c*) = *m*
    *δw*，这给出了 *δL*/*δw* = *m*，一个常数。根据定义，导数 *dL*/*dw* = lim[*δw*→0] *δL*/*δw*，这导致 *dL*/*dw*
    = *m*。因此，对于直线 *L*(*w*) = *mw* + *c*，*L* 相对于 *w* 的变化率在每处都是常数，并且等于斜率 *m*。将这些放在一起，我们得到
    *δL* = *m δw* = *dL*/*dw* *δw*。
- en: Let’s now study a slightly more complex, non-linear but still one dimensional
    case—a parabolic loss function illustrated in figure [3.4](#fig-parabola_tangent).
    This parabola can be written as *L*(*w*) = *w*². If we change the parameter *w*
    by a small amount *δw*, what is the corresponding change in in loss *δL*? We have
    *δL* = *L*(*w* + *δw*) − *L*(*w*) = (*w* + *δw*)² − *w*² = (2*wδw* + *δw*²). For
    infinitesimally small *δw*, *δw*² becomes negligibly small and we get lim[*δw*→0]
    *δL* = lim[*δw*→0] (2*wδw* + *δw*²) = 2*wδw* and *dL*/*dw* = lim[*δw*→0] *δL*/*δw*
    = 2w. Combining all these we get the same equation as the linear case *δL* = *dL*/*dw*
    *δw*. Of course, in case of the straight line this expression holds for all *δw*
    while in the non-linear curves the expression holds only for small *δw*.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来研究一个稍微复杂一些、非线性但仍为一维的情况——一个抛物线损失函数，如图 [3.4](#fig-parabola_tangent) 所示。这个抛物线可以表示为
    *L*(*w*) = *w*²。如果我们通过一个小的量 *δw* 改变参数 *w*，损失 *δL* 的相应变化是多少？我们有 *δL* = *L*(*w*
    + *δw*) − *L*(*w*) = (*w* + *δw*)² − *w*² = (2*wδw* + *δw*²)。对于无限小的 *δw*，*δw*²
    变得可以忽略不计，我们得到 lim[*δw*→0] *δL* = lim[*δw*→0] (2*wδw* + *δw*²) = 2*wδw* 和 *dL*/*dw*
    = lim[*δw*→0] *δL*/*δw* = 2w。结合所有这些，我们得到与线性情况相同的方程 *δL* = *dL*/*dw* *δw*。当然，在直线的情况下，这个表达式对所有
    *δw* 都成立，而在非线性曲线上，这个表达式只在小的 *δw* 下成立。
- en: '*δL* and *δw*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*δL* 和 *δw*'
- en: 'In general, for all one-dimensional loss functions *L*(*w*), the change *δL*
    caused by a change *δw* in parameters can be expressed as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于所有一维损失函数 *L*(*w*)，参数变化 *δw* 引起的改变 *δL* 可以表示如下：
- en: '![](../../OEBPS/Images/eq_03-03.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-03.png)'
- en: Equation 3.3
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3.3
- en: To decrease *L*, *δL* must be negative. From equation [3.3](#eq-derivative_total_change),
    we can see that this requires *δw* (change in *w*) and *dL*/*dw* (derivative)
    to have opposite signs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少 *L*，*δL* 必须是负的。从方程 [3.3](#eq-derivative_total_change) 中，我们可以看出这需要 *δw*（*w*
    的变化）和 *dL*/*dw*（导数）具有相反的符号。
- en: Geometrically speaking, the loss function represents a curve with the loss *L*(*w*)
    plotted along the *Y* axis against the parameter *w* plotted along the *X* axis
    (see figure [3.4](#fig-tangent-1d) for examples). The tangent at any point can
    be viewed as the local approximation to the curve itself for an infinitesimally
    small neighborhood around the point. The derivative at any point represents the
    slope of the tangent to the curve at that point.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从几何角度来说，损失函数表示一条曲线，其中损失 *L*(*w*) 沿着 *Y* 轴绘制，与参数 *w* 沿着 *X* 轴绘制（见图 [3.4](#fig-tangent-1d)
    中的示例）。任何点的切线可以看作是该点无限小邻域内曲线本身的局部近似。任何点的导数代表该点处曲线切线的斜率。
- en: NOTE Equation [3.3](#eq-derivative_total_change) basically tells us that to
    reduce the loss value, we have to follow the tangent, moving to the right (i.e.,
    positive *δw*) if the derivative is negative and moving to the left (i.e., negative
    *δw*) if the derivative is positive.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：方程 [3.3](#eq-derivative_total_change) 基本上告诉我们，为了减少损失值，我们必须沿着切线移动，如果导数是负的，则向右（即正
    *δw*）移动；如果导数是正的，则向左（即负 *δw*）移动。
- en: Multidimensional loss functions
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多维损失函数
- en: If there are many tunable parameters, our loss function will be a function of
    many variables, which implies that we have a high-dimensional vector ![](../../OEBPS/Images/AR_w.png)
    and a loss function *L*(![](../../OEBPS/Images/AR_w.png)). Our goal is to compute
    the change *δL* in *L*(![](../../OEBPS/Images/AR_w.png)) caused by a small vector
    displacement ![](../../OEBPS/Images/AR_delta.png).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有多个可调参数，我们的损失函数将是多个变量的函数，这意味着我们有一个高维向量 ![](../../OEBPS/Images/AR_w.png) 和一个损失函数
    *L*(![](../../OEBPS/Images/AR_w.png))。我们的目标是计算由小向量位移 ![](../../OEBPS/Images/AR_delta.png)
    引起的 *L*(![](../../OEBPS/Images/AR_w.png)) 的变化 *δL*。
- en: 'We immediately note a fundamental difference from the one-dimensional case:
    the parameter change is a vector, ![](../../OEBPS/Images/AR_delta.png), which
    has not only a magnitude denoted ||![](../../OEBPS/Images/AR_delta.png)|| but
    also a direction denoted by the unit vector ![](../../OEBPS/Images/AR_bw_hat.png).
    We can take a step of the same size in the *w* space, and the change in *L*(![](../../OEBPS/Images/AR_w.png))
    will be different for different directions. The situation is illustrated in figure
    [3.5](#fig-surface_gradient), which shows an example loss function *L*(![](../../OEBPS/Images/AR_w.png))
    ≡ *L*(*w*[0], *w*[1]) = 2*w*[0]² + 3*w*[1]² for two independent variables *w*[0]
    and *w*[1]. Let’s examine how this loss function changes with a few concrete examples.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到与一维情况的一个基本区别：参数变化是一个向量，![](../../OEBPS/Images/AR_delta.png)，它不仅有一个表示
    ||![](../../OEBPS/Images/AR_delta.png)|| 的大小，还有一个表示单位向量 ![](../../OEBPS/Images/AR_bw_hat.png)
    的方向。我们可以在 *w* 空间中迈出相同大小的步伐，而 *L*(![](../../OEBPS/Images/AR_w.png)) 的变化将因方向不同而不同。这种情况在图
    [3.5](#fig-surface_gradient) 中得到说明，该图展示了示例损失函数 *L*(![](../../OEBPS/Images/AR_w.png))
    ≡ *L*(*w*[0], *w*[1]) = 2*w*[0]² + 3*w*[1]²，其中 *w*[0] 和 *w*[1] 是两个独立变量。让我们通过几个具体的例子来考察这个损失函数是如何变化的。
- en: '![](../../OEBPS/Images/CH03_F05_Chaudhury.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F05_Chaudhury.png)'
- en: 'Figure 3.5 Plot for surface *L*(![](../../OEBPS/Images/AR_w.png)) ≡ *L*(*w*[0],
    *w*[1]) = 2*w*[0]² + 3*w*[1]² against ![](../../OEBPS/Images/AR_w.png) ≡ (*w*[0],
    *w*[1]). From an example point *P* ≡ (*w*[0]=3, *w*[1]=4, *L* = 66) on the surface,
    we can travel in many directions to reduce *L*. Some of these are shown byarrows.
    The maximum reduction occurs when we travel along the dark arrow: this happens
    when ![](../../OEBPS/Images/AR_w.png) is changed along ![](../../OEBPS/Images/AR_delta.png)
    = [-12, -24]*^T*, which is the negative of the gradient of *L*(![](../../OEBPS/Images/AR_w.png))
    at *P*.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 展示了表面 *L*(![](../../OEBPS/Images/AR_w.png)) ≡ *L*(*w*[0], *w*[1]) = 2*w*[0]²
    + 3*w*[1]² 对 ![](../../OEBPS/Images/AR_w.png) ≡ (*w*[0], *w*[1]) 的绘图。从表面上的一个示例点
    *P* ≡ (*w*[0]=3, *w*[1]=4, *L* = 66) 出发，我们可以向许多方向移动以减少 *L*。其中一些方向用箭头表示。最大减少发生在我们沿着深色箭头移动时：这发生在
    ![](../../OEBPS/Images/AR_w.png) 沿着 ![](../../OEBPS/Images/AR_delta.png) = [-12,
    -24]*^T* 变化时，这是 *P* 点处 *L*(![](../../OEBPS/Images/AR_w.png)) 的负梯度。
- en: 'Suppose we are at ![](../../OEBPS/Images/eq_03-03-a.png). The corresponding
    value of *L*(![](../../OEBPS/Images/AR_w.png)) is 2 ∗ 3² + 3 ∗ 4² = 66. Now, suppose
    we undergo a small displacement from this point: ![](../../OEBPS/Images/eq_03-03-b.png).
    The new value is *L*(![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_delta.png))
    = *L*(3.0003, 4.0004) = 2 ∗ 3.0003² + 3 ∗ 4.0004² ≈ 66.0132066. Thus this displacement
    vector ![](../../OEBPS/Images/eq_03-03-b.png) causes a change *δL* = 66.01320066
    – 66 = 0.01320066 in *L*.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于 ![](../../OEBPS/Images/eq_03-03-a.png)。*L*(![](../../OEBPS/Images/AR_w.png))
    的对应值是 2 ∗ 3² + 3 ∗ 4² = 66。现在，假设我们从这一点进行一个小位移：![](../../OEBPS/Images/eq_03-03-b.png)。新的值是
    *L*(![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_delta.png)) =
    *L*(3.0003, 4.0004) = 2 ∗ 3.0003² + 3 ∗ 4.0004² ≈ 66.0132066。因此，这个位移向量 ![](../../OEBPS/Images/eq_03-03-b.png)
    导致 *L* 的变化 *δL* = 66.01320066 – 66 = 0.01320066。
- en: On the other hand, if the displacement is ![](../../OEBPS/Images/eq_03-03-c.png),
    we get *L*(![](../../OEBPS/Images/AR_w.png) + ![](../../OEBPS/Images/AR_delta.png))
    = *L*(3.0004, 4.0003) = 2 ∗ 3.0004² + 3 ∗ 4.0003² ≈ 66.0120006. Thus, this displacement
    causes a change *δL* = 66.0120006 − 66 = 0.0120006 in *L*. The displacement ![](../../OEBPS/Images/eq_03-03-b.png)
    and ![](../../OEBPS/Images/eq_03-03-c.png) have the same length ![](../../OEBPS/Images/eq_03-03-d.png)
    but different directions. The change they cause to the function value is different.
    This exemplifies our thesis that in multivariable loss function, the change in
    the loss function depends not only on the magnitude but also on the direction
    of the displacement in the parameter space.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果位移是 ![](../../OEBPS/Images/eq_03-03-c.png)，我们得到 *L*(![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_delta.png)) = *L*(3.0004, 4.0003) = 2 ∗ 3.0004² +
    3 ∗ 4.0003² ≈ 66.0120006。因此，这个位移导致 *L* 的变化 *δL* = 66.0120006 − 66 = 0.0120006。位移向量
    ![](../../OEBPS/Images/eq_03-03-b.png) 和 ![](../../OEBPS/Images/eq_03-03-c.png)
    有相同的长度 ![](../../OEBPS/Images/eq_03-03-d.png) 但不同的方向。它们对函数值造成的变化是不同的。这证明了我们的论点：在多变量损失函数中，损失函数的变化不仅取决于位移在参数空间中的大小，还取决于位移的方向。
- en: What is the general relationship between the displacement vector ![](../../OEBPS/Images/AR_delta.png)
    in the parameter space and the overall change in loss *L*(![](../../OEBPS/Images/AR_w.png))?
    To examine this question, we need to know what a partial derivative is.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 参数空间中位移向量 ![](../../OEBPS/Images/AR_delta.png) 与损失函数 *L*(![](../../OEBPS/Images/AR_w.png))
    的整体变化之间有什么一般关系？为了考察这个问题，我们需要知道什么是偏导数。
- en: Partial derivatives
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数
- en: The derivative *dL*/*dw* of a function *L*(*w*) indicates the rate of change
    of the function with respect to *w*. But if *L* is a function of many variables,
    how does it change if only one of those variables is changed? This question leads
    to the notion of partialderivatives.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *L*(*w*) 的导数 *dL*/*dw* 表示函数相对于 *w* 的变化率。但如果 *L* 是多个变量的函数，当只有一个变量变化时，它会如何变化？这个问题引出了偏导数的概念。
- en: The *partial derivative* of a function of many variables is a derivative taken
    with respect to exactly one variable, treating all other variables as constants.
    For instance, given *L*(![](../../OEBPS/Images/AR_w.png)) ≡ *L*(*w*[0], *w*[1])
    = 2*w*[0]² + 3*w*[1]², the partial derivatives with respect to *w*[0] , *w*[1]
    are
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 多变量函数的偏导数是相对于恰好一个变量的导数，将其他变量视为常数。例如，给定 *L*(![](../../OEBPS/Images/AR_w.png))
    ≡ *L*(*w*[0], *w*[1]) = 2*w*[0]² + 3*w*[1]²，相对于 *w*[0] 和 *w*[1] 的偏导数是
- en: '![](../../OEBPS/Images/eq_03-03-e.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-03-e.png)'
- en: Total change in a multidimensional function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 多维函数的总变化
- en: Partial derivatives estimate the change in a function if a single variable changes
    and the others stay constant. How do we estimate the change in a function’s value
    if all the variables change together?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 偏导数估计在只有一个变量变化而其他变量保持不变的情况下函数的变化。如果我们想估计所有变量同时变化时函数值的改变，我们应该如何估计？
- en: 'The total change can be estimated by taking a weighted combination of the partial
    derivatives. Let ![](../../OEBPS/Images/AR_w.png) and ![](../../OEBPS/Images/AR_delta.png)
    denote the point and the displacement vector, respectively:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总变化可以通过对偏导数的加权组合来估计。令 ![](../../OEBPS/Images/AR_w.png) 和 ![](../../OEBPS/Images/AR_delta.png)
    分别表示点和位移向量：
- en: '![](../../OEBPS/Images/eq_03-03-f.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-03-f.png)'
- en: Then
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后
- en: '![](../../OEBPS/Images/eq_03-04.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-04.png)'
- en: Equation 3.4
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 3.4
- en: 'Equation [3.4](#eq-partial_deriv_total_change) essentially says that the total
    change in *L* is obtained by adding up the changes caused by displacements in
    individual variables. The rate of change of *L* with respect to the change in
    *w[i]* only is *∂L*/*∂w[i]*. The displacement along the variable *w[i]* is *δw[i]*.
    Hence, the change caused by the *i*th element of the displacement is *∂L*/*∂w[i]*
    *δw[i]*— this follows from equation [3.3](#eq-derivative_total_change). The total
    change is obtained by adding the changes caused by individual elements of the
    displacement vector: that is, summing over all *i* from 0 to *n*. This leads to
    equation [3.4](#eq-partial_deriv_total_change). Thus equation [3.4](#eq-partial_deriv_total_change)
    is simply the multidimensional version of equation [3.3](#eq-derivative_total_change).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [3.4](#eq-partial_deriv_total_change) 实际上说明，*L* 的总变化是通过将各个变量位移引起的变化相加得到的。*L*
    相对于 *w[i]* 变化的变化率是 *∂L*/*∂w[i]*。变量 *w[i]* 的位移是 *δw[i]*。因此，位移的第 *i* 个元素引起的变化是 *∂L*/*∂w[i]*
    *δw[i]*——这可以从方程 [3.3](#eq-derivative_total_change) 得出。总变化是通过将位移向量的各个元素引起的变化相加得到的：即对所有
    *i* 从 0 到 *n* 进行求和。这导致了方程 [3.4](#eq-partial_deriv_total_change)。因此，方程 [3.4](#eq-partial_deriv_total_change)
    只是方程 [3.3](#eq-derivative_total_change) 的多维版本。
- en: Gradients
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度
- en: 'It would be nice to be able to represent equation [3.4](#eq-partial_deriv_total_change)
    compactly. To do this, we define a quantity called a *gradient*: the vector of
    all the partial derivatives.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 能够紧凑地表示方程 [3.4](#eq-partial_deriv_total_change) 会很好。为了做到这一点，我们定义了一个称为 *梯度* 的量：所有偏导数的向量。
- en: Given an *n*-dimensional function *L*(![](../../OEBPS/Images/AR_w.png)), its
    gradient is defined as
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个 *n* 维函数 *L*(![](../../OEBPS/Images/AR_w.png))，其梯度定义为
- en: '![](../../OEBPS/Images/eq_03-05.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-05.png)'
- en: Equation 3.5
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 3.5
- en: Using gradients, we can rewrite equation [3.4](#eq-partial_deriv_total_change)
    as
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度，我们可以将方程 [3.4](#eq-partial_deriv_total_change) 重新写为
- en: '![](../../OEBPS/Images/eq_03-06.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-06.png)'
- en: Equation 3.6
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 公式 3.6
- en: Equation [3.6](#eq-gradient_total_change) tells us that the total change, *δL*
    in *L*(![](../../OEBPS/Images/AR_w.png)), caused by displacement ![](../../OEBPS/Images/AR_delta.png)
    from ![](../../OEBPS/Images/AR_w.png) in parameter space is the dot product between
    the gradient vector ∇*L*(![](../../OEBPS/Images/AR_w.png)) and the displacement
    vector ![](../../OEBPS/Images/AR_delta.png). This is the exact multidimensional
    analog of equation [3.3](#eq-derivative_total_change).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [3.6](#eq-gradient_total_change) 告诉我们，由参数空间中从 ![](../../OEBPS/Images/AR_w.png)
    到 ![](../../OEBPS/Images/AR_delta.png) 的位移 ![](../../OEBPS/Images/AR_delta.png)
    引起的 *L*(![](../../OEBPS/Images/AR_w.png)) 的总变化 *δL*，是梯度向量 ∇*L*(![](../../OEBPS/Images/AR_w.png))
    和位移向量 ![](../../OEBPS/Images/AR_delta.png) 的点积。这是方程 [3.3](#eq-derivative_total_change)
    的精确多维类似物。
- en: 'Recall from section [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement) that
    the dot product of two vectors (of fixed magnitude) attains a maximum value when
    the vectors are aligned in direction. This yields a physical interpretation of
    the gradient vector: its direction is the direction in parameter space *along
    which the multidimensional function is changing fastest*. It is the multidimensional
    counterpart of the derivative. This is why, in machine learning, when we want
    to minimize the loss function, we change the parameter values along the direction
    of the gradient vector of the loss function.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，从第 [2.5.6.2](02.xhtml#subsubsec-dotproduct_as_agreement) 节中，两个向量（固定大小）的点积在向量方向对齐时达到最大值。这为梯度向量提供了一个物理解释：其方向是参数空间中多维函数变化最快的方向。它是导数的多维对应物。这就是为什么在机器学习中，当我们想要最小化损失函数时，我们会沿着损失函数梯度向量的方向改变参数值。
- en: The gradient is zero at the minimum
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度在最小值处为零
- en: 'any *optimum* (that is, maximum or minimum) of a function is a point of inflection.
    This means the function turns around at the optimum point. In other words, the
    gradient direction on one side of the optimum is the opposite of that on the other
    side. If we try to travel smoothly from positive values to negative values, we
    must cross zero somewhere in between. Thus, the gradient is zero at the exact
    point of inflection maximum or minimum). This is easiest to see in 2D and is depicted
    in figure [3.6](#fig-gradient_zero_at_minima). However, the idea is general: it
    works in higher dimensions, too. The fact that the gradient becomes zero at the
    optimum is often used to algebraically compute the optimum. The following example
    illustrates this.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的任何 *最优值*（即最大值或最小值）都是拐点。这意味着函数在最优值点转折。换句话说，最优值一侧的梯度方向与另一侧相反。如果我们试图平滑地从正值过渡到负值，我们必须在中间某处穿过零。因此，梯度在精确的拐点最大值或最小值处为零）。这在二维中最容易看出，如图
    [3.6](#fig-gradient_zero_at_minima) 所示。然而，这个想法是通用的：它也适用于更高维的情况。梯度在最优值处变为零的事实常被用来代数计算最优值。以下例子说明了这一点。
- en: '![](../../OEBPS/Images/CH03_F06_Chaudhury.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F06_Chaudhury.png)'
- en: Figure 3.6 The minimum is always a point of inflection, meaning the function
    turns around at that point. If we consider any two points *P*[−] and *P* [+] on
    both sides of the minimum, the gradient is positive on one side and negative on
    the other. Assuming the gradient changes smoothly, it must be zero in between,
    at the minimum.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 最小值总是拐点，意味着函数在该点转折。如果我们考虑最小值两侧的任意两点 *P*[−] 和 *P* [+]，则梯度在一侧为正，在另一侧为负。假设梯度变化平滑，那么在最小值之间必须为零。
- en: 'Consider the simple example function *L*(*w*[0], *w*[1]) = √*w*[0]² + *w*[1]².
    Its optimum occurs when its gradient is zero:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑简单的示例函数 *L*(*w*[0], *w*[1]) = √*w*[0]² + *w*[1]²。其最优值出现在梯度为零时：
- en: '![](../../OEBPS/Images/eq_03-06-a.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-06-a.png)'
- en: The solution is *w*[0] = 0, *w*[1] = 0 The function attains its minimum value
    at the origin, which agrees with our intuition.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 解为 *w*[0] = 0, *w*[1] = 0 函数在其原点达到最小值，这与我们的直觉相符。
- en: 3.3.2 Level surface representation and loss minimization
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 水平面表示和损失最小化
- en: In figure [3.5](#fig-surface_gradient), we plotted the loss function *L*(![](../../OEBPS/Images/AR_w.png))
    against the parameter values ![](../../OEBPS/Images/AR_w.png). In this section,
    we study a different way of visualizing loss surfaces. This will lend further
    insight into gradients and minimization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [3.5](#fig-surface_gradient) 中，我们绘制了损失函数 *L*(![](../../OEBPS/Images/AR_w.png))
    与参数值 ![](../../OEBPS/Images/AR_w.png) 的关系。在本节中，我们研究了一种不同的可视化损失表面的方法。这将进一步揭示梯度和最小化的见解。
- en: 'We will continue with our simple example function from the last subsection.
    Consider a field *L*(*w*[0], *w*[1]) = √(*w*[0]² + *w*[1]²). Its domain is the
    infinite 2D plane defined by the axes *W*[0] and *W*[1]. Note that the function
    has constant values along concentric circles centered on the origin. For instance,
    at all points on the circumference of the circle *w*[0]² + *w*[1]² = 1, the function
    has a constant function value of 1. At all points on the circumference of the
    circle *w*[0]² + *w*[1]² = 25, the function has a constant function value of 5.
    Such constant function value curves on the domain are called *level contours*
    in 2D. This is shown as a heat map in figure [3.7](#fig-circle-field-2D). The
    idea of level contours can be generalized to higher dimensions where we have level
    surfaces or level hypersurfaces. Note that while the ![](../../OEBPS/Images/AR_w.png),
    *L*(![](../../OEBPS/Images/AR_w.png)) in figure [3.5](#fig-surface_gradient) was
    on an (*n*+1)-dimensional space (where *n* is the dimensionality of ![](../../OEBPS/Images/AR_w.png)),
    the level surface/contour representation is in *n*-dimensional space. At any point
    on the domain, what is the direction along which the biggest *change* in the function
    value occurs? The answer is *along the direction of the gradient*. The magnitude
    of the change corresponds to the magnitude of the gradient. In the current example,
    say we are at a point (*w*[0], *w*[1]). There exists a level contour through this
    point: the circle with origin at the center passing through (*w*[0], *w*[1]).
    If we move along the circumference of this circle—that is, along the tangent to
    this circle—the function value does not change. In other words, at any point,
    the tangent to the level contour through that point is the direction of *minimal*
    change. On the other hand, *if we move perpendicular to the tangent, maximum change
    in the function value occurs*. The perpendicular to the tangent is known as a
    *normal*. This is the direction of the gradient. *The gradient at any point on
    the domain is always normal to the level contour through that point, indicating
    the direction of maximum change in the function value*. In figure [3.7](#fig-circle-field-2D),
    the gradients are all parallel to the radii of the concentric circles.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用上一个小节中的简单示例函数。考虑一个场 *L*(*w*[0], *w*[1]) = √(*w*[0]² + *w*[1]²)。其定义域是由轴
    *W*[0] 和 *W*[1] 定义的无限二维平面。请注意，该函数在以原点为中心的同心圆上具有常数值。例如，在圆 *w*[0]² + *w*[1]² = 1
    的所有点上，函数具有常数函数值 1。在圆 *w*[0]² + *w*[1]² = 25 的所有点上，函数具有常数函数值 5。这种定义域上的常数函数值曲线在二维中称为
    *等高线*。这如图 [3.7](#fig-circle-field-2D) 中的热图所示。等高线的概念可以推广到更高维度，其中我们具有等高面或等高超曲面。请注意，虽然图
    [3.5](#fig-surface_gradient) 中的 ![](../../OEBPS/Images/AR_w.png)，*L*(![](../../OEBPS/Images/AR_w.png))
    在 (*n*+1)-维空间中（其中 *n* 是 ![](../../OEBPS/Images/AR_w.png) 的维度），但等高面/等高线表示是在 *n*-维空间中。在定义域上的任何一点，沿着哪个方向函数值的变化最大？答案是
    *沿着梯度的方向*。变化的幅度对应于梯度的幅度。在当前示例中，假设我们位于点 (*w*[0], *w*[1])。通过此点存在一个等高线：以原点为中心通过 (*w*[0],
    *w*[1]) 的圆。如果我们沿着这个圆的周长移动——即沿着这个圆的切线——函数值不会改变。换句话说，在任何一点，通过该点的等高线的切线是 *最小变化*的方向。另一方面，*如果我们垂直于切线移动，函数值的变化最大*。切线的垂线称为
    *法线*。这是梯度的方向。*在定义域上的任何一点，梯度总是垂直于通过该点的等高线，指示函数值最大变化的方向*。在图 [3.7](#fig-circle-field-2D)
    中，梯度都与同心圆的半径平行。
- en: Recall that while training a machine learning model, we essentially define a
    loss function in terms of a tunable set of parameters and try to minimize the
    loss by adjusting (tuning) the parameters. We start at a random point and iteratively
    progress toward the minimum. Geometrically, this can be viewed as starting at
    an arbitrary point on the domain and continuing to move in a direction that minimizes
    the function value. Of course, we would like to progress to the minimum of the
    function value in as few iterations as possible. In figure [3.7](#fig-circle-field-2D),
    the minimum is at the origin, which is also the center of all the concentric circles.
    Wherever we start, we will have to always travel radially inward to reach the
    minimum (0,0) of the function √(*w*[0]² + *w*[1]²).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在训练机器学习模型时，我们本质上是在一组可调参数的术语下定义一个损失函数，并通过调整（微调）参数来最小化损失。我们从随机点开始，迭代地向最小值前进。从几何上看，这可以看作是从定义域上的任意一点开始，并继续沿着最小化函数值的方向移动。当然，我们希望尽可能少地进行迭代，以到达函数值的最低点。在图[3.7](#fig-circle-field-2D)中，最小值在原点，这也是所有同心圆的中心。无论我们从哪里开始，我们都需要始终沿着径向向内移动，以达到函数√(w*[0]²
    + w*[1]²)的最小值（0,0）。
- en: 'In higher dimensions, level contours become level surfaces. Given any function
    *L*(![](../../OEBPS/Images/AR_w.png)) with ![](../../OEBPS/Images/AR_w.png)] ∈
    ℝ*^n*, we define level surfaces as *L*(![](../../OEBPS/Images/AR_w.png)) = *constant*.
    If we move along the level surface, the change in *L*(![](../../OEBPS/Images/AR_w.png))
    is minimal (0). The gradient of a function at any point is normal to the level
    surface through that point. This is the direction along which the function value
    is changing fastest. Moving along the gradient, we pass from one level surface
    to another, as shown in figure [3.8](#fig-grad_sphere). Here the function is 3D:
    *L*(![](../../OEBPS/Images/AR_w.png)) = *L*(*w*[0], *w*[1], *w*[2]) = *w*[0]²
    + *w*[1]² + *w*[2]². The level surfaces *w*[0]² + *w*[1]² + *w*[2]² = *constant*
    for various values of the constant are concentric spheres, with the origin as
    their center. The gradient vector at any point is along the outward-pointing radius
    of the sphere through that point.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高维的情况下，等高线变为等高面。给定任何函数*L*(![图片](../../OEBPS/Images/AR_w.png))，其中![图片](../../OEBPS/Images/AR_w.png)]
    ∈ ℝ*^n*，我们定义等高面为*L*(![图片](../../OEBPS/Images/AR_w.png)) = *constant*。如果我们沿着等高面移动，L(![图片](../../OEBPS/Images/AR_w.png))的变化是最小的（0）。函数在任意点的梯度是穿过该点的等高面的法线。这是函数值变化最快的方向。沿着梯度移动，我们从一个等高面过渡到另一个等高面，如图[3.8](#fig-grad_sphere)所示。在这里，函数是3D的：*L*(![图片](../../OEBPS/Images/AR_w.png))
    = *L*(w*[0], w*[1], w*[2]) = w*[0]² + w*[1]² + w*[2]²。对于各种常数值，等高面*w*[0]² + w*[1]²
    + w*[2]² = *constant*是同心球面，以原点为中心。任意点的梯度向量沿着通过该点的球面向外辐射的半径方向。
- en: '![](../../OEBPS/Images/CH03_F07_Chaudhury.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F07_Chaudhury.png)'
- en: Figure 3.7 The domain of *L*(*w*[0], *w*[1]) = √(*w*[0]² + *w*[1]²) shown as
    a heat map of function values. Gradients point radially outward, as shown by the
    arrowed line. The intensity of the heat map changes fastest along the gradient
    (that is, radii). This is the direction to follow to rapidly reach lower values
    of the function represented by the heat map.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7展示了函数*L*(w*[0], w*[1]) = √(w*[0]² + w*[1]²)的定义域，以函数值的热图形式呈现。梯度方向向外辐射，如箭头线所示。热图强度沿梯度（即半径）变化最快。这是快速达到热图所表示函数的较低值的方向。
- en: '![](../../OEBPS/Images/CH03_F08_Chaudhury.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F08_Chaudhury.png)'
- en: 'Figure 3.8 Gradient example in 3D: the function *L*(*w*[0], *w*[1], *w*[2])
    = *L*(![](../../OEBPS/Images/AR_w.png)) = *w*[0]² + *w*[1]² + *w*[2]². The levelsurfaces
    *L*(![](../../OEBPS/Images/AR_w.png)) = *constant* are concentric spheres with
    the origin as their center. One such surface is partially shown in the diagram.
    ∇*L*(![](../../OEBPS/Images/AR_w.png)) = *k*[*w*[0] *w*[1] *w*[2]]*^T*—the gradient
    points radially outward. along the gradient, we go from one level surface to another,
    corresponding to maximum change in *L*(![](../../OEBPS/Images/AR_w.png)). Moving
    along any direction orthogonal to the gradient, we stay on the same level surface
    (sphere), which corresponds to zero change in the function value. *D[θ]*(![](../../OEBPS/Images/AR_w.png))
    denotes the directional derivative along the displacement direction making angle
    *θ* with the gradient. If *l̂* denotes this displacement direction, *D[θ]*(![](../../OEBPS/Images/AR_w.png))
    = ∇*L*(![](../../OEBPS/Images/AR_w.png)) ⋅ *l̂*.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 3D中的梯度示例：函数 *L*(*w*[0], *w*[1], *w*[2]) = *L*(![](../../OEBPS/Images/AR_w.png))
    = *w*[0]² + *w*[1]² + *w*[2]². 等值面 *L*(![](../../OEBPS/Images/AR_w.png)) = *constant*
    是以原点为中心的同心球体。图中部分展示了这样一个表面。∇*L*(![](../../OEBPS/Images/AR_w.png)) = *k*[*w*[0]
    *w*[1] *w*[2]]*^T*—梯度指向径向外。沿着梯度移动，我们从一级表面移动到另一级表面，对应于 *L*(![](../../OEBPS/Images/AR_w.png))
    的最大变化。沿着与梯度垂直的任何方向移动，我们保持在同一级表面（球体）上，这对应于函数值的变化为零。*D[θ]*(![](../../OEBPS/Images/AR_w.png))
    表示沿着与梯度成 *θ* 角的位移方向的偏导数。如果 *l̂* 表示这个位移方向，*D[θ]*(![](../../OEBPS/Images/AR_w.png))
    = ∇*L*(![](../../OEBPS/Images/AR_w.png)) ⋅ *l̂*。
- en: 'Another example is shown in figure [3.9](#fig-grad_cylinder). Here the function
    is 3D: *L*(![](../../OEBPS/Images/AR_w.png)) = *f*(*w*[0], *w*[1], *w*[2]) = *w*[0]²
    + *w*[1]². The level surfaces *w*[0]² + *w*[1]² = *constant* for various values
    of the constant are coaxial cylinders, with *w*[2] as the axis. The gradient vector
    at any point is along the outward-pointing radius of the planar circle belonging
    to the cylinder through that point.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子如图[3.9](#fig-grad_cylinder)所示。这里函数是3D的：*L*(![](../../OEBPS/Images/AR_w.png))
    = *f*(*w*[0], *w*[1], *w*[2]) = *w*[0]² + *w*[1]². 对于常数的各种值，等值面 *w*[0]² + *w*[1]²
    = *constant* 是以 *w*[2] 为轴的同轴圆柱体。图中部分展示了这样一个表面。在任意点的梯度向量沿着属于该点的圆柱体的平面圆的向外指向的半径。沿着梯度移动，我们从一级表面移动到另一级表面，对应于
    *L*(![](../../OEBPS/Images/AR_w.png)) 的最大变化。沿着与梯度垂直的任何方向移动，我们保持在同一级表面（圆柱体）上，这对应于函数值的变化为零。*D[θ]*(![](../../OEBPS/Images/AR_w.png))
    表示沿着与梯度成 *θ* 角的位移方向的偏导数。如果 *l̂* 表示这个位移方向，*D[θ]*(![](../../OEBPS/Images/AR_w.png))
    = ∇*L*(![](../../OEBPS/Images/AR_w.png)) ⋅ *l̂*。
- en: '![](../../OEBPS/Images/CH03_F09_Chaudhury.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F09_Chaudhury.png)'
- en: 'Figure 3.9 Gradient example in 3D: the function *L*(*w*[0], *w*[1], *w*[2])
    = *L*(![](../../OEBPS/Images/AR_w.png)) = *w*[0]² + *w*[1]². The level surfaces
    *f*(![](../../OEBPS/Images/AR_w.png)) = *constant* are coaxial cylinders. One
    such surface is partially shown in the diagram: ∇*L*(![](../../OEBPS/Images/AR_w.png))
    = *k*[*w*[0] *w*[1] 0]*^T*. The gradient is normal to the curved surface of the
    cylinder along the outward radiusof the circle. Moving along the gradient, we
    go from one level surface to another, corresponding to themaximum change in *L*(![](../../OEBPS/Images/AR_w.png)).
    Moving along any direction orthogonal to the gradient, we stay on thesame level
    surface (cylinder), which corresponds to zero change in the function value.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 3D中的梯度示例：函数 *L*(*w*[0], *w*[1], *w*[2]) = *L*(![](../../OEBPS/Images/AR_w.png))
    = *w*[0]² + *w*[1]². 等值面 *f*(![](../../OEBPS/Images/AR_w.png)) = *constant* 是同轴的圆柱体。图中部分展示了这样一个表面：∇*L*(![](../../OEBPS/Images/AR_w.png))
    = *k*[*w*[0] *w*[1] 0]*^T*. 梯度垂直于圆柱体的曲面，沿着圆的向外半径。沿着梯度移动，我们从一级表面移动到另一级表面，对应于 *L*(![](../../OEBPS/Images/AR_w.png))
    的最大变化。沿着与梯度垂直的任何方向移动，我们保持在同一级表面（圆柱体）上，这对应于函数值的变化为零。
- en: So far, we have studied the change in loss value resulting from infinitesimally
    small displacements in the parameter space. In practice, the programmatic displacements
    undergone during parameter updates while training are small, but not infinitesimally
    so. Is there any way to improve the approximation in these cases? This is discussed
    in the following section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了参数空间中无限小位移引起的损失值的变化。在实践中，训练过程中参数更新时程序性位移是小的，但不是无限小的。在这些情况下，有没有什么方法可以改进近似？这将在下一节中讨论。
- en: 3.4 Local approximation for the loss function
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 损失函数的局部近似
- en: 'Equation [3.6](#eq-gradient_total_change) expresses the change *δL* in the
    loss value corresponding to displacement ![](../../OEBPS/Images/AR_delta.png)
    in the parameter space. The equation is exactly true if and only if the loss function
    is linear or the magnitude of the displacement is infinitesimally small. In practice,
    we adjust parameter values by small—but not infinitesimally small—amounts. Under
    these circumstances, equation [3.6](#eq-gradient_total_change) is only approximately
    true: the larger the magnitude of ||![](../../OEBPS/Images/AR_delta.png)||, the
    worse the approximation.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [3.6](#eq-gradient_total_change) 表达了参数空间中位移 ![图片](../../OEBPS/Images/AR_delta.png)
    对应的损失值变化 *δL*。当损失函数是线性的或位移的幅度无限小的时候，方程是完全正确的。在实践中，我们通过小量（但不是无限小量）调整参数值。在这种情况下，方程
    [3.6](#eq-gradient_total_change) 只是大致正确的：||![图片](../../OEBPS/Images/AR_delta.png)||
    的幅度越大，近似越差。
- en: A Taylor series offers a way to approximate a multidimensional function in the
    local neighborhood of any point by expressing it in terms of the displacements
    in the parameter space. It is an infinite series, meaning the equation is exactly
    true (zero approximation) only when we have summed an infinite number of terms.
    Of course, we cannot add an infinite number of terms with a computer program.
    But we can improve the accuracy of the approximation as much as we like by including
    more and more terms. In practice, we include at most up to the second term. Anything
    beyond that is redundant because the improvement is too small to be realized by
    the floating point system of current computers. First we will study a Taylor series
    in one dimension.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数提供了一种方法，通过在参数空间中的位移来近似任何点的局部邻域中的多维函数。它是一个无限级数，这意味着方程只有在我们将无限多个项相加时才是完全正确的（零近似）。当然，我们无法用计算机程序添加无限多个项。但我们可以通过包括越来越多的项来尽可能提高近似的精度。在实践中，我们最多包括到第二项。任何超过这一项的都是多余的，因为改进太小，无法通过当前计算机的浮点系统实现。首先，我们将研究一维的泰勒级数。
- en: 3.4.1 1*D* Taylor series recap
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 1*D* 泰勒级数回顾
- en: 'Suppose we are trying to describe the curve *L*(*w*) in the neighborhood of
    a particular point *w*. If we stay infinitesimally close to *w*, then, as described
    in section [3.3](../Text/03.xhtml#sec-grad), we can approximate the curve with
    a straight line:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们试图描述特定点 *w* 附近的曲线 *L*(*w*)。如果我们无限接近于 *w*，那么，如第 [3.3](../Text/03.xhtml#sec-grad)
    节所述，我们可以用直线来近似曲线：
- en: '![](../../OEBPS/Images/eq_03-06-b.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-06-b.png)'
- en: 'But in the general case, if we are describing a continuous (smooth) function
    in the neighborhood of a specific point, we use a Taylor series. A Taylor series
    allows us to describe a function in the neighborhood of a specific point in terms
    of the value of the function and its derivatives at that point. Doing so is relatively
    simple in 1D:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 但在一般情况下，如果我们描述的是特定点附近的连续（光滑）函数，我们使用泰勒级数。泰勒级数允许我们用函数在该点的值及其导数来描述该点附近的函数。在1D中这样做相对简单：
- en: '![](../../OEBPS/Images/eq_03-07.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-07.png)'
- en: Equation 3.7
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 3.7
- en: Note that the terms become progressively smaller (since they involve higher
    and higher powers of a small number *δw*). Hence, although the series goes on
    to infinity, in practice we entail a negligible loss in accuracy by dropping higher-order
    terms. We often use the first-order approximation (or, at most, second-order).
    Equation [3.7](#eq-taylor-onedim) can be rewritten as
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，项会逐渐变小（因为它们涉及到越来越小的数 *δw* 的高次幂）。因此，尽管级数延伸到无穷大，但在实践中，我们通过省略高阶项来忽略可忽略的精度损失。我们通常使用一阶近似（或者最多二阶）。方程
    [3.7](#eq-taylor-onedim) 可以重写为
- en: '![](../../OEBPS/Images/eq_03-07-a.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-07-a.png)'
- en: Note that the second term has (*δw*)² as a factor, which is nearly zero at small
    values of the displacement *δw*. So, for really small *δw*, we include only the
    first term. Then we get *δL* = (*δw*/1!) (*dL*/*dw)*, which is the same as equation
    [3.3](#eq-derivative_total_change). If *δw* is a bit larger and we want greater
    accuracy, we can include the second-order term. In practice, as mentioned earlier,
    that is hardly ever done.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第二项包含 (*δw*)² 作为因子，在位移 *δw* 很小的时候几乎为零。所以，对于非常小的 *δw*，我们只包括第一项。然后我们得到 *δL*
    = (*δw*/1!) (*dL*/*dw)*，这与方程 [3.3](#eq-derivative_total_change) 相同。如果 *δw* 稍大，并且我们想要更高的精度，我们可以包括二阶项。在实践中，如前所述，这种情况几乎从未发生过。
- en: A handy example of a Taylor series is the expansion of the exponential function
    *e^x* near *x* = 0
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 泰勒级数的一个实用例子是指数函数*e^x*在*x* = 0附近的展开
- en: '![](../../OEBPS/Images/eq_03-07-b.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-07-b.png)'
- en: where we use the fact that *d^n*/*dx^n* (*e^x*)|[*x* = 0] = *e^x*|[*x* = 0]
    = 1 for all *n*.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其中我们使用了以下事实：*d^n*/*dx^n* (*e^x*)|[*x* = 0] = *e^x*|[*x* = 0] = 1对所有*n*成立。
- en: 3.4.2 Multidimensional Taylor series and the Hessian matrix
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 多维泰勒级数和Hessian矩阵
- en: 'In equation [3.7](#eq-taylor-onedim), we express a function of one variable
    in a small neighborhood around a point in terms of the derivatives. Can we do
    a similar thing in higher dimensions? Yes. We simply need to replace the first
    derivative with the gradient. We replace the second derivative with its multidimensional
    counterpart: the Hessian matrix. The multidimensional Taylor series is as follows'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程[3.7](#eq-taylor-onedim)中，我们用导数在点附近的小邻域内表示一个单变量函数。在更高维度中，我们能做类似的事情吗？是的。我们只需将一阶导数替换为梯度。我们将二阶导数替换为其多维对应物：Hessian矩阵。多维泰勒级数如下
- en: '![](../../OEBPS/Images/eq_03-08.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-08.png)'
- en: Equation 3.8
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 方程3.8
- en: where *H*(*L*(![](../../OEBPS/Images/AR_w.png))), called the *Hessian matrix*,
    is defined as
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*H*(*L*(![图片](../../OEBPS/Images/AR_w.png)))，称为Hessian矩阵，定义为
- en: '![](../../OEBPS/Images/eq_03-09.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-09.png)'
- en: Equation 3.9
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 方程3.9
- en: The Hessian matrix is symmetric since ![](../../OEBPS/Images/eq_03-09-a.png).
    Also, note that the Taylor expansion assumes that the function is continuous in
    the neighborhood.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Hessian矩阵是对称的，因为![图片](../../OEBPS/Images/eq_03-09-a.png)。此外，请注意，泰勒展开假设函数在邻域内是连续的。
- en: Equation [3.8](../Text/03.xhtml#eq-taylor-multidim) allows us to compute the
    value of *L* in a small neighborhood around point ![](../../OEBPS/Images/AR_w.png)
    in the parameter space. If we displace from ![](../../OEBPS/Images/AR_w.png) by
    the vector ![](../../OEBPS/Images/AR_delta.png), we arrive at ![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_delta.png). The loss there is *L*(![](../../OEBPS/Images/AR_w.png)
    + ![](../../OEBPS/Images/AR_delta.png)), which is expressed by equation [3.8](../Text/03.xhtml#eq-taylor-multidim)
    in terms of the loss *L*(![](../../OEBPS/Images/AR_w.png)) at the original point
    and the displacement ![](../../OEBPS/Images/AR_delta.png).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 方程[3.8](../Text/03.xhtml#eq-taylor-multidim)允许我们在参数空间中点![图片](../../OEBPS/Images/AR_w.png)附近的小邻域内计算*L*的值。如果我们从![图片](../../OEBPS/Images/AR_w.png)通过向量![图片](../../OEBPS/Images/AR_delta.png)移动，我们到达![图片](../../OEBPS/Images/AR_w.png)
    + ![图片](../../OEBPS/Images/AR_delta.png)。那里的损失是*L*(![图片](../../OEBPS/Images/AR_w.png)
    + ![图片](../../OEBPS/Images/AR_delta.png))，它通过方程[3.8](../Text/03.xhtml#eq-taylor-multidim)以原始点的损失*L*(![图片](../../OEBPS/Images/AR_w.png))和位移![图片](../../OEBPS/Images/AR_delta.png)来表示。
- en: '![](../../OEBPS/Images/eq_03-10.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/eq_03-10.png)'
- en: Equation 3.10
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 方程3.10
- en: Note that the first term is same as equation [3.6](#eq-gradient_total_change)
    and the second term has squares of the displacement. Since the square of a small
    quantity is even smaller, for very small displacements, the second term disappears,
    and we essentially get back equation [3.6](#eq-gradient_total_change). This is
    called *first-order approximation*. For slightly larger displacements, we can
    include the second term, involving Hessians to improve the approximation. As stated
    earlier, this is hardly ever done in practice.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第一项与方程[3.6](#eq-gradient_total_change)相同，第二项是位移的平方。由于小量的平方更小，对于非常小的位移，第二项消失，我们本质上又回到了方程[3.6](#eq-gradient_total_change)。这被称为一阶近似。对于稍微大一点的位移，我们可以包括涉及Hessian的二阶项，以改进近似。如前所述，这在实践中几乎从未做过。
- en: 3.5 PyTorch code for gradient descent, error minimization,and model training
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 梯度下降、误差最小化和模型训练的PyTorch代码
- en: In this section, we study PyTorch examples in which models are trained by minimizing
    errors via gradient descent. Before we present the code, we briefly recap the
    main ideas from a practical point of view. Complete code for this section can
    be found at [http://mng.bz/4Zya](http://mng.bz/4Zya).)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究PyTorch示例，其中模型通过最小化误差通过梯度下降进行训练。在我们展示代码之前，我们从实际的角度简要回顾一下主要思想。本节的完整代码可以在[http://mng.bz/4Zya](http://mng.bz/4Zya)找到。
- en: 3.5.1 PyTorch code for linear models
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.1 线性模型的PyTorch代码
- en: 'If the true underlying function we are trying to predict is very simple, linear
    models suffice. Otherwise, we require nonlinear models. Here we will look at the
    linear model. In machine learning, we identify the input and output variables
    pertaining to the problem at hand and cast the problem as generating outputs from
    input variables. All the inputs are represented together by the vector ![](../../OEBPS/Images/AR_x.png).
    Sometimes there are multiple outputs, and sometimes there is a single output.
    Accordingly, we have an output vector ![](../../OEBPS/Images/AR_y.png) or an output
    scalar *y*. Let’s denote the function that generates the output from the input
    vector as *f*: that is, *y* = *f*(![](../../OEBPS/Images/AR_x.png)).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图预测的真实基础函数非常简单，线性模型就足够了。否则，我们需要非线性模型。在这里，我们将研究线性模型。在机器学习中，我们确定与手头问题相关的输入和输出变量，并将问题表述为从输入变量生成输出。所有输入都由向量
    ![](../../OEBPS/Images/AR_x.png) 一起表示。有时有多个输出，有时只有一个输出。相应地，我们有一个输出向量 ![](../../OEBPS/Images/AR_y.png)
    或输出标量 *y*。让我们用 *f* 表示从输入向量生成输出的函数：即，*y* = *f*(![](../../OEBPS/Images/AR_x.png))。
- en: In real-life problems, we do not know *f*. The crux of machine learning is to
    estimate *f* from a set of observed inputs ![](../../OEBPS/Images/AR_x.png)*[i]*
    and their corresponding outputs *y[i]*. Each observation can be depicted as a
    pair ⟨![](../../OEBPS/Images/AR_x.png)*[i]*, *y[i]*⟩. We model the unknown function
    *f* with a known function *ϕ*. *ϕ* is a parameterized function. Although the nature
    of *ϕ* is known, its parameter values are unknown. These parameter values are
    “learned” via training. This means we estimate the parameter values such that
    the overall error on the observations is minimized.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中的问题中，我们不知道 *f*。机器学习的核心是从一组观察到的输入 ![](../../OEBPS/Images/AR_x.png)*[i]*
    和它们相应的输出 *y[i]* 中估计 *f*。每个观察结果可以表示为一个对 ⟨![](../../OEBPS/Images/AR_x.png)*[i]*,
    *y[i]*⟩。我们用已知函数 *ϕ* 来建模未知函数 *f*。*ϕ* 是一个参数化函数。尽管 *ϕ* 的性质是已知的，但其参数值是未知的。这些参数值通过训练“学习”得到。这意味着我们估计参数值，以使观察的整体误差最小化。
- en: If ![](../../OEBPS/Images/AR_w.png), *b* denotes the current set of parameters
    (weights, bias), then the model will output *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*,
    ![](../../OEBPS/Images/AR_w.png), *b*) on the observed input ![](../../OEBPS/Images/AR_x.png)*[i]*.
    Thus the error on this *i*th observation is *e[i]*² = (*ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*,
    ![](../../OEBPS/Images/AR_w.png), *b*)−*y[i]*)². We can batch several observations
    and add up the errors into a batch error *L* = Σ[*i* = 0]^(*i* = *N*) (*e^((i))*)².
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ![](../../OEBPS/Images/AR_w.png)，*b* 表示当前的参数集（权重，偏差），那么模型将在观察到的输入 ![](../../OEBPS/Images/AR_x.png)*[i]*
    上输出 *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*, ![](../../OEBPS/Images/AR_w.png),
    *b*)。因此，这个 *i* 次观察的误差是 *e[i]*² = (*ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*, ![](../../OEBPS/Images/AR_w.png),
    *b*)−*y[i]*)²。我们可以批量处理几个观察结果，并将误差加起来得到批量误差 *L* = Σ[*i* = 0]^(*i* = *N*) (*e^((i))*)²。
- en: 'The error is a function of the parameter set ![](../../OEBPS/Images/AR_w.png).
    The question is, how do we adjust ![](../../OEBPS/Images/AR_w.png) so that the
    error *e[i]*² decreases? We know a function’s value changes most when we move
    along the direction of the gradient of the parameters. Hence, we adjust the parameters
    ![](../../OEBPS/Images/AR_w.png), *b* as follows:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 误差是参数集 ![](../../OEBPS/Images/AR_w.png) 的函数。问题是，我们如何调整 ![](../../OEBPS/Images/AR_w.png)
    以使误差 *e[i]*² 减少？我们知道函数的值在沿着参数梯度的方向移动时变化最大。因此，我们调整参数 ![](../../OEBPS/Images/AR_w.png)，*b*
    如下：
- en: '![](../../OEBPS/Images/eq_03-10-a.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-10-a.png)'
- en: Each adjustment reduces the error. Starting from a random set of parameter values
    and doing this a sufficiently large number of times ields the desired model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 每次调整都会减少误差。从一组随机的参数值开始，并足够多次地这样做，可以得到期望的模型。
- en: 'A simple and popular model *ϕ* is the linear function (the predicted value
    is the dot product between the input vector and parameters vector plus bias):
    *ỹ[i]* = *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*, ![](../../OEBPS/Images/AR_w.png),
    *b*) = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png) +
    *b* = ∑*[j]w[j]x[j]* + *b*. Our initial implementation (listing [3.1](#listing3-1))
    simply mimics this formula. For more complicated models *ϕ* (with millions of
    parameters and nonlinearities), we cannot obtain closed-form gradients like this.
    In such cases, we use a technique called autograd (automatic gradient computation),
    which does not required closed form gradients. This is discussed in the next section.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单且流行的模型*ϕ*是线性函数（预测值是输入向量和参数向量之间的点积加上偏置）：*ỹ[i]* = *ϕ*(![](../../OEBPS/Images/AR_x.png)*[i]*,
    ![](../../OEBPS/Images/AR_w.png), *b*) = ![](../../OEBPS/Images/AR_w.png)*^T*![](../../OEBPS/Images/AR_x.png)
    + *b* = ∑*[j]w[j]x[j]* + *b*. 我们最初的实现（列表[3.1](#listing3-1)）只是简单地模仿了这个公式。对于更复杂的模型*ϕ*（具有数百万个参数和非线性），我们无法获得这样的闭合形式梯度。在这种情况下，我们使用一种称为自动微分（自动梯度计算）的技术，它不需要闭合形式梯度。这将在下一节中讨论。
- en: NOTE In real-world problems, we will not know the true underlying function mapping
    inputs to outputs. But here, for the sake of gaining insight, we will assume known
    output functions and perturb them with noise to make them slightly more realistic.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：在现实世界的问题中，我们不会知道将输入映射到输出的真实底层函数。但在这里，为了获得洞察力，我们将假设已知的输出函数，并通过添加噪声来使它们稍微更真实。
- en: Listing 3.1 PyTorch linear model (closed-form gradient formula needed)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 PyTorch线性模型（需要闭合形式的梯度公式）
- en: '[PRE0]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Generates random input values
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成随机输入值
- en: ② Generates output values by applying a simple known function to the input and
    then adds noise. Let’s see if our learned function matches the known underlying
    function.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ② 通过将一个简单的已知函数应用于输入并添加噪声来生成输出值。让我们看看我们学习到的函数是否与已知的底层函数相匹配。
- en: ③ Our model, initialized with arbitrary parameter values
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们用任意参数值初始化的模型
- en: ④ Model error is the (squared) difference between the observed and predicted
    values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 模型误差是观测值与预测值之间的（平方）差。
- en: ⑤ Calculates the gradient of the error using calculus. Possible only with such
    simple models.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用微积分计算误差的梯度。仅适用于此类简单模型。
- en: ⑥ Adjusts the weight, bias along the gradient of error
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 沿着误差梯度调整权重和偏置
- en: 'The output is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE1]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3.5.2 Autograd: PyTorch automatic gradient computation'
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.2 Autograd：PyTorch自动梯度计算
- en: In the PyTorch code in listing [3.1](#listing3-1), for this specific model architecture,
    we computed the gradient using calculus. This approach does not scale to more
    complex models with millions of weights and perhaps nonlinear complex functions.
    For scalability, we use an *automatic differentiation* software library like PyTorch
    Autograd. Users of the library need not worry about how to compute the gradients—they
    just construct the model function. Once the function is specified, PyTorch figures
    out how to compute its gradient through the Autograd technology.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表[3.1](#listing3-1)中的PyTorch代码，对于这个特定的模型架构，我们使用了微积分来计算梯度。这种方法不适用于具有数百万个权重和可能非线性复杂函数的更复杂模型。为了可扩展性，我们使用像PyTorch
    Autograd这样的自动微分软件库。库的用户无需担心如何计算梯度——他们只需构建模型函数。一旦函数被指定，PyTorch就会通过Autograd技术找出如何计算其梯度。
- en: To use Autograd, we explicitly tell PyTorch to track gradients for a variable
    by setting `requires_grad = True` when creating the variable. PyTorch remembers
    a computation graph that is updated every time we create an expression using tracked
    variables. Figure [3.10](#fig-auto-grad) shows an example of a computation graph.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Autograd，我们明确告诉PyTorch在创建变量时跟踪该变量的梯度，通过设置`requires_grad = True`。PyTorch会记住一个计算图，每次我们使用跟踪变量创建表达式时，该图都会更新。图[3.10](#fig-auto-grad)展示了计算图的一个示例。
- en: '![](../../OEBPS/Images/CH03_F10_Chaudhury.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F10_Chaudhury.png)'
- en: Figure 3.10 Autograd analysis
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 Autograd分析
- en: The following listing, which implements a linear model in PyTorch, relies on
    PyTorch’s Autograd for gradient computation. Note that this method does not require
    the closed-form gradient.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表，实现了PyTorch中的线性模型，依赖于PyTorch的Autograd进行梯度计算。请注意，此方法不需要闭合形式的梯度。
- en: Listing 3.2 Linear modeling with PyTorch
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 使用PyTorch进行线性建模
- en: '[PRE2]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '① Updates parameters: adjusts the weight, bias along the gradient of error'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ① 更新参数：沿着误差梯度调整权重和偏置
- en: ② Doesn’t track gradients during parameter updates
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在参数更新期间不跟踪梯度
- en: ③ Restores gradient tracking
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 恢复梯度跟踪
- en: ④ Generates random training input
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 生成随机训练输入
- en: '⑤ Generates training output: applies a simple known function to the input and
    then adds noise. Let’s see if our learned function matches the known underlying
    function.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 生成训练输出：将简单的已知函数应用于输入，然后添加噪声。让我们看看我们学习到的函数是否与已知的底层函数匹配。
- en: ⑥ Our model, initialized with arbitrary parameter values
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 我们的模型，使用任意参数值初始化
- en: ⑦ The model error is the (squared) difference
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 模型误差是（平方的）误差相对于
- en: between the observed and predicted values.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 之间。
- en: '⑧ Backpropagates: computes the partial'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 反向传播：计算偏导数
- en: derivatives of the error with respect to
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 误差相对于每个变量的导数
- en: each variable and stores them in the “grad” field within the variable
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每个变量之间的观察值和预测值之间的偏导数，并将它们存储在变量的“grad”字段中
- en: ⑨ Updates parameters using those partial derivatives
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用这些偏导数更新参数
- en: 'The output is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE3]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 3.5.3 Nonlinear Models in PyTorch
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.3 PyTorch 中的非线性模型
- en: In listings [3.1](#listing3-1) and [3.2](#listing3-2), we fit a linear model
    to a data distribution that we know to be linear. From the output, we can see
    that those models converged to a pretty good approximation of the underlying output
    function. This is also shown graphically in figure [3.11](#fig-gradients-pytorch-linear).
    But what happens if the underlying output function is nonlinear?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [3.1](#listing3-1) 和 [3.2](#listing3-2) 列表中，我们将线性模型拟合到已知为线性的数据分布。从输出中，我们可以看到这些模型收敛到了底层输出函数的良好近似。这也在图
    [3.11](#fig-gradients-pytorch-linear) 中以图形方式展示。但如果底层输出函数是非线性的呢？
- en: First, listing [3.3](#listing3-3) tries to use a linear model on a nonlinear
    data distribution. As expected (and demonstrated via the output as well as figure
    [3.12](#fig-gradients-pytorch-non-linear-using-linear)), this model does not do
    well, because we are using an inadequate model architecture. Further training
    will not help.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，[3.3](#listing3-3) 列表尝试在非线性数据分布上使用线性模型。正如预期（并通过输出以及图 [3.12](#fig-gradients-pytorch-non-linear-using-linear)
    所展示），这个模型表现不佳，因为我们使用了不合适的模型架构。进一步的训练将不会有所帮助。
- en: '![](../../OEBPS/Images/CH03_F11_Chaudhury.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F11_Chaudhury.png)'
- en: Figure 3.11 Linear approximation of linear data. By step 1,000, the model has
    more or less converged to the true underlying function.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 线性数据的线性近似。到第 1,000 步，模型已经或多或少收敛到真实的潜在函数。
- en: '![](../../OEBPS/Images/CH03_F12_Chaudhury.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F12_Chaudhury.png)'
- en: Figure 3.12 Linear approximation of nonlinear data. Clearly the model is not
    converging to anything close to the desired/true function. Our model architecture
    is inadequate.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12 非线性数据的线性近似。显然，模型并没有收敛到接近期望/真实函数的任何东西。我们的模型架构是不充分的。
- en: Listing 3.3 Linear approximation of nonlinear data
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 非线性数据的线性近似
- en: '[PRE4]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Generates random input training data
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成随机输入训练数据
- en: '② Generates training output: applies a known nonlinear function to the input
    and then perturbs it with noise'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ② 生成训练输出：将已知的非线性函数应用于输入，然后添加噪声
- en: ③ Trains a linear model as in listing 3.2
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 按照列表 3.2 的方式训练线性模型
- en: 'Here is the output:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE5]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Next, listing [3.4](#listing3-4) tries a nonlinear model. As expected (and demonstrated
    via the output as well as figure [3.13](#fig-gradients-pytorch-nonlinear)), the
    nonlinear model does well. In real-life problems, we usually assume nonlinearity
    and choose a model architecture accordingly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，[3.4](#listing3-4) 列表尝试使用非线性模型。正如预期（并通过输出以及图 [3.13](#fig-gradients-pytorch-non-linear)
    所展示），非线性模型表现良好。在现实生活中的问题中，我们通常假设非线性，并相应地选择模型架构。
- en: '![](../../OEBPS/Images/CH03_F13_Chaudhury.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F13_Chaudhury.png)'
- en: Figure 3.13 If we use a nonlinear model, it more or less converges to the true
    underlying function.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 如果我们使用非线性模型，它或多或少会收敛到真实的潜在函数。
- en: Listing 3.4 Nonlinear modeling with PyTorch
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 使用 PyTorch 进行非线性建模
- en: '[PRE6]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the output:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '[PRE7]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 3.5.4 A linear model for the cat brain in PyTorch
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5.4 PyTorch 中猫脑的线性模型
- en: In section [2.12.5](02.xhtml#python-overdet), we solved the cat-brain problem
    directly via pseudo-inverse. Now, let’s train a PyTorch model over the same dataset.
    As expected, the model parameters will converge to a solution close to that obtained
    by the pseudo-inverse technique this being a simple training dataset); but in
    the following listing, we demonstrate our first somewhat sophisticated PyTorch
    model.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2.12.5](02.xhtml#python-overdet) 节中，我们通过伪逆直接解决了猫脑问题。现在，让我们在相同的数据集上训练一个 PyTorch
    模型。正如预期，模型参数将收敛到一个接近伪逆技术获得的解（这是一个简单的训练数据集）；但在下面的列表中，我们展示了我们的第一个相对复杂的 PyTorch 模型。
- en: Listing 3.5 Our first realistic PyTorch model (solves the cat-brain problem)
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 我们第一个现实的 PyTorch 模型（解决猫脑问题）
- en: '[PRE8]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '① *X*, ![](../../OEBPS/Images/AR_y.png) created (see section [2.12.3](02.xhtml#subsec-over-under-determined-linsys))
    as per equation [2.22](02.xhtml#eq-lin-model) It is easy to verify that the solution
    to equation [2.22](02.xhtml#eq-lin-model) is roughly *w*[0] = 1, *w*[1] = 1, *b*
    = −1. But the equations are not consistent: no one solution perfectly fits all
    of them. We expect the learned model to be close to *y* = *x*[0] + *x*[1] − 1.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ① *X*，![图片](../../OEBPS/Images/AR_y.png)根据方程 [2.22](02.xhtml#eq-lin-model) 创建（见第
    [2.12.3](02.xhtml#subsec-over-under-determined-linsys) 节），它很容易验证方程 [2.22](02.xhtml#eq-lin-model)
    的解大致为 *w*[0] = 1，*w*[1] = 1，*b* = −1。但是方程不一致：没有一种解能完美地适合所有这些。我们期望学习到的模型接近 *y*
    = *x*[0] + *x*[1] − 1。
- en: ② Adds a column of all 1s to augment the data matrix *X*
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ② 向数据矩阵 *X* 添加一列全 1 的列以增强数据矩阵
- en: ③ Parameter is a type (subclass) of Torch Tensor suitable for model parameters
    (weights+bias).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 参数是 Torch Tensor 的类型（子类），适用于模型参数（权重+偏置）
- en: '④ Linear model: ![](../../OEBPS/Images/AR_y.png) = *X*![](../../OEBPS/Images/AR_w.png)(*X*
    is augmented, and ![](../../OEBPS/Images/AR_w.png) includes bias)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 线性模型：![图片](../../OEBPS/Images/AR_y.png) = *X*![图片](../../OEBPS/Images/AR_w.png)（*X*
    被增强，且 ![图片](../../OEBPS/Images/AR_w.png) 包含偏置）
- en: ⑤ Ready-made class for computing squared error loss/
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 预制类用于计算平方误差损失
- en: ⑥ Ready-made class for updating weights using the gradient of error
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 预制类用于使用误差梯度更新权重
- en: ⑦ Zeros out all partial derivatives
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 将所有偏导数置为零
- en: ⑧ Computes partial derivatives via autograd
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 通过自动微分计算偏导数
- en: ⑨ Updates the parameters using gradients computed in the backward() step
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用 backward() 步骤中计算的梯度更新参数
- en: 'The output is as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 3.6 Convex and nonconvex functions, and global and local minima
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 凸和非凸函数，以及全局和局部最小值
- en: 'A convex surface (see figure [3.14](#fig-convex_diagram)) has a single optimum
    (maximum/minimum): the global one.[²](#fn10) Wherever we are on such a surface,
    if we keep moving along the gradient in parameter space, we will eventually reach
    the global minimum. On the other hand, on a nonconvex surface, we might get stuck
    in a local minimum. For instance, in figure [3.14b](#fig-nonconvex_diagram), if
    we start at the point marked with the arrowed line indicating a gradient and move
    downward following the gradient, we will arrive at a local minimum. At the minimum,
    the gradient is zero, and we will never move out of that point.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 凸表面（见图 [3.14](#fig-convex_diagram)）有一个唯一的最佳点（最大值/最小值）：全局最佳点。[²](#fn10) 在这样的表面上，无论我们处于何处，如果我们沿着参数空间中的梯度移动，我们最终会达到全局最小值。另一方面，在非凸表面上，我们可能会陷入局部最小值。例如，在图
    [3.14b](#fig-nonconvex_diagram) 中，如果我们从箭头线标记的梯度点开始，沿着梯度向下移动，我们将到达一个局部最小值。在最小值处，梯度为零，我们将永远不会从这个点移动出去。
- en: '![](../../OEBPS/Images/CH03_F14a_Chaudhury.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F14a_Chaudhury.png)'
- en: (a) A convex function
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 凸函数
- en: '![](../../OEBPS/Images/CH03_F14b_Chaudhury.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F14b_Chaudhury.png)'
- en: (b) A nonconvex function
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 非凸函数
- en: Figure 3.14 Convex vs. nonconvex functions. Convex functions have only a global
    optimum (minimum or maximum), nolocal optimum. Following the gradient downward
    is guaranteed to reach the global minimum. Friendly error functions are convex.
    A nonconvex function has one or more local optima. Following the gradient may
    reach a local minimum and never discover the global minimum. Unfriendly error
    functions are nonconvex.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14 凸与非凸函数。凸函数只有一个全局最佳点（最小值或最大值），没有局部最佳点。沿着梯度向下移动可以保证达到全局最小值。友好的误差函数是凸的。非凸函数有一个或多个局部最佳点。沿着梯度移动可能会达到局部最小值，并且永远发现不了全局最小值。不友好的误差函数是非凸的。
- en: There was a time when researchers put a lot of effort into trying to avoid local
    minima. Special techniques (such as simulated annealing) were developed to avoid
    them. However, neural networks typically do not do anything special to deal with
    local minima and nonconvex functions. Often, the local minimum is good enough.
    Or we can retrain by starting from a different random point, which may help us
    escape the local minimum.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一段时间，研究人员投入大量精力试图避免局部最小值。为此开发了特殊技术（如模拟退火）来避免它们。然而，神经网络通常不会做任何特别的事情来处理局部最小值和非凸函数。通常，局部最小值已经足够好。或者我们可以从不同的随机点重新训练，这可能会帮助我们逃离局部最小值。
- en: 3.7 Convex sets and functions
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 凸集和函数
- en: In section [3.6](../Text/03.xhtml#sec-locglob-minima), we briefly encountered
    convex functions and how convexity tells us whether a function has local minima.
    In this section, we look at convex functions in more detail. In particular, we
    learn how to tell whether a given function is convex. We also discuss some important
    properties of convex functions that will come in handy later—for instance, when
    we study Jensen’s inequality in probability and statistics, in the appendix. We
    will mostly illustrate the ideas in 2D space, but they can be easily extended
    to higher dimensions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 [3.6](../Text/03.xhtml#sec-locglob-minima) 节中，我们简要介绍了凸函数以及凸性如何告诉我们函数是否有局部极小值。在本节中，我们将更详细地研究凸函数。特别是，我们学习如何判断给定的函数是否是凸函数。我们还讨论了一些凸函数的重要性质，这些性质将在以后很有用——例如，当我们研究概率和统计中的
    Jensen 不等式时，在附录中。我们将主要在二维空间中阐述这些思想，但它们可以很容易地扩展到更高维。
- en: 3.7.1 Convex sets
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.1 凸集
- en: Informally speaking, a set of points is said to be convex if and only if the
    straight line joining any pair of points in the set lies entirely within the set.
    For example, if we join any pair of points in the shaded region on the left-hand
    side of figure [3.15](#fig-convex-set) with a straight line, all points on that
    line will also be in the shaded region. This is illustrated by points A and B
    in the figure. The complete set of points in any such region constitutes a convex
    set.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 不正式地说，一个点集被称为凸集，当且仅当连接该集中任意两点之间的直线完全位于该集内。例如，如果我们用直线连接图 [3.15](#fig-convex-set)
    左侧阴影区域内的任意一对点，那么该直线上的所有点也将位于阴影区域内。这如图中的点 A 和 B 所示。任何此类区域中的所有点集构成一个凸集。
- en: '![](../../OEBPS/Images/CH03_F15_Chaudhury.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH03_F15_Chaudhury.png)'
- en: 'Figure 3.15 Convex and nonconvex sets. The points in the left-hand shaded region
    form a convex set.The line joining any pair of points in that shaded region lies
    entirely in the shaded region: for example, AB.The points in the right-hand shaded
    region form a nonconvex set. For instance, the line joining points C and D passes
    through a nonshaded region even though both end points belong to a shaded region.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15 凸集和非凸集。左侧阴影区域中的点构成一个凸集。连接该阴影区域内任意一对点的直线完全位于阴影区域内：例如，AB。右侧阴影区域中的点构成一个非凸集。例如，连接点
    C 和 D 的直线穿过非阴影区域，尽管两个端点都属于阴影区域。
- en: Conversely, a set of points is nonconvex if it contains at least one pair of
    points whose joining line contains a point not belonging to the set. For instance,
    the shaded region on the right-hand side of figure [3.15](#fig-convex-set) contains
    a pair of points C and D whose joining line passes through points not belonging
    to the shaded region.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，如果一个点集包含至少一对点，它们的连接线包含不属于该集的点，则该点集是非凸的。例如，图 [3.15](#fig-convex-set) 右侧的阴影区域包含一对点
    C 和 D，它们的连接线穿过不属于阴影区域的点。
- en: The boundary of a convex set is always a convex curve.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 凸集的边界总是凸曲线。
- en: 3.7.2 Convex curves and surfaces
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.2 凸曲线和曲面
- en: 'Consider a function *g*(*x*). Let’s pick any two points on the curve *y* =
    *g*(*x*): *A* ≡ (*x*[1], *y*[1] = *g*(*x*[1])) and *B* ≡ (*x*[2], *y*[2] = *g*(*x*[2])).
    Now consider the line segment *L* joining *A* and *B*. From section [2.8.1](02.xhtml#sec-multidim-line-eq)
    (equation [2.12](02.xhtml#eq-collinearity) and figure [2.8](02.xhtml#fig-multi-dim-lineeq)),
    we know that all points *C* on *L* can be expressed as a weighted average of the
    coordinates of *A* and *B*, with the sum of weights being 1. Thus, *C* ≡ (*α*[1]*x*[1]
    + *α*[2]*x*[2], *α*[1]*y*[1] + *α*[2]*y*[2]), where *α*[1] + *α*[2] = 1. Compare
    *C* with its corresponding point *D* on the curve, which has the same *X* coordinate:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个函数 *g*(*x*). 让我们选取曲线 *y* = *g*(*x*) 上的任意两点：*A* ≡ (*x*[1], *y*[1] = *g*(*x*[1]))
    和 *B* ≡ (*x*[2], *y*[2] = *g*(*x*[2])). 现在考虑连接 *A* 和 *B* 的线段 *L*. 从第 [2.8.1](02.xhtml#sec-multidim-line-eq)
    节（方程 [2.12](02.xhtml#eq-collinearity) 和图 [2.8](02.xhtml#fig-multi-dim-lineeq)），我们知道线段
    *L* 上的所有点 *C* 可以表示为 *A* 和 *B* 坐标的加权平均，权重之和为 1。因此，*C* ≡ (*α*[1]*x*[1] + *α*[2]*x*[2],
    *α*[1]*y*[1] + *α*[2]*y*[2])，其中 *α*[1] + *α*[2] = 1。比较 *C* 与其在曲线上的对应点 *D*，它具有相同的
    *X* 坐标：
- en: '*D* ≡ (*α*[1]*x*[1] + *α*[2]*x*[2], *g*(*α*[1]*x*[1] + *α*[2]*x*[2])).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '*D* ≡ (*α*[1]*x*[1] + *α*[2]*x*[2], *g*(*α*[1]*x*[1] + *α*[2]*x*[2])).'
- en: If and only if *g*(*x*) is a convex function, *C* will always be above *D*,
    or
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当 *g*(*x*) 是凸函数时，*C* 才会始终位于 *D* 上，或者
- en: '*α*[1]*y*[1] + *α*[2]*y*[2] = *α*[1]*g*(*x*[1]) + *α*[2]*g*(*x*[2]) ≥ *g*(*α*[1]*x*[1]
    + *α*[2]*x*[2])'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '*α*[1]*y*[1] + *α*[2]*y*[2] = *α*[1]*g*(*x*[1]) + *α*[2]*g*(*x*[2]) ≥ *g*(*α*[1]*x*[1]
    + *α*[2]*x*[2])'
- en: Viewed another way, if we drop a perpendicular to the *X*-axis from any point
    on the secant line joining a pair of points on the curve, that perpendicular will
    cut the curve at a lower point (that is, smaller in its *Y*-coordinate).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 从连接曲线上一对点的割线上的任何一点向 *X*-轴引垂线，这条垂线将在曲线的较低点（即在 *Y*-坐标上较小）处截断曲线。
- en: 'This is illustrated on the left-hand side of figure [3.19](#fig-convex-wt-avg)
    with the function *g*(*x*) = *x*² known to be convex) and *A* ≡ (−3,9) and *B*
    ≡ (5,25), *α*[1] = 0.3, *α*[2] = 0.7. It can be seen that the weighted average
    point *C* on the line lies above the corresponding point on the curve *D*. The
    right-hand side illustrates the nonconvex function *g*(*x*) = *x*³, with *A* ≡
    (−8,−512) and *B* ≡ (5,125), *α*[1] = 0.3, *α*[2] = 0.7. The figure shows one
    weighted average point (*C*) on the line joining points *A* and *B* on the curve:
    *C* lies below point *D* on the curve, which has the same *X*-coordinate.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图[3.19](#fig-convex-wt-avg)的左侧得到了说明，其中函数 *g*(*x*) = *x*²（已知是凸函数）和 *A* ≡ (−3,9)
    以及 *B* ≡ (5,25)，*α*[1] = 0.3，*α*[2] = 0.7。可以看出，线上的加权平均点 *C* 位于曲线对应点 *D* 的上方。右侧说明了非凸函数
    *g*(*x*) = *x*³，其中 *A* ≡ (−8,−512) 和 *B* ≡ (5,125)，*α*[1] = 0.3，*α*[2] = 0.7。图中显示了连接曲线上的点
    *A* 和 *B* 的线上的一个加权平均点 (*C*)：*C* 位于曲线上的点 *D* 下方，该点的 *X*-坐标相同。
- en: '![](../../OEBPS/Images/CH03_F16_Chaudhury.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F16_Chaudhury.png)'
- en: 'Figure 3.16 Convex and nonconvex curves. *A* and *B* are a pair of points on
    the curve. *C* = 0.3*A* + 0.7*B* is a weighted average of the coordinates of A
    and B, with weights summing to 1. *C* lies on the line joining *A* and *B*. The
    left-hand curve is convex: *C* lies above the corresponding curve point *D*. The
    right-hand curve is nonconvex: *C* lies below the corresponding curve point *D*.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.16 凸和非凸曲线。*A* 和 *B* 是曲线上的一个点对。*C* = 0.3*A* + 0.7*B* 是 A 和 B 坐标的加权平均，权重之和为
    1。*C* 位于连接 *A* 和 *B* 的线上。左侧的曲线是凸的：*C* 位于对应的曲线点 *D* 之上。右侧的曲线是非凸的：*C* 位于对应的曲线点 *D*
    之下。
- en: We need not restrict ourselves to two points. We can take the weighted average
    of an arbitrary number of points on the curve, with the weights summing to one.
    The point corresponding to the weighted average will lie above the curve (that
    is, above the point on the curve with the same *X*-coordinate). The idea also
    extends to higher dimensions, as discussed next.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必限制自己只考虑两个点。我们可以对曲线上任意数量的点进行加权平均，权重之和为 1。对应于加权平均的点将位于曲线之上（即位于具有相同 *X*-坐标的曲线上点之上）。这一思想也扩展到更高维度，如以下所述。
- en: Definition 1
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 1
- en: In general, a multidimensional function *g*(![](../../OEBPS/Images/AR_x.png))
    is convex if and only if
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，一个多维函数 *g*(![](../../OEBPS/Images/AR_x.png)) 是凸的，当且仅当
- en: Given an arbitrary set of points on the function surface (curve, if the function
    is 1D), (![](../../OEBPS/Images/AR_x.png)[1], *g*(![](../../OEBPS/Images/AR_x.png)[1])),
    (![](../../OEBPS/Images/AR_x.png)[2], *g*(![](../../OEBPS/Images/AR_x.png)[2])),
    ⋯, (*![](../../OEBPS/Images/AR_x.png)[n]*, *g*(*![](../../OEBPS/Images/AR_x.png)[n]*)),
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定函数表面（如果函数是一维的，则为曲线）上的任意一组点（(![](../../OEBPS/Images/AR_x.png)[1]，*g*(![](../../OEBPS/Images/AR_x.png)[1]))，（![](../../OEBPS/Images/AR_x.png)[2]，*g*(![](../../OEBPS/Images/AR_x.png)[2]))，⋯，（*![](../../OEBPS/Images/AR_x.png)[n]*，*g*(*![](../../OEBPS/Images/AR_x.png)[n]*))，
- en: And given an arbitrary set of *n* weights *α*[1], *α*[2], ⋯, *α[n]* that sum
    to 1 (that is, Σ*[i]^n*[= 1] *α[i]* = 1),
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定一个任意权重集 *α*[1]，*α*[2]，⋯，*α[n]*，它们的和为 1（即，Σ*[i]^n*[= 1] *α[i]* = 1），
- en: '*The weighted sum of the function outputs exceeds or equals the function output
    on the weighted sums*:'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*函数输出的加权总和超过或等于加权总和的函数输出*：'
- en: '![](../../OEBPS/Images/eq_03-11.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-11.png)'
- en: Equation 3.11
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 3.11
- en: A little thought will reveal that definition 1 implies that convex curves always
    curl upward and/or rightward everywhere. This leads to another equivalent definition
    of convexity.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细思考可以发现，定义 1 意味着凸曲线始终向上和/或向右卷曲。这导致凸性的另一个等价定义。
- en: Definition 2
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 2
- en: In general, a multidimensional function *g*(![](../../OEBPS/Images/AR_x.png))
    is convex if and only if
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，一个多维函数 *g*(![](../../OEBPS/Images/AR_x.png)) 是凸的，当且仅当
- en: 'A 1D function *g*(*x*) is convex if and only if its curvature is positive everywhere:'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一维函数 *g*(*x*) 是凸的，当且仅当它在任何地方都具有正曲率：
- en: '![](../../OEBPS/Images/eq_03-12.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_03-12.png)'
- en: Equation 3.12
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 3.12
- en: A multidimensional function *g*(![](../../OEBPS/Images/AR_x.png)) is convex
    if and only if its Hessian matrix (see section [3.4.2](#sec-taylor-multidim),
    equation [3.9](../Text/03.xhtml#eq-hessian)) is positive semi-definite that is,
    all the eigenvalues of the Hessian matrix are greater than or equal to zero).
    This is just the multidimensional analog of equation [3.12](#eq-convexity-2nd-deriv).
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多维函数 *g*(![](../../OEBPS/Images/AR_x.png)) 是凸的，当且仅当其Hessian矩阵（见[3.4.2](#sec-taylor-multidim)，方程[3.9](../Text/03.xhtml#eq-hessian)）是正半定的，即Hessian矩阵的所有特征值都大于或等于零。这正是方程[3.12](#eq-convexity-2nd-deriv)的多维类似物。
- en: One subtle point to note is that if the second derivative is negative everywhere
    or the Hessian is negative semi-definite, the curve or surface is said to be *concave*.
    This is different from nonconvex curves, where the second derivative is positive
    in some places and negative in others. The negative of a concave function is a
    convex function. But the negative of a nonconvex function is again nonconvex.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个微妙之处是，如果二阶导数在所有地方都是负的，或者Hessian矩阵是负半定的，则曲线或表面被称为*凹的*。这与非凸曲线不同，在非凸曲线中，二阶导数在某些地方是正的，在其他地方是负的。凹函数的负值是凸函数。但非凸函数的负值仍然是非凸的。
- en: A function that curves upward everywhere always lies above its tangent. This
    leads to another equivalent definition of a convex function.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何地方都向上弯曲的函数始终位于其切线之上。这导致凸函数的另一个等价定义。
- en: Definition 3
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 定义3
- en: In general, a multidimensional function *g*(![](../../OEBPS/Images/AR_x.png))
    is convex if and only if
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个多维函数 *g*(![](../../OEBPS/Images/AR_x.png)) 是凸的，当且仅当
- en: A function *g*(*x*) is convex if and only if all the points on the curve *S*
    ≡ (*x*, *g*(*x*)) lie above the tangent line *T* at any point *A* on *S*, with
    *S* touching *T* only at *A*.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个函数 *g*(*x*) 是凸的，当且仅当曲线 *S* ≡ (*x*, *g*(*x*)) 上的所有点在任何点 *A* 处都位于切线 *T* 之上，且
    *S* 仅在 *A* 处接触 *T*。
- en: A function *g*(![](../../OEBPS/Images/AR_x.png)) is convex if and only if all
    the points on the surface *S* ≡ (![](../../OEBPS/Images/AR_x.png), *g*(![](../../OEBPS/Images/AR_x.png)))
    lie above the tangent plane *T* at any point *A* on *S*, with *S* touching *T*
    only at *A*.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个函数 *g*(![](../../OEBPS/Images/AR_x.png)) 是凸的，当且仅当表面 *S* ≡ (![](../../OEBPS/Images/AR_x.png),
    *g*(![](../../OEBPS/Images/AR_x.png))) 上的所有点在任何点 *A* 处都位于切平面 *T* 之上，且 *S* 仅在 *A*
    处接触 *T*。
- en: This is illustrated in figure [3.17](#fig-convex-tangent).
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这在图[3.17](#fig-convex-tangent)中得到了说明。
- en: '![](../../OEBPS/Images/CH03_F17_Chaudhury.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH03_F17_Chaudhury.png)'
- en: 'Figure 3.17 The left-hand curve is convex. If we draw a tangent line at any
    point *A* on the curve, the entire curve is *above* the tangent line, touching
    it only at *A*. The right-hand cuve is nonconvex: part of the curve lies above
    the tangent and part of it below.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 左侧的曲线是凸的。如果我们曲线上的任何点 *A* 处画一条切线，整个曲线都位于切线之上，仅在 *A* 处接触切线。右侧的曲线是非凸的：曲线的一部分位于切线之上，另一部分位于切线之下。
- en: 3.7.3 Convexity and the Taylor series
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.3 凸性与泰勒级数
- en: In section [3.4.1](#sec-taylor-onedim), equation [3.7](#eq-taylor-onedim), we
    saw the one-dimensional Taylor expansion for a function in the neighborhood of
    a point *x*. If we retain the terms in the Taylor expansion only up to the first
    derivative and ignore all subsequent terms, that is equivalent to approximating
    the function at *x* with its tangent at *x* (see figure [3.17](#fig-convex-tangent)).
    This is the linear approximation to the curve. If we retain one more term (that
    is, up to the second derivative), we get the quadratic approximation to the curve.
    If the second derivative of the function is always positive (as in convex functions),
    the quadratic approximation to the function will always be greater than or equal
    to the linear approximation. In other words, locally, the curve will curve so
    that it lies above the tangent. This connects the second derivative definition
    (definition 2) with the tangent definition (definition 3) of convexity.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.4.1](#sec-taylor-onedim)节中，方程[3.7](#eq-taylor-onedim)，我们看到了函数在点 *x* 附近的单变量泰勒展开。如果我们只保留泰勒展开中的直到一阶导数的项，并忽略所有后续项，那么这相当于用
    *x* 处的切线来近似 *x* 处的函数（见图[3.17](#fig-convex-tangent)）。这是曲线的线性近似。如果我们保留一个额外的项（即，直到二阶导数），我们得到曲线的二次近似。如果函数的二阶导数始终为正（如凸函数），则函数的二次近似将始终大于或等于线性近似。换句话说，局部地，曲线将弯曲，使其位于切线之上。这将二阶导数定义（定义2）与切线定义（定义3）的凸性联系起来。
- en: 3.7.4 Examples of convex functions
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7.4 凸函数的例子
- en: The function *g*(*x*) = *x*² is convex. The easiest way to verify this is to
    compute *d*²*g*/*dx*² = *d*2*x*/*dx* = 2, which is always positive. In fact, any
    even power of *x*, *g*(*x*) = *x*^(2*n*) for an integer *n*, such as *x*⁴ or *x*⁶,
    is convex. *g*(*x*) = *e^x* is also convex. This can be easily verified by taking
    its second derivative. *g*(*x*) = *logx* is concave. Hence, *g*(*x*) = −*logx*
    is convex.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 *g*(*x*) = *x*² 是凸函数。验证这一点最简单的方法是计算 *d*²*g*/*dx*² = *d*2*x*/*dx* = 2，这始终是正的。事实上，任何偶数次幂的
    *x*，例如 *x*⁴ 或 *x*⁶，都是凸函数。*g*(*x*) = *e^x* 也是凸函数。这可以通过计算其二阶导数来轻松验证。*g*(*x*) = *logx*
    是凹函数。因此，*g*(*x*) = −*logx* 是凸函数。
- en: Multiplication by a positive scalar preserves convexity. The sum of convex functions
    is also a convex function.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 乘以正标量保持凸性。凸函数的和也是一个凸函数。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We would like to leave you with the following mental pictures from this chapter:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您能从本章留下以下心理图像：
- en: Inputs for a machine learning problem can be viewed as vectors or, equivalently,
    points in a high-dimensional feature space. Classification is nothing but separating
    clusters of points belonging to individual classes in this space.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习问题的输入可以被视为向量，或者等价地，是高维特征空间中的点。分类不过是将属于单个类别的点簇在这个空间中分离出来。
- en: A classifier is can be viewed geometrically as the hypersurface (aka decision
    boundary) in the high-dimensional feature space, separating the point clusters
    corresponding to individual classes. During training, we collect sample inputs
    with known classes and identify the surface that best separates the corresponding
    points. During inferencing, given an unknown input, we determine which side of
    the decision boundary this point lies in—this tells us the class.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分类器可以从几何上视为高维特征空间中的超曲面（也称为决策边界），它将对应于单个类别的点簇分开。在训练过程中，我们收集具有已知类别的样本输入并识别最佳分离相应点的表面。在推理过程中，给定一个未知输入，我们确定这个点位于决策边界的哪一侧——这告诉我们类别。
- en: For two-class classifiers (aka binary classifiers), if we plug in the point
    in the function for the classifier hypersurface, the sign of the corresponding
    output yields the class.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于二分类器（也称为二元分类器），如果我们把点代入分类器超平面的函数中，相应输出的符号就给出了类别。
- en: To compute the hypersurface decision boundary that best separates the training
    data, we first choose a parametric function family to model this surface (for
    example, a hyperplane for simple problems). Then we estimate the optimal parameter
    values that best separate the training data, usually in an iterative fashion.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了计算最佳分离训练数据的超曲面决策边界，我们首先选择一个参数函数族来模拟这个表面（例如，对于简单问题，使用超平面）。然后我们估计最佳参数值，以最佳方式分离训练数据，通常以迭代的方式进行。
- en: To estimate the parameter values that optimally separates the training data,
    we define a loss function that measures the difference between the model output
    and the known desired output over the entire training dataset. Then, starting
    from random initial values, we iteratively adjust the parameter values so that
    the loss value decreases progressively.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了估计最佳分离训练数据的参数值，我们定义一个损失函数来衡量模型输出与整个训练数据集中已知期望输出之间的差异。然后，从随机初始值开始，我们迭代调整参数值，使损失值逐渐降低。
- en: At every iteration, the adjustment to the parameter values that optimally reduces
    the loss is estimated by computing the gradient of the loss function.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每次迭代中，通过计算损失函数的梯度来估计最优减少损失的参数值的调整。
- en: The gradient of a multidimensional function identifies the direction in the
    parameter space corresponding to the maximum change in the function. Thus, the
    gradient of the loss function identifies the direction in which we can adjust
    the parameters to maximally decrease the loss.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维函数的梯度确定了参数空间中对应函数最大变化的方向。因此，损失函数的梯度确定了我们可以调整参数以最大程度减少损失的方向。
- en: 'The gradient is zero at the maximum or minimum point of a function, which is
    always a point of inflection. This can be used to recognize when we have reached
    the minimum. However, in practice, in machine learning we often do an early stop:
    terminate training iterations when the loss is sufficiently low.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数的最大值或最小值处的梯度为零，这始终是一个拐点。这可以用来识别我们何时达到最小值。然而，在实践中，在机器学习中，我们经常进行早期停止：当损失足够低时终止训练迭代。
- en: A multidimensional Taylor series can be used to create local approximations
    to a smooth function in the neighborhood of a point. The function is expressed
    in terms of the displacement from the point, the first-order derivatives (gradient),
    second-order derivatives Hessian matrix), and so on. This can be used to make
    higher-accuracy approximations to the change in loss value resulting from a displacement
    in the parameter space.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多维泰勒级数可用于在一点邻域内创建对光滑函数的局部近似。该函数用从该点的位移、一阶导数（梯度）、二阶导数（海森矩阵）等来表示。这可以用来对参数空间中位移引起的损失值变化进行更高精度的近似。
- en: Loss functions can be*convex* or *nonconvex*. In a convex function, there is
    no local minimum, only a single global minimum. Hence, gradient descent is guaranteed
    to converge to the global minimum. A nonconvex function can have both a local
    and a global minimum. So, gradient-based descent may get stuck in a local minimum.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数可以是*凸的*或*非凸的*。在凸函数中，没有局部最小值，只有一个全局最小值。因此，梯度下降法保证收敛到全局最小值。非凸函数可以同时具有局部和全局最小值。所以，基于梯度的下降法可能会陷入局部最小值。
- en: '* * *'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹  If the change in a quantity such as *w* is infinitesimally small, we use
    the symbol *dw* to denote the change. If the change is small but not infinitesimally
    so, we use the symbol *δw*. [↩](#fnref9)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ¹  如果某个量如 *w* 的变化极小，我们使用符号 *dw* 来表示该变化。如果变化小但不是极小的，我们使用符号 *δw*。[↩](#fnref9)
- en: ²  Although the theory applies to either optimum, maximum or minimum, for brevity’s
    sake, here we will only talk in terms of the minimum [↩](#fnref10)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ²  虽然该理论适用于最优、最大或最小值，但为了简洁起见，这里我们只谈论最小值[↩](#fnref10)
