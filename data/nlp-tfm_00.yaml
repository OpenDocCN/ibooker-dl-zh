- en: Foreword
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'A miracle is taking place as you read these lines: the squiggles on this page
    are transforming into words and concepts and emotions as they navigate their way
    through your cortex. My thoughts from November 2021 have now successfully invaded
    your brain. If they manage to catch your attention and survive long enough in
    this harsh and highly competitive environment, they may have a chance to reproduce
    again as you share these thoughts with others. Thanks to language, thoughts have
    become airborne and highly contagious brain germs—and no vaccine is coming.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这些文字时，一个奇迹正在发生：这页上的涂鸦正在在你的大脑皮层中转化为单词、概念和情感。我的2021年11月的思想现在已成功侵入你的大脑。如果它们设法引起你的注意并在这个严酷而竞争激烈的环境中存活足够长的时间，它们可能有机会再次通过你与他人分享这些思想而繁殖。由于语言，思想已经变得空气中传播并且高度传染的大脑细菌——而且没有疫苗即将出现。
- en: 'Luckily, most brain germs are harmless,^([1](foreword01.xhtml#idm46238741809312))
    and a few are wonderfully useful. In fact, humanity’s brain germs constitute two
    of our most precious treasures: knowledge and culture. Much as we can’t digest
    properly without healthy gut bacteria, we cannot think properly without healthy
    brain germs. Most of your thoughts are not actually yours: they arose and grew
    and evolved in many other brains before they infected you. So if we want to build
    intelligent machines, we will need to find a way to infect them too.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，大多数大脑细菌是无害的，而一些则非常有用。事实上，人类的大脑细菌构成了我们最宝贵的两个财富：知识和文化。就像我们没有健康的肠道细菌就无法正常消化一样，没有健康的大脑细菌，我们也无法正常思考。你的大部分想法实际上并不是你自己的：它们在感染你之前在许多其他大脑中产生、生长和演变。因此，如果我们想要构建智能机器，我们将需要找到一种方法来感染它们。
- en: 'The good news is that another miracle has been unfolding over the last few
    years: several breakthroughs in deep learning have given birth to powerful language
    models. Since you are reading this book, you have probably seen some astonishing
    demos of these language models, such as GPT-3, which given a short prompt such
    as “a frog meets a crocodile” can write a whole story. Although it’s not quite
    Shakespeare yet, it’s sometimes hard to believe that these texts were written
    by an artificial neural network. In fact, GitHub’s Copilot system is helping me
    write these lines: you’ll never know how much I really wrote.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，在过去几年中，另一个奇迹已经在发生：深度学习的几项突破为强大的语言模型奠定了基础。由于你正在阅读这本书，你可能已经看到了一些令人惊讶的这些语言模型的演示，比如GPT-3，它可以根据“一只青蛙遇到了一只鳄鱼”这样的简短提示来写一个完整的故事。虽然它还不是莎士比亚，但有时很难相信这些文本是由人工神经网络写的。事实上，GitHub的Copilot系统正在帮助我写这些文字：你永远不会知道我真正写了多少。
- en: The revolution goes far beyond text generation. It encompasses the whole realm
    of natural language processing (NLP), from text classification to summarization,
    translation, question answering, chatbots, natural language understanding (NLU),
    and more. Wherever there’s language, speech or text, there’s an application for
    NLP. You can already ask your phone for tomorrow’s weather, or chat with a virtual
    help desk assistant to troubleshoot a problem, or get meaningful results from
    search engines that seem to truly understand your query. But the technology is
    so new that the best is probably yet to come.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 这一革命远不止于文本生成。它涵盖了自然语言处理（NLP）的整个领域，从文本分类到摘要、翻译、问答、聊天机器人、自然语言理解（NLU）等等。无论何处有语言、语音或文本，都有NLP的应用。你已经可以向你的手机询问明天的天气，或者与虚拟帮助台助手聊天以解决问题，或者从似乎真正理解你的查询的搜索引擎中获得有意义的结果。但是这项技术是如此新颖，以至于最好的可能还在未来。
- en: 'Like most advances in science, this recent revolution in NLP rests upon the
    hard work of hundreds of unsung heroes. But three key ingredients of its success
    do stand out:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数科学进步一样，NLP领域的这一最新革命也是建立在数百位无名英雄的辛勤工作之上的。但是它的成功有三个关键因素是显而易见的：
- en: The *transformer* is a neural network architecture proposed in 2017 in a groundbreaking
    paper called [“Attention Is All You Need”](https://arxiv.org/abs/1706.03762),
    published by a team of Google researchers. In just a few years it swept across
    the field, crushing previous architectures that were typically based on recurrent
    neural networks (RNNs). The Transformer architecture is excellent at capturing
    patterns in long sequences of data and dealing with huge datasets—so much so that
    its use is now extending well beyond NLP, for example to image processing tasks.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变压器*是一种神经网络架构，它是在2017年由一组谷歌研究人员发表的一篇开创性论文[“注意力机制是你所需要的一切”](https://arxiv.org/abs/1706.03762)中提出的。在短短几年内，它席卷了整个领域，击败了通常基于循环神经网络（RNNs）的先前架构。变压器架构在捕捉长序列数据中的模式和处理庞大数据集方面表现出色，以至于它的使用现在已经远远超出了NLP领域，例如扩展到图像处理任务。'
- en: 'In most projects, you won’t have access to a huge dataset to train a model
    from scratch. Luckily, it’s often possible to download a model that was *pretrained*
    on a generic dataset: all you need to do then is fine-tune it on your own (much
    smaller) dataset. Pretraining has been mainstream in image processing since the
    early 2010s, but in NLP it was restricted to contextless word embeddings (i.e.,
    dense vector representations of individual words). For example, the word “bear”
    had the same pretrained embedding in “teddy bear” and in “to bear.” Then, in 2018,
    several papers proposed full-blown language models that could be pretrained and
    fine-tuned for a variety of NLP tasks; this completely changed the game.'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数项目中，你可能无法访问一个庞大的数据集来从头开始训练模型。幸运的是，通常可以下载一个在通用数据集上*预训练*的模型：然后你所需要做的就是在自己的（小得多的）数据集上进行微调。自2010年代初以来，在图像处理领域，预训练已经成为主流，但在自然语言处理（NLP）领域，它仅限于无上下文的词嵌入（即，个别词的密集向量表示）。例如，“bear”这个词在“teddy
    bear”和“to bear”中具有相同的预训练嵌入。然后，在2018年，几篇论文提出了完整的语言模型，可以在各种NLP任务中进行预训练和微调；这彻底改变了游戏规则。
- en: '*Model hubs* like Hugging Face’s have also been a game-changer. In the early
    days, pretrained models were just posted anywhere, so it wasn’t easy to find what
    you needed. Murphy’s law guaranteed that PyTorch users would only find TensorFlow
    models, and vice versa. And when you did find a model, figuring out how to fine-tune
    it wasn’t always easy. This is where Hugging Face’s Transformers library comes
    in: it’s open source, it supports both TensorFlow and PyTorch, and it makes it
    easy to download a state-of-the-art pretrained model from the Hugging Face Hub,
    configure it for your task, fine-tune it on your dataset, and evaluate it. Use
    of the library is growing quickly: in Q4 2021 it was used by over five thousand
    organizations and was installed using `pip` over four million times per month.
    Moreover, the library and its ecosystem are expanding beyond NLP: image processing
    models are available too. You can also download numerous datasets from the Hub
    to train or evaluate your models.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型中心*，如Hugging Face的，也是一个改变游戏规则的存在。在早期，预训练模型随处可见，所以很难找到你需要的东西。墨菲定律保证了PyTorch用户只能找到TensorFlow模型，反之亦然。而且，当你找到一个模型时，弄清楚如何对其进行微调并不总是容易的。这就是Hugging
    Face的Transformers库发挥作用的地方：它是开源的，支持TensorFlow和PyTorch，而且可以轻松地从Hugging Face Hub下载最先进的预训练模型，为你的任务配置它，对你的数据集进行微调，并进行评估。该库的使用量正在迅速增长：2021年第四季度，它被超过五千家组织使用，并且每月使用`pip`安装超过四百万次。此外，该库及其生态系统正在扩展到NLP之外：图像处理模型也可用。您还可以从Hub下载大量数据集来训练或评估您的模型。'
- en: 'So what more can you ask for? Well, this book! It was written by open source
    developers at Hugging Face—including the creator of the Transformers library!—and
    it shows: the breadth and depth of the information you will find in these pages
    is astounding. It covers everything from the Transformer architecture itself,
    to the Transformers library and the entire ecosystem around it. I particularly
    appreciated the hands-on approach: you can follow along in Jupyter notebooks,
    and all the code examples are straight to the point and simple to understand.
    The authors have extensive experience in training very large transformer models,
    and they provide a wealth of tips and tricks for getting everything to work efficiently.
    Last but not least, their writing style is direct and lively: it reads like a
    novel.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你还能要求什么呢？嗯，就是这本书！它是由Hugging Face的开源开发人员撰写的，包括Transformers库的创造者！——这一点可以看出：你将在这些页面中找到的信息的广度和深度令人震惊。它涵盖了从Transformer架构本身到Transformers库及其周围整个生态系统的一切。我特别欣赏这种实践方法：你可以在Jupyter笔记本中跟着做，所有的代码示例都直截了当，易于理解。作者在训练非常大的变压器模型方面拥有丰富的经验，并提供了大量的技巧和窍门，以确保一切都能高效运行。最后但同样重要的是，他们的写作风格直接而生动：读起来就像一部小说。
- en: In short, I thoroughly enjoyed this book, and I’m certain you will too. Anyone
    interested in building products with state-of-the-art language-processing features
    needs to read it. It’s packed to the brim with all the right brain germs!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我非常喜欢这本书，我相信你也会喜欢。任何对使用最先进的语言处理功能构建产品感兴趣的人都需要阅读它。它充满了所有正确的大脑细菌！
- en: Aurélien Géron
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Aurélien Géron
- en: November 2021, Auckland, NZ
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年11月，新西兰奥克兰
- en: ^([1](foreword01.xhtml#idm46238741809312-marker)) For brain hygiene tips, see
    CGP Grey’s [excellent video on memes](https://youtu.be/rE3j_RHkqJc).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](foreword01.xhtml#idm46238741809312-marker)) 对于大脑卫生提示，请参阅CGP Grey的[关于模因的优秀视频](https://youtu.be/rE3j_RHkqJc)。
