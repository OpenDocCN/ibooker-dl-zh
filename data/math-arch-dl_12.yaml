- en: 13 Fully Bayes model parameter estimation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 å…¨è´å¶æ–¯æ¨¡å‹å‚æ•°ä¼°è®¡
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–
- en: Fully Bayes parameter estimation for unsupervised modeling
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ— ç›‘ç£å»ºæ¨¡çš„å…¨è´å¶æ–¯å‚æ•°ä¼°è®¡
- en: Injecting prior belief into parameter estimation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°†å…ˆéªŒä¿¡å¿µæ³¨å…¥å‚æ•°ä¼°è®¡
- en: Estimating Gaussian likelihood parameters with known or unknown mean and precision
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å·²çŸ¥æˆ–æœªçŸ¥å‡å€¼å’Œç²¾åº¦çš„é«˜æ–¯ä¼¼ç„¶å‚æ•°ä¼°è®¡
- en: Normal-gamma and Wishart distributions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ­£æ€-ä¼½é©¬å’ŒWishartåˆ†å¸ƒ
- en: 'Suppose we have a data set of interest: say, all images containing a cat. If
    we represent images as points in a high-dimensional feature space, our data set
    of interest forms a subspace of that feature space. Now we want to create an *unsupervised*
    model for our data set of interest. This means we want to identify a probability
    density function *p*(![](../../OEBPS/Images/AR_x.png)) whose sample cloud (the
    set of points obtained by repeatedly sampling the probability distribution many
    times) largely overlaps our subspace of interest. Of course, we do not know the
    exact subspace of interest, but we have collected a set of samples *X* from the
    data set of interest: that is, the training data. We can use the point cloud for
    *X* as a surrogate for the unknown subspace of interest. Thus, we are essentially
    trying to identify a probability density function *p*(![](../../OEBPS/Images/AR_x.png))
    whose sample cloud, by and large, overlaps *X*.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ„Ÿå…´è¶£çš„æ•°æ®é›†ï¼šæ¯”å¦‚è¯´ï¼Œæ‰€æœ‰åŒ…å«çŒ«çš„å›¾åƒã€‚å¦‚æœæˆ‘ä»¬æŠŠå›¾åƒè¡¨ç¤ºä¸ºé«˜ç»´ç‰¹å¾ç©ºé—´ä¸­çš„ç‚¹ï¼Œé‚£ä¹ˆæˆ‘ä»¬æ„Ÿå…´è¶£çš„æ•°æ®é›†å°±æ„æˆäº†è¯¥ç‰¹å¾ç©ºé—´çš„ä¸€ä¸ªå­ç©ºé—´ã€‚ç°åœ¨æˆ‘ä»¬æƒ³è¦ä¸ºæˆ‘ä»¬çš„æ•°æ®é›†åˆ›å»ºä¸€ä¸ª*æ— ç›‘ç£*æ¨¡å‹ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æƒ³è¦è¯†åˆ«ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°
    *p*(![](../../OEBPS/Images/AR_x.png))ï¼Œå…¶æ ·æœ¬äº‘ï¼ˆé€šè¿‡å¤šæ¬¡é‡å¤é‡‡æ ·æ¦‚ç‡åˆ†å¸ƒè€Œè·å¾—çš„ç‚¹çš„é›†åˆï¼‰ä¸æˆ‘ä»¬çš„æ„Ÿå…´è¶£å­ç©ºé—´å¤§è‡´é‡å ã€‚å½“ç„¶ï¼Œæˆ‘ä»¬ä¸çŸ¥é“æ„Ÿå…´è¶£çš„å­ç©ºé—´çš„ç¡®åˆ‡ä½ç½®ï¼Œä½†æˆ‘ä»¬å·²ç»ä»æ„Ÿå…´è¶£çš„æ•°æ®é›†ä¸­æ”¶é›†äº†ä¸€ç»„æ ·æœ¬
    *X*ï¼šå³è®­ç»ƒæ•°æ®ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ *X* çš„ç‚¹äº‘ä½œä¸ºæœªçŸ¥æ„Ÿå…´è¶£å­ç©ºé—´çš„ä¸€ä¸ªæ›¿ä»£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ¬è´¨ä¸Šæ˜¯åœ¨å°è¯•è¯†åˆ«ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•° *p*(![](../../OEBPS/Images/AR_x.png))ï¼Œå…¶æ ·æœ¬äº‘å¤§è‡´ä¸
    *X* é‡å ã€‚
- en: Once we have the model *p*(![](../../OEBPS/Images/AR_x.png)), we can use it
    to generate more data samples; these will be computer-generated cat images. This
    is generative modeling. Also, given a new image ![](../../OEBPS/Images/AR_a.png),
    we can estimate the probability of it being an image of a cat by evaluating *p*(![](../../OEBPS/Images/AR_a.png)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æœ‰äº†æ¨¡å‹ *p*(![](../../OEBPS/Images/AR_x.png))ï¼Œæˆ‘ä»¬å°±å¯ä»¥ç”¨å®ƒæ¥ç”Ÿæˆæ›´å¤šçš„æ•°æ®æ ·æœ¬ï¼›è¿™äº›å°†æ˜¯è®¡ç®—æœºç”Ÿæˆçš„çŒ«å›¾åƒã€‚è¿™æ˜¯ç”Ÿæˆæ¨¡å‹ã€‚æ­¤å¤–ï¼Œç»™å®šä¸€ä¸ªæ–°å›¾åƒ
    ![](../../OEBPS/Images/AR_a.png)ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è¯„ä¼° *p*(![](../../OEBPS/Images/AR_a.png))
    æ¥ä¼°è®¡å®ƒæˆä¸ºçŒ«å›¾åƒçš„æ¦‚ç‡ã€‚
- en: '13.1 Fully Bayes estimation: An informal introduction'
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 å…¨è´å¶æ–¯ä¼°è®¡ï¼šéæ­£å¼ä»‹ç»
- en: 'Letâ€™s recap Bayesâ€™ theorem:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾è´å¶æ–¯å®šç†ï¼š
- en: '![](../../OEBPS/Images/eq_13-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-01](../../OEBPS/Images/eq_13-01.png)'
- en: Equation 13.1
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 13.1
- en: 'Here, *X* = {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],â‹¯}
    denotes the training data set. Our ultimate goal is to identify the likelihood
    function *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*). Estimating the likelihood
    function has two aspects: selecting the function family and estimating the parameters.
    We usually preselect the family from our knowledge of the problem and then estimate
    the model parameters. For instance, the family for our model likelihood function
    might be Gaussian: *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*) = ğ’©(![](../../OEBPS/Images/AR_x.png);
    ![](../../OEBPS/Images/AR_micro.png),Â **Î£**) as before, the semicolon separates
    the model variables from model parameters). Then *Î¸* = {![](../../OEBPS/Images/AR_micro.png),Â **Î£**}
    are the model parameters to estimate. We estimate *Î¸* such that the overall likelihood
    *p*(*X*|*Î¸*) = âˆ*[iÂ ]p*(![](../../OEBPS/Images/AR_x.png)*[i]*|*Î¸*) best fits the
    training data *X*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ*X* = {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],â‹¯}
    è¡¨ç¤ºè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„æœ€ç»ˆç›®æ ‡æ˜¯è¯†åˆ«ä¼¼ç„¶å‡½æ•° *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*). ä¼°è®¡ä¼¼ç„¶å‡½æ•°æœ‰ä¸¤ä¸ªæ–¹é¢ï¼šé€‰æ‹©å‡½æ•°æ—å’Œä¼°è®¡å‚æ•°ã€‚æˆ‘ä»¬é€šå¸¸æ ¹æ®å¯¹é—®é¢˜çš„äº†è§£é¢„å…ˆé€‰æ‹©å‡½æ•°æ—ï¼Œç„¶åä¼°è®¡æ¨¡å‹å‚æ•°ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬æ¨¡å‹ä¼¼ç„¶å‡½æ•°çš„å‡½æ•°æ—å¯èƒ½æ˜¯é«˜æ–¯åˆ†å¸ƒï¼š*p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)
    = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),Â **Î£**)ï¼Œæ­£å¦‚ä¹‹å‰æ‰€è¿°ï¼Œåˆ†å·å°†æ¨¡å‹å˜é‡ä¸æ¨¡å‹å‚æ•°åˆ†å¼€ï¼‰ã€‚ç„¶å
    *Î¸* = {![](../../OEBPS/Images/AR_micro.png),Â **Î£**} æ˜¯éœ€è¦ä¼°è®¡çš„æ¨¡å‹å‚æ•°ã€‚æˆ‘ä»¬ä¼°è®¡ *Î¸* ä»¥ä½¿æ•´ä½“ä¼¼ç„¶
    *p*(*X*|*Î¸*) = âˆ*[iÂ ]p*(![](../../OEBPS/Images/AR_x.png)*[i]*|*Î¸*) æœ€å¥½åœ°æ‹Ÿåˆè®­ç»ƒæ•°æ® *X*ã€‚
- en: We want to re-emphasize the mental picture that *best fit* implies that the
    sample cloud of the likelihood function (repeated samples from *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*))
    largely overlaps the training data set *X*. For the Gaussian case, this implies
    that the mean ![](../../OEBPS/Images/AR_micro.png) should fall at a place where
    there is a very high concentration of training data points, and the covariance
    matrix Î£ should be such that the elliptical base of the likelihood function tightly
    contains as many training data points as possible.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³å†æ¬¡å¼ºè°ƒ*æœ€ä½³æ‹Ÿåˆ*æ„å‘³ç€ä¼¼ç„¶å‡½æ•°çš„æ ·æœ¬äº‘ï¼ˆä»*p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)é‡å¤æŠ½å–çš„æ ·æœ¬ï¼‰ä¸è®­ç»ƒæ•°æ®é›†*X*å¤§ä½“é‡å ã€‚å¯¹äºé«˜æ–¯æƒ…å†µï¼Œè¿™æ„å‘³ç€å‡å€¼![](../../OEBPS/Images/AR_micro.png)åº”è¯¥è½åœ¨è®­ç»ƒæ•°æ®ç‚¹éå¸¸é›†ä¸­çš„åœ°æ–¹ï¼Œåæ–¹å·®çŸ©é˜µÎ£åº”è¯¥æ˜¯è¿™æ ·çš„ï¼Œå³ä¼¼ç„¶å‡½æ•°çš„æ¤­åœ†åº•éƒ¨å°½å¯èƒ½ç´§å¯†åœ°åŒ…å«å°½å¯èƒ½å¤šçš„è®­ç»ƒæ•°æ®ç‚¹ã€‚
- en: 13.1.1 Parameter estimation and belief injection
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 å‚æ•°ä¼°è®¡å’Œä¿¡å¿µæ³¨å…¥
- en: There are various possible approaches to parameter estimation. The simplest
    approach is *maximum likelihood parameter estimation* MLE), introduced in section
    [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation). In MLE, we choose the
    parameter values that maximize *p*(*X*|*Î¸*), the likelihood of observing the training
    data set. This makes some sense. After all, the only thing we know to be true
    is that the input data set *X* *has been observed*â€”this being unsupervised data,
    we do not know anything else. It is reasonable to choose the parameters that maximize
    the probability of that known truth. If the training data set is large, MLE estimation
    works well.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å‚æ•°ä¼°è®¡æœ‰å„ç§å¯èƒ½çš„æ–¹æ³•ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯*æœ€å¤§ä¼¼ç„¶å‚æ•°ä¼°è®¡*ï¼ˆMLEï¼‰ï¼Œåœ¨ç¬¬[6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation)èŠ‚ä¸­ä»‹ç»ã€‚åœ¨MLEä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€å¤§åŒ–è§‚å¯Ÿåˆ°çš„è®­ç»ƒæ•°æ®é›†ä¼¼ç„¶*p*(*X*|*Î¸*)çš„å‚æ•°å€¼ã€‚è¿™æœ‰äº›é“ç†ã€‚æ¯•ç«Ÿï¼Œæˆ‘ä»¬çŸ¥é“å”¯ä¸€çœŸå®çš„äº‹æƒ…æ˜¯è¾“å…¥æ•°æ®é›†*X*å·²ç»è¢«è§‚å¯Ÿåˆ°äº†â€”â€”ç”±äºè¿™æ˜¯æ— ç›‘ç£æ•°æ®ï¼Œæˆ‘ä»¬ä¸çŸ¥é“å…¶ä»–ä»»ä½•äº‹æƒ…ã€‚é€‰æ‹©æœ€å¤§åŒ–å·²çŸ¥çœŸç›¸æ¦‚ç‡çš„å‚æ•°æ˜¯åˆç†çš„ã€‚å¦‚æœè®­ç»ƒæ•°æ®é›†å¾ˆå¤§ï¼ŒMLEä¼°è®¡æ•ˆæœè‰¯å¥½ã€‚
- en: However, in the absence of a sufficiently large amount of training data, it
    often helps to inject our prior knowledge about the system into the estimationâ€”prior
    knowledge can cover for the lack of data. This injection of guess/belief into
    the system is done via the prior probability density. To do this, we can no longer
    maximize the likelihood, as likelihood ignores the prior. We have to do maximum
    a posteriori (MAP) estimation, which maximizes the posterior probability. The
    posterior probability is the product of likelihood (which depends on the data)
    and the prior (which does not depend on data; we will make it reflect our prior
    belief).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œåœ¨æ²¡æœ‰è¶³å¤Ÿå¤§é‡çš„è®­ç»ƒæ•°æ®çš„æƒ…å†µä¸‹ï¼Œå°†æˆ‘ä»¬å¯¹ç³»ç»Ÿçš„å…ˆéªŒçŸ¥è¯†æ³¨å…¥åˆ°ä¼°è®¡ä¸­é€šå¸¸æ˜¯æœ‰å¸®åŠ©çš„â€”â€”å…ˆéªŒçŸ¥è¯†å¯ä»¥å¼¥è¡¥æ•°æ®ä¸è¶³ã€‚è¿™ç§å°†çŒœæµ‹/ä¿¡å¿µæ³¨å…¥ç³»ç»Ÿçš„è¿‡ç¨‹æ˜¯é€šè¿‡å…ˆéªŒæ¦‚ç‡å¯†åº¦å®Œæˆçš„ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸èƒ½å†æœ€å¤§åŒ–ä¼¼ç„¶ï¼Œå› ä¸ºä¼¼ç„¶å¿½ç•¥äº†å…ˆéªŒã€‚æˆ‘ä»¬å¿…é¡»è¿›è¡Œæœ€å¤§åéªŒï¼ˆMAPï¼‰ä¼°è®¡ï¼Œå®ƒæœ€å¤§åŒ–åéªŒæ¦‚ç‡ã€‚åéªŒæ¦‚ç‡æ˜¯ä¼¼ç„¶ï¼ˆä¾èµ–äºæ•°æ®ï¼‰å’Œå…ˆéªŒï¼ˆä¸ä¾èµ–äºæ•°æ®ï¼›æˆ‘ä»¬å°†ä½¿å…¶åæ˜ æˆ‘ä»¬çš„å…ˆéªŒä¿¡å¿µï¼‰çš„ä¹˜ç§¯ã€‚
- en: There are two possible MAP paradigms. We saw one of them in section [6.6.3](../Text/06.xhtml#sec-MAP_estimation),
    where we injected our belief that the unknown parameters must be *small* in magnitude
    and set *p*(*Î¸*) âˆ *e*^(âˆ’||*Î¸*||Â²) as a *regularizer*. The system was incentivized
    to select parameter values that are relatively smaller in magnitude. In this chapter,
    we study a different paradigm; letâ€™s illustrate it with an example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸¤ç§å¯èƒ½çš„MAPèŒƒå¼ã€‚æˆ‘ä»¬åœ¨ç¬¬[6.6.3](../Text/06.xhtml#sec-MAP_estimation)èŠ‚ä¸­çœ‹åˆ°äº†å…¶ä¸­ä¹‹ä¸€ï¼Œåœ¨é‚£é‡Œæˆ‘ä»¬æ³¨å…¥äº†æˆ‘ä»¬çš„ä¿¡å¿µï¼Œå³æœªçŸ¥å‚æ•°çš„å¹…åº¦å¿…é¡»*å°*ï¼Œå¹¶å°†*p*(*Î¸*)
    âˆ *e*^(âˆ’||*Î¸*||Â²)ä½œä¸º*æ­£åˆ™åŒ–å™¨*ã€‚ç³»ç»Ÿè¢«æ¿€åŠ±é€‰æ‹©ç›¸å¯¹è¾ƒå°çš„å‚æ•°å€¼ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶ä¸€ä¸ªä¸åŒçš„èŒƒå¼ï¼›è®©æˆ‘ä»¬ç”¨ä¸€ä¸ªä¾‹å­æ¥è¯´æ˜å®ƒã€‚
- en: 'Suppose we model the likelihood as a Gaussian: *p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*)
    = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),Â **Î£**).
    We have to estimate the parameters *Î¸* = {![](../../OEBPS/Images/AR_micro.png),Â **Î£**}
    from the training data *X*, for which we must maximize the posterior probability.
    To compute the posterior probability, we need the prior probability. In addition,
    we must somehow inject constant values as our belief (lacking observed data) about
    the parameter values. How about modeling the prior probability as a Gaussian probability
    density function in the parameter space? Ignoring the covariance matrix parameter
    for the sake of simplicity, we can model the probability density of the mean parameter
    as *p*(![](../../OEBPS/Images/AR_micro.png)) = ğ’©(![](../../OEBPS/Images/AR_micro.png);
    ![](../../OEBPS/Images/AR_micro.png)[0], **Î£**[0]). We are essentially saying
    that we believe the parameter ![](../../OEBPS/Images/AR_micro.png) is likely to
    have a value near ![](../../OEBPS/Images/AR_micro.png)[0] with a confidence **Î£**[0].
    In other words, we are injecting a constant value as our belief in the parameter
    ![](../../OEBPS/Images/AR_micro.png). We can treat the covariance similarly. Later,
    we prove that in this paradigm, with a low volume of training data, the prior
    dominates. Once sufficient training data is digested, the effect of the prior
    fades, and the solution gets closer and closer to the MLE. This is the fully Bayes
    parameter estimation technique in a nutshell.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬å°†ä¼¼ç„¶å‡½æ•°å»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼š*p*(![](../../OEBPS/Images/AR_x.png)|*Î¸*) = ğ’©(![](../../OEBPS/Images/AR_x.png);
    ![](../../OEBPS/Images/AR_micro.png), **Î£**). æˆ‘ä»¬å¿…é¡»ä»è®­ç»ƒæ•°æ® *X* ä¸­ä¼°è®¡å‚æ•° *Î¸* = {![](../../OEBPS/Images/AR_micro.png),
    **Î£**}ï¼Œä¸ºæ­¤æˆ‘ä»¬å¿…é¡»æœ€å¤§åŒ–åéªŒæ¦‚ç‡ã€‚ä¸ºäº†è®¡ç®—åéªŒæ¦‚ç‡ï¼Œæˆ‘ä»¬éœ€è¦å…ˆéªŒæ¦‚ç‡ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å¿…é¡»ä»¥æŸç§æ–¹å¼æ³¨å…¥å¸¸æ•°å€¼ä½œä¸ºæˆ‘ä»¬å¯¹å‚æ•°å€¼çš„ä¿¡å¿µï¼ˆç¼ºä¹è§‚å¯Ÿæ•°æ®ï¼‰ã€‚é‚£ä¹ˆï¼Œå°†å…ˆéªŒæ¦‚ç‡å»ºæ¨¡ä¸ºå‚æ•°ç©ºé—´ä¸­çš„é«˜æ–¯æ¦‚ç‡å¯†åº¦å‡½æ•°å¦‚ä½•ï¼Ÿä¸ºäº†ç®€åŒ–èµ·è§ï¼Œæˆ‘ä»¬å¿½ç•¥åæ–¹å·®çŸ©é˜µå‚æ•°ï¼Œå¯ä»¥å°†å‡å€¼å‚æ•°çš„æ¦‚ç‡å¯†åº¦å»ºæ¨¡ä¸º
    *p*(![](../../OEBPS/Images/AR_micro.png)) = ğ’©(![](../../OEBPS/Images/AR_micro.png);
    ![](../../OEBPS/Images/AR_micro.png)[0], **Î£**[0])ã€‚æˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨è¯´ï¼Œæˆ‘ä»¬ç›¸ä¿¡å‚æ•° ![](../../OEBPS/Images/AR_micro.png)
    å¾ˆå¯èƒ½å…·æœ‰æ¥è¿‘ ![](../../OEBPS/Images/AR_micro.png)[0] çš„å€¼ï¼Œå¹¶ä¸”å…·æœ‰ **Î£**[0] çš„ç½®ä¿¡åº¦ã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬æ­£åœ¨å°†ä¸€ä¸ªå¸¸æ•°å€¼ä½œä¸ºå¯¹å‚æ•°
    ![](../../OEBPS/Images/AR_micro.png) çš„ä¿¡å¿µæ³¨å…¥å…¶ä¸­ã€‚æˆ‘ä»¬å¯ä»¥ç±»ä¼¼åœ°å¤„ç†åæ–¹å·®ã€‚åæ¥ï¼Œæˆ‘ä»¬è¯æ˜åœ¨è¿™ä¸ªèŒƒå¼ä¸‹ï¼Œåœ¨è®­ç»ƒæ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œå…ˆéªŒæ¦‚ç‡å ä¸»å¯¼åœ°ä½ã€‚ä¸€æ—¦æ¶ˆåŒ–äº†è¶³å¤Ÿå¤šçš„è®­ç»ƒæ•°æ®ï¼Œå…ˆéªŒæ¦‚ç‡çš„å½±å“å°±ä¼šå‡å¼±ï¼Œè§£å†³æ–¹æ¡ˆå°±ä¼šè¶Šæ¥è¶Šæ¥è¿‘æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰ã€‚è¿™å°±æ˜¯ç®€è€Œè¨€ä¹‹çš„å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡æŠ€æœ¯ã€‚
- en: In this chapter, we discuss Bayes estimation of parameters for a Gaussian likelihood
    function for a series of increasingly complex scenarios. In section [13.3](#sec-bayesinf-muonly),
    we deal with the case where the variance of the parameters to be estimated is
    known (constant) but the mean is unknown, so the mean is expressed as a (Gaussian)
    random variable. Then, in section [13.6](#sec-bayesinf-sigmaonly), we examine
    the case where the mean is known (constant) but the variance is unknown. Finally,
    in section [13.7](#sec-bayesinf-musigma), we examine the case where both are unknown.
    Both the univariate and multivariate cases are dealt with for each scenario.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†ä¸€ç³»åˆ—è¶Šæ¥è¶Šå¤æ‚çš„åœºæ™¯ä¸­é«˜æ–¯ä¼¼ç„¶å‡½æ•°å‚æ•°çš„è´å¶æ–¯ä¼°è®¡ã€‚åœ¨ç¬¬ [13.3](#sec-bayesinf-muonly) èŠ‚ä¸­ï¼Œæˆ‘ä»¬å¤„ç†äº†è¦ä¼°è®¡çš„å‚æ•°æ–¹å·®å·²çŸ¥ï¼ˆå¸¸æ•°ï¼‰ä½†å‡å€¼æœªçŸ¥çš„æƒ…å†µï¼Œå› æ­¤å‡å€¼è¢«è¡¨ç¤ºä¸ºä¸€ä¸ªï¼ˆé«˜æ–¯ï¼‰éšæœºå˜é‡ã€‚ç„¶åï¼Œåœ¨ç¬¬
    [13.6](#sec-bayesinf-sigmaonly) èŠ‚ä¸­ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†å‡å€¼å·²çŸ¥ï¼ˆå¸¸æ•°ï¼‰ä½†æ–¹å·®æœªçŸ¥çš„æƒ…å†µã€‚æœ€åï¼Œåœ¨ç¬¬ [13.7](#sec-bayesinf-musigma)
    èŠ‚ä¸­ï¼Œæˆ‘ä»¬è€ƒå¯Ÿäº†ä¸¤è€…éƒ½æœªçŸ¥çš„æƒ…å†µã€‚å¯¹äºæ¯ä¸ªåœºæ™¯ï¼Œéƒ½å¤„ç†äº†å•å˜é‡å’Œå¤šå˜é‡æƒ…å†µã€‚
- en: NOTE Fully functional code for this chapter, executable via Jupyter Notebook,
    can be found at [http://mng.bz/woYW](http://mng.bz/woYW).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæœ¬ç« çš„å®Œæ•´åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯ä»¥åœ¨ [http://mng.bz/woYW](http://mng.bz/woYW)
    æ‰¾åˆ°ã€‚
- en: 13.2 MLE for Gaussian parameter values recap)
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 é«˜æ–¯å‚æ•°å€¼çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å›é¡¾ï¼‰
- en: We have discussed the details of this in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation).
    Here we recap the main results. Suppose we have a data set *X* = {*x*^((1)), *x*^((2)),â‹¯,
    *x*^((*n*))}. We have decided to model the data distribution as a Gaussian ğ’©(*x*;*Î¼*,
    *Ïƒ*)â€”we want to estimate the parameters *Î¼*, *Ïƒ* that best â€œexplain" or â€œfit"
    the observed data set *X*. MLE is one of the simplest approaches to solving this
    problem. Here we estimate the parameters such that *the likelihood of the data
    observed during training is maximized*. This can be loosely visualized as estimating
    a probability density function whose peak coincides with the region in the input
    space with the densest population of training data. We looked at MLE in section
    [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation). Here we simply restate
    the expressions.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»åœ¨ç¬¬ [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation) èŠ‚ä¸­è®¨è®ºäº†è¿™ä¸€ç»†èŠ‚ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å›é¡¾ä¸€ä¸‹ä¸»è¦ç»“æœã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›†
    *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}ã€‚æˆ‘ä»¬å†³å®šå°†æ•°æ®åˆ†å¸ƒå»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒ ğ’©(*x*;*Î¼*, *Ïƒ*)â€”æˆ‘ä»¬æƒ³è¦ä¼°è®¡å‚æ•°
    *Î¼*, *Ïƒ*ï¼Œä»¥æœ€å¥½åœ°â€œè§£é‡Šâ€æˆ–â€œæ‹Ÿåˆâ€è§‚å¯Ÿåˆ°çš„æ•°æ®é›† *X*ã€‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡æ˜¯è§£å†³æ­¤é—®é¢˜çš„ä¸€ç§ç®€å•æ–¹æ³•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¼°è®¡å‚æ•°ï¼Œä½¿å¾— *è®­ç»ƒè¿‡ç¨‹ä¸­è§‚å¯Ÿåˆ°çš„æ•°æ®çš„ä¼¼ç„¶æ€§æœ€å¤§åŒ–*ã€‚è¿™å¯ä»¥ç²—ç•¥åœ°ç†è§£ä¸ºä¼°è®¡ä¸€ä¸ªæ¦‚ç‡å¯†åº¦å‡½æ•°ï¼Œå…¶å³°å€¼ä¸è¾“å…¥ç©ºé—´ä¸­è®­ç»ƒæ•°æ®å¯†é›†åŒºåŸŸç›¸å»åˆã€‚æˆ‘ä»¬åœ¨ç¬¬
    [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation) èŠ‚ä¸­è®¨è®ºäº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬åªæ˜¯é‡æ–°é™ˆè¿°äº†è¿™äº›è¡¨è¾¾å¼ã€‚
- en: Letâ€™s denote the (as yet unknown) mean and variance of the data distribution
    as *Î¼* and *Ïƒ*. Then from equation [5.22](../Text/05.xhtml#eq-univar-normal),
    we get
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç”¨ *Î¼* å’Œ *Ïƒ* è¡¨ç¤ºæ•°æ®åˆ†å¸ƒçš„ï¼ˆå°šæœªçŸ¥çš„ï¼‰å‡å€¼å’Œæ–¹å·®ã€‚ç„¶åä»æ–¹ç¨‹ [5.22](../Text/05.xhtml#eq-univar-normal)
    ä¸­ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../../OEBPS/Images/eq_13-01-a.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-01-a.png)'
- en: 'Maximizing the log-likelihood *p*(*X*|*Î¼*, *Ïƒ*) has a closed-form solution:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å¤§åŒ–å¯¹æ•°ä¼¼ç„¶ *p*(*X*|*Î¼*, *Ïƒ*) æœ‰ä¸€ä¸ªé—­å¼è§£ï¼š
- en: '![](../../OEBPS/Images/eq_13-02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-02.png)'
- en: Equation 13.2
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 13.2
- en: Thus the MLE mean and variance are essentially the mean and variance of the
    training data samples (see section [6.6](../Text/06.xhtml#sec-model_param_estimation)
    for the derivation of these expressions).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å‡å€¼å’Œæ–¹å·®æœ¬è´¨ä¸Šå°±æ˜¯è®­ç»ƒæ•°æ®æ ·æœ¬çš„å‡å€¼å’Œæ–¹å·®ï¼ˆå‚è§ç¬¬ [6.6](../Text/06.xhtml#sec-model_param_estimation)
    èŠ‚ä¸­è¿™äº›è¡¨è¾¾å¼çš„æ¨å¯¼ï¼‰ã€‚
- en: The corresponding expressions for multivariate Gaussian MLE are
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šå˜é‡é«˜æ–¯æœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„å¯¹åº”è¡¨è¾¾å¼æ˜¯
- en: '![](../../OEBPS/Images/eq_13-03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-03.png)'
- en: Equation 13.3
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 13.3
- en: These MLE parameter values are to be used to evaluate *p*(*x*) = ğ’©(*x*;*Î¼*,
    *Ïƒ*)â€”the probability of an unknown data point *x* coming from the distribution
    represented by the training data set *X*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æœ€å¤§ä¼¼ç„¶ä¼°è®¡å‚æ•°å€¼ç”¨äºè¯„ä¼° *p*(*x*) = ğ’©(*x*;*Î¼*, *Ïƒ*)â€”å³æœªçŸ¥æ•°æ®ç‚¹ *x* æ¥è‡ªç”±è®­ç»ƒæ•°æ®é›† *X* è¡¨ç¤ºçš„åˆ†å¸ƒçš„æ¦‚ç‡ã€‚
- en: '13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision'
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼šé«˜æ–¯åˆ†å¸ƒï¼ŒæœªçŸ¥å‡å€¼ï¼Œå·²çŸ¥ç²¾åº¦
- en: 'MLE may not be that accurate when the available data set is small that is,
    *n*, size of the data set *X*, is small). In many problems, we have a prior idea
    of the mean and sigma of the data set. Unfortunately, MLE provides no way to bake
    such a prior belief into the estimation. Fully Bayes parameter estimation techniques
    try to fix this drawback: here we are not simply maximizing the likelihood of
    the observed data. Instead, we maximize the posterior probability of the estimated
    parameter(s). This posterior probability involves the product of the likelihood
    and a prior probability (see equation [13.1](#eq-posterior-prior-likelihood-evidence-ch12)).
    The likelihood term captures the effect of the training dataâ€”maximizing it alone
    is MLEâ€”but does not capture the effect of a prior belief. On the other hand, the
    prior term does not depend on the data. This is where we bake in our belief or
    guess or prior knowledge about the data distribution. Thus, our estimate for the
    data distribution parameters will consider the data and the prior guess. We will
    soon see that the estimation is such that as the size of the data set (*n*, length
    of *X*) increases, the effect of the prior term decreases, and the effect of the
    likelihood term increases. In the limit, at infinite data availability, the Bayesian
    inference ields the MLE. At the other extreme, when no data is available (*n*
    = 0), the Fully Bayes estimates for the parameters are the same as the prior estimates.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¯ç”¨æ•°æ®é›†è¾ƒå°ï¼ˆå³ï¼Œæ•°æ®é›† *X* çš„å¤§å° *n* è¾ƒå°ï¼‰æ—¶ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼ˆMLEï¼‰å¯èƒ½å¹¶ä¸é‚£ä¹ˆå‡†ç¡®ã€‚åœ¨è®¸å¤šé—®é¢˜ä¸­ï¼Œæˆ‘ä»¬å¯¹æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®æœ‰ä¸€ä¸ªå…ˆéªŒçš„æƒ³æ³•ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒMLE
    æä¾›äº†æ²¡æœ‰å°†è¿™ç§å…ˆéªŒä¿¡å¿µèå…¥ä¼°è®¡çš„æ–¹æ³•ã€‚å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡æŠ€æœ¯è¯•å›¾è§£å†³è¿™ä¸ªé—®é¢˜ï¼šåœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä¸ä»…ä»…æ˜¯æœ€å¤§åŒ–è§‚å¯Ÿæ•°æ®çš„ä¼¼ç„¶æ€§ã€‚ç›¸åï¼Œæˆ‘ä»¬æœ€å¤§åŒ–ä¼°è®¡å‚æ•°çš„åéªŒæ¦‚ç‡ã€‚è¿™ä¸ªåéªŒæ¦‚ç‡æ¶‰åŠåˆ°ä¼¼ç„¶æ€§å’Œå…ˆéªŒæ¦‚ç‡çš„ä¹˜ç§¯ï¼ˆè§æ–¹ç¨‹
    [13.1](#eq-posterior-prior-likelihood-evidence-ch12)ï¼‰ã€‚ä¼¼ç„¶é¡¹æ•æ‰äº†è®­ç»ƒæ•°æ®çš„å½±å“â€”â€”ä»…æœ€å¤§åŒ–å®ƒæ˜¯ MLEï¼Œä½†å®ƒä¸æ•æ‰å…ˆéªŒä¿¡å¿µçš„å½±å“ã€‚å¦ä¸€æ–¹é¢ï¼Œå…ˆéªŒé¡¹ä¸ä¾èµ–äºæ•°æ®ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å°†æˆ‘ä»¬çš„ä¿¡å¿µã€çŒœæµ‹æˆ–å…ˆéªŒçŸ¥è¯†å…³äºæ•°æ®åˆ†å¸ƒåµŒå…¥çš„åœ°æ–¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯¹æ•°æ®åˆ†å¸ƒå‚æ•°çš„ä¼°è®¡å°†è€ƒè™‘æ•°æ®å’Œå…ˆéªŒçŒœæµ‹ã€‚æˆ‘ä»¬å°†å¾ˆå¿«çœ‹åˆ°ï¼Œè¿™ç§ä¼°è®¡æ˜¯éšç€æ•°æ®é›†çš„å¤§å°ï¼ˆ*n*ï¼Œ*X*
    çš„é•¿åº¦ï¼‰å¢åŠ ï¼Œå…ˆéªŒé¡¹çš„å½±å“å‡å°ï¼Œä¼¼ç„¶é¡¹çš„å½±å“å¢åŠ ã€‚åœ¨æé™æƒ…å†µä¸‹ï¼Œåœ¨æ•°æ®å¯ç”¨æ€§æ— é™çš„æƒ…å†µä¸‹ï¼Œè´å¶æ–¯æ¨ç†äº§ç”Ÿ MLEã€‚åœ¨å¦ä¸€ä¸ªæç«¯æƒ…å†µä¸‹ï¼Œå½“æ²¡æœ‰æ•°æ®å¯ç”¨ï¼ˆ*n*
    = 0ï¼‰æ—¶ï¼Œå®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ä¸å…ˆéªŒä¼°è®¡ç›¸åŒã€‚
- en: 'Letâ€™s we examine Bayesian parameter estimation. For starters, we deal with
    a relatively simple case where we have a Gaussian data distribution with a known
    (constant) variance but unknown and modeled mean. The data distribution is Gaussian
    (as usual, the semicolon in ğ’©(*x*;*Î¼[n]*, *Ïƒ*) separates the variables from parameters):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒå¯Ÿè´å¶æ–¯å‚æ•°ä¼°è®¡ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å¤„ç†ä¸€ä¸ªç›¸å¯¹ç®€å•çš„æƒ…å†µï¼Œå³æˆ‘ä»¬æœ‰ä¸€ä¸ªå…·æœ‰å·²çŸ¥ï¼ˆå¸¸æ•°ï¼‰æ–¹å·®ä½†æœªçŸ¥ä¸”å·²å»ºæ¨¡çš„å‡å€¼çš„é«˜æ–¯æ•°æ®åˆ†å¸ƒã€‚æ•°æ®åˆ†å¸ƒæ˜¯é«˜æ–¯åˆ†å¸ƒï¼ˆé€šå¸¸ï¼Œğ’©(*x*;*Î¼[n]*,
    *Ïƒ*) ä¸­çš„åˆ†å·å°†å˜é‡ä¸å‚æ•°åˆ†å¼€ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_13-03-a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼13-03-a](../../OEBPS/Images/eq_13-03-a.png)'
- en: The training data set is denoted *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))},
    and its overall likelihood is
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®é›†è¡¨ç¤ºä¸º *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}ï¼Œå…¶æ•´ä½“ä¼¼ç„¶æ€§ä¸º
- en: '![](../../OEBPS/Images/eq_13-03-b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼13-03-b](../../OEBPS/Images/eq_13-03-b.png)'
- en: The variance is known by assumptionâ€”hence it is treated as a constant instead
    of a random variable. The mean *Î¼* is unknown and is treated as a Gaussian random
    variable, with mean *Î¼*[0] and variance *Ïƒ*[0] (not to be confused with *Î¼* and
    *Ïƒ*, the mean and variance of the data itself ). So, the prior is
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æ–¹å·®æ˜¯å·²çŸ¥çš„â€”â€”å› æ­¤å®ƒè¢«è§†ä¸ºä¸€ä¸ªå¸¸æ•°è€Œä¸æ˜¯éšæœºå˜é‡ã€‚å‡å€¼ *Î¼* æ˜¯æœªçŸ¥çš„ï¼Œè¢«è§†ä¸ºä¸€ä¸ªå…·æœ‰å‡å€¼ *Î¼*[0] å’Œæ–¹å·® *Ïƒ*[0]ï¼ˆä¸è¦ä¸ *Î¼*
    å’Œ *Ïƒ* æ··æ·†ï¼Œå®ƒä»¬æ˜¯æ•°æ®æœ¬èº«çš„å‡å€¼å’Œæ–¹å·®ï¼‰çš„é«˜æ–¯éšæœºå˜é‡ã€‚å› æ­¤ï¼Œå…ˆéªŒæ˜¯
- en: '![](../../OEBPS/Images/eq_13-03-c.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼13-03-c](../../OEBPS/Images/eq_13-03-c.png)'
- en: The posterior probability of the unknown *Î¼* parameter is a product of two Gaussians,
    which is a Gaussian itself. Letâ€™s denote the as-yet-unknown) mean and variance
    of this product Gaussian as *Î¼[n]* and *Ïƒ[n]*. Here the subscript *n* is to remind
    us that the posterior has been obtained by digesting *n* data instances from *X*
    = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}. Thus, the Gaussian posterior can be denoted
    as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æœªçŸ¥å‚æ•° *Î¼* çš„åéªŒæ¦‚ç‡æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ä¹˜ç§¯ï¼Œè¿™æœ¬èº«ä¹Ÿæ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚è®©æˆ‘ä»¬ç”¨ *Î¼[n]* å’Œ *Ïƒ[n]* è¡¨ç¤ºè¿™ä¸ªä¹˜ç§¯é«˜æ–¯åˆ†å¸ƒçš„å°šæœªçŸ¥çš„å‡å€¼å’Œæ–¹å·®ã€‚åœ¨è¿™é‡Œï¼Œä¸‹æ ‡
    *n* æ˜¯ä¸ºäº†æé†’æˆ‘ä»¬ï¼ŒåéªŒæ˜¯é€šè¿‡å¤„ç† *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))} ä¸­çš„ *n* ä¸ªæ•°æ®å®ä¾‹æ¥è·å¾—çš„ã€‚å› æ­¤ï¼Œé«˜æ–¯åéªŒå¯ä»¥è¡¨ç¤ºä¸º
- en: '![](../../OEBPS/Images/eq_13-03-d.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼13-03-d](../../OEBPS/Images/eq_13-03-d.png)'
- en: Using Bayesâ€™ theorem,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å¶æ–¯å®šç†ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-03-e.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼13-03-e](../../OEBPS/Images/eq_13-03-e.png)'
- en: 'By comparing the coefficients of *Î¼*Â² and *Î¼* on the exponents of the left
    and right sides, we determine the unknown parameters of the posterior distribution:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¯”è¾ƒå·¦å³ä¸¤ä¾§æŒ‡æ•°ä¸Šçš„ *Î¼*Â² å’Œ *Î¼* çš„ç³»æ•°ï¼Œæˆ‘ä»¬ç¡®å®šåéªŒåˆ†å¸ƒçš„æœªçŸ¥å‚æ•°ï¼š
- en: '![](../../OEBPS/Images/eq_13-04.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-04.png)'
- en: Equation 13.4
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 13.4
- en: 'The significance of various closely named variables should be clearly understood:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åº”è¯¥æ¸…æ¥šåœ°ç†è§£å„ç§åç§°ç›¸è¿‘çš„å˜é‡çš„æ„ä¹‰ï¼š
- en: '*Î¼*, *Ïƒ* are the mean and variance of the *data* distribution *p*(*x*)â€”assumed
    to be Gaussian. The final goal is to estimate *Î¼*, *Ïƒ* that best fits the data
    set *X*. On the other hand, *Î¼*[0], *Ïƒ*[0] are the mean and variance of the *parameter*
    distribution *p*(*Î¼*), which captures our prior belief about the value of the
    data mean *Î¼* (remember, by assumption, the data mean is also a Gaussian random
    variable). *Î¼[n]*, *Ïƒ[n]* are the mean and variance of the posterior distribution
    *p*(*Î¼*|*X*) for the data mean *Î¼* as computed from *n* data point samples. This
    is a Gaussian random variable because it is a product of two Gaussians.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î¼* å’Œ *Ïƒ* æ˜¯æ•°æ®åˆ†å¸ƒ *p*(*x*) çš„å‡å€¼å’Œæ–¹å·®â€”â€”å‡è®¾ä¸ºé«˜æ–¯åˆ†å¸ƒã€‚æœ€ç»ˆç›®æ ‡æ˜¯ä¼°è®¡ *Î¼* å’Œ *Ïƒ*ï¼Œä»¥æœ€ä½³åœ°æ‹Ÿåˆæ•°æ®é›† *X*ã€‚å¦ä¸€æ–¹é¢ï¼Œ*Î¼*[0]
    å’Œ *Ïƒ*[0] æ˜¯å‚æ•°åˆ†å¸ƒ *p*(*Î¼*) çš„å‡å€¼å’Œæ–¹å·®ï¼Œå®ƒæ•æ‰äº†æˆ‘ä»¬å…³äºæ•°æ®å‡å€¼ *Î¼* çš„å…ˆéªŒä¿¡å¿µï¼ˆè®°ä½ï¼Œæ ¹æ®å‡è®¾ï¼Œæ•°æ®å‡å€¼ä¹Ÿæ˜¯ä¸€ä¸ªé«˜æ–¯éšæœºå˜é‡ï¼‰ã€‚*Î¼[n]*
    å’Œ *Ïƒ[n]* æ˜¯ä» *n* ä¸ªæ•°æ®ç‚¹æ ·æœ¬è®¡ç®—å‡ºçš„åéªŒåˆ†å¸ƒ *p*(*Î¼*|*X*) çš„å‡å€¼å’Œæ–¹å·®ã€‚å› ä¸ºå®ƒæ˜¯ç”±ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ä¹˜ç§¯ï¼Œæ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªé«˜æ–¯éšæœºå˜é‡ã€‚'
- en: The posterior distribution of the unknown mean parameter, *p*(*Î¼*|*X*), is a
    Gaussian with mean *Î¼[n]*. So, it will attain a maximum when *Î¼* = *Î¼[n]*. In
    other words, the MAP estimate for the unknown mean *Î¼* is *Î¼*[MAP] = *Î¼[n]*.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœªçŸ¥å‡å€¼å‚æ•°çš„åéªŒåˆ†å¸ƒ *p*(*Î¼*|*X*) æ˜¯ä¸€ä¸ªä»¥ *Î¼[n]* ä¸ºå‡å€¼çš„æ­£æ€åˆ†å¸ƒã€‚å› æ­¤ï¼Œå½“ *Î¼* = *Î¼[n]* æ—¶ï¼Œå®ƒå°†è¾¾åˆ°æœ€å¤§å€¼ã€‚æ¢å¥è¯è¯´ï¼ŒæœªçŸ¥å‡å€¼
    *Î¼* çš„ MAP ä¼°è®¡ä¸º *Î¼*[MAP] = *Î¼[n]*ã€‚
- en: Even though *Î¼[n]* is the best estimate of *Î¼*, *Ïƒ[n]* is not approximating
    the *Ïƒ* of the data, *Ïƒ* is known in this case by assumption. Here, *Ïƒ[n]* is
    the variance of the posterior distribution, reflecting our *uncertainty* about
    the estimate of *Î¼*. That is why, as the number of data instances becomes very
    large, *Ïƒ[n]* approaches 0 (indicating we have zero uncertainty or full confidence
    in the estimate of the mean.)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°½ç®¡ *Î¼[n]* æ˜¯ *Î¼* çš„æœ€ä½³ä¼°è®¡ï¼Œä½† *Ïƒ[n]* å¹¶æ²¡æœ‰è¿‘ä¼¼æ•°æ®çš„ *Ïƒ*ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ*Ïƒ* æ˜¯é€šè¿‡å‡è®¾è€ŒçŸ¥çš„ã€‚åœ¨è¿™é‡Œï¼Œ*Ïƒ[n]* æ˜¯åéªŒåˆ†å¸ƒçš„æ–¹å·®ï¼Œåæ˜ äº†æˆ‘ä»¬å¯¹
    *Î¼* ä¼°è®¡çš„ä¸ç¡®å®šæ€§ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆï¼Œå½“æ•°æ®å®ä¾‹çš„æ•°é‡å˜å¾—éå¸¸å¤§æ—¶ï¼Œ*Ïƒ[n]* è¶‹è¿‘äº 0ï¼ˆè¡¨ç¤ºæˆ‘ä»¬å¯¹å‡å€¼ä¼°è®¡æ²¡æœ‰ä¸ç¡®å®šæ€§æˆ–å®Œå…¨æœ‰ä¿¡å¿ƒï¼‰ã€‚
- en: The estimate for our data distribution is *p*(*x*) = ğ’©(*x*;*Î¼[n]*, *Ïƒ*), where
    *Î¼[n]* is given by equation [13.4](#eq-bayesinf-muonlyunivar). Note that it is
    a combination of the MLE *xÌ„* and prior guess *Î¼*[0]. Using this, given any arbitrary
    data instance *x*, we can infer the probability of *x* belonging to the class
    of the training data set *X*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ•°æ®åˆ†å¸ƒçš„ä¼°è®¡ä¸º *p*(*x*) = ğ’©(*x*;*Î¼[n]*, *Ïƒ*)ï¼Œå…¶ä¸­ *Î¼[n]* ç”±æ–¹ç¨‹ [13.4](#eq-bayesinf-muonlyunivar)
    ç»™å‡ºã€‚è¯·æ³¨æ„ï¼Œå®ƒæ˜¯ MLE *xÌ„* å’Œå…ˆéªŒçŒœæµ‹ *Î¼*[0] çš„ç»„åˆã€‚ä½¿ç”¨è¿™ä¸ªï¼Œå¯¹äºä»»ä½•ä»»æ„æ•°æ®å®ä¾‹ *x*ï¼Œæˆ‘ä»¬å¯ä»¥æ¨æ–­ *x* å±äºè®­ç»ƒæ•°æ®é›† *X*
    çš„ç±»çš„æ¦‚ç‡ã€‚
- en: NOTE Fully functional code for Bayesian estimation with unknown mean and known
    variance, executable via Jupyter Notebook, can be found at [http://mng.bz](http://mng.bz/ZA75)
    [/ZA75](http://mng.bz/ZA75).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå®Œå…¨åŠŸèƒ½çš„è´å¶æ–¯ä¼°è®¡ä»£ç ï¼Œå…·æœ‰æœªçŸ¥å‡å€¼å’Œå·²çŸ¥æ–¹å·®ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯ä»¥åœ¨ [http://mng.bz](http://mng.bz/ZA75)
    [/ZA75](http://mng.bz/ZA75) æ‰¾åˆ°ã€‚
- en: Listing 13.1 PyTorch- Bayesian estimation with unknown mean, known variance
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 13.1 PyTorch- å¸¦æœ‰æœªçŸ¥å‡å€¼å’Œå·²çŸ¥æ–¹å·®çš„è´å¶æ–¯ä¼°è®¡
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: â‘  Parameters of the prior
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å…ˆéªŒå‚æ•°
- en: â‘¡ Mean of the posterior, following equation [13.4](#eq-bayesinf-muonlyunivar)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ åéªŒå‡å€¼ï¼Œæ ¹æ®æ–¹ç¨‹ [13.4](#eq-bayesinf-muonlyunivar)
- en: â‘¢ Standard deviation of the posterior, following equation [13.4](#eq-bayesinf-muonlyunivar)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ åéªŒæ ‡å‡†å·®ï¼Œæ ¹æ®æ–¹ç¨‹ [13.4](#eq-bayesinf-muonlyunivar)
- en: 13.4 Small and large volumes of training data, and strong and weak priors
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 å°é‡å’Œå¤§é‡çš„è®­ç»ƒæ•°æ®ï¼Œä»¥åŠå¼ºå’Œå¼±çš„å…ˆéªŒ
- en: 'Letâ€™s examine the behavior of equation [13.4](#eq-bayesinf-muonlyunivar) when
    *n* = 0 (no data) and when *n* â†’ âˆ (lots of data):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒå¯Ÿæ–¹ç¨‹ [13.4](#eq-bayesinf-muonlyunivar) åœ¨ *n* = 0ï¼ˆæ— æ•°æ®ï¼‰å’Œ *n* â†’ âˆï¼ˆå¤§é‡æ•°æ®ï¼‰æ—¶çš„è¡Œä¸ºï¼š
- en: '![](../../OEBPS/Images/eq_13-04-a.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-04-a.png)'
- en: This agrees with our notion that with little data, the posterior is dominated
    by the prior, while with lots of data, the posterior is dominated by the likelihood.
    With lots of data, the variance of the parameter is zero (we are saying with *full
    certainty* that the best value for the mean is the sample mean for the data, aka
    the MLE estimate for the mean). In general, with more training data (that is,
    larger values of *n*), the posterior shifts closer to the likelihood. This can
    be seen by analyzing equation [13.4](#eq-bayesinf-muonlyunivar). It agrees with
    our intuition that with little data, we try to compensate with our pre-existing
    (prior) belief as to the value of the parameters. As the number of training data
    instances increases, the effect of the prior is reduced, and the likelihood (which
    is a function of the data) begins to dominate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æˆ‘ä»¬å…³äºå°‘é‡æ•°æ®æ—¶åéªŒä¸»è¦ç”±å…ˆéªŒå†³å®šï¼Œè€Œå¤§é‡æ•°æ®æ—¶åéªŒä¸»è¦ç”±ä¼¼ç„¶å†³å®šçš„è§‚å¿µç›¸ç¬¦ã€‚åœ¨å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå‚æ•°çš„æ–¹å·®ä¸ºé›¶ï¼ˆæˆ‘ä»¬å¯ä»¥è¯´æˆ‘ä»¬æœ‰**å®Œå…¨çš„ç¡®å®šæ€§**ï¼Œå³å‡å€¼çš„æœ€ä¼˜å€¼æ˜¯æ•°æ®çš„æ ·æœ¬å‡å€¼ï¼Œä¹Ÿå°±æ˜¯å‡å€¼çš„MLEä¼°è®¡ï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œéšç€è®­ç»ƒæ•°æ®çš„å¢åŠ ï¼ˆå³*n*çš„å€¼å¢å¤§ï¼‰ï¼ŒåéªŒä¼šè¶Šæ¥è¶Šæ¥è¿‘ä¼¼ç„¶ã€‚è¿™å¯ä»¥é€šè¿‡åˆ†ææ–¹ç¨‹[13.4](#eq-bayesinf-muonlyunivar)æ¥çœ‹åˆ°ã€‚è¿™ä¸æˆ‘ä»¬çš„ç›´è§‰ç›¸ç¬¦ï¼Œå³å°‘é‡æ•°æ®æ—¶ï¼Œæˆ‘ä»¬è¯•å›¾ç”¨æˆ‘ä»¬é¢„å…ˆå­˜åœ¨çš„ï¼ˆå…ˆéªŒï¼‰å…³äºå‚æ•°å€¼çš„ä¿¡å¿µæ¥è¡¥å¿ã€‚éšç€è®­ç»ƒæ•°æ®å®ä¾‹æ•°é‡çš„å¢åŠ ï¼Œå…ˆéªŒçš„å½±å“å‡å°ï¼Œä¼¼ç„¶ï¼ˆå®ƒæ˜¯æ•°æ®çš„å‡½æ•°ï¼‰å¼€å§‹å ä¸»å¯¼åœ°ä½ã€‚
- en: 'A low variance for the prior (that is, small *Ïƒ*[0]) essentially means we have
    low uncertainty in our prior belief (remember, the entropy/uncertainty of a Gaussian
    is proportional to its variance). Such high-confidence priors resist being overwhelmed
    by the data and are called *strong priors*. On the other hand, a large *Ïƒ*[0]
    implies low /confidence in the prior mean value. This is a *weak prior* that is
    easily overwhelmed by the data. We can see this in the final expression for mean
    in equation [13.4](#eq-bayesinf-muonlyunivar): we have *nÏƒ*[0]Â²/*Ïƒ*Â² in the denominator
    of the second term. In general, the second term vanishes with larger *n*, thereby
    removing the effect of the prior *Î¼*[0] and making the posterior mean coincide
    with the MLE mean. But the smaller the *Ïƒ*[0], the larger the *n* required to
    achieve this, and vice versa.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å…ˆéªŒçš„æ–¹å·®ä½ï¼ˆå³å°çš„*Ïƒ*[0]ï¼‰æœ¬è´¨ä¸Šæ„å‘³ç€æˆ‘ä»¬å¯¹å…ˆéªŒä¿¡å¿µçš„ä¸ç¡®å®šæ€§ä½ï¼ˆè®°ä½ï¼Œé«˜æ–¯ç†µ/ä¸ç¡®å®šæ€§ä¸å…¶æ–¹å·®æˆæ­£æ¯”ï¼‰ã€‚è¿™æ ·çš„é«˜ç½®ä¿¡åº¦å…ˆéªŒæŠµæŠ—è¢«æ•°æ®æ·¹æ²¡ï¼Œè¢«ç§°ä¸º**å¼ºå…ˆéªŒ**ã€‚å¦ä¸€æ–¹é¢ï¼Œå¤§çš„*Ïƒ*[0]æ„å‘³ç€å¯¹å…ˆéªŒå‡å€¼å€¼çš„ä¿¡å¿ƒä½ã€‚è¿™æ˜¯ä¸€ä¸ª**å¼±å…ˆéªŒ**ï¼Œå®¹æ˜“è¢«æ•°æ®æ·¹æ²¡ã€‚æˆ‘ä»¬å¯ä»¥åœ¨æ–¹ç¨‹[13.4](#eq-bayesinf-muonlyunivar)ä¸­å‡å€¼çš„æœ€ç»ˆæƒè¡¡è¡¨è¾¾å¼ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼šåœ¨ç¬¬äºŒé¡¹çš„åˆ†æ¯ä¸­æˆ‘ä»¬æœ‰*nÏƒ*[0]Â²/*Ïƒ*Â²ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œéšç€*n*çš„å¢å¤§ï¼Œç¬¬äºŒé¡¹æ¶ˆå¤±ï¼Œä»è€Œæ¶ˆé™¤äº†å…ˆéªŒ*Î¼*[0]çš„å½±å“ï¼Œä½¿å¾—åéªŒå‡å€¼ä¸MLEå‡å€¼ç›¸ä¸€è‡´ã€‚ä½†æ˜¯ï¼Œ*Ïƒ*[0]è¶Šå°ï¼Œéœ€è¦æ›´å¤§çš„*n*æ¥å®ç°è¿™ä¸€ç‚¹ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: 13.5 Conjugate priors
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.5 å…±è½­å…ˆéªŒ
- en: In section [13.3](#sec-bayesinf-muonly), given a Gaussian likelihood, choosing
    the Gaussian family for the prior made the posterior also belong to the Gaussian
    family. This simplified things considerably. If the prior was chosen from another
    family, the posteriorâ€”which is the product of the likelihood and priorâ€”may not
    belong to a simple or even known distribution family.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[13.3](#sec-bayesinf-muonly)èŠ‚ä¸­ï¼Œç»™å®šé«˜æ–¯ä¼¼ç„¶ï¼Œé€‰æ‹©é«˜æ–¯å…ˆéªŒä½¿å¾—åéªŒä¹Ÿå±äºé«˜æ–¯å®¶æ—ã€‚è¿™å¤§å¤§ç®€åŒ–äº†é—®é¢˜ã€‚å¦‚æœå…ˆéªŒæ¥è‡ªå¦ä¸€ä¸ªå®¶æ—ï¼Œé‚£ä¹ˆåéªŒâ€”â€”ä¼¼ç„¶å’Œå…ˆéªŒçš„ä¹˜ç§¯â€”â€”å¯èƒ½ä¸å±äºç®€å•æˆ–ç”šè‡³å·²çŸ¥çš„åˆ†å¸ƒå®¶æ—ã€‚
- en: Thus, a Gaussian likelihood with a Gaussian prior results in a Gaussian posterior
    for the mean. Such priors are said to be *conjugate*. Formally, for a specific
    family of likelihood, the choice of the prior that results in the posterior belonging
    to the same family as the prior is called a conjugate prior. For instance, Gaussians
    for the mean (with known variance) are conjugate to a Gaussian likelihood. Soon
    we will see that for a Gaussian likelihood, a gamma distribution for the precision
    (inverse of the variance) results in a gamma posterior. In other words, a gamma
    prior to the precision is conjugate to a Gaussian likelihood. In the multivariate
    case, instead of gamma, we have the Wishart distribution as a conjugate prior.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œå…·æœ‰é«˜æ–¯å…ˆéªŒçš„é«˜æ–¯ä¼¼ç„¶å¯¼è‡´å‡å€¼åéªŒä¹Ÿæ˜¯é«˜æ–¯ã€‚è¿™æ ·çš„å…ˆéªŒè¢«ç§°ä¸º**å…±è½­**ã€‚å½¢å¼ä¸Šï¼Œå¯¹äºç‰¹å®šçš„ä¼¼ç„¶å®¶æ—ï¼Œé€‰æ‹©ä½¿å¾—åéªŒå±äºä¸å…ˆéªŒç›¸åŒå®¶æ—çš„å…ˆéªŒè¢«ç§°ä¸ºå…±è½­å…ˆéªŒã€‚ä¾‹å¦‚ï¼Œå‡å€¼çš„æ­£æ€åˆ†å¸ƒï¼ˆå·²çŸ¥æ–¹å·®ï¼‰ä¸æ­£æ€ä¼¼ç„¶æ˜¯å…±è½­çš„ã€‚å¾ˆå¿«æˆ‘ä»¬å°±ä¼šçœ‹åˆ°ï¼Œå¯¹äºæ­£æ€ä¼¼ç„¶ï¼Œç²¾åº¦çš„ä¼½é©¬åˆ†å¸ƒï¼ˆæ–¹å·®çš„å€’æ•°ï¼‰å¯¼è‡´ä¼½é©¬åéªŒã€‚æ¢å¥è¯è¯´ï¼Œç²¾åº¦çš„ä¼½é©¬å…ˆéªŒä¸æ­£æ€ä¼¼ç„¶æ˜¯å…±è½­çš„ã€‚åœ¨å¤šå…ƒæƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ç”¨Wishartåˆ†å¸ƒä½œä¸ºå…±è½­å…ˆéªŒã€‚
- en: '13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean'
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.6 å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼šé«˜æ–¯ï¼ŒæœªçŸ¥ç²¾åº¦ï¼Œå·²çŸ¥å‡å€¼
- en: In section [13.3](#sec-bayesinf-muonly), we discussed fully Bayes parameter
    estimation with the assumption that we somehow know the variance *Ïƒ* and only
    want to estimate the mean *Î¼*. Now we examine the case where the mean is known
    but the variance is unknown and expressed as a random variable. The computations
    become simpler if we use precision *Î»* instead of variance *Ïƒ*. They are related
    by the expression 1/*Ïƒ*Â². Thus we have a data set *X*, which is assumed to be
    sampled from a Gaussian distribution with a constant mean *Î¼*, while the precision
    *Î»* is a random variable with a gamma distribution. The probability density function
    for the data is thus *p*(*x*|*Î¼*, *Î»*) = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ [13.3](#sec-bayesinf-muonly) èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†åœ¨å‡è®¾æˆ‘ä»¬ä»¥æŸç§æ–¹å¼çŸ¥é“æ–¹å·® *Ïƒ* å¹¶ä¸”åªæƒ³ä¼°è®¡å‡å€¼ *Î¼* çš„æ¡ä»¶ä¸‹ï¼Œå®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ã€‚ç°åœ¨æˆ‘ä»¬è€ƒå¯Ÿå‡å€¼å·²çŸ¥ä½†æ–¹å·®æœªçŸ¥ä¸”è¡¨ç¤ºä¸ºéšæœºå˜é‡çš„æƒ…å†µã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç²¾åº¦
    *Î»* è€Œä¸æ˜¯æ–¹å·® *Ïƒ*ï¼Œè®¡ç®—å°†å˜å¾—ç®€å•ã€‚å®ƒä»¬é€šè¿‡è¡¨è¾¾å¼ 1/*Ïƒ*Â² ç›¸å…³è”ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ•°æ®é›† *X*ï¼Œå®ƒè¢«å‡å®šä¸ºä»å…·æœ‰å¸¸æ•°å‡å€¼ *Î¼* çš„é«˜æ–¯åˆ†å¸ƒä¸­æŠ½å–çš„ï¼Œè€Œç²¾åº¦
    *Î»* æ˜¯å…·æœ‰ä¼½é©¬åˆ†å¸ƒçš„éšæœºå˜é‡ã€‚å› æ­¤ï¼Œæ•°æ®çš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯ *p*(*x*|*Î¼*, *Î»*) = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*)ã€‚
- en: We model the prior random variable for precision with a gamma distribution.
    The likelihood is Gaussian, and since the product of a Gaussian and gamma is another
    gamma (due to the conjugate prior property of gamma), the resulting posterior
    is a gamma. The gamma function parameters for the posterior can be derived via
    coefficient comparison. The maximum of the posterior is our estimate for the parameter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨ä¼½é©¬åˆ†å¸ƒæ¥å»ºæ¨¡ç²¾åº¦çš„å…ˆéªŒéšæœºå˜é‡ã€‚ä¼¼ç„¶å‡½æ•°æ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œç”±äºé«˜æ–¯å’Œä¼½é©¬çš„ä¹˜ç§¯ä»ç„¶æ˜¯ä¼½é©¬ï¼ˆç”±äºä¼½é©¬åˆ†å¸ƒçš„å…±è½­å…ˆéªŒæ€§è´¨ï¼‰ï¼Œå› æ­¤å¾—åˆ°çš„ç»“æœåéªŒæ˜¯ä¼½é©¬åˆ†å¸ƒã€‚åéªŒçš„ä¼½é©¬å‡½æ•°å‚æ•°å¯ä»¥é€šè¿‡ç³»æ•°æ¯”è¾ƒæ¨å¯¼å‡ºæ¥ã€‚åéªŒçš„æœ€å¤§å€¼æ˜¯æˆ‘ä»¬å¯¹å‚æ•°çš„ä¼°è®¡ã€‚
- en: Gamma distribution
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼½é©¬åˆ†å¸ƒ
- en: 'The gamma distribution is introduced in the appendix; if necessary, please
    read that first. Here we state the relevant properties. The probability density
    function for a random variable *Î»* having a gamma distribution is a function with
    two parameters *Î±*, *Î²*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼½é©¬åˆ†å¸ƒåœ¨é™„å½•ä¸­ä»‹ç»ï¼›å¦‚æœ‰å¿…è¦ï¼Œè¯·å…ˆé˜…è¯»ã€‚è¿™é‡Œæˆ‘ä»¬é™ˆè¿°ç›¸å…³çš„æ€§è´¨ã€‚å…·æœ‰ä¼½é©¬åˆ†å¸ƒçš„éšæœºå˜é‡ *Î»* çš„æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯å…·æœ‰ä¸¤ä¸ªå‚æ•° *Î±*ï¼Œ*Î²* çš„å‡½æ•°ï¼š
- en: '![](../../OEBPS/Images/eq_13-05.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-05.png)'
- en: Equation 13.5
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 13.5
- en: Maximum of a gamma distribution
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ä¼½é©¬åˆ†å¸ƒçš„æœ€å¤§å€¼
- en: 'To maximize the gamma probability density function *p*(*Î»*|*X*) = *Î»*^((*Î±[n]*
    âˆ’ 1))*e*^(âˆ’*Î²[n]Î»*) for a random variable *Î»*, we take the derivative and equate
    to zero:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æœ€å¤§åŒ–ä¼½é©¬æ¦‚ç‡å¯†åº¦å‡½æ•° *p*(*Î»*|*X*) = *Î»*^((*Î±[n]* âˆ’ 1))*e*^(âˆ’*Î²[n]Î»*) å¯¹äºéšæœºå˜é‡ *Î»*ï¼Œæˆ‘ä»¬å¯¹å®ƒæ±‚å¯¼å¹¶ä»¤å…¶ç­‰äºé›¶ï¼š
- en: '![](../../OEBPS/Images/eq_13-05-a.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-05-a.png)'
- en: 13.6.1 Estimating the precision parameter
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.6.1 ä¼°è®¡ç²¾åº¦å‚æ•°
- en: 'Letâ€™s return to the fully Bayes estimation of the precision parameter when
    the mean is known. We model the data distribution with a Gaussian: *p*(*x*|*Î¼*,
    *Î»*) = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*) (we have expressed this Gaussian in terms of the precision,
    *Î»*, which is related to the variance *Ïƒ* as *Î»* = 1/*Ïƒ* Â²). The training data
    set is *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}, and its overall likelihood
    is'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›åˆ°å½“å‡å€¼å·²çŸ¥æ—¶å¯¹ç²¾åº¦å‚æ•°çš„å®Œå…¨è´å¶æ–¯ä¼°è®¡ã€‚æˆ‘ä»¬ç”¨é«˜æ–¯åˆ†å¸ƒæ¥å»ºæ¨¡æ•°æ®åˆ†å¸ƒï¼š*p*(*x*|*Î¼*, *Î»*) = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*)ï¼ˆæˆ‘ä»¬å·²ç»ç”¨ç²¾åº¦
    *Î»* è¡¨è¾¾äº†è¿™ç§é«˜æ–¯åˆ†å¸ƒï¼Œè€Œ *Î»* ä¸æ–¹å·® *Ïƒ* ç›¸å…³ï¼Œ*Î»* = 1/*Ïƒ* Â²ï¼‰ã€‚è®­ç»ƒæ•°æ®é›†æ˜¯ *X* = {*x*^((1)), *x*^((2)),â‹¯,
    *x*^((*n*))}ï¼Œå…¶æ•´ä½“ä¼¼ç„¶å‡½æ•°æ˜¯
- en: '![](../../OEBPS/Images/eq_13-05-b.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-05-b.png)'
- en: 'We model the prior for the precision with a gamma distribution with parameters
    *Î±*[0], *Î²*[0]:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”¨å‚æ•° *Î±*[0]ï¼Œ*Î²*[0] çš„ä¼½é©¬åˆ†å¸ƒæ¥å»ºæ¨¡ç²¾åº¦çš„å…ˆéªŒï¼š
- en: '*p*(*Î»*) = *Î³*(*Î»*;*Î±*[0], *Î²*[0]) âˆ *Î»*^((*Î±*[0]âˆ’1))*e*^(âˆ’*Î²*[0]*Î»*)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*Î»*) = *Î³*(*Î»*;*Î±*[0], *Î²*[0]) âˆ *Î»*^((*Î±*[0]âˆ’1))*e*^(âˆ’*Î²*[0]*Î»*)'
- en: We know the corresponding posteriorâ€”a product of a Gaussian and a gammaâ€”is gamma
    distribution (due to the conjugate prior property of gamma distribution). Letâ€™s
    denote the posterior as
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“ç›¸åº”çš„åéªŒâ€”â€”é«˜æ–¯å’Œä¼½é©¬åˆ†å¸ƒçš„ä¹˜ç§¯â€”â€”æ˜¯ä¼½é©¬åˆ†å¸ƒï¼ˆç”±äºä¼½é©¬åˆ†å¸ƒçš„å…±è½­å…ˆéªŒæ€§è´¨ï¼‰ã€‚è®©æˆ‘ä»¬å°†åéªŒè¡¨ç¤ºä¸º
- en: '*p*(*Î»*|*X*) = *Î³*(*Î»*;*Î±[n]*, *Î²[n]*)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*Î»*|*X*) = *Î³*(*Î»*;*Î±[n]*, *Î²[n]*)'
- en: From Bayesâ€™ theorem,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä»è´å¶æ–¯å®šç†ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-05-c.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-05-c.png)'
- en: Substituting
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£å…¥
- en: '![](../../OEBPS/Images/eq_13-05-d.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-05-d.png)'
- en: and comparing the powers of *Î»* and *e*, we get
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶æ¯”è¾ƒ *Î»* å’Œ *e* çš„å¹‚ï¼Œæˆ‘ä»¬å¾—åˆ°
- en: '![](../../OEBPS/Images/eq_13-06.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06.png)'
- en: Equation 13.6
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 13.6
- en: Notice that as before, at low values of *n*, the posterior is dominated by the
    prior but gets closer and closer to the likelihood estimate as *n* increases.
    In other words, in the absence of sufficient data, we let our belief take over
    the estimation; but if and when data is available, the estimation is dominated
    by the data-based entity likelihood.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œä¸ä¹‹å‰ä¸€æ ·ï¼Œåœ¨ *n* è¾ƒä½æ—¶ï¼ŒåéªŒè¢«å…ˆéªŒä¸»å¯¼ï¼Œä½†éšç€ *n* çš„å¢åŠ ï¼ŒåéªŒè¶Šæ¥è¶Šæ¥è¿‘ä¼¼ç„¶ä¼°è®¡ã€‚æ¢å¥è¯è¯´ï¼Œåœ¨æ²¡æœ‰è¶³å¤Ÿæ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è®©æˆ‘ä»¬çš„ä¿¡å¿µä¸»å¯¼ä¼°è®¡ï¼›ä½†å¦‚æœæ•°æ®å¯ç”¨ï¼Œä¼°è®¡åˆ™ç”±åŸºäºæ•°æ®çš„å®ä½“ä¼¼ç„¶ä¸»å¯¼ã€‚
- en: The MAP point estimate for the parameter *Î»* given data set *X* is obtained
    by maximizing this posterior distribution *p*(*Î»*|*X*) = *Î³*(*Î»*;*Î±[n]*, *Î²[n]*),
    which yields *Î»[MAP]* = 1/*Ïƒ[MAP]*Â² = (*Î±[n]*â€“1/*Î²[n]*). (Section A.5 in the appendix
    shows how to obtain the maximum of a gamma distribution.) Thus our estimate for
    the training data distribution is *p*(*x*) = ğ’©(*x*; *Î¼*, *Ïƒ*[MAP]), where 1/*Ïƒ[MAP]*Â²
    = (*Î±[n]*â€“1/*Î²[n]*).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™å®šæ•°æ®é›† *X*ï¼Œå‚æ•° *Î»* çš„ MAP ç‚¹ä¼°è®¡æ˜¯é€šè¿‡æœ€å¤§åŒ–è¿™ä¸ªåéªŒåˆ†å¸ƒ *p*(*Î»*|*X*) = *Î³*(*Î»*;*Î±[n]*, *Î²[n]*)ï¼Œå¾—åˆ°
    *Î»[MAP]* = 1/*Ïƒ[MAP]*Â² = (*Î±[n]*â€“1/*Î²[n]*)ã€‚ (é™„å½• A.5 å±•ç¤ºäº†å¦‚ä½•è·å¾—ä¼½é©¬åˆ†å¸ƒçš„æœ€å¤§å€¼ã€‚) å› æ­¤ï¼Œæˆ‘ä»¬å¯¹è®­ç»ƒæ•°æ®åˆ†å¸ƒçš„ä¼°è®¡æ˜¯
    *p*(*x*) = ğ’©(*x*; *Î¼*, *Ïƒ*[MAP])ï¼Œå…¶ä¸­ 1/*Ïƒ[MAP]*Â² = (*Î±[n]*â€“1/*Î²[n]*)ã€‚
- en: 'Given a large volume of data, the MAP estimate for the unknown precision/variance
    becomes identical to the MLE estimate (proof outline shown):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™äºˆå¤§é‡æ•°æ®ï¼ŒæœªçŸ¥ç²¾åº¦/æ–¹å·®çš„ MAP ä¼°è®¡ä¸ MLE ä¼°è®¡ç›¸åŒï¼ˆè¯æ˜æ¦‚è¦å¦‚ä¸‹ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_13-06-a.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-a.png)'
- en: 'On the other hand, given no data, the MAP estimate for the unknown precision/variance
    is completely determined by the prior (proof outline shown):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œåœ¨æ²¡æœ‰æ•°æ®çš„æƒ…å†µä¸‹ï¼ŒæœªçŸ¥ç²¾åº¦/æ–¹å·®çš„ MAP ä¼°è®¡å®Œå…¨ç”±å…ˆéªŒå†³å®šï¼ˆè¯æ˜æ¦‚è¦å¦‚ä¸‹ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_13-06-b.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-b.png)'
- en: NOTE Fully functional code for Bayesian estimation with a known mean and unknown
    variance, executable via Jupyter Notebook, can be found at [http://mng.bz](http://mng.bz/2nZ9)
    [/2nZ9](http://mng.bz/2nZ9).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå…·æœ‰å·²çŸ¥å‡å€¼å’ŒæœªçŸ¥æ–¹å·®çš„è´å¶æ–¯ä¼°è®¡çš„å®Œæ•´åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯åœ¨ [http://mng.bz](http://mng.bz/2nZ9)
    [/2nZ9](http://mng.bz/2nZ9) æ‰¾åˆ°ã€‚
- en: Listing 13.2 PyTorch- Bayesian estimation with unknown variance, known mean
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 13.2 PyTorch- å…·æœ‰æœªçŸ¥æ–¹å·®å’Œå·²çŸ¥å‡å€¼çš„è´å¶æ–¯ä¼°è®¡
- en: '[PRE1]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: â‘  Parameters of the prior
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å…ˆéªŒå‚æ•°
- en: â‘¡ Parameters of the posterior
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ åéªŒå‚æ•°
- en: '13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision'
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.7 å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼šé«˜æ–¯ï¼ŒæœªçŸ¥å‡å€¼ï¼ŒæœªçŸ¥ç²¾åº¦
- en: In section [13.3](#sec-bayesinf-muonly), we saw that if the variance is known,
    the conjugate prior to the mean is a Gaussian (aka normal) distribution. Likewise,
    when the mean is known, the conjugate prior to the precision is a gamma distribution.
    If both are unknown, we end up with a normal-gamma distribution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ [13.3](#sec-bayesinf-muonly) èŠ‚ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¦‚æœæ–¹å·®å·²çŸ¥ï¼Œå‡å€¼çš„å…±è½­å…ˆéªŒæ˜¯é«˜æ–¯åˆ†å¸ƒï¼ˆä¹Ÿç§°ä¸ºæ­£æ€åˆ†å¸ƒï¼‰ã€‚åŒæ ·ï¼Œå½“å‡å€¼å·²çŸ¥æ—¶ï¼Œç²¾åº¦çš„å…±è½­å…ˆéªŒæ˜¯ä¼½é©¬åˆ†å¸ƒã€‚å¦‚æœä¸¤è€…éƒ½æœªçŸ¥ï¼Œæˆ‘ä»¬æœ€ç»ˆå¾—åˆ°æ­£æ€-ä¼½é©¬åˆ†å¸ƒã€‚
- en: 13.7.1 Normal-gamma distribution
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.1 æ­£æ€-ä¼½é©¬åˆ†å¸ƒ
- en: 'Normal-gamma is a probability distribution of two random variables, say, *Î¼*
    and *Î»*, whose density is defined in terms of four parameters *Î¼*^â€², *Î»*^â€², *Î±*^â€²,
    and *Î²*^â€², as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£æ€-ä¼½é©¬æ˜¯ä¸¤ä¸ªéšæœºå˜é‡ï¼Œä¾‹å¦‚ *Î¼* å’Œ *Î»* çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå…¶å¯†åº¦ç”±å››ä¸ªå‚æ•° *Î¼*^â€²ï¼Œ*Î»*^â€²ï¼Œ*Î±*^â€²ï¼Œå’Œ *Î²*^â€² å®šä¹‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../../OEBPS/Images/eq_13-06-c.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-c.png)'
- en: Although it looks complicated, a simple way to remember it is a product of a
    normal and a gamma distribution.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡çœ‹èµ·æ¥å¾ˆå¤æ‚ï¼Œä½†è®°ä½å®ƒçš„ç®€å•æ–¹æ³•æ˜¯å®ƒæ˜¯æ­£æ€åˆ†å¸ƒå’Œä¼½é©¬åˆ†å¸ƒçš„ä¹˜ç§¯ã€‚
- en: The normal-gamma distribution attains a maximum at
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£æ€-ä¼½é©¬åˆ†å¸ƒè¾¾åˆ°æœ€å¤§å€¼åœ¨
- en: '![](../../OEBPS/Images/eq_13-06-d.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-d.png)'
- en: 13.7.2 Estimating the mean and precision parameters
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.7.2 ä¼°è®¡å‡å€¼å’Œç²¾åº¦å‚æ•°
- en: 'As before, we model the data distribution with a Gaussian: *p*(*x*|*Î¼* , *Î»*)
    = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*) we have expressed this Gaussian in terms of the precision,
    *Î»*, which is related to the variance *Ïƒ* as *Î»* = 1/*Ïƒ*Â²). The training data
    set is *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}, and its overall likelihood
    is'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬ç”¨é«˜æ–¯åˆ†å¸ƒå»ºæ¨¡æ•°æ®åˆ†å¸ƒï¼š*p*(*x*|*Î¼* , *Î»*) = ğ’©(*x*; *Î¼*, 1/âˆš*Î»*)ï¼Œæˆ‘ä»¬ç”¨ç²¾åº¦ *Î»* è¡¨è¾¾äº†è¿™ä¸ªé«˜æ–¯åˆ†å¸ƒï¼Œå®ƒä¸æ–¹å·®
    *Ïƒ* ç›¸å…³ï¼Œ*Î»* = 1/*Ïƒ*Â²ï¼‰ã€‚è®­ç»ƒæ•°æ®é›†æ˜¯ *X* = {*x*^((1)), *x*^((2)),â‹¯, *x*^((*n*))}ï¼Œå…¶æ•´ä½“ä¼¼ç„¶ä¸º
- en: '![](../../OEBPS/Images/eq_13-06-e.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-e.png)'
- en: 'We model the prior for the mean as a Gaussian with mean *Î¼*[0] and precision
    *Î»*[0]*Î»*:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…ˆéªŒçš„å‡å€¼å»ºæ¨¡ä¸ºå‡å€¼ *Î¼*[0] å’Œç²¾åº¦ *Î»*[0]*Î»* çš„é«˜æ–¯åˆ†å¸ƒï¼š
- en: '![](../../OEBPS/Images/eq_13-06-f.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-f.png)'
- en: 'We model the prior for the precision as a gamma distribution with parameters
    *Î±*[0], *Î²*[0]:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å…ˆéªŒçš„ç²¾åº¦å»ºæ¨¡ä¸ºå‚æ•° *Î±*[0]ï¼Œ*Î²*[0] çš„ä¼½é©¬åˆ†å¸ƒï¼š
- en: '![](../../OEBPS/Images/eq_13-06-g.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-g.png)'
- en: 'The overall prior probability for the mean and precision parameters is the
    product of the two, a normal-gamma distribution with parameters *Î¼*â°, *Î»*â°, *Î±*â°,
    *Î²*â°:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼å’Œç²¾åº¦å‚æ•°çš„æ•´ä½“å…ˆéªŒæ¦‚ç‡æ˜¯ä¸¤è€…çš„ä¹˜ç§¯ï¼Œå³å‚æ•° *Î¼*â°, *Î»*â°, *Î±*â°, *Î²*â° çš„æ­£æ€-ä¼½é©¬åˆ†å¸ƒï¼š
- en: '![](../../OEBPS/Images/eq_13-06-h.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-h.png)'
- en: 'The posterior probability for the mean and precision parameters is the joint
    (that is, product) of the likelihood and the prior. As such, we know it is another
    normal-gamma distribution (due to the conjugate prior property of normal-gamma):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼å’Œç²¾åº¦å‚æ•°çš„åéªŒæ¦‚ç‡æ˜¯ä¼¼ç„¶å’Œå…ˆéªŒçš„è”åˆï¼ˆå³ä¹˜ç§¯ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çŸ¥é“å®ƒåˆæ˜¯å¦ä¸€ä¸ªæ­£æ€-ä¼½é©¬åˆ†å¸ƒï¼ˆç”±äºæ­£æ€-ä¼½é©¬åˆ†å¸ƒçš„å…±è½­å…ˆéªŒæ€§è´¨ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_13-06-i.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-i.png)'
- en: Using Bayesâ€™ theorem,
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å¶æ–¯å®šç†ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-06-j.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-j.png)'
- en: Substituting
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä»£å…¥
- en: '![](../../OEBPS/Images/eq_13-06-k.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-06-k.png)'
- en: 'and comparing coefficients, the unknown parameters of the posterior distribution
    can be determined:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¯”è¾ƒç³»æ•°ï¼Œå¯ä»¥ç¡®å®šåéªŒåˆ†å¸ƒçš„æœªçŸ¥å‚æ•°ï¼š
- en: '![](../../OEBPS/Images/eq_13-07.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07.png)'
- en: Equation 13.7
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹ 13.7
- en: 'To obtain the fully Bayes parameter estimate, we take the maximum of the normal-gamma
    posterior probability density function:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è·å¾—å®Œå…¨çš„è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼Œæˆ‘ä»¬å–æ­£æ€-ä¼½é©¬åéªŒæ¦‚ç‡å¯†åº¦å‡½æ•°çš„æœ€å¤§å€¼ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-a.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-a.png)'
- en: Thus the final probability density function for the data is
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ•°æ®çš„æœ€ç»ˆæ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º
- en: '![](../../OEBPS/Images/eq_13-07-b.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-b.png)'
- en: NOTE Fully functional code for Bayesian estimation with an unknown mean and
    unknown variance, executable via Jupyter Notebook, can be found at [http://mng.bz/1oQy](http://mng.bz/1oQy).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå®Œå…¨åŠŸèƒ½çš„è´å¶æ–¯ä¼°è®¡ä»£ç ï¼Œç”¨äºæœªçŸ¥å‡å€¼å’ŒæœªçŸ¥æ–¹å·®ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯åœ¨[http://mng.bz/1oQy](http://mng.bz/1oQy)æ‰¾åˆ°ã€‚
- en: Listing 13.3 PyTorch code for a normal-gamma distribution
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 13.3 PyTorch ä»£ç ï¼šæ­£æ€-ä¼½é©¬åˆ†å¸ƒ
- en: '[PRE2]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: â‘  Since PyTorch doesnâ€™t implement normal-gamma distribution, we implement a
    bare-bones version.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  ç”±äº PyTorch æ²¡æœ‰å®ç°æ­£æ€-ä¼½é©¬åˆ†å¸ƒï¼Œæˆ‘ä»¬å®ç°äº†ä¸€ä¸ªç®€åŒ–çš„ç‰ˆæœ¬ã€‚
- en: 'Listing 13.4 PyTorch: Bayesian estimation with unknown mean, unknown variance'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 13.4 PyTorchï¼šå…·æœ‰æœªçŸ¥å‡å€¼å’ŒæœªçŸ¥æ–¹å·®çš„è´å¶æ–¯ä¼°è®¡
- en: '[PRE3]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: â‘  Parameters of the prior
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å…ˆéªŒå‚æ•°
- en: â‘¡ Parameters of the posterior
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ åéªŒå‚æ•°
- en: '13.8 Example: Fully Bayesian inferencing'
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.8 ç¤ºä¾‹ï¼šå®Œå…¨è´å¶æ–¯æ¨ç†
- en: Letâ€™s revisit the problem discussed in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)
    of predicting whether a resident of Statsville is female based on height. For
    this purpose, we have collected height samples from adult female residents of
    Statsville. Unfortunately, due to unforeseen circumstances, we collected a very
    small sample. Armed with our knowledge of Bayesian inference, we do not want to
    let this deter us from trying to build a model. Based on physical considerations,
    we can assume that the distribution of heights is Gaussian. Our goal is to estimate
    the parameters (*Î¼*, *Ïƒ*) of this Gaussian.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹ç¬¬ [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation) èŠ‚ä¸­è®¨è®ºçš„é—®é¢˜ï¼Œå³æ ¹æ®èº«é«˜é¢„æµ‹
    Statsville å±…æ°‘æ˜¯å¦ä¸ºå¥³æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å·²ç»æ”¶é›†äº† Statsville æˆå¹´å¥³æ€§å±…æ°‘çš„èº«é«˜æ ·æœ¬ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç”±äºä¸å¯é¢„è§çš„æƒ…å†µï¼Œæˆ‘ä»¬æ”¶é›†çš„æ ·æœ¬éå¸¸å°ã€‚å‡­å€Ÿæˆ‘ä»¬å¯¹è´å¶æ–¯æ¨ç†çš„äº†è§£ï¼Œæˆ‘ä»¬ä¸æƒ³å› æ­¤æ”¾å¼ƒå°è¯•æ„å»ºæ¨¡å‹ã€‚æ ¹æ®ç‰©ç†è€ƒè™‘ï¼Œæˆ‘ä»¬å¯ä»¥å‡è®¾èº«é«˜çš„åˆ†å¸ƒæ˜¯é«˜æ–¯åˆ†å¸ƒã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡è¿™ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‚æ•°
    (*Î¼*, *Ïƒ*)ã€‚
- en: NOTE Fully functional code for this example, executable via Jupyter Notebook,
    can be found at [http://mng.bz/Pn4g](http://mng.bz/Pn4g).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šæ­¤ä¾‹çš„å®Œå…¨åŠŸèƒ½ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯åœ¨[http://mng.bz/Pn4g](http://mng.bz/Pn4g)æ‰¾åˆ°ã€‚
- en: 'Letâ€™s first create the data set by sampling five points from a Gaussian distribution
    with *Î¼* = 152 and *Ïƒ* = 8. In real-life scenarios, we do not know the mean and
    standard deviation of the true distribution. But for the sake of this example,
    letâ€™s assume that the mean height is 152 cm and the standard deviation is 8 cm.
    Our data matrix, *X*, is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬é€šè¿‡ä»å…·æœ‰ *Î¼* = 152 å’Œ *Ïƒ* = 8 çš„é«˜æ–¯åˆ†å¸ƒä¸­é‡‡æ ·äº”ä¸ªç‚¹æ¥åˆ›å»ºæ•°æ®é›†ã€‚åœ¨ç°å®åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬ä¸çŸ¥é“çœŸå®åˆ†å¸ƒçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚ä½†ä¸ºäº†è¿™ä¸ªä¾‹å­ï¼Œè®©æˆ‘ä»¬å‡è®¾å¹³å‡èº«é«˜ä¸º
    152 å˜ç±³ï¼Œæ ‡å‡†å·®ä¸º 8 å˜ç±³ã€‚æˆ‘ä»¬çš„æ•°æ®çŸ©é˜µï¼Œ*X*ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![](../../OEBPS/Images/eq_13-07-c.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-c.png)'
- en: 13.8.1 Maximum likelihood estimation
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.8.1 æœ€å¤§ä¼¼ç„¶ä¼°è®¡
- en: 'If we relied on MLE, our approach would be to compute the mean and standard
    deviation of the data set and use this normal distribution as our model. We use
    the following equations to compute the mean and standard deviation of our normal
    distribution:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä¾èµ–MLEï¼Œæˆ‘ä»¬çš„æ–¹æ³•å°†æ˜¯è®¡ç®—æ•°æ®é›†çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼Œå¹¶ä½¿ç”¨è¿™ä¸ªæ­£æ€åˆ†å¸ƒä½œä¸ºæˆ‘ä»¬çš„æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹æ–¹ç¨‹æ¥è®¡ç®—æ­£æ€åˆ†å¸ƒçš„å‡å€¼å’Œæ ‡å‡†å·®ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-d.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![å…¬å¼å›¾](../../OEBPS/Images/eq_13-07-d.png)'
- en: The mean, *Î¼*, comes out to be 149.68, and the standard deviation, *Ïƒ*, is 11.52\.
    This differs significantly from the true mean (152) and standard deviation (8)
    because the number of data points is low. In such low-data scenarios, the maximum
    likelihood estimates are not very reliable.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å€¼ï¼Œ*Î¼*ï¼Œè®¡ç®—ç»“æœä¸º149.68ï¼Œæ ‡å‡†å·®ï¼Œ*Ïƒ*ï¼Œä¸º11.52ã€‚è¿™ä¸çœŸå®çš„å‡å€¼ï¼ˆ152ï¼‰å’Œæ ‡å‡†å·®ï¼ˆ8ï¼‰æœ‰æ˜¾è‘—å·®å¼‚ï¼Œå› ä¸ºæ•°æ®ç‚¹çš„æ•°é‡å¾ˆå°‘ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæœ€å¤§ä¼¼ç„¶ä¼°è®¡å¹¶ä¸å¯é ã€‚
- en: 13.8.2 Bayesian inference
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.8.2 è´å¶æ–¯æ¨ç†
- en: Can we do better than MLE? One potential method is to use Bayesian inference
    with a good prior. How do we select a good prior? Well, letâ€™s say that we know
    from an old survey that the average and standard deviation of the height of adult
    female residents of Neighborville, the neighboring town, are 150 cm and 9 cm,
    respectively. Additionally, we have no reason to believe that the distribution
    of heights at Statsville is significantly different. So we can use this information
    to â€œinitialize" our prior. The prior distribution encodes our beliefs about the
    parameter values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èƒ½å¦æ¯”MLEåšå¾—æ›´å¥½ï¼Ÿä¸€ç§æ½œåœ¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨å…·æœ‰è‰¯å¥½å…ˆéªŒçš„è´å¶æ–¯æ¨ç†ã€‚æˆ‘ä»¬å¦‚ä½•é€‰æ‹©ä¸€ä¸ªå¥½çš„å…ˆéªŒï¼Ÿå¥½å§ï¼Œè®©æˆ‘ä»¬å‡è®¾æˆ‘ä»¬çŸ¥é“ä»ä¸€é¡¹æ—§è°ƒæŸ¥ä¸­ï¼Œé‚»é•‡Neighborvilleæˆå¹´å¥³æ€§å±…æ°‘çš„èº«é«˜å¹³å‡å€¼å’Œæ ‡å‡†å·®åˆ†åˆ«ä¸º150å˜ç±³å’Œ9å˜ç±³ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æ²¡æœ‰ç†ç”±ç›¸ä¿¡Statsvilleçš„èº«é«˜åˆ†å¸ƒä¸é‚»é•‡æœ‰æ˜¾è‘—å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™äº›ä¿¡æ¯æ¥â€œåˆå§‹åŒ–â€æˆ‘ä»¬çš„å…ˆéªŒã€‚å…ˆéªŒåˆ†å¸ƒç¼–ç äº†æˆ‘ä»¬å…³äºå‚æ•°å€¼çš„ä¿¡å¿µã€‚
- en: 'Given that we are dealing with an unknown mean and unknown variance, we model
    the prior as a normal-gamma distribution:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å¤„ç†çš„æ˜¯æœªçŸ¥å‡å€¼å’ŒæœªçŸ¥æ–¹å·®ï¼Œæˆ‘ä»¬å°†å…ˆéªŒæ¨¡å‹è®¾ä¸ºæ­£æ€ä¼½é©¬åˆ†å¸ƒï¼š
- en: '*p*(*Î¸*) = ğ’©*Î³*(*Î¼*[0], *Î»*[0], *Î±*[0], *Î²*[0])'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*Î¸*) = ğ’©*Î³*(*Î¼*[0], *Î»*[0], *Î±*[0], *Î²*[0])'
- en: We choose *p*(*Î¸*) such that *Î¼*[0] = 150, *Î»*[0] = 100, *Î±*[0] = 10.5, and
    *Î²*[0] = 810\. This implies that
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€‰æ‹©*p*(*Î¸*)ï¼Œä½¿å¾—*Î¼*[0] = 150ï¼Œ*Î»*[0] = 100ï¼Œ*Î±*[0] = 10.5ï¼Œå’Œ*Î²*[0] = 810ã€‚è¿™æ„å‘³ç€
- en: '*p*(*Î¸*) = ğ’©*Î³*(150, 100, 10.5, 810)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*Î¸*) = ğ’©*Î³*(150, 100, 10.5, 810)'
- en: '*p*(*Î¸*|*X*) is a normal-gamma distribution whose parameters can be computed
    using equations described in section [13.7](#eq-normal-gamma). The PyTorch code
    for computing the posterior is shown next.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '*p*(*Î¸*|*X*)æ˜¯ä¸€ä¸ªæ­£æ€ä¼½é©¬åˆ†å¸ƒï¼Œå…¶å‚æ•°å¯ä»¥ä½¿ç”¨[13.7](#eq-normal-gamma)èŠ‚ä¸­æè¿°çš„æ–¹ç¨‹è®¡ç®—ã€‚ä¸‹é¢å±•ç¤ºäº†è®¡ç®—åéªŒçš„PyTorchä»£ç ã€‚'
- en: Listing 13.5 PyTorch- Computing posterior probability using Bayesian inference
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨13.5 PyTorch- ä½¿ç”¨è´å¶æ–¯æ¨ç†è®¡ç®—åéªŒæ¦‚ç‡
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: â‘  Initializes the normal-gamma distribution
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  åˆå§‹åŒ–æ­£æ€ä¼½é©¬åˆ†å¸ƒ
- en: â‘¡ Computes the posterior
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ è®¡ç®—åéªŒæ¦‚ç‡
- en: â‘¢ The mode of the distribution refers to parameter values with the highest probability
    density.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¢ åˆ†å¸ƒçš„æ¨¡æŒ‡çš„æ˜¯å…·æœ‰æœ€é«˜æ¦‚ç‡å¯†åº¦çš„å‚æ•°å€¼ã€‚
- en: â‘£ Computes the standard deviation using precision
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: â‘£ ä½¿ç”¨ç²¾åº¦è®¡ç®—æ ‡å‡†å·®
- en: â‘¤ map_mu and map_std refer to the parameter values that maximize the posterior
    distribution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¤ map_muå’Œmap_stdæŒ‡çš„æ˜¯æœ€å¤§åŒ–åéªŒåˆ†å¸ƒçš„å‚æ•°å€¼ã€‚
- en: The MAP estimates for *Î¼* and *Ïƒ* obtained using Bayesian inference are 149.98
    and 9.56, respectively, which are better than the MLE estimates of 149.68 and
    11.52 (the true *Î¼* and *Ïƒ* are 152 and 9, respectively).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å¶æ–¯æ¨ç†è·å¾—çš„*Î¼*å’Œ*Ïƒ*çš„MAPä¼°è®¡å€¼åˆ†åˆ«ä¸º149.98å’Œ9.56ï¼Œåˆ†åˆ«ä¼˜äºMLEä¼°è®¡å€¼149.68å’Œ11.52ï¼ˆçœŸå®çš„*Î¼*å’Œ*Ïƒ*åˆ†åˆ«ä¸º152å’Œ9ï¼‰ã€‚
- en: Now that weâ€™ve estimated the parameters, we can find out the probability that
    a sample lies in the range using the formula
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¼°è®¡äº†å‚æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å…¬å¼æ‰¾å‡ºæ ·æœ¬è½åœ¨æŸä¸ªèŒƒå›´å†…çš„æ¦‚ç‡
- en: '![](../../OEBPS/Images/eq_13-07-e.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![å…¬å¼å›¾](../../OEBPS/Images/eq_13-07-e.png)'
- en: The details of this can be found in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å…³æ­¤å†…å®¹çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…[6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)èŠ‚ã€‚
- en: '13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean,
    known precision'
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.9 å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼šå¤šå…ƒé«˜æ–¯åˆ†å¸ƒï¼ŒæœªçŸ¥å‡å€¼ï¼Œå·²çŸ¥ç²¾åº¦
- en: This is the multivariate case; the univariate version is discussed in section
    [13.3](#sec-bayesinf-muonly). The computations follow along the same lines as
    the univariate ones.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å¤šå…ƒæƒ…å†µï¼›ä¸€å…ƒç‰ˆæœ¬åœ¨[13.3](#sec-bayesinf-muonly)èŠ‚ä¸­è®¨è®ºã€‚è®¡ç®—æ–¹æ³•ä¸ä¸€å…ƒæƒ…å†µç±»ä¼¼ã€‚
- en: We model the data distribution as a Gaussian *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Î›**) = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Î›**^(âˆ’1)), where we have expressed the Gaussian in terms of the *precision matrix*
    Î› instead of the covariance matrix Î£, where **Î›** = **Î£**^(âˆ’1). The training data
    set is *X* â‰¡ {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*i*)), â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    and its overall likelihood is
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†æ•°æ®åˆ†å¸ƒå»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒ *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Î›**) = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Î›**^(âˆ’1))ï¼Œå…¶ä¸­æˆ‘ä»¬ç”¨ç²¾åº¦çŸ©é˜µ **Î›** è€Œä¸æ˜¯åæ–¹å·®çŸ©é˜µ **Î£** æ¥è¡¨ç¤ºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­ **Î›** = **Î£**^(âˆ’1)ã€‚è®­ç»ƒæ•°æ®é›†æ˜¯
    *X* â‰¡ {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*i*)), â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}ï¼Œå…¶æ•´ä½“ä¼¼ç„¶ä¸º
- en: '![](../../OEBPS/Images/eq_13-07-f.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-f](../../OEBPS/Images/eq_13-07-f.png)'
- en: 'We model the prior for the mean as a Gaussian:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†å‡å€¼çš„å‰éªŒå»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒï¼š
- en: '![](../../OEBPS/Images/eq_13-07-g.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-g](../../OEBPS/Images/eq_13-07-g.png)'
- en: The posterior probability density is a Gaussian (because it is the product of
    two Gaussians). Letâ€™s denote it as
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒæ¦‚ç‡å¯†åº¦æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼ˆå› ä¸ºå®ƒæ˜¯ç”±ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ä¹˜ç§¯ï¼‰ã€‚è®©æˆ‘ä»¬ç”¨ä»¥ä¸‹ç¬¦å·è¡¨ç¤ºå®ƒ
- en: '![](../../OEBPS/Images/eq_13-07-h.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-h](../../OEBPS/Images/eq_13-07-h.png)'
- en: Using Bayesâ€™ theorem,
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å¶æ–¯å®šç†ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-07-i.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-i](../../OEBPS/Images/eq_13-07-i.png)'
- en: Letâ€™s examine the exponent of the rightmost expression.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ£€æŸ¥æœ€å³è¾¹è¡¨è¾¾å¼çš„æŒ‡æ•°ã€‚
- en: '![](../../OEBPS/Images/eq_13-07-j.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-j](../../OEBPS/Images/eq_13-07-j.png)'
- en: We ignored the last constant terms because they will be rolled into the overall
    constant of proportionality). Thus
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¿½ç•¥äº†æœ€åä¸€ä¸ªå¸¸æ•°é¡¹ï¼Œå› ä¸ºå®ƒä»¬å°†è¢«åˆå¹¶åˆ°æ•´ä½“æ¯”ä¾‹å¸¸æ•°ä¸­ã€‚å› æ­¤
- en: '![](../../OEBPS/Images/eq_13-07-k.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-k](../../OEBPS/Images/eq_13-07-k.png)'
- en: 'Comparing coefficients:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”è¾ƒç³»æ•°ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-l.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-l](../../OEBPS/Images/eq_13-07-l.png)'
- en: The posterior probability maximizes at *![](../../OEBPS/Images/AR_micro.png)[n]*.
    Thus *![](../../OEBPS/Images/AR_micro.png)[MAP]* = *![](../../OEBPS/Images/AR_micro.png)[n]*
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒæ¦‚ç‡åœ¨ *![](../../OEBPS/Images/AR_micro.png)[n]* å¤„æœ€å¤§åŒ–ã€‚å› æ­¤ *![](../../OEBPS/Images/AR_micro.png)[MAP]*
    = *![](../../OEBPS/Images/AR_micro.png)[n]*
- en: 'is the MAP estimate for the mean parameter of the multivariate Gaussian data
    distribution: *p*(![](../../OEBPS/Images/AR_x.png)) = ğ’©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[n]*, **Î›**^(-1)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯å¤šå…ƒé«˜æ–¯æ•°æ®åˆ†å¸ƒå‡å€¼å‚æ•°çš„ MAP ä¼°è®¡ï¼š*p*(![](../../OEBPS/Images/AR_x.png)) = ğ’©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[n]*, **Î›**^(-1))ã€‚
- en: 'Note the following:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ä»¥ä¸‹å†…å®¹ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-m.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![æ–¹ç¨‹å¼ 13-07-m](../../OEBPS/Images/eq_13-07-m.png)'
- en: With a large volume of data, the estimated mean parameter *![](../../OEBPS/Images/AR_micro.png)[MAP]*
    = *![](../../OEBPS/Images/AR_micro.png)[n]* approaches the MLE *![](../../OEBPS/Images/AR_micro.png)[MLE]*
    = ![](../../OEBPS/Images/AR_x2.png).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é‡è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡çš„å‡å€¼å‚æ•° *![](../../OEBPS/Images/AR_micro.png)[MAP]* = *![](../../OEBPS/Images/AR_micro.png)[n]*
    æ¥è¿‘ MLE *![](../../OEBPS/Images/AR_micro.png)[MLE]* = ![](../../OEBPS/Images/AR_x2.png)ã€‚
- en: With a low volume of data, the estimated posterior mean parameter *![](../../OEBPS/Images/AR_micro.png)[MAP]*
    = *![](../../OEBPS/Images/AR_micro.png)[n]* approaches the prior ![](../../OEBPS/Images/AR_micro.png)[0].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œä¼°è®¡çš„åéªŒå‡å€¼å‚æ•° *![](../../OEBPS/Images/AR_micro.png)[MAP]* = *![](../../OEBPS/Images/AR_micro.png)[n]*
    æ¥è¿‘å…ˆéªŒ ![](../../OEBPS/Images/AR_micro.png)[0]ã€‚
- en: NOTE Fully functional code for multivariate Bayesian inferencing of the mean
    of a Gaussian likelihood with known precision, executable via Jupyter Notebook,
    can be found at [http://mng.bz/J2AP](http://mng.bz/J2AP).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šå®Œå…¨åŠŸèƒ½çš„å¤šå…ƒè´å¶æ–¯æ¨æ–­é«˜æ–¯ä¼¼ç„¶å‡å€¼ï¼ˆå·²çŸ¥ç²¾åº¦ï¼‰çš„ä»£ç ï¼Œå¯é€šè¿‡ Jupyter Notebook æ‰§è¡Œï¼Œå¯åœ¨ [http://mng.bz/J2AP](http://mng.bz/J2AP)
    æ‰¾åˆ°ã€‚
- en: Listing 13.6 PyTorch- Multivariate Bayesian inferencing, unknown mean
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—è¡¨ 13.6 PyTorch- å¤šå…ƒè´å¶æ–¯æ¨æ–­ï¼ŒæœªçŸ¥å‡å€¼
- en: '[PRE5]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: â‘  Parameters of the prior
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: â‘  å…ˆéªŒå‚æ•°
- en: â‘¡ Parameters of the posterior
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: â‘¡ åéªŒå‚æ•°
- en: '13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known
    mean'
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.10 å®Œå…¨è´å¶æ–¯å‚æ•°ä¼°è®¡ï¼šå¤šå…ƒï¼ŒæœªçŸ¥ç²¾åº¦ï¼Œå·²çŸ¥å‡å€¼
- en: In section [13.6](#sec-bayesinf-sigmaonly), we discussed the univariate case,
    and now we examine the multivariate case. For the univariate case, we had to look
    at the gamma distribution. For the multivariate case, we have to look at the Wishart
    distribution.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬ [13.6](#sec-bayesinf-sigmaonly) èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å•å˜é‡æƒ…å†µï¼Œç°åœ¨æˆ‘ä»¬æ¥è€ƒå¯Ÿå¤šå…ƒæƒ…å†µã€‚å¯¹äºå•å˜é‡æƒ…å†µï¼Œæˆ‘ä»¬å¿…é¡»æŸ¥çœ‹ä¼½é©¬åˆ†å¸ƒã€‚å¯¹äºå¤šå…ƒæƒ…å†µï¼Œæˆ‘ä»¬å¿…é¡»æŸ¥çœ‹å¨æ²™ç‰¹åˆ†å¸ƒã€‚
- en: 13.10.1 Wishart distribution
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.10.1 å¨æ²™ç‰¹åˆ†å¸ƒ
- en: 'Suppose we have a Gaussian random data vector ![](../../OEBPS/Images/AR_x.png)
    with probability density function ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Î£**). Once again, we use *precision matrix* Î› instead of the covariance matrix
    Î£, where **Î›** = **Î£**^(âˆ’1). Consider the case where we know the mean ![](../../OEBPS/Images/AR_micro.png)
    but want to estimate the precision Î›. How do we express the prior? Note that *p*(**Î›**)
    is the probability density function of a *matrix*. So far, we have encountered
    probability distributions of scalars and vectors, not a matrix. Also, this is
    not an arbitrary matrix. We are talking about a *symmetric, non-negative definite*
    matrix (all covariance and precision matrices belong to this category). Consequently,
    the distribution we are looking for is not a joint distribution of all the *d*Â²
    matrix elements *d* denotes the dimensionality of the data: that is, all ![](../../OEBPS/Images/AR_x.png)
    and ![](../../OEBPS/Images/AR_micro.png) vectors are *d* Ã— 1). Rather, it is a
    joint distribution of (*d*(*d* + 1))/2 elements in the matrixâ€”the diagonal and
    those above or below (diagonal elements above and below are identical because
    the matrix is symmetric).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªé«˜æ–¯éšæœºæ•°æ®å‘é‡ ![](../../OEBPS/Images/AR_x.png) ï¼Œå…¶æ¦‚ç‡å¯†åº¦å‡½æ•°ä¸º ğ’©(![](../../OEBPS/Images/AR_x.png);
    ![](../../OEBPS/Images/AR_micro.png), **Î£**). å†æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ *ç²¾åº¦çŸ©é˜µ* Î› è€Œä¸æ˜¯åæ–¹å·®çŸ©é˜µ Î£ï¼Œå…¶ä¸­ **Î›**
    = **Î£**^(âˆ’1). è€ƒè™‘æˆ‘ä»¬å·²çŸ¥å‡å€¼ ![](../../OEBPS/Images/AR_micro.png) ä½†æƒ³ä¼°è®¡ç²¾åº¦ Î› çš„æƒ…å†µã€‚æˆ‘ä»¬å¦‚ä½•è¡¨è¾¾å…ˆéªŒï¼Ÿè¯·æ³¨æ„ï¼Œ*p*(**Î›**)
    æ˜¯ä¸€ä¸ª *çŸ©é˜µ* çš„æ¦‚ç‡å¯†åº¦å‡½æ•°ã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬é‡åˆ°äº†æ ‡é‡å’Œå‘é‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè€Œä¸æ˜¯çŸ©é˜µã€‚æ­¤å¤–ï¼Œè¿™ä¹Ÿä¸æ˜¯ä¸€ä¸ªä»»æ„çš„çŸ©é˜µã€‚æˆ‘ä»¬è°ˆè®ºçš„æ˜¯ä¸€ä¸ª *å¯¹ç§°ã€éè´Ÿå®š*
    çš„çŸ©é˜µï¼ˆæ‰€æœ‰åæ–¹å·®å’Œç²¾åº¦çŸ©é˜µéƒ½å±äºè¿™ä¸€ç±»ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯»æ‰¾çš„åˆ†å¸ƒä¸æ˜¯æ‰€æœ‰ *d*Â² çŸ©é˜µå…ƒç´ çš„è”åˆåˆ†å¸ƒ *d* è¡¨ç¤ºæ•°æ®çš„ç»´åº¦ï¼šå³æ‰€æœ‰ ![](../../OEBPS/Images/AR_x.png)
    å’Œ ![](../../OEBPS/Images/AR_micro.png) å‘é‡éƒ½æ˜¯ *d* Ã— 1ï¼‰ã€‚è€Œæ˜¯çŸ©é˜µä¸­ (*d*(*d* + 1))/2 ä¸ªå…ƒç´ çš„è”åˆåˆ†å¸ƒâ€”â€”å¯¹è§’çº¿åŠå…¶ä¸Šæˆ–ä¸‹çš„å…ƒç´ ï¼ˆå¯¹è§’çº¿ä¸Šçš„å…ƒç´ ä¸Šä¸‹ç›¸åŒï¼Œå› ä¸ºçŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼‰ã€‚
- en: 'The space of such matrices is called a *Wishart ensemble*. The probability
    of a random-precision matrix Î› of size *d* Ã— *d* can be expressed as a Wishart
    distribution. This distribution has two parameters:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·çš„çŸ©é˜µç©ºé—´è¢«ç§°ä¸º *Wishart ç³»ç»Ÿé›†*ã€‚ä¸€ä¸ªå¤§å°ä¸º *d* Ã— *d* çš„éšæœºç²¾åº¦çŸ©é˜µ Î› çš„æ¦‚ç‡å¯ä»¥è¡¨ç¤ºä¸ºWishartåˆ†å¸ƒã€‚è¿™ä¸ªåˆ†å¸ƒæœ‰ä¸¤ä¸ªå‚æ•°ï¼š
- en: '*Î½*, a scalar, satisfying *Î½* > *d* âˆ’ 1'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î½*ï¼Œä¸€ä¸ªæ»¡è¶³ *Î½* > *d* âˆ’ 1 çš„æ ‡é‡'
- en: W, a *d* Ã— *d* symmetric non-negative definite matrix
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wï¼Œä¸€ä¸ª *d* Ã— *d* çš„å¯¹ç§°éè´Ÿå®šçŸ©é˜µ
- en: The probability density function is
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: æ¦‚ç‡å¯†åº¦å‡½æ•°æ˜¯
- en: '![](../../OEBPS/Images/eq_13-07-n1.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_13-07-n1.png)'
- en: where
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: ğ’² denotes Wishart.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ğ’² è¡¨ç¤ºWishartã€‚
- en: '|**W**|, |**Î›**| denote the determinants of the matrices W and Î›, respectively.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '|**W**|, |**Î›**| åˆ†åˆ«è¡¨ç¤ºçŸ©é˜µ W å’Œ Î› çš„è¡Œåˆ—å¼ã€‚'
- en: '*Tr*(*A*) denotes the trace of a matrix *A* (sum of the diagonal elements).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Tr*(*A*) è¡¨ç¤ºçŸ©é˜µ *A* çš„è¿¹ï¼ˆå¯¹è§’çº¿å…ƒç´ ä¹‹å’Œï¼‰ã€‚'
- en: '*Î“* denotes the multivariate gamma function'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Î“* è¡¨ç¤ºå¤šå…ƒä¼½é©¬å‡½æ•°'
- en: '![](../../OEBPS/Images/eq_13-07-n2.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_13-07-n2.png)'
- en: The Wishart is the multivariate version of the gamma distribution. Its expected
    value is
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Wishart æ˜¯ä¼½é©¬åˆ†å¸ƒçš„å¤šå˜é‡ç‰ˆæœ¬ã€‚å…¶æœŸæœ›å€¼æ˜¯
- en: ğ”¼(**Î›**) = *Î½***W**
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ğ”¼(**Î›**) = *Î½***W**
- en: Its maxima occur at
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶æœ€å¤§å€¼å‡ºç°åœ¨
- en: '**Î›** = (*Î½* âˆ’ *d* âˆ’ 1)**W** for *Î½* â‰¥ *d* + 1'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**Î›** = (*Î½* âˆ’ *d* âˆ’ 1)**W** å¯¹äº *Î½* â‰¥ *d* + 1'
- en: 13.10.2 Estimating precision
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.10.2 ä¼°è®¡ç²¾åº¦
- en: As before, we model the data distribution as a Gaussian *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Î›**) = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Î›**^(âˆ’1)), where we have expressed the Gaussian in terms of the *precision matrix*
    Î› instead of the covariance matrix Î£, where **Î›** = **Î£**^(âˆ’1).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬å°†æ•°æ®åˆ†å¸ƒå»ºæ¨¡ä¸ºé«˜æ–¯åˆ†å¸ƒ *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Î›**) = ğ’©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Î›**^(âˆ’1))ï¼Œå…¶ä¸­æˆ‘ä»¬ç”¨ *ç²¾åº¦çŸ©é˜µ* Î› è€Œä¸æ˜¯åæ–¹å·®çŸ©é˜µ Î£ æ¥è¡¨ç¤ºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶ä¸­ **Î›** = **Î£**^(âˆ’1)ã€‚
- en: The training data set is *X* â‰¡ {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),â‹¯,
    ![](../../OEBPS/Images/AR_x.png)^((*i*)),â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    and its overall likelihood is
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒæ•°æ®é›†æ˜¯ *X* â‰¡ {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),â‹¯,
    ![](../../OEBPS/Images/AR_x.png)^((*i*)),â‹¯, ![](../../OEBPS/Images/AR_x.png)^((*n*))}ï¼Œå…¶æ•´ä½“ä¼¼ç„¶å‡½æ•°æ˜¯
- en: '![](../../OEBPS/Images/eq_13-07-n3.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_13-07-n3.png)'
- en: We model the prior probability of the precision matrix as a Wishart distribution.
    Hence,
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç²¾åº¦çŸ©é˜µçš„å…ˆéªŒæ¦‚ç‡å»ºæ¨¡ä¸ºWishartåˆ†å¸ƒã€‚å› æ­¤ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-07-n4.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_13-07-n4.png)'
- en: 'The posterior is another Wishart (owing to the Wishart conjugate prior property):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒåˆ†å¸ƒæ˜¯å¦ä¸€ä¸ªWishartåˆ†å¸ƒï¼ˆå½’åŠŸäºWishartå…±è½­å…ˆéªŒæ€§è´¨ï¼‰ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-n5.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../../OEBPS/Images/eq_13-07-n5.png)'
- en: Using Bayesâ€™ theorem for the training data set *X*,
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è´å¶æ–¯å®šç†å¯¹è®­ç»ƒæ•°æ®é›†*X*è¿›è¡Œä¼°è®¡ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-07-o1.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o1.png)'
- en: Letâ€™s study a pair of simple lemmas that will come in handy.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç ”ç©¶ä¸€å¯¹æœ‰ç”¨çš„ç®€å•å¼•ç†ã€‚
- en: '![](../../OEBPS/Images/eq_13-07-o2.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o2.png)'
- en: where *Tr* refers to Trace of a matrix (sum of diagonal elements).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­*Tr*æŒ‡çš„æ˜¯çŸ©é˜µçš„è¿¹ï¼ˆå¯¹è§’å…ƒç´ ä¹‹å’Œï¼‰ã€‚
- en: The first lemma is almost trivialâ€”the quadratic form ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    is a scalar, so of course it is the same as its trace.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå¼•ç†å‡ ä¹æ˜¯æ˜¾è€Œæ˜“è§çš„â€”â€”äºŒæ¬¡å‹![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)æ˜¯ä¸€ä¸ªæ ‡é‡ï¼Œæ‰€ä»¥å½“ç„¶å®ƒä¸å®ƒçš„è¿¹æ˜¯ç›¸åŒçš„ã€‚
- en: 'The second lemma follows directly from the matrix property of a trace: *Tr*(*BC*)
    = *Tr*(*CB*).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªå¼•ç†ç›´æ¥æ¥è‡ªè¿¹çš„çŸ©é˜µå±æ€§ï¼š*Tr*(*BC*) = *Tr*(*CB*).
- en: Using the lemmas, the exponent of the likelihood term is
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å¼•ç†ï¼Œä¼¼ç„¶é¡¹çš„æŒ‡æ•°ä¸º
- en: '![](../../OEBPS/Images/eq_13-07-o3.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o3.png)'
- en: where
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../../OEBPS/Images/eq_13-07-o4.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o4.png)'
- en: Thus, the posterior density is
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼ŒåéªŒå¯†åº¦ä¸º
- en: '![](../../OEBPS/Images/eq_13-07-o5.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o5.png)'
- en: Since *Tr*(*A*) + *Tr*(*B*) = *Tr*(*A* + *B*),
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äº*Tr*(*A*) + *Tr*(*B*) = *Tr*(*A* + *B*),
- en: '![](../../OEBPS/Images/eq_13-07-o6.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o6.png)'
- en: 'Comparing coefficients, we determine the unknown parameters of the posterior
    distribution:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æ¯”è¾ƒç³»æ•°ï¼Œæˆ‘ä»¬ç¡®å®šåéªŒåˆ†å¸ƒçš„æœªçŸ¥å‚æ•°ï¼š
- en: '![](../../OEBPS/Images/eq_13-07-o7.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o7.png)'
- en: where
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­
- en: '![](../../OEBPS/Images/eq_13-07-o8.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o8.png)'
- en: 'The maximum of the posterior density function, **ğ’²**(**Î›**;*Î½[n]*, **W**[n]),
    ields an estimate for the precision parameter of the data distribution: **Î›**
    = (*Î½[n]* âˆ’ *d* âˆ’ 1)**W**[n] for *Î½[n]* â‰¥ *d* + 1 i.e.,'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: åéªŒå¯†åº¦å‡½æ•°çš„æœ€å¤§å€¼ï¼Œ**ğ’²**(**Î›**;*Î½[n]*, **W**[n])ï¼Œç»™å‡ºäº†æ•°æ®åˆ†å¸ƒç²¾åº¦å‚æ•°çš„ä¼°è®¡ï¼š**Î›** = (*Î½[n]* âˆ’ *d*
    âˆ’ 1)**W**[n] å¯¹äº *Î½[n]* â‰¥ *d* + 1ï¼Œå³ï¼Œ
- en: '![](../../OEBPS/Images/eq_13-07-o9.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/eq_13-07-o9.png)'
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: A generative model that models the underlying data distribution can be more
    powerful than a black box discriminative model. Once we choose a model family,
    we need to estimate the model parameters, *Î¸*. We can estimate the best values
    of *Î¸* from the training data *X* using Bayesâ€™ theorem.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªèƒ½å¤Ÿæ¨¡æ‹Ÿåº•å±‚æ•°æ®åˆ†å¸ƒçš„ç”Ÿæˆæ¨¡å‹å¯èƒ½æ¯”ä¸€ä¸ªé»‘ç›’åˆ¤åˆ«æ¨¡å‹æ›´å¼ºå¤§ã€‚ä¸€æ—¦æˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªæ¨¡å‹æ—ï¼Œæˆ‘ä»¬éœ€è¦ä¼°è®¡æ¨¡å‹å‚æ•°ï¼Œ*Î¸*ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨è´å¶æ–¯å®šç†ä»è®­ç»ƒæ•°æ®*X*ä¸­ä¼°è®¡*Î¸*çš„æœ€ä½³å€¼ã€‚
- en: The posterior distribution *p*(*Î¸*|*X*) is a function of the product of likelihood
    *p*(*X*|*Î¸*) and the prior *p*(*Î¸*). The prior expresses our belief in the value
    of the parameters. The posterior is dominated by the prior for small data sets
    and the likelihood for large data sets. Injecting belief via a good prior distribution
    can be helpful in settings with very little training data.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åéªŒåˆ†å¸ƒ*p*(*Î¸*|*X*)æ˜¯ä¼¼ç„¶ç‡*p*(*X*|*Î¸*)å’Œå…ˆéªŒ*p*(*Î¸*)çš„ä¹˜ç§¯çš„å‡½æ•°ã€‚å…ˆéªŒè¡¨è¾¾äº†æˆ‘ä»¬å¯¹å‚æ•°å€¼çš„ä¿¡å¿µã€‚å¯¹äºå°æ•°æ®é›†ï¼ŒåéªŒä¸»è¦ç”±å…ˆéªŒå†³å®šï¼Œå¯¹äºå¤§æ•°æ®é›†ï¼ŒåéªŒä¸»è¦ç”±ä¼¼ç„¶ç‡å†³å®šã€‚é€šè¿‡ä¸€ä¸ªå¥½çš„å…ˆéªŒåˆ†å¸ƒæ³¨å…¥ä¿¡å¿µï¼Œåœ¨è®­ç»ƒæ•°æ®éå¸¸å°‘çš„æƒ…å†µä¸‹å¯èƒ½æ˜¯æœ‰å¸®åŠ©çš„ã€‚
- en: Maximum likelihood estimation only relies on the data, in contrast to maximum
    a posteriori (MAP) estimation, which relies on the data as well as the prior information.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä»…ä¾èµ–äºæ•°æ®ï¼Œä¸æœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰ç›¸å¯¹ï¼Œåè€…æ—¢ä¾èµ–äºæ•°æ®ä¹Ÿä¾èµ–äºå…ˆéªŒä¿¡æ¯ã€‚
- en: We can use Bayesian estimation for the mean of a Gaussian likelihood when the
    variance is known. When the likelihood is Gaussian *p*(*X*) âˆ¼ *N*(*Î¼*, *Ïƒ*), we
    model the prior as a normal distribution *p*(*Î¼*) âˆ¼ *N*(*Î¼*[0], *Ïƒ*[0]). The posterior
    distribution is also a normal distribution *p*(*Î¼*|*X*) âˆ¼ *N*(*Î¼[n]*, *Ïƒ[n]*),
    where ![](../../OEBPS/Images/eq_13-07-p1a.png) and ![](../../OEBPS/Images/eq_13-07-p2a.png).
    We can also use the estimated parameter to make predictions about new instances
    of data.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“æ–¹å·®å·²çŸ¥æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è´å¶æ–¯ä¼°è®¡é«˜æ–¯ä¼¼ç„¶ç‡çš„å‡å€¼ã€‚å½“ä¼¼ç„¶ç‡æ˜¯é«˜æ–¯åˆ†å¸ƒ*p*(*X*) âˆ¼ *N*(*Î¼*, *Ïƒ*), æˆ‘ä»¬å°†å…ˆéªŒæ¨¡å‹å»ºæ¨¡ä¸ºæ­£æ€åˆ†å¸ƒ*p*(*Î¼*)
    âˆ¼ *N*(*Î¼*[0], *Ïƒ*[0])ã€‚åéªŒåˆ†å¸ƒä¹Ÿæ˜¯ä¸€ä¸ªæ­£æ€åˆ†å¸ƒ*p*(*Î¼*|*X*) âˆ¼ *N*(*Î¼[n]*, *Ïƒ[n]*)ï¼Œå…¶ä¸­![](../../OEBPS/Images/eq_13-07-p1a.png)å’Œ![](../../OEBPS/Images/eq_13-07-p2a.png)ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä½¿ç”¨ä¼°è®¡çš„å‚æ•°å¯¹æ–°æ•°æ®å®ä¾‹è¿›è¡Œé¢„æµ‹ã€‚
- en: Weak priors imply a high degree of uncertainty/lower confidence in our prior
    belief and can easily be overwhelmed by the data. In contrast, strong priors imply
    a lower degree of uncertainty/higher confidence in our prior belief and will resist
    data overload.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¼±å…ˆéªŒæ„å‘³ç€æˆ‘ä»¬å¯¹å…ˆéªŒä¿¡å¿µçš„ä¸ç¡®å®šæ€§ç¨‹åº¦é«˜/ä¿¡å¿ƒä½ï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“è¢«æ•°æ®æ‰€æ·¹æ²¡ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¼ºå…ˆéªŒæ„å‘³ç€æˆ‘ä»¬å¯¹å…ˆéªŒä¿¡å¿µçš„ä¸ç¡®å®šæ€§ç¨‹åº¦ä½/ä¿¡å¿ƒé«˜ï¼Œå¹¶ä¸”èƒ½å¤ŸæŠµæŠ—æ•°æ®è¿‡è½½ã€‚
- en: For a specific family of likelihood, the choice of the prior that results in
    the posterior belonging to the same family as the prior is called a conjugate
    prior.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹äºç‰¹å®šçš„ä¼¼ç„¶æ—ï¼Œå¯¼è‡´åéªŒå±äºä¸å…ˆéªŒç›¸åŒæ—çš„å…ˆéªŒé€‰æ‹©è¢«ç§°ä¸ºå…±è½­å…ˆéªŒã€‚
- en: The gamma function is ![](../../OEBPS/Images/eq_13-07-p3a.png), and the gamma
    distribution is ![](../../OEBPS/Images/eq_13-07-p4a.png). The gamma distribution
    varies with different values of *Î±* and *Î²*.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¼½é©¬å‡½æ•°æ˜¯ ![](../../OEBPS/Images/eq_13-07-p3a.png)ï¼Œä¼½é©¬åˆ†å¸ƒæ˜¯ ![](../../OEBPS/Images/eq_13-07-p4a.png)ã€‚ä¼½é©¬åˆ†å¸ƒéš
    *Î±* å’Œ *Î²* çš„ä¸åŒå€¼è€Œå˜åŒ–ã€‚
- en: In the case of Bayesian estimation of the precision of the Gaussian likelihood
    for a known mean, the precision *Î»* is the inverse of the variance. We can model
    the prior as a gamma distribution ![](../../OEBPS/Images/eq_13-07-p5a.png). The
    posterior distribution is also a gamma distribution, ![](../../OEBPS/Images/eq_13-07-p6a.png),
    where ![](../../OEBPS/Images/eq_13-07-p7a.png) and ![](../../OEBPS/Images/eq_13-07-p8a.png).
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å·²çŸ¥å‡å€¼çš„æƒ…å†µä¸‹ï¼Œå¯¹é«˜æ–¯ä¼¼ç„¶ç²¾åº¦çš„è´å¶æ–¯ä¼°è®¡ä¸­ï¼Œç²¾åº¦ *Î»* æ˜¯æ–¹å·®çš„å€’æ•°ã€‚æˆ‘ä»¬å¯ä»¥å°†å…ˆéªŒæ¨¡å‹åŒ–ä¸ºä¸€ä¸ªä¼½é©¬åˆ†å¸ƒ ![](../../OEBPS/Images/eq_13-07-p5a.png)ã€‚åéªŒåˆ†å¸ƒä¹Ÿæ˜¯ä¸€ä¸ªä¼½é©¬åˆ†å¸ƒï¼Œ![](../../OEBPS/Images/eq_13-07-p6a.png)ï¼Œå…¶ä¸­
    ![](../../OEBPS/Images/eq_13-07-p7a.png) å’Œ ![](../../OEBPS/Images/eq_13-07-p8a.png)ã€‚
- en: In Bayesian estimation of both the mean and precision of a Gaussian likelihood,
    we model the prior as a normal-gamma distribution. The posterior is another normal-gamma
    distribution. The posterior distribution can be used to predict new data instances.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨å¯¹é«˜æ–¯ä¼¼ç„¶çš„å‡å€¼å’Œç²¾åº¦è¿›è¡Œè´å¶æ–¯ä¼°è®¡çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†å…ˆéªŒæ¨¡å‹åŒ–ä¸ºä¸€ä¸ªæ­£æ€-ä¼½é©¬åˆ†å¸ƒã€‚åéªŒä¹Ÿæ˜¯ä¸€ä¸ªæ­£æ€-ä¼½é©¬åˆ†å¸ƒã€‚åéªŒåˆ†å¸ƒå¯ä»¥ç”¨æ¥é¢„æµ‹æ–°çš„æ•°æ®å®ä¾‹ã€‚
- en: The multivariate setting of Bayesian inferencing of the mean of a Gaussian likelihood
    is known as precision. We can model the prior as a multivariate normal distribution;
    the posterior is also a multivariate normal distribution.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é«˜æ–¯ä¼¼ç„¶å‡å€¼çš„å¤šå˜é‡è´å¶æ–¯æ¨æ–­è¢«ç§°ä¸ºç²¾åº¦ã€‚æˆ‘ä»¬å¯ä»¥å°†å…ˆéªŒæ¨¡å‹åŒ–ä¸ºä¸€ä¸ªå¤šå…ƒæ­£æ€åˆ†å¸ƒï¼›åéªŒä¹Ÿæ˜¯ä¸€ä¸ªå¤šå…ƒæ­£æ€åˆ†å¸ƒã€‚
- en: The Wishart distribution is the multivariate version of the gamma distribution.
    With multivariate Bayesian inferencing of the precision of a Gaussian likelihood
    with a known mean, we can model the prior as a Wishart distribution. The corresponding
    posterior is also a Wishart distribution.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wishart åˆ†å¸ƒæ˜¯ä¼½é©¬åˆ†å¸ƒçš„å¤šå˜é‡ç‰ˆæœ¬ã€‚åœ¨å·²çŸ¥å‡å€¼çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨å¤šå…ƒè´å¶æ–¯æ¨æ–­é«˜æ–¯ä¼¼ç„¶ç²¾åº¦çš„ç²¾åº¦ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…ˆéªŒæ¨¡å‹åŒ–ä¸ºä¸€ä¸ª Wishart åˆ†å¸ƒã€‚ç›¸åº”åœ°ï¼ŒåéªŒä¹Ÿæ˜¯ä¸€ä¸ª
    Wishart åˆ†å¸ƒã€‚
