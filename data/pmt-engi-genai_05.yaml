- en: Chapter 5\. Vector Databases with FAISS and Pinecone
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章\. 使用FAISS和Pinecone的向量数据库
- en: This chapter introduces the concept of embeddings and vector databases, discussing
    how they can be used to provide relevant context in prompts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了嵌入和向量数据库的概念，讨论了它们如何用于在提示中提供相关上下文。
- en: A *vector database* is a tool most commonly used for storing text data in a
    way that enables querying based on similarity or semantic meaning. This technology
    is used to decrease hallucinations (where the AI model makes something up) by
    referencing data the model isn’t trained on, significantly improving the accuracy
    and quality of the LLM’s response. Use cases for vector databases also include
    reading documents, recommending similar products, or remembering past conversations.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量数据库*是一种最常用于以支持基于相似性或语义意义查询的方式存储文本数据的工具。这项技术通过引用模型未训练的数据来减少幻觉（即AI模型编造内容），显著提高了LLM（大型语言模型）响应的准确性和质量。向量数据库的应用案例还包括阅读文档、推荐类似产品或记住过去的对话。'
- en: '*Vectors* are lists of numbers representing text (or images), which you might
    think of as coordinates for a location. The vector for the word *mouse* using
    OpenAI’s text-embedding-ada-002 model is a list of 1,536 numbers, each representing
    the value for a feature the embedding model learned in training:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量*是表示文本（或图像）的数字列表，你可以将其视为位置的坐标。使用OpenAI的text-embedding-ada-002模型，单词*mouse*的向量是一个包含1,536个数字的列表，每个数字代表嵌入模型在训练中学习的特征值：'
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When these models are trained, texts that appear together in the training data
    will be pushed closer together in values, and texts that are unrelated will be
    pushed further away. Imagine we trained a simple model with only two parameters,
    `Cartoon` and `Hygiene`, that must describe the entire world, but only in terms
    of these two variables. Starting from the word *mouse*, increasing the value for
    the parameter `Cartoon` we would travel toward the most famous cartoon mouse,
    `mickey mouse`, as shown in [Figure 5-1](#figure-5-1). Decreasing the value for
    the `Hygiene` parameter would take us toward `rat`, because rats are rodents similar
    to mice, but are associated with plague and disease (i.e., being unhygenic).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些模型被训练时，训练数据中一起出现的文本在值上会被推得更近，而无关的文本会被推得更远。想象我们训练了一个只有两个参数的简单模型，`Cartoon`和`Hygiene`，必须用这两个变量来描述整个世界。从单词*mouse*开始，增加`Cartoon`参数的值，我们会走向最著名的卡通老鼠，`mickey
    mouse`，如图[图5-1](#figure-5-1)所示。减少`Hygiene`参数的值会带我们走向`rat`，因为老鼠和老鼠是啮齿类动物，但与瘟疫和疾病（即不卫生）有关。
- en: '![pega 0501](assets/pega_0501.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0501](assets/pega_0501.png)'
- en: Figure 5-1\. 2-D vector distances
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-1\. 2-D向量距离
- en: Each location on the graph can be found by two numbers on the x- and y-axes,
    which represent the features of the model `Cartoon` and `Hygiene`. In reality,
    vectors can have thousands of parameters, because having more parameters allows
    the model to capture a wider range of similarities and differences. Hygiene is
    not the only difference between mice and rats, and Mickey Mouse isn’t just a cartoon
    mouse. These features are learned from the data in a way that makes them hard
    for humans to interpret, and we would need a graph with thousands of axes to display
    a location in *latent space* (the abstract multidimensional space formed by the
    model’s parameters). Often there is no human-understandable explanation of what
    a feature means. However, we can create a simplified two-dimensional projection
    of the distances between vectors, as has been done in [Figure 5-2](#figure-5-2).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图表上的每个位置都可以通过x轴和y轴上的两个数字找到，这些数字代表模型的`Cartoon`和`Hygiene`特征。实际上，向量可以有数千个参数，因为更多的参数允许模型捕捉更广泛的相似性和差异性。卫生并不是老鼠和老鼠之间唯一的区别，米老鼠也不只是一个卡通老鼠。这些特征是以一种使人类难以解释的方式从数据中学习到的，我们需要一个有数千个轴的图表来显示*潜在空间*（由模型参数形成的抽象的多维空间）中的位置。通常，没有人类可以理解的特征含义的解释。然而，我们可以创建一个简化的二维向量距离投影，如图[图5-2](#figure-5-2)所示。
- en: To conduct a vector search, you first get the vector (or location) of what you
    want to look up and find the `k` closest records in the database. In this case
    the word *mouse* is closest to `mickey mouse`, `cheese`, and `trap` where `k=3`
    (return the three nearest records). The word *rat* is excluded if `k=3`, but would
    be included if `k=4` as it is the next closest vector. The word *airplane* in
    this example is far away because it is rarely associated with the word *mouse*
    in the training data. The word *ship* is still colocated near the other forms
    of transport but is closer to `mouse` and `rat` because they are often found on
    ships, as per the training data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行向量搜索，你首先获取你想要查找的内容的向量（或位置），然后在数据库中找到最近的`k`条记录。在这种情况下，单词`mouse`与`mickey mouse`、`cheese`和`trap`最接近，其中`k=3`（返回三条最近的记录）。如果`k=3`，则排除单词`rat`，但如果`k=4`，则将其包括在内，因为它是最接近的下一个向量。在这个例子中，单词`airplane`离得很远，因为它在训练数据中很少与单词`mouse`相关联。单词`ship`仍然与其他交通方式一起出现在同一位置，但它比`mouse`和`rat`更接近，因为根据训练数据，它们经常出现在船上。
- en: '![pega 0502](assets/pega_0502.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![pega 0502](assets/pega_0502.png)'
- en: Figure 5-2\. Multidimensional vector distances
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-2. 多维向量距离
- en: A vector database stores the text records with their vector representation as
    the key. This is unlike other types of databases, where you might find records
    based on an ID, relation, or where the text contains a string. For example, if
    you queried a relational database based on the text in [Figure 5-2](#figure-5-2)
    to find records where text contains `mouse`, you’d return the record `mickey mouse`
    but nothing else, as no other record contains that exact phrase. With vectors
    search you could also return the records `cheese` and `trap`, because they are
    closely associated, even though they aren’t an exact match for your query.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库以文本记录及其向量表示作为键来存储文本。这与其他类型的数据库不同，在其他类型的数据库中，你可能根据ID、关系或文本中包含的字符串来查找记录。例如，如果你基于[图5-2](#figure-5-2)中的文本查询关系型数据库以查找包含`mouse`的记录，你会返回记录`mickey
    mouse`，但没有其他记录，因为没有其他记录包含该确切短语。使用向量搜索，你也可以返回记录`cheese`和`trap`，尽管它们与你的查询不是完全匹配，但它们密切相关。
- en: 'The ability to query based on similarity is extremely useful, and vector search
    powers a lot of AI functionality. For example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基于相似性进行查询的能力非常有用，向量搜索为许多AI功能提供了动力。例如：
- en: Document reading
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 文档阅读
- en: Find related sections of text to read in order to provide a more accurate answer.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 找到相关的文本部分以便阅读，以提供更准确的答案。
- en: Recommendation systems
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Discover similar products or items in order to suggest them to a user.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 发现类似的产品或项目，以便向用户推荐。
- en: Long-term memory
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆
- en: Look up relevant snippets of conversation history so a chatbot remembers past
    interactions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 查找相关的对话历史片段，以便聊天机器人记住过去的交互。
- en: AI models are able to handle these tasks at small scale, as long as your documents,
    product list, or conversation memory fits within the token limits of the model
    you’re using. However, at scale you quite quickly run into token limits and excess
    cost from passing too many tokens in each prompt. OpenAI’s `gpt-4-1106-preview`
    was [released in November 2023](https://oreil.ly/KMNU8) with an enormous 128,000
    token context window, but it costs 10 times more per token than `gpt-3.5-turbo`,
    which has 88% fewer tokens and was released a year earlier. The more efficient
    approach is to look up only the most relevant records to pass into the prompt
    at runtime in order to provide the most relevant context to form a response. This
    practice is typically referred to as RAG.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AI模型能够处理这些任务，只要你的文档、产品列表或对话记忆适合你使用的模型的代币限制。然而，在规模上，你很快就会遇到代币限制和过多的费用，因为每个提示中传递的代币太多。OpenAI的`gpt-4-1106-preview`于2023年11月发布，拥有巨大的128,000个代词上下文窗口，但每个代词的成本是`gpt-3.5-turbo`的10倍，后者有88%的代词更少，并且比它早一年发布。更有效的方法是在运行时只查找最相关的记录并将其传递到提示中，以提供最相关的上下文来形成回答。这种做法通常被称为RAG。
- en: Retrieval Augmented Generation (RAG)
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检索增强生成（RAG）
- en: Vector databases are a key component of RAG, which typically involves searching
    by similarity to the query, retrieving the most relevant documents, and inserting
    them into the prompt as context. This lets you stay within what fits in the current
    context window, while avoiding spending money on wasted tokens by inserting irrelevant
    text documents in the context.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库是RAG的关键组成部分，通常涉及根据查询的相似性进行搜索，检索最相关的文档，并将它们作为上下文插入到提示中。这让你可以保持在当前上下文窗口内，同时避免通过插入无关文本文档来浪费代币而花费不必要的费用。
- en: Retrieval can also be done using traditional database searches or web browsing,
    and in many cases a vector search by semantic similarity is not necessary. RAG
    is typically used to solve hallucinations in open-ended scenarios, like a user
    talking to a chatbot that is prone to making things up when asked about something
    not in its training data. Vector search can insert documents that are semantically
    similar to the user query into the prompt, greatly decreasing the chances the
    chatbot will hallucinate.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 检索也可以使用传统的数据库搜索或网络浏览来完成，在许多情况下，使用语义相似性的向量搜索并不是必要的。RAG通常用于解决开放场景中的幻觉问题，例如用户与一个在询问不在其训练数据中的内容时容易编造东西的聊天机器人交谈。向量搜索可以将与用户查询语义相似的文档插入到提示中，大大降低聊天机器人产生幻觉的可能性。
- en: 'For example, if your author Mike told a chatbot “My name is Mike,” then three
    messages later asked, “What is my name?” it can easily recall the right answer.
    The message containing Mike’s name is still within the context window of the chat.
    However, if it was 3,000 messages ago, the text of those messages may be too large
    to fit inside the context window. Without this important context, it might hallucinate
    a name or refuse to answer for lack of information. A keyword search might help
    but could return too many irrelevant documents or fail to recall the right context
    in which the information was captured in the past. There may be many times Mike
    mentioned the word *name* in different formats, and for different reasons. By
    passing the question to the vector database, it can return the top three similar
    messages from the chat that match what the user asked:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的作者Mike告诉聊天机器人“我的名字是Mike”，然后过了三条消息后问“我的名字是什么？”它可以轻松地回忆起正确的答案。包含Mike名字的消息仍然在聊天上下文窗口内。然而，如果它是在3,000条消息之前，这些消息的文本可能太大而无法放入上下文窗口。没有这个重要的上下文，它可能会幻想一个名字或者因为没有信息而拒绝回答。关键词搜索可能会有所帮助，但可能会返回太多不相关的文档，或者无法回忆起过去捕获信息时的正确上下文。Mike可能多次在不同格式和不同原因下提到单词*名字*。通过将问题传递到向量数据库，它可以返回与用户询问内容最相似的聊天中的前三条消息：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It’s impossible to pass all 3,000 past messages into the prompt for most models,
    and for a traditional search the AI model would have to formulate the right search
    query, which can be unreliable. Using the RAG pattern, you would pass the current
    user message to a vector search function, and return the most relevant three records
    as context, which the chatbot can then use to respond correctly.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数模型来说，将所有3,000条历史消息传递到提示中是不可能的，而对于传统的搜索，AI模型必须制定正确的搜索查询，这可能会不可靠。使用RAG模式，您会将当前用户消息传递给向量搜索函数，并返回最相关的三条记录作为上下文，然后聊天机器人可以使用这些上下文来正确地回答。
- en: Give Direction
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定方向
- en: Rather than inserting static knowledge into the prompt, vector search allows
    you to dynamically insert the most relevant knowledge into the prompt.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与将静态知识插入提示中不同，向量搜索允许您动态地将最相关的知识插入到提示中。
- en: 'Here’s how the process works for production applications using RAG:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用RAG的生产应用程序的工作流程：
- en: Break documents into chunks of text.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将文档分解成文本块。
- en: Index chunks in a vector database.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在向量数据库中索引数据块。
- en: Search by vector for similar records.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过向量搜索相似记录。
- en: Insert records into the prompt as context.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将记录作为上下文插入到提示中。
- en: In this instance, the documents would be all the 3,000 past user messages to
    serve as the chatbot’s memory, but it could also be sections of a PDF document
    we uploaded to give the chatbot the ability to read, or a list of all the relevant
    products you sell to enable the chatbot to make a recommendation. The ability
    of our vector search to find the most similar texts is wholly dependent on the
    AI model used to generate the vectors, referred to as *embeddings* when you’re
    dealing with semantic or contextual information.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，文档将是所有3,000条过去用户消息，作为聊天机器人的记忆，但它也可以是上传到聊天机器人的PDF文档的部分，以使其能够阅读，或者是一份所有相关产品的列表，以使聊天机器人能够做出推荐。我们向量搜索找到最相似文本的能力完全取决于用于生成向量的AI模型，当处理语义或上下文信息时，这些向量被称为*嵌入*。
- en: Introducing Embeddings
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入嵌入
- en: The word *embeddings* typically refers to the vector representation of the text
    returned from a pretrained AI model. At the time of writing, the standard model
    for generating embeddings is OpenAI’s text-embedding-ada-002, although embedding
    models have been available long before the advent of generative AI.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *embeddings* 通常指的是从预训练 AI 模型返回的文本的向量表示。在撰写本文时，生成嵌入的标准模型是 OpenAI 的 text-embedding-ada-002，尽管嵌入模型在生成式
    AI 出现之前就已经存在。
- en: Although it is helpful to visualize vector spaces as a two-dimensional chart,
    as in [Figure 5-2](#figure-5-2), in reality the embeddings returned from text-embedding-ada-002
    are in 1,536 dimensions, which is difficult to depict graphically. Having more
    dimensions allows the model to capture deeper semantic meaning and relationships.
    For example, while a 2-D space might be able to separate cats from dogs, a 300-D
    space could capture information about the differences between breeds, sizes, colors,
    and other intricate details. The following code shows how to retrieve embeddings
    from the OpenAI API. The code for the following examples is included in the [GitHub
    repository](https://oreil.ly/6RzTy) for this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将向量空间可视化为二维图表很有帮助，如 [图 5-2](#figure-5-2) 所示，但实际上从 text-embedding-ada-002 返回的嵌入是在
    1,536 维度上，这在图形上很难表示。更多的维度允许模型捕捉更深的语义意义和关系。例如，一个二维空间可能能够将猫和狗分开，而一个 300 维的空间可以捕捉关于品种、大小、颜色和其他复杂细节的信息。以下代码展示了如何从
    OpenAI API 中检索嵌入。以下示例的代码包含在此书的 [GitHub 仓库](https://oreil.ly/6RzTy) 中。
- en: 'Input:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Output:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This code uses the OpenAI API to create an embedding for a given input text
    using a specific embedding model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用 OpenAI API 通过特定的嵌入模型为给定输入文本创建嵌入：
- en: '`from openai import OpenAI` imports the OpenAI library, and `client = OpenAI()`
    sets up the client. It retrieves your OpenAI API key from an environment variable
    `OPENAI_API_KEY` in order to charge the cost of the embeddings to your account.
    You need to set this in your environment (usually in an *.env* file), which can
    be obtained by creating an account and visiting [*https://oreil.ly/apikeys*](https://oreil.ly/apikeys).'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`from openai import OpenAI` 导入 OpenAI 库，并使用 `client = OpenAI()` 设置客户端。它从环境变量
    `OPENAI_API_KEY` 中检索您的 OpenAI API 密钥，以便将嵌入的成本记入您的账户。您需要在您的环境中设置此变量（通常在 *.env*
    文件中），这可以通过创建账户并访问 [*https://oreil.ly/apikeys*](https://oreil.ly/apikeys) 来获取。'
- en: '`response = client.embeddings.create(...)`: This line calls the `create` method
    of the `Embedding` class from the `client` from the OpenAI library. The method
    takes two arguments:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`response = client.embeddings.create(...)`：此行调用 OpenAI 库中 `Embedding` 类的 `create`
    方法。该方法接受两个参数：'
- en: '`input`: This is where you provide the text string for which you want to generate
    an embedding.'
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：这是您提供要生成嵌入的文本字符串的地方。'
- en: '`model`: This specifies the embedding model you want to use. In this case,
    it is `text-embedding-ada-002`, which is a model within the OpenAI API.'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`model`：这指定了您想要使用的嵌入模型。在这种情况下，它是 `text-embedding-ada-002`，这是 OpenAI API 中的一个模型。'
- en: '`embeddings = [r.embedding for r in response.data]`: After the API call, the
    `response` object contains the generated embeddings in JSON format. This line
    extracts the actual numerical embedding from the response, by iterating through
    a list of embeddings in `response.data`.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`embeddings = [r.embedding for r in response.data]`：在 API 调用之后，`response` 对象包含以
    JSON 格式生成的嵌入。此行通过遍历 `response.data` 中的嵌入列表，从响应中提取实际的数值嵌入。'
- en: After executing this code, the `embeddings` variable will hold the numerical
    representation (embedding) of the input text, which can then be used in various
    NLP tasks or machine learning models. This process of retrieving or generating
    embeddings is sometimes referred to as *document loading*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，`embeddings` 变量将包含输入文本的数值表示（嵌入），然后可以用于各种 NLP 任务或机器学习模型。检索或生成嵌入的过程有时被称为
    *文档加载*。
- en: The term *loading* in this context refers to the act of computing or retrieving
    the numerical (vector) representations of text from a model and storing them in
    a variable for later use. This is distinct from the concept of *chunking*, which
    typically refers to breaking down a text into smaller, manageable pieces or chunks
    to facilitate processing. These two techniques are regularly used in conjunction
    with each other, as it’s often useful to break large documents up into pages or
    paragraphs to facilitate more accurate matching and to only pass the most relevant
    tokens into the prompt.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在此上下文中，术语 *loading* 指的是从模型中计算或检索文本的数值（向量）表示并将其存储在变量中以供以后使用的行为。这与 *chunking*
    的概念不同，通常指的是将文本分解成更小、更易于管理的片段或块，以促进处理。这两种技术通常一起使用，因为通常将大型文档拆分成页面或段落以促进更准确的匹配，并且只将最相关的标记传递到提示中。
- en: There is a cost associated with retrieving embeddings from OpenAI, but it is
    relatively inexpensive at $0.0004 per 1,000 tokens at the time of writing. For
    instance, the King James version of the Bible, which comprises around 800,000
    words or approximately 4,000,000 tokens, would cost about $1.60 to retrieve all
    the embeddings for the entire document.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 从 OpenAI 获取嵌入存在成本，但在写作时每 1,000 个标记的费用相对较低，为 0.0004 美元。例如，包含大约 800,000 个单词或约
    4,000,000 个标记的《圣经》钦定版，检索整个文档的所有嵌入将花费大约 1.60 美元。
- en: Paying for embeddings from OpenAI is not your only option. There are also open-source
    models you can use, for example, the [Sentence Transformers library](https://oreil.ly/8OV3c)
    provided by Hugging Face, which has 384 dimensions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为 OpenAI 的嵌入付费并不是你的唯一选择。你还可以使用开源模型，例如 Hugging Face 提供的 [Sentence Transformers
    库](https://oreil.ly/8OV3c)，它具有 384 维度。
- en: 'Input:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This code uses the Hugging Face API to obtain embeddings for a list of text
    inputs using a pre-trained model. The model used here is the `sentence-transformers/all-MiniLM-L6-v2`,
    which is a smaller version of BERT, an open source NLP model introduced by Google
    in 2017 (based on the transformer model), which is optimized for sentence-level
    tasks. Here’s how it works step-by-step:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用 Hugging Face API 通过预训练模型为文本输入列表获取嵌入。这里使用的模型是 `sentence-transformers/all-MiniLM-L6-v2`，这是
    BERT 的一个较小版本，BERT 是 Google 在 2017 年（基于 Transformer 模型）引入的开源 NLP 模型，它针对句子级任务进行了优化。以下是它的工作步骤：
- en: '`model_id` is assigned the identifier of the pre-trained model, `sentence-transformers/all-MiniLM-L6-v2`.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`model_id` 被分配了预训练模型的标识符，`sentence-transformers/all-MiniLM-L6-v2`。'
- en: '`hf_token = os.getenv("HF_TOKEN")` retrieves the API key for the Hugging Face
    API token from your environment. You need to set this in your environment with
    your own token, which can be obtained by creating an account and visiting [*https://hf.co/settings/tokens*](https://hf.co/settings/tokens).'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`hf_token = os.getenv("HF_TOKEN")` 从你的环境中检索 Hugging Face API 令牌。你需要使用自己的令牌设置此环境，该令牌可以通过创建账户并访问
    [*https://hf.co/settings/tokens*](https://hf.co/settings/tokens) 获取。'
- en: The `requests` library is imported to make HTTP requests to the API.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入了 `requests` 库以向 API 发送 HTTP 请求。
- en: '`api_url` is assigned the URL for the Hugging Face API, with the model ID included
    in the URL.'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`api_url` 被分配了 Hugging Face API 的 URL，其中包含模型 ID。'
- en: '`headers` is a dictionary containing the authorization header with your Hugging
    Face API token.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`headers` 是一个包含授权头部的字典，其中包含你的 Hugging Face API 令牌。'
- en: The `query()` function is defined, which takes a list of text inputs and sends
    a `POST` request to the Hugging Face API with the appropriate headers and JSON
    payload containing the inputs and an option to wait for the model to become available.
    The function then returns the JSON response from the API.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义了 `query()` 函数，它接受一个文本输入列表，并通过适当的头部和包含输入以及等待模型可用的选项的 JSON 有效负载向 Hugging Face
    API 发送 `POST` 请求。然后，该函数返回 API 的 JSON 响应。
- en: '`texts` is a list of strings from your database.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`texts` 是从你的数据库中获取的字符串列表。'
- en: '`output` is assigned the result of calling the `query()` function with the
    `texts` list.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`output` 被分配为调用 `query()` 函数并使用 `texts` 列表的结果。'
- en: The `output` variable is printed, which will display the feature embeddings
    for the input texts.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印了 `output` 变量，它将显示输入文本的特征嵌入。
- en: When you run this code, the script will send text to the Hugging Face API, and
    the API will return embeddings for each string of text sent.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行此代码时，脚本将向 Hugging Face API 发送文本，API 将返回每个文本字符串的嵌入。
- en: If you pass the same text into an embedding model, you’ll get the same vector
    back every time. However, vectors are not usually comparable across models (or
    versions of models) due to differences in training. The embeddings you get from
    OpenAI are different from those you get from BERT or spaCy (a natural language
    processing library).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将相同的文本输入到嵌入模型中，你将每次都得到相同的向量。然而，由于训练的不同，向量通常不可跨模型（或模型的版本）比较。从OpenAI获得的嵌入与从BERT或spaCy（一个自然语言处理库）获得的嵌入不同。
- en: The main difference with embeddings generated by modern transformer models is
    that the vectors are contextual rather than static, meaning the word *bank* would
    have different embeddings in the context of a *riverbank* versus *financial bank*.
    The embeddings you get from OpenAI Ada 002 and HuggingFace Sentence Transformers
    are examples of dense vectors, where each number in the array is almost always
    nonzero (i.e., they contain semantic information). There are also [sparse vectors](https://oreil.ly/d1cmb),
    which normally have a large number of dimensions (e.g., 100,000+) with many of
    the dimensions having a value of zero. This allows capturing specific important
    features (each feature can have its own dimension), which tends to be important
    for performance in keyword-based search applications. Most AI applications use
    dense vectors for retrieval, although hybrid search (both dense and sparse vectors)
    is rising in popularity, as both similarity and keyword search can be useful in
    combination.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与现代转换器模型生成的嵌入相比，主要区别在于向量是上下文相关的而不是静态的，这意味着单词*bank*在*河岸*和*金融机构*的上下文中会有不同的嵌入。从OpenAI
    Ada 002和HuggingFace Sentence Transformers获得的嵌入是密集向量的例子，其中数组中的每个数字几乎总是非零的（即，它们包含语义信息）。还有[稀疏向量](https://oreil.ly/d1cmb)，通常具有大量维度（例如，100,000+），其中许多维度具有零值。这允许捕捉特定的、重要的特征（每个特征可以有自己的维度），这对于基于关键字的搜索应用中的性能往往很重要。大多数AI应用使用密集向量进行检索，尽管混合搜索（密集和稀疏向量）越来越受欢迎，因为相似性和关键字搜索可以结合使用。
- en: The accuracy of the vectors is wholly reliant on the accuracy of the model you
    use to generate the embeddings. Whatever biases or knowledge gaps the underlying
    models have will also be an issue for vector search. For example, the `text-embedding-ada-002`
    model is currently only trained up to August 2020 and therefore is unaware of
    any new words or new cultural associations that formed after that cutoff date.
    This can cause a problem for use cases that need more recent context or niche
    domain knowledge not available in the training data, which may necessitate training
    a custom model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的准确性完全依赖于你用来生成嵌入的模型的准确性。底层模型中存在的任何偏差或知识差距也将成为向量搜索的问题。例如，`text-embedding-ada-002`模型目前仅训练到2020年8月，因此对该截止日期之后形成的新词或新的文化关联一无所知。这可能会对需要更多近期上下文或训练数据中不可用的特定领域知识的用例造成问题，这可能需要训练自定义模型。
- en: In some instances it might make sense to train your own embedding model. For
    instance, you might do this if the text used has a domain-specific vocabulary
    where specific words have a meaning separate from the generally accepted meaning
    of the word. One example might be tracing the language used by toxic groups on
    social media like Q-Anon, who evolve the language they use in posts to bypass
    moderation actions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，训练自己的嵌入模型可能是有意义的。例如，如果你使用的文本具有特定领域的词汇，其中某些单词的含义与通常接受的含义不同，你可能就会这样做。一个例子可能是追踪社交媒体上如Q-Anon等有毒群体使用的语言，他们通过在帖子中演变语言来规避审查措施。
- en: Training your own embeddings can be done with tools like word2vec, a method
    to represent words in a vector space, enabling you to capture the semantic meanings
    of words. More advanced models may be used, like GloVe (Global Vectors for Word
    Representation), which is used by spaCy for its embeddings, which are trained
    on the Common Crawl dataset, an open source snapshot of the web. The library Gensim
    offers a simple process for training your own custom embeddings using the [open
    source algorithm](https://oreil.ly/RmXVR) word2vec.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用像word2vec这样的工具可以训练自己的嵌入，这是一种在向量空间中表示单词的方法，使你能够捕捉单词的语义含义。可以使用更高级的模型，如GloVe（全球单词表示向量），它被spaCy用于其嵌入，该嵌入是在Common
    Crawl数据集上训练的，这是一个开源的网页快照。Gensim库提供了一个简单的流程，使用[开源算法](https://oreil.ly/RmXVR) word2vec来训练自己的自定义嵌入。
- en: 'Input:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This code creates a word2vec model using the Gensim library and then uses the
    model to determine words that are similar to a given word. Let’s break it down:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用 Gensim 库创建了一个 word2vec 模型，然后使用该模型确定与给定单词相似的单词。让我们将其分解：
- en: The variable `sentences` contains a list of sentences, where each sentence is
    a list of words. This is the data on which the Word2Vec model will be trained.
    In a real application, instead of such hardcoded sentences, you’d often load a
    large corpus of text and preprocess it to obtain such a list of tokenized sentences.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量 `sentences` 包含一个句子列表，其中每个句子都是一个单词列表。这是 Word2Vec 模型将要训练的数据。在实际应用中，你通常会加载一个大型文本语料库，并对其进行预处理以获得这样的标记句子列表。
- en: 'An instance of the `word2vec` class is created to represent the model. While
    initializing this instance, several parameters are provided:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个 `word2vec` 类的实例来表示模型。在初始化这个实例时，提供了几个参数：
- en: '`sentences`: This is the training data.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sentences`：这是训练数据。'
- en: '`vector_size=100`: This defines the size of the word vectors. So each word
    will be represented as a 100-dimensional vector.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vector_size=100`：这定义了单词向量的大小。因此，每个单词都将表示为一个100维的向量。'
- en: '`window=5`: This represents the maximum distance between the current and predicted
    word within a sentence.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`window=5`：这表示句子中当前单词和预测单词之间的最大距离。'
- en: '`min_count=1`: This ensures that even words that appear only once in the dataset
    will have vectors created for them.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_count=1`：这确保了即使只出现在数据集中一次的单词也会为其创建向量。'
- en: '`workers=4`: Number of CPU cores to use during training. It speeds up training
    on multicore machines.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`workers=4`：在训练期间使用的 CPU 核心数。它可以在多核机器上加速训练。'
- en: '`seed=36`: This is set for reproducibility so that the random processes in
    training deliver the same result each time (not guaranteed with multiple workers).'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed=36`：这是为了可重复性而设置的，以确保训练中的随机过程每次都能产生相同的结果（不保证在多个工作者的情况下）。'
- en: After training, the model is saved to a file named `custom_word2vec_model.model`
    using the `save` method. This allows you to reuse the trained model later without
    needing to train it again.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，使用 `save` 方法将模型保存到名为 `custom_word2vec_model.model` 的文件中。这允许你在以后重用训练好的模型，而无需再次进行训练。
- en: There is a commented-out line that shows how to load the model back from the
    saved file. This is useful when you want to load a pre-trained model in a different
    script or session.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 文件中存在一条被注释掉的行，展示了如何从保存的文件中重新加载模型。这在你想在不同的脚本或会话中加载预训练模型时非常有用。
- en: The variable `vector` is assigned the vector representation of the word *cake*.
    This vector can be used for various purposes, like similarity calculations, arithmetic
    operations, etc.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变量 `vector` 被分配了单词 *cake* 的向量表示。这个向量可以用于各种目的，如相似度计算、算术运算等。
- en: The `most_similar` method is used to find words that are most similar to the
    provided vector (in this case, the vector for *cake*). The method returns the
    top five (`topn=5`) most similar words.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`most_similar` 方法用于查找与提供的向量（在这种情况下，是 *cake* 的向量）最相似的单词。该方法返回最相似的五个单词（`topn=5`）。'
- en: The `similarity` method queries the similarity between *cake* and *lie* direction,
    showing a small positive value.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`similarity` 方法查询了 *cake* 和 *lie* 方向之间的相似度，显示了一个小的正值。'
- en: The dataset is small and heavily repetitive, which might not provide a diverse
    context to properly learn the relationship between the words. Normally, word2vec
    benefits from larger and more diverse corpora and typically won’t get good results
    until you’re into the tens of millions of words. In the example we set a seed
    value to cherrypick one instance where *lie* came back in the top five results,
    but if you remove that seed, you’ll find it rarely discovers the association successfully.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集很小且高度重复，这可能无法提供足够多样的上下文来正确学习单词之间的关系。通常，word2vec 从更大、更多样化的语料库中受益，并且通常需要达到数千万个单词才能获得良好的结果。在我们的例子中，我们设置了一个种子值来挑选出一个实例，其中
    *lie* 出现在前五个结果中，但如果你移除那个种子，你会发现它很少能够成功发现这种关联。
- en: For smaller document sizes a simpler technique *TF-IDF* (Term Frequency-Inverse
    Document Frequency) is recommended, a statistical measure used to evaluate the
    importance of a word in a document relative to a collection of documents. The
    TF-IDF value increases proportionally to the number of times a word appears in
    the document but is offset by the frequency of the word in the wider corpus, which
    helps to adjust for the fact that some words are generally more common than others.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较小的文档大小，建议使用更简单的技术*TF-IDF*（词频-逆文档频率），这是一种用于评估单词在文档中相对于文档集合的重要性的一种统计度量。TF-IDF值与单词在文档中出现的次数成正比，但被单词在更广泛的语料库中的频率所抵消，这有助于调整某些单词通常比其他单词更常见的事实。
- en: To compute the similarity between *cake* and *lie* using TF-IDF, you can use
    the open source [scientific library](https://oreil.ly/gHb3F) scikit-learn and
    compute the *cosine similarity* (a measure of distance between two vectors). Words
    that are frequently colocated in sentences will have high cosine similarity (approaching
    1), whereas words that appear infrequently will show a low value (or 0, if not
    co-located at all). This method is robust to even small documents like our toy
    example.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用TF-IDF计算`cake`和`lie`之间的相似度，可以使用开源[科学库](https://oreil.ly/gHb3F) scikit-learn并计算余弦相似度（两个向量之间距离的度量）。在句子中经常共现的单词将具有高余弦相似度（接近1），而出现频率低的单词将显示低值（或0，如果根本不共现）。这种方法甚至对像我们的玩具示例那样的小文档也具有鲁棒性。
- en: 'Input:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s break down this code step-by-step:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地分解这段代码：
- en: The `sentences` variable is reused from the previous example. The code converts
    these lists of words into full sentences (strings) using a list comprehension,
    resulting in `document_list`.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从上一个示例中重用了`sentences`变量。代码使用列表推导将这些单词列表转换为完整的句子（字符串），从而得到`document_list`。
- en: An instance of `TfidfVectorizer` is created. The `fit_transform` method of the
    vectorizer is then used to convert the `document_list` into a matrix of TF-IDF
    features, which is stored in `tfidf_matrix`.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个`TfidfVectorizer`实例。然后使用向量器的`fit_transform`方法将`document_list`转换为TF-IDF特征矩阵，该矩阵存储在`tfidf_matrix`中。
- en: The code extracts the position (or index) of the words *cake* and *lie* in the
    feature matrix using the `vocabulary_` attribute of the vectorizer.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码使用向量器的`vocabulary_`属性提取了单词`cake`和`lie`在特征矩阵中的位置（或索引）。
- en: The TF-IDF vector corresponding to the word *cake* is extracted from the matrix
    and reshaped.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从矩阵中提取与单词`cake`对应的TF-IDF向量并将其重塑。
- en: The cosine similarity between the vector for *cake* and all other vectors in
    the TF-IDF matrix is computed. This results in a list of similarity scores.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算了`cake`向量与TF-IDF矩阵中所有其他向量的余弦相似度。这产生了一个相似度分数列表。
- en: The indices of the top six most similar words (including *cake*) are identified.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定了最相似的前六个单词（包括`cake`）的索引。
- en: Using these indices, the top five words (excluding *cake*) with the highest
    similarity to *cake* are retrieved and printed.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用这些索引，检索与`cake`最相似的前五个单词（不包括`cake`）并打印出来。
- en: The cosine similarity between the TF-IDF vectors of the words *cake* and *lie*
    is computed. Since the result is a matrix, the code computes the mean similarity
    value across all values in this matrix and then prints the average similarity.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算单词`cake`和`lie`的TF-IDF向量的余弦相似度。由于结果是矩阵，代码计算矩阵中所有值的平均相似度，然后打印平均值。
- en: Now we compute the similarity between *cake* and *sing*. The average similarity
    value is calculated and printed to show that the two words are not commonly colocated
    (close to zero).
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们计算单词`cake`和`sing`之间的相似度。计算并打印平均相似度值以显示这两个单词通常不共现（接近零）。
- en: As well as the embedding model used, the strategy for what you embed is also
    important, because there is a trade-off between context and similarity. If you
    embed a large block of text, say an entire book, the vector you get back will
    be the average of the locations of the tokens that make up the full text. As you
    increase the size of the chunk, there is a regression to the mean where it approaches
    the average of all the vectors and no longer contains much semantic information.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用的嵌入模型外，嵌入内容的策略也很重要，因为存在上下文和相似性之间的权衡。如果你嵌入一大块文本，比如整本书，你得到的向量将是构成全文的标记位置的均值。随着块大小的增加，会回归到均值，接近所有向量的均值，并且不再包含很多语义信息。
- en: Smaller chunks of text will be more specific in terms of location in vector
    space and as such might be more useful when you need close similarity. For example,
    isolating smaller sections of text from a novel may better separate comedic from
    tragic moments in the story, whereas embedding a whole page or chapter may mix
    both together. However, making the chunks of text too small might also cause them
    to lose meaning if the text is cut off in the middle of a sentence or paragraph.
    Much of the art of working with vector databases is in the way you load the document
    and break it into chunks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 较小的文本块在向量空间中的位置将更加具体，因此当你需要接近的相似性时可能更有用。例如，从小说中隔离较小的文本部分可能更好地将故事中的喜剧时刻和悲剧时刻分开，而嵌入整个页面或章节可能会将两者混合在一起。然而，如果文本块太小，也可能导致它们在句子或段落中间被截断而失去意义。与向量数据库一起工作的很大一部分艺术在于你如何加载文档并将其分割成块。
- en: Document Loading
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档加载
- en: One common use case of AI is to be able to search across documents based on
    similarity to the text of the user query. For example, you may have a series of
    PDFs representing your employee handbook, and you want to return the correct snippet
    of text from those PDFs that relates to an employee question. The way you load
    documents into your vector database will be dictated by the structure of your
    documents, how many examples you want to return from each query, and the number
    of tokens you can afford in each prompt.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的一个常见用途是能够根据用户查询文本的相似性在文档中进行搜索。例如，你可能有一系列代表你的员工手册的PDF文件，你希望从这些PDF文件中返回与员工问题相关的正确文本片段。你将文档加载到向量数据库中的方式将由你的文档结构、你希望从每个查询中返回多少示例以及每个提示中可以承受的token数量决定。
- en: For example, `gpt-4-0613` has an [8,192 token limit](https://oreil.ly/wbx1f),
    which needs to be shared between the prompt template, the examples inserted into
    the prompt, and the completion the model provides in response. Setting aside around
    2,000 words or approximately 3,000 tokens for the prompt and response, you could
    pull the five most similar chunks of 1,000 tokens of text each into the prompt
    as context. However, if you naively split the document into 1,000-token chunks,
    you will run into a problem. The arbitrary place where each split takes place
    might be in the middle of a paragraph or sentence, so you risk losing the meaning
    of what’s being conveyed. LangChain has a series of [text splitters](https://oreil.ly/qsG7J),
    including the commonly used recursive character text splitter. It tries to split
    on line breaks and then spaces until the chunks are small enough. This keeps all
    paragraphs (and then sentences, and then words) together as much as possible to
    retain semantic groupings inherent in the structure of the text.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`gpt-4-0613`有一个[8,192 token限制](https://oreil.ly/wbx1f)，这个限制需要在提示模板、插入到提示中的示例以及模型提供的响应之间共享。为提示和响应预留大约2,000个单词或大约3,000个token，你可以在提示中作为上下文提取出五个最相似的、每个1,000
    token的文本块。然而，如果你天真地将文档分割成1,000-token的块，你将遇到问题。每次分割的任意位置可能位于段落或句子的中间，这样你可能会丢失所传达的意义。LangChain有一系列[文本分割器](https://oreil.ly/qsG7J)，包括常用的递归字符文本分割器。它试图在行中断和空格处分割，直到块足够小。这尽可能多地保持所有段落（然后是句子，然后是单词）在一起，以保留文本结构中固有的语义分组。
- en: 'Input:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here’s how this code works step-by-step:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这段代码按步骤工作的方式：
- en: '*Create text splitter instance*: An instance of `RecursiveCharacterTextSplitter`
    is created using the `from_tiktoken_encoder` method. This method is specifically
    designed to handle the splitting of text based on token counts.'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建文本分割器实例*：使用`from_tiktoken_encoder`方法创建`RecursiveCharacterTextSplitter`的实例。此方法专门设计用于根据token计数来分割文本。'
- en: The `chunk_size` parameter, set to 100, ensures that each chunk of text will
    contain approximately 100 tokens. This is a way of controlling the size of each
    text segment.
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`chunk_size`参数设置为100，确保每个文本块将包含大约100个token。这是一种控制每个文本段大小的方法。'
- en: The `chunk_overlap` parameter, set to 20, specifies that there will be an overlap
    of 20 tokens between consecutive chunks. This overlap ensures that the context
    is not lost between chunks, which is crucial for understanding and processing
    the text accurately.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`chunk_overlap`参数设置为20，表示连续的块之间将有20个token的重叠。这种重叠确保了块与块之间不会丢失上下文，这对于准确理解和处理文本至关重要。'
- en: '*Prepare the text*: The variable `text` contains a multiparagraph string, representing
    the content to be split into chunks.'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*准备文本*：变量 `text` 包含一个多段落字符串，表示要分割成块的文本内容。'
- en: '*Split the text*: The `split_text` method of the `text_splitter` instance is
    used to split the text into chunks based on the previously defined `chunk_size`
    and `chunk_overlap`. This method processes the text and returns a list of text
    chunks.'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*分割文本*：`text_splitter` 实例的 `split_text` 方法用于根据先前定义的 `chunk_size` 和 `chunk_overlap`
    将文本分割成块。此方法处理文本并返回一个文本块列表。'
- en: '*Output the chunks*: The code prints the first three chunks of the split text
    to demonstrate how the text has been divided. This output is helpful for verifying
    that the text has been split as expected, adhering to the specified chunk size
    and overlap.'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*输出块*：代码打印出分割文本的前三个块，以演示文本是如何被分割的。这种输出有助于验证文本是否按预期分割，遵循指定的块大小和重叠。'
- en: Specify Format
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指定格式
- en: The relevance of the chunk of text provided to the prompt will depend heavily
    on your chunking strategy. Shorter chunks of text without overlap may not contain
    the right answer, whereas longer chunks of text with too much overlap may return
    too many irrelevant results and confuse the LLM or cost you too many tokens.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的文本块与提示的相关性将很大程度上取决于你的分块策略。没有重叠的短文本块可能不包含正确的答案，而重叠过多的长文本块可能会返回太多不相关的结果，并使
    LLM 迷惑或花费你太多的令牌。
- en: Memory Retrieval with FAISS
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 FAISS 进行内存检索
- en: Now that you have your documents processed into chunks, you need to store them
    in a vector database. It is common practice to store vectors in a database so
    that you do not need to recompute them, as there is typically some cost and latency
    associated with doing so. If you don’t change your embedding model, the vectors
    won’t change, so you do not typically need to update them once stored. You can
    use an open source library to store and query your vectors called FAISS, a library
    developed by [Facebook AI](https://oreil.ly/gIcTI) that provides efficient similarity
    search and clustering of dense vectors. First install FAISS in the terminal with
    `pip install faiss-cpu`. The code for this example is included in the [GitHub
    repository](https://oreil.ly/4wR7o) for this book.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经将文档处理成块，你需要将它们存储在向量数据库中。将向量存储在数据库中是一种常见的做法，这样你就不需要重新计算它们，因为这样做通常会有一些成本和延迟。如果你不更改你的嵌入模型，向量就不会改变，所以一旦存储后通常不需要更新它们。你可以使用一个名为
    FAISS 的开源库来存储和查询你的向量，这是一个由 [Facebook AI](https://oreil.ly/gIcTI) 开发的库，它提供了密集向量的高效相似性搜索和聚类。首先在终端中使用
    `pip install faiss-cpu` 安装 FAISS。本例的代码包含在本书的 [GitHub 仓库](https://oreil.ly/4wR7o)
    中。
- en: 'Input:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is an explanation of the preceding code:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对前面代码的解释：
- en: Import the Facebook AI Similarity Search (FAISS) library with `import faiss`.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `import faiss` 导入 Facebook AI 相似性搜索（FAISS）库。
- en: '`vectors = np.array([get_vector_embeddings(chunk) for chunk in chunks])` applies
    `get_vector_embeddings` to each element in `chunks`, which returns a vector representation
    (embedding) of each element. These vectors are then used to create a numpy array,
    which is stored in the variable `vectors`.'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`vectors = np.array([get_vector_embeddings(chunk) for chunk in chunks])` 将
    `get_vector_embeddings` 函数应用于 `chunks` 中的每个元素，该函数返回每个元素的向量表示（嵌入）。然后，这些向量被用来创建一个
    numpy 数组，该数组存储在变量 `vectors` 中。'
- en: The line `index = faiss.IndexFlatL2(vectors.shape[1])` creates a FAISS index
    for efficient similarity search. The argument `vectors.shape[1]` is the dimension
    of the vectors that will be added to the index. This kind of index (`IndexFlatL2`)
    performs brute-force L2 distance search, which looks for the closest items to
    a particular item in a collection by measuring the straight-line distance between
    them, checking each item in the collection one by one.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代码行 `index = faiss.IndexFlatL2(vectors.shape[1])` 创建了一个用于高效相似性搜索的 FAISS 索引。参数
    `vectors.shape[1]` 是将要添加到索引中的向量的维度。这种索引（`IndexFlatL2`）执行 brute-force L2 距离搜索，通过测量它们之间的直线距离来寻找集合中与特定项目最接近的项目，逐个检查集合中的每个项目。
- en: Then you add the array of vectors to the created FAISS index with `index.add(vectors)`.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后你使用 `index.add(vectors)` 将向量数组添加到创建的 FAISS 索引中。
- en: '`def vector_search(query_text, k=1)` defines a new function named `vector_search`
    that accepts two parameters: `query_text` and `k` (with a default value of 1).
    The function will retrieve the embeddings for the `query_text`, and then use that
    to search the index for the `k` closest vectors.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`def vector_search(query_text, k=1)`定义了一个名为`vector_search`的新函数，该函数接受两个参数：`query_text`和`k`（默认值为1）。该函数将检索`query_text`的嵌入，然后使用该嵌入在索引中搜索最近的`k`个向量。'
- en: Inside the `vector_search` function, `query_vector = get_vector_embeddings(query_text)`
    generates a vector embedding for the query text using the `get_vector_embeddings`
    function.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`vector_search`函数内部，`query_vector = get_vector_embeddings(query_text)`使用`get_vector_embeddings`函数生成查询文本的向量嵌入。
- en: 'The `distances, indices = index.search(np.array([query_vector]), k)` line performs
    a search in the FAISS index. It looks for the `k` closest vectors to `query_vector`.
    The method returns two arrays: `distances` (the squared L2 distances to the query
    vector) and `indices` (the indices'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`distances, indices = index.search(np.array([query_vector]), k)`这一行在FAISS索引中执行搜索。它寻找与`query_vector`最近的`k`个向量。该方法返回两个数组：`distances`（到查询向量的平方L2距离）和`indices`（索引'
- en: '`return [(chunks[i], float(dist)) for dist, i in zip(distances[0], indices[0])]`
    returns a list of tuples. Each tuple contains a chunk (retrieved using the indices
    found in the search) and the corresponding distance from the query vector. Note
    that the distance is converted to a float before returning.'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`return [(chunks[i], float(dist)) for dist, i in zip(distances[0], indices[0])]`返回一个元组列表。每个元组包含一个块（使用在搜索中找到的索引检索到的）和查询向量对应的距离。注意，在返回之前，距离被转换为浮点数。'
- en: 'Finally, you perform a vector search for the string containing the user query:
    `search_results = vector_search(user_query)`. The result (the closest chunk and
    its distance) is stored in the variable `search_results`.'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你执行字符串包含用户查询的向量搜索：`search_results = vector_search(user_query)`。结果（最近的块及其距离）存储在变量`search_results`中。
- en: Once the vector search is complete, the results can be injected into the prompt
    to provide useful context. It’s also important to set the system message so that
    the model is focused on answering based on the context provided rather than making
    an answer up. The RAG technique as demonstrated here is widely used in AI to help
    protect against hallucination.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦向量搜索完成，结果可以注入到提示中，以提供有用的上下文。同时，设置系统消息也很重要，以确保模型专注于根据提供的上下文回答问题，而不是随意作出回答。这里展示的RAG技术被广泛应用于AI领域，以帮助防止幻觉。
- en: 'Input:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here is a step-by-step explanation of what the function does:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是对函数执行步骤的逐步解释：
- en: Using a function named `vector_search`, the program performs a vector search
    with `user_query` as the search string and `k` as the number of search results
    to return. The results are stored in `search_results`.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用名为`vector_search`的函数，程序执行向量搜索，以`user_query`作为搜索字符串，以`k`作为要返回的搜索结果数量。结果存储在`search_results`中。
- en: The search results are then printed to the console.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将搜索结果打印到控制台。
- en: A `prompt_with_context` is created by concatenating the `search_results` and
    `user_query`. The goal is to provide the model with context from the search results
    and a question to answer.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过连接`search_results`和`user_query`创建了一个`prompt_with_context`。目的是向模型提供搜索结果中的上下文和一个需要回答的问题。
- en: A list of messages is created. The first message is a system message that instructs
    the model to answer questions provided by the user using only the given context.
    If the model doesn’t know the answer, it’s advised to respond with *I don’t know*.
    The second message is a user message containing the `prompt_with_context`.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建了一个消息列表。第一条消息是一个系统消息，指示模型仅使用给定上下文回答用户提出的问题。如果模型不知道答案，建议回答“I don’t know”。第二条消息是包含`prompt_with_context`的用户消息。
- en: The `openai.ChatCompletion.create()` function is used to get the model’s response.
    It’s provided with the model name (`gpt-3.5-turbo`) and the list of messages.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`openai.ChatCompletion.create()`函数来获取模型的响应。它提供了模型名称（`gpt-3.5-turbo`）和消息列表。
- en: At the end of the code, the `search_and_chat()` function is called with the
    question as the `user_query`.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在代码的末尾，使用`user_query`作为`user_query`参数调用了`search_and_chat()`函数。
- en: Provide Examples
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供示例
- en: Without testing the writing style, it would be hard to guess which prompting
    strategy would win. Now you can be confident this is the correct approach.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 没有测试写作风格，很难猜测哪种提示策略会获胜。现在你可以确信这是正确的方法。
- en: 'Although our code is working end to end now, it doesn’t make sense to be collecting
    embeddings and creating a vector database with every query. Even if you’re using
    an open source model for embeddings, there will be a cost in terms of compute
    and latency. You can save the FAISS index to a file using the `faiss.write_index`
    function:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的代码现在从头到尾都能工作，但每次查询都收集嵌入并创建向量数据库是没有意义的。即使您使用开源模型进行嵌入，也会在计算和延迟方面产生成本。您可以使用`faiss.write_index`函数将FAISS索引保存到文件中：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will create a file called *my_index_file.index* in your current directory,
    which contains the serialized index. You can load this index back into memory
    later with `faiss.read_index`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在您的当前目录中创建一个名为`my_index_file.index`的文件，其中包含序列化的索引。您可以使用`faiss.read_index`将此索引加载回内存中：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This way, you can persist your index across different sessions, or even share
    it between different machines or environments. Just make sure to handle these
    files carefully, as they can be quite large for big indexes.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，您可以在不同的会话之间持久化索引，甚至在不同机器或环境中共享它。只是请确保小心处理这些文件，因为对于大型索引，它们可能相当大。
- en: If you have more than one saved vector database, it’s also possible to merge
    them. This can be useful when serializing the loading of documents or making batch
    updates to your records.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有多个已保存的向量数据库，合并它们也是可能的。这在序列化文档加载或对记录进行批量更新时可能很有用。
- en: 'You can merge two FAISS indices using the `faiss.IndexFlatL2` index’s `add`
    method:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`faiss.IndexFlatL2`索引的`add`方法合并两个FAISS索引：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this code, `reconstruct_n(0, index2.ntotal)` is used to fetch all vectors
    from `index2`, and then `index1.add()` is used to add those vectors to `index1`,
    effectively merging the two indices.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，使用`reconstruct_n(0, index2.ntotal)`来获取`index2`中的所有向量，然后使用`index1.add()`将这些向量添加到`index1`中，从而有效地合并了两个索引。
- en: This should work because `faiss.IndexFlatL2` supports the `reconstruct` method
    to retrieve vectors. However, please note that this process will not move any
    IDs associated with the vectors from `index2` to `index1`. After merging, the
    vectors from `index2` will have new IDs in `index1`.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会工作，因为`faiss.IndexFlatL2`支持`reconstruct`方法来检索向量。但是，请注意，此过程不会将任何与向量关联的ID从`index2`移动到`index1`。合并后，`index2`中的向量将在`index1`中具有新的ID。
- en: If you need to preserve vector IDs, you’ll need to manage this externally by
    keeping a separate mapping from vector IDs to your data items. Then, when you
    merge the indices, you also merge these mappings.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要保留向量ID，您需要通过外部管理从向量ID到您的数据项的单独映射来管理这一点。然后，当您合并索引时，也要合并这些映射。
- en: Tip
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Be aware that this method may not work for all types of indices, especially
    for those that do not support the `reconstruct` method like `IndexIVFFlat` or
    if the two indices have different configurations. In those cases, it may be better
    to keep the original vectors used to build each index and then merge and rebuild
    the index.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种方法可能不适用于所有类型的索引，特别是对于不支持`reconstruct`方法（如`IndexIVFFlat`）或两个索引具有不同配置的情况。在这种情况下，保留用于构建每个索引的原始向量，然后合并并重建索引可能更好。
- en: RAG with LangChain
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于LangChain的RAG
- en: 'As one of the most popular frameworks for AI engineering, LangChain has a wide
    coverage of RAG techniques. Other frameworks like [LlamaIndex](https://www.llamaindex.ai)
    focus specifically on RAG and are worth exploring for sophisticated use cases.
    As you are familiar with LangChain from [Chapter 4](ch04.html#advanced_text_04),
    we’ll continue in this framework for the examples in this chapter. After manually
    performing RAG based on a desired context, let’s create a similar example using
    LCEL on four small text documents with FAISS:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最受欢迎的AI工程框架之一，LangChain涵盖了广泛的RAG技术。其他框架，如[LlamaIndex](https://www.llamaindex.ai)，专注于RAG，对于复杂用例值得探索。由于您熟悉LangChain（[第4章](ch04.html#advanced_text_04)），我们将继续在这个框架中展示本章节的示例。在手动根据所需上下文执行RAG之后，让我们使用LCEL在四个小型文本文档和FAISS上创建一个类似的示例：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The code begins by importing necessary modules from the LangChain library and
    defines a list of text documents to be processed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 代码首先从LangChain库导入必要的模块，并定义要处理的文本文档列表。
- en: It utilizes `FAISS`, a library for efficient similarity search, to create a
    vector store from the text documents. This involves converting the texts into
    vector embeddings using OpenAI’s embedding model.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 它利用了`FAISS`，一个用于高效相似性搜索的库，从文本文档中创建向量存储。这涉及到使用OpenAI的嵌入模型将文本转换为向量嵌入。
- en: A prompt template for handling questions and a `ChatOpenAI` model are initialized
    for generating responses. Additionally, the prompt template enforces that the
    LLM only replies using the context provided from the retriever.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为处理问题和初始化`ChatOpenAI`模型以生成响应提供了一个提示模板。此外，提示模板强制LLM只使用检索器提供的上下文进行回复。
- en: 'You’ll need to create an LCEL chain that will contain the `"context"` and `"question"`
    keys:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要创建一个包含`"context"`和`"question"`键的LCEL链：
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By adding a retriever to `"context"`, it will automatically fetch four documents
    that are converted into a string value. Combined with the `"question"` key, these
    are then used to format the prompt. The LLM generates a response that is then
    parsed into a string value by `StrOutputParser()`.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向`"context"`添加检索器，它将自动获取四个文档，这些文档被转换为字符串值。结合`"question"`键，这些被用于格式化提示。LLM生成一个响应，然后通过`StrOutputParser()`解析为字符串值。
- en: 'You’ll invoke the chain and pass in your question that gets assigned to `"question"`
    and manually test three different queries:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您将调用链并传入您的问题，该问题被分配给`"question"`，并手动测试三个不同的查询：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice how the LLM only appropriately answered the first two queries because
    it didn’t have any relevant context contained within the vector database to answer
    the third query!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意LLM只适当地回答了前两个查询，因为它在向量数据库中没有包含任何相关上下文来回答第三个查询！
- en: The LangChain implementation uses significantly less code, is easy to read,
    and allows you to rapidly implement retrieval augmented generation.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: LangChain实现使用的代码量显著减少，易于阅读，并允许您快速实现检索增强生成。
- en: Hosted Vector Databases with Pinecone
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有Pinecone的托管向量数据库
- en: There are a number of hosted vector database providers emerging to support AI
    use cases, including [Chroma](https://www.trychroma.com), [Weaviate](https://weaviate.io),
    and [Pinecone](https://www.pinecone.io). Hosts of other types of databases are
    also offering vector search functionality, such as [Supabase](https://supabase.com)
    with the [pgvector add-on](https://oreil.ly/pgvector). Examples in this book use
    Pinecone, as it is the current leader at the time of writing, but usage patterns
    are relatively consistent across providers and concepts should be transferrable.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多托管向量数据库提供商出现，以支持AI用例，包括[Chroma](https://www.trychroma.com)、[Weaviate](https://weaviate.io)和[Pinecone](https://www.pinecone.io)。其他类型数据库的托管方也提供向量搜索功能，例如[Supabase](https://supabase.com)与[pgvector插件](https://oreil.ly/pgvector)。本书中的示例使用Pinecone，因为它是撰写时的领导者，但使用模式在提供商之间相对一致，概念应该是可转移的。
- en: 'Hosted vector databases offer several advantages over open source local vector
    stores:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 与开源本地向量存储相比，托管向量数据库具有以下优势：
- en: Maintainance
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 维护
- en: With a hosted vector database, you don’t need to worry about setting up, managing,
    and maintaining the database yourself. This can save significant time and resources,
    especially for businesses that may not have dedicated DevOps or database management
    teams.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管向量数据库，您无需担心自行设置、管理和维护数据库。这可以节省大量时间和资源，尤其是对于可能没有专门的DevOps或数据库管理团队的企业。
- en: Scalability
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Hosted vector databases are designed to scale with your needs. As your data
    grows, the database can automatically scale to handle the increased load, ensuring
    that your applications continue to perform efficiently.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 托管向量数据库旨在根据您的需求进行扩展。随着数据量的增长，数据库可以自动扩展以处理增加的负载，确保您的应用程序继续高效运行。
- en: Reliability
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性
- en: Managed services typically offer high availability with service-level agreements,
    as well as automatic backups and disaster recovery features. This can provide
    peace of mind and save you from potential data loss.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 管理服务通常提供高可用性以及服务级别协议，以及自动备份和灾难恢复功能。这可以提供安心，并让您免受潜在数据丢失的困扰。
- en: Performance
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 性能
- en: Hosted vector databases often have optimized infrastructure and algorithms that
    can provide better performance than self-managed, open source solutions. This
    can be particularly important for applications that rely on real-time or near-real-time
    vector search capabilities.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 托管向量数据库通常具有优化了的基础设施和算法，可以提供比自行管理的开源解决方案更好的性能。这对于依赖于实时或近实时向量搜索功能的应用程序尤为重要。
- en: Support
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 支持
- en: With a hosted service, you typically get access to support from the company
    providing the service. This can be very helpful if you experience issues or need
    help optimizing your use of the database.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管服务，您通常可以访问提供服务的公司的支持。如果您遇到问题或需要帮助优化数据库的使用，这可能非常有帮助。
- en: Security
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: Managed services often have robust security measures in place to protect your
    data, including things like encryption, access control, and monitoring. Major
    hosted providers are more likely to have the necessary compliance certificates
    and be in compliance with privacy legislation in regions like the EU.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 管理服务通常已经实施了强大的安全措施来保护您的数据，包括加密、访问控制和监控等。主要的托管提供商更有可能拥有必要的合规证书，并符合欧盟等地区的隐私法规。
- en: Of course, this extra functionality comes at a cost, as well as a risk of overspending.
    As is the case with using Amazon Web Services, Microsoft Azure, or Google Cloud,
    stories of developers accidentally spending thousands of dollars through incorrect
    configuration or mistakes in code abound. There is also some risk of vendor lock-in,
    because although each vendor has similar functionality, they differ in certain
    areas, and as such it’s not quite straightforward to migrate between them. The
    other major consideration is privacy, because sharing data with a third party
    comes with security risks and potential legal implications.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这种额外的功能是有代价的，同时也存在过度支出的风险。就像使用Amazon Web Services、Microsoft Azure或Google
    Cloud一样，开发者因配置错误或代码错误而意外花费数千美元的故事屡见不鲜。还存在一些供应商锁定风险，因为尽管每个供应商都有类似的功能，但在某些方面它们是不同的，因此迁移它们并不完全直接。另一个主要考虑因素是隐私，因为与第三方共享数据会带来安全风险和潜在的法律后果。
- en: The steps for working with a hosted vector database remain the same as when
    you set up your open source FAISS vector store. First, you chunk your documents
    and retrieve vectors; you then index your document chunks in the vector database,
    allowing you to retrieve similar records to your query, in order to insert into
    the prompt as context. First, let’s create an index in [Pinecone](https://www.pinecone.io),
    a popular commercial vector database vendor. Then log into Pinecone and retrieve
    your API key (visit API Keys in the side menu and click “create API Key”). The
    code for this example is provided in the [GitHub repository](https://oreil.ly/Q0rIw)
    for this book.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管向量数据库的步骤与设置开源FAISS向量存储时相同。首先，您需要对文档进行分块并检索向量；然后，您在向量数据库中索引文档块，以便检索与查询相似的记录，以便将其作为上下文插入到提示中。首先，让我们在[Pinecone](https://www.pinecone.io)，一个流行的商业向量数据库供应商中创建一个索引。然后登录Pinecone并检索您的API密钥（在侧菜单中访问API密钥并点击“创建API密钥”）。本例的代码在本书的[GitHub仓库](https://oreil.ly/Q0rIw)中提供。
- en: 'Input:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE22]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s go through this code step-by-step:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步地通过这段代码：
- en: 1\. Importing libraries
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 导入库
- en: The script begins with importing the necessary modules. `from pinecone import
    Pinecone, ServerlessSpec,` `import os` is used for accessing and setting environment
    variables.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本从导入必要的模块开始。`from pinecone import Pinecone, ServerlessSpec,` `import os`用于访问和设置环境变量。
- en: 2\. Setting up the Pinecone API key
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 设置Pinecone API密钥
- en: The Pinecone API key, which is crucial for authentication, is set as an environment
    variable using `os.environ["PINECONE_API_KEY"] = "insert-your-api-key-here"`.
    It’s important to replace `"insert-your-api-key-here"` with your actual Pinecone
    API key.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于认证至关重要的Pinecone API密钥，使用`os.environ["PINECONE_API_KEY"] = "insert-your-api-key-here"`设置为环境变量。重要的是将`"insert-your-api-key-here"`替换为您的实际Pinecone
    API密钥。
- en: 3\. Defining index name and environment
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 定义索引名称和环境
- en: The variables `index_name` and `environment` are set up. `index_name` is given
    the value `"employee-handbook"`, which is the name of the index to be created
    or accessed in the Pinecone database. The `environment` variable is assigned `"us-west-2"`,
    indicating the server’s location.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 变量`index_name`和`environment`已设置。`index_name`被赋予值`"employee-handbook"`，这是要在Pinecone数据库中创建或访问的索引名称。`environment`变量被分配为`"us-west-2"`，表示服务器的位置。
- en: 4\. Initializing Pinecone connection
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 初始化Pinecone连接
- en: The connection to Pinecone is initialized with the `Pinecone()` constructor.
    This constructor automatically reads the `PINECONE_API_KEY` from the environment
    variable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Pinecone()`构造函数初始化与Pinecone的连接。此构造函数会自动从环境变量中读取`PINECONE_API_KEY`。
- en: 5\. Checking for existing index
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 检查现有索引
- en: The script checks whether an index with the name `index_name` already exists
    in the Pinecone database. This is done through `pc.list_indexes().names()` functions,
    which returns a list of all existing index names.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本检查名为`index_name`的索引是否已存在于Pinecone数据库中。这是通过`pc.list_indexes().names()`函数完成的，该函数返回所有现有索引名称的列表。
- en: 6\. Creating the index
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 创建索引
- en: 'If the index doesn’t exist, it is created using the `pc.​cre⁠ate_index()` function.
    This function is invoked with several parameters that configure the new index:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如果索引不存在，则使用 `pc.create_index()` 函数创建它。此函数使用配置新索引的几个参数调用：
- en: '`index_name`: Specifies the name of the index'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`index_name`：指定索引的名称'
- en: '`dimension=1536`: Sets the dimensionality of the vectors to be stored in the
    index'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dimension=1536`：设置要存储在索引中的向量的维度'
- en: '`metric=''cosine''`: Determines that the cosine similarity metric will be used
    for vector comparisons'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metric=''cosine''`：确定将使用余弦相似度度量来比较向量'
- en: 7\. Connecting to the index
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 连接到索引
- en: After verifying or creating the index, the script connects to it using `pc.Index(index_name)`.
    This connection is necessary for subsequent operations like inserting or querying
    data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证或创建索引后，脚本使用 `pc.Index(index_name)` 连接到它。这种连接对于后续操作，如插入或查询数据是必要的。
- en: 8\. Index statistics
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 索引统计
- en: The script concludes with calling `index.describe_​index_stats()`, which retrieves
    and displays various statistics about the index, such as its dimensionality and
    the total count of vectors stored.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本以调用 `index.describe_index_stats()` 结束，该函数检索并显示有关索引的各种统计信息，例如其维度和存储的向量的总数。
- en: Next, you need to store your vectors in the newly created index, by looping
    through all the text chunks and vectors and upserting them as records in Pinecone.
    The database operation `upsert` is a combination of *update* and *insert*, and
    it either updates an existing record or inserts a new record if the record does
    not already exist (refer to [this Jupyter Notebook](https://oreil.ly/YC-nV) for
    the chunks variable).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要通过遍历所有文本块和向量，并将它们作为 Pinecone 中的记录更新或插入来存储你的向量到新创建的索引中。数据库操作 `upsert`
    是 `update` 和 `insert` 的组合，如果记录已存在则更新，如果不存在则插入新记录（有关 `chunks` 变量的更多信息，请参阅[这个 Jupyter
    Notebook](https://oreil.ly/YC-nV)）。
- en: 'Input:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE24]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s break this code down:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解这段代码：
- en: Import the necessary libraries `tqdm` and `time`. The library `tqdm` displays
    progress bars, and `time` provides the `sleep()` function, which is used in this
    script for retry logic.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库 `tqdm` 和 `time`。库 `tqdm` 显示进度条，而 `time` 提供了 `sleep()` 函数，该函数在本脚本中用于重试逻辑。
- en: Set the variable `batch_size` to 10 (normally set to 100 for real workloads),
    representing how many items will be processed at once in the upcoming loop. Also
    set the `retry_limit` to make sure we stop after five tries.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将变量 `batch_size` 设置为 10（通常对于实际工作负载设置为 100），表示在即将到来的循环中一次将处理多少个项目。同时设置 `retry_limit`
    以确保我们在尝试五次后停止。
- en: The `tqdm(range(0, len(chunks), batch_size))` part is a loop that will run from
    0 to the length of `chunks` (defined previously), with a step of `batch_size`.
    `chunks` is a list of text to be processed. `tqdm` is used here to display a progress
    bar for this loop.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tqdm(range(0, len(chunks), batch_size))` 部分是一个循环，将从 0 运行到 `chunks`（之前定义）的长度，步长为
    `batch_size`。"chunks" 是要处理的文本列表。"tqdm" 在这里用于显示循环的进度条。'
- en: The `i_end` variable is calculated to be the smaller of the length of `chunks`
    or `i + batch_size`. This is used to prevent an Index Error if `i + batch_size`
    exceeds the length of `chunks`.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`i_end` 变量被计算为 `chunks` 长度与 `i + batch_size` 中的较小值。这是用来防止 `i + batch_size`
    超过 `chunks` 长度时发生索引错误。'
- en: '`meta_batch` is a subset of `chunks` for the current batch. This is created
    by slicing the `chunks` list from index `i` to `i_end`.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`meta_batch` 是当前批次 `chunks` 的子集。这是通过从索引 `i` 到 `i_end` 切片 `chunks` 列表来创建的。'
- en: '`ids_batch` is a list of string representations of the range `i` to `i_end`.
    These are IDs that are used to identify each item in `meta_batch`.'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ids_batch` 是从 `i` 到 `i_end` 范围的字符串表示形式的列表。这些是用于识别 `meta_batch` 中每个项目的 ID。'
- en: '`texts` list is just the text from `meta_batch`, ready for processing for embeddings.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`texts` 列表仅仅是 `meta_batch` 中的文本，已准备好进行嵌入处理。'
- en: Try to get the embeddings by calling `get_vector_embeddings()` with the `texts`
    as the argument. The result is stored in the variable `embeds`. This is done inside
    a `try-except` block to handle any exceptions that might be raised by this function,
    such as a rate limit error.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用 `get_vector_embeddings()` 并将 `texts` 作为参数来尝试获取嵌入。结果存储在变量 `embeds` 中。这是在
    `try-except` 块中完成的，以处理该函数可能引发的任何异常，例如速率限制错误。
- en: If an exception is raised, the script enters a `while` loop where it will sleep
    for five seconds and then tries again to retrieve the embeddings. It will continue
    this until successful or the number of retries is reached, at which point it sets
    `done = True` to exit the `while` loop.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果抛出异常，脚本将进入一个 `while` 循环，其中它将休眠五秒钟，然后再次尝试检索嵌入。它将继续这样做，直到成功或达到重试次数，此时它将 `done
    = True` 设置为退出 `while` 循环。
- en: 'Modify the `meta_batch` to be a list of dictionaries. Each dictionary has two
    keys: `batch`, which is set to the current batch number `i`, and `text`, which
    is set to the corresponding item in `meta_batch`. This is where you could add
    additional metadata for filtering queries later, such as the page, title, or chapter.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `meta_batch` 修改为字典列表。每个字典有两个键：`batch`，设置为当前批次号 `i`，和 `text`，设置为 `meta_batch`
    中的对应项。这里你可以添加额外的元数据以供后续过滤查询使用，例如页面、标题或章节。
- en: Create the `to_upsert` list by using the `zip` function to combine `ids_batch`,
    `embeds`, and `meta_batch` into tuples, and then turning that into a list. Each
    tuple contains the ID, the corresponding embedding, and the corresponding metadata
    for each item in the batch.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 `zip` 函数将 `ids_batch`、`embeds` 和 `meta_batch` 组合成元组，然后将其转换为列表来创建 `to_upsert`
    列表。每个元组包含批次中每个项目的 ID、相应的嵌入和相应的元数据。
- en: The last line of the loop calls a method `upsert` on `index`, a Pinecone (a
    vector database service) index. The `vectors=to_upsert` argument passes the `to_upsert`
    list as the data to be inserted or updated in the index. If a vector with a given
    ID already exists in the index, it will be updated; if it doesn’t exist, it will
    be inserted.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 循环的最后一行在 `index`（一个 Pinecone 索引，即向量数据库服务）上调用 `upsert` 方法。`vectors=to_upsert`
    参数将 `to_upsert` 列表作为要插入或更新到索引中的数据。如果索引中已存在具有给定 ID 的向量，它将被更新；如果不存在，它将被插入。
- en: Once the records are stored in Pinecone, you can query them as you need, just
    like when you saved your vectors locally with FAISS. Embeddings remain the same
    so long as you’re using the same embedding model to retrieve vectors for your
    query, so you do not need to update your database unless you have additional records
    or metadata to add.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦记录存储在 Pinecone 中，你就可以按需查询它们，就像你使用 FAISS 本地保存向量时一样。只要你在查询向量时使用相同的嵌入模型，嵌入就会保持不变，因此除非你有额外的记录或元数据要添加，否则你不需要更新数据库。
- en: 'Input:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: '[PRE26]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Output:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE27]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This script performs a nearest neighbors search using Pinecone’s API to identify
    the most similar vectors to a given input vector in a high-dimensional space.
    Here’s a step-by-step breakdown:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本使用 Pinecone 的 API 执行最近邻搜索，以在高维空间中识别与给定输入向量最相似的向量。以下是逐步分解：
- en: 'The function `pinecone_vector_search` is defined with two parameters: `user_query`
    and `k`. `user_query` is the input text from the user, ready to be converted into
    a vector, and `k` indicates the number of closest vectors you want to retrieve.'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`pinecone_vector_search` 函数定义了两个参数：`user_query` 和 `k`。`user_query` 是用户输入的文本，准备转换为向量，而
    `k` 表示你想要检索的最近向量数量。'
- en: Within the function, `xq` is defined by calling another function, `get_​vec⁠tor_embeddings(user_query)`.
    This function (defined previously) is responsible for transforming the `user_query`
    into a vector representation.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在函数内部，`xq` 通过调用另一个函数 `get_​vec⁠tor_embeddings(user_query)` 来定义。这个函数（之前已定义）负责将
    `user_query` 转换为向量表示。
- en: 'The next line performs a query on an object named `index`, a Pinecone index
    object, using the `query` method. The `query` method takes three parameters:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一条语句使用 `index` 对象（一个 Pinecone 索引对象）执行查询，使用 `query` 方法。`query` 方法接受三个参数：
- en: The first parameter is `vector=xq`, the vector representation of our `user_query`.
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个参数是 `vector=xq`，即 `user_query` 的向量表示。
- en: The second parameter, `top_k=k`, specifies that you want to return only the
    `k` closest vectors in the Pinecone index.
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二个参数 `top_k=k` 指定你只想返回 Pinecone 索引中的 `k` 个最接近的向量。
- en: 'The third parameter, `include_metadata=True`, specifies that you want to include
    metadata (such as IDs or other associated data) with the returned results. If
    you wanted to [filter the results by metadata](https://oreil.ly/BBYD4), for example
    specifying the batch (or any other metadata you upserted), you could do this here
    by adding a fourth parameter: `filter={"batch": 1}`.'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第三个参数 `include_metadata=True` 指定你想要包含返回结果中的元数据（如 ID 或其他相关数据）。例如，如果你想通过元数据[过滤结果](https://oreil.ly/BBYD4)，例如指定批次（或任何其他你已上载的元数据），你可以在这里通过添加第四个参数
    `filter={"batch": 1}` 来实现。'
- en: The results of the `query` method are assigned to `res` and then returned by
    the function.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`query` 方法的返回结果被分配给 `res`，然后由函数返回。'
- en: Finally, the function `pinecone_vector_search` is called with arguments `user_query`
    and `k`, returning the response from Pinecone.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用参数 `user_query` 和 `k` 调用函数 `pinecone_vector_search`，返回 Pinecone 的响应。
- en: You have now effectively emulated the job FAISS was doing, returning the relevant
    record from the handbook with a similarity search by vector. If you replace `vector_search(user_query,
    k)` with `pinecone_vector_search(user_query, k)` in the `search_and_chat` function
    (from the previous example), the chatbot will run the same, except that the vectors
    will be stored in a hosted Pinecone database instead of locally using FAISS.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经有效地模拟了 FAISS 的工作，通过向量搜索从手册中返回相关记录。如果你在 `search_and_chat` 函数（来自上一个示例）中将
    `vector_search(user_query, k)` 替换为 `pinecone_vector_search(user_query, k)`，聊天机器人将运行相同，只是向量将存储在托管的
    Pinecone 数据库中，而不是本地使用 FAISS。
- en: 'When you upserted the records into Pinecone, you passed the batch number as
    metadata. Pinecone supports the following formats of metadata:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将记录上载到 Pinecone 时，你将批次号作为元数据传递。Pinecone 支持以下元数据格式：
- en: String
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串
- en: Number (integer or floating-point, gets converted to a 64-bit floating point)
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字（整数或浮点数，转换为 64 位浮点数）
- en: Booleans (true, false)
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布尔值（true，false）
- en: List of strings
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串列表
- en: 'The metadata strategy for storing records can be just as important as the chunking
    strategy, as you can use metadata to filter queries. For example, if you wanted
    to only search for similarity limited to a specific batch number, you could add
    a filter to the `index.query`:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 存储记录的元数据策略与分块策略一样重要，因为你可以使用元数据来过滤查询。例如，如果你想仅搜索特定批次号的相似性，你可以在 `index.query` 中添加一个过滤器：
- en: '[PRE28]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This can be useful for limiting the scope of where you are searching for similarity.
    For example, it would allow you to store past conversations for all chatbots in
    the same vector database and then query only for past conversations related to
    a specific chatbot ID when querying to add context to that chatbot’s prompt. Other
    common uses of metadata filters include searching for more recent timestamps,
    for specific page numbers of documents, or for products over a certain price.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来限制搜索相似性的范围。例如，这允许你将所有聊天机器人的过去对话存储在同一个向量数据库中，然后在查询以添加上下文时，仅查询与特定聊天机器人 ID
    相关的过去对话。元数据过滤器的其他常见用途包括搜索更近的时间戳、特定页码的文档或价格超过一定限度的产品。
- en: Note
  id: totrans-263
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: Note that more metadata storage is likely to increase storage costs, as is storing
    large chunks that are infrequently referenced. Understanding how vector databases
    work should give you license to experiment with different chunking and metadata
    strategies and see what works for your use cases.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，存储更多元数据可能会增加存储成本，正如存储不常引用的大块数据一样。了解向量数据库的工作原理应该让你有理由尝试不同的分块和元数据策略，看看哪些适用于你的用例。
- en: Self-Querying
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自查询
- en: Retrieval can get quite sophisticated, and you’re not limited to a basic retriever
    that fetches documents from a vector database based purely on semantic relevance.
    For example, consider using metadata from within a user’s query. By recognizing
    and extracting such filters, your retriever can autonomously generate a new query
    to execute against the vector database, as in the structure depicted in [Figure 5-3](#figure-5-3).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 检索可以变得相当复杂，你不仅限于基于语义相关性的基本检索器从向量数据库中检索文档。例如，考虑使用用户查询中的元数据。通过识别和提取这样的过滤器，你的检索器可以自主生成一个新的查询以执行向量数据库，如图
    5-3 所示的结构。
- en: Note
  id: totrans-267
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: This approach also generalizes to NoSQL, SQL, or any common database, and it
    is not solely limited to vector databases.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法也适用于 NoSQL、SQL 或任何常见数据库，而不仅仅局限于向量数据库。
- en: '![Self querying](assets/pega_0503.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![自查询](assets/pega_0503.png)'
- en: Figure 5-3\. A self-querying retriever architecture
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. 一个自查询检索器架构
- en: '[Self-querying](https://oreil.ly/39rgU) yields several significant benefits:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '[自查询](https://oreil.ly/39rgU)带来了几个显著的好处：'
- en: Schema definition
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 架构定义
- en: You can establish a schema reflecting anticipated user descriptions, enabling
    a structured understanding of the information sought by users.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以建立一个架构，反映预期的用户描述，使用户能够结构化地理解所寻求的信息。
- en: Dual-layer retrieval
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 双层检索
- en: The retriever performs a two-tier operation. First, it gauges the semantic similarity
    between the user’s input and the database’s contents. Simultaneously, it discerns
    and applies filters based on the metadata of the stored documents or rows, ensuring
    an even more precise and relevant retrieval.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 检索器执行双层操作。首先，它评估用户输入与数据库内容之间的语义相似度。同时，它根据存储文档或行的元数据识别并应用过滤器，确保更精确和相关的检索。
- en: This method maximizes the retriever’s potential in serving user-specific requests.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法最大化了检索器在满足用户特定请求方面的潜力。
- en: Install `lark` on the terminal with `pip install lark`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端上使用`pip install lark`安装`lark`。
- en: 'In the subsequent code, essential modules such as `langchain`, `lark`, `getpass`,
    and `chroma` are imported. For a streamlined experience, potential warnings are
    suppressed:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的代码中，导入如`langchain`、`lark`、`getpass`和`chroma`等基本模块。为了获得流畅的体验，潜在的警告被抑制：
- en: '[PRE29]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'In the upcoming section, you’ll craft a list named `docs`, filling it with
    detailed instances of the `Document` class. Each `Document` lets you capture rich
    details of a book. Within the metadata dictionary, you’ll store valuable information
    such as the title, author, and genre. You’ll also include data like the ISBN,
    the publisher, and a concise summary to give you a snapshot of each story. The
    `"rating"` offers a hint of its popularity. By setting up your data this way,
    you’re laying the groundwork for a systematic and insightful exploration of a
    diverse library:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将创建一个名为`docs`的列表，并用`Document`类的详细实例填充它。每个`Document`让你能够捕捉到一本书的丰富细节。在元数据字典中，你将存储诸如标题、作者和类型等有价值的信息。你还将包括ISBN、出版社和简短摘要，以提供每个故事的快照。"评分"提供了其受欢迎程度的线索。通过这种方式设置数据，你正在为对多样化图书馆的系统性和洞察性探索打下基础：
- en: '[PRE30]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: You’ll import `ChatOpenAI`, `SelfQueryRetriever`, and `OpenAIEmbeddings`. Following
    this, you’ll create a new vector database using the `Chroma.from_documents(..)`
    method.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你将导入`ChatOpenAI`、`SelfQueryRetriever`和`OpenAIEmbeddings`。随后，你将使用`Chroma.from_documents(..)`方法创建一个新的向量数据库。
- en: 'Next, the `AttributeInfo` class is used to structure metadata for each book.
    Through this class, you’ll systematically specify the attribute’s name, description,
    and type. By curating a list of `AttributeInfo` entries, the self-query retriever
    can perform metadata filtering:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`AttributeInfo`类为每本书构建元数据结构。通过这个类，你可以系统地指定属性名称、描述和类型。通过整理`AttributeInfo`条目列表，自我查询检索器可以执行元数据过滤：
- en: '[PRE31]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Let’s run this through step-by-step:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步运行这个操作：
- en: Import `ChatOpenAI`, `SelfQueryRetriever`, and `AttributeInfo` from the LangChain
    modules for chat model integration, self-querying, and defining metadata attributes.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从LangChain模块导入`ChatOpenAI`、`SelfQueryRetriever`和`AttributeInfo`，以实现聊天模型集成、自我查询和定义元数据属性。
- en: Create an `OpenAIEmbeddings` instance for handling OpenAI model embeddings.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个用于处理OpenAI模型嵌入的`OpenAIEmbeddings`实例。
- en: A `Chroma` vector database is created from the documents.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文档中创建了一个`Chroma`向量数据库。
- en: Define three lists (`basic_info`, `detailed_info`, `analysis`), each containing
    `AttributeInfo` objects for different types of book metadata.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义三个列表（`basic_info`、`detailed_info`、`analysis`），每个列表包含不同类型书籍元数据的`AttributeInfo`对象。
- en: Combine these lists into a single list, `metadata_field_info`, for comprehensive
    book metadata management.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些列表合并为一个名为`metadata_field_info`的单个列表，以实现全面的书籍元数据管理。
- en: Now, set up a `ChatOpenAI` model and assign a `document_content_description`
    to specify what content type you’re working with. The `SelfQueryRetriever` then
    uses this along with your LLM to fetch relevant documents from your `vectorstore`.
    With a simple query, such as asking for sci-fi books, the `invoke` method scans
    through the dataset and returns a list of `Document` objects.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，设置一个`ChatOpenAI`模型，并分配一个`document_content_description`以指定你正在处理的内容类型。然后，`SelfQueryRetriever`使用此信息以及你的LLM从你的`vectorstore`中检索相关文档。通过简单的查询，例如请求科幻书籍，`invoke`方法将扫描数据集并返回一个`Document`对象列表。
- en: 'Each `Document` encapsulates valuable metadata about the book, like the genre,
    author, and a brief summary, transforming the results into organized, rich data
    for your application:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`Document`封装了关于书籍的有价值元数据，如类型、作者和简短摘要，将结果转化为适用于你应用程序的有序、丰富数据：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Evaluate Quality
  id: totrans-294
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估质量
- en: By setting the temperature to zero, you instruct the model to prioritize generating
    consistent metadata filtering outputs, rather than being more creative and therefore
    inconsistent. These metadata filters are then leveraged against the vector database
    to retrieve relevant documents.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将温度设置为零，您指示模型优先生成一致的元数据过滤输出，而不是更富有创造性，从而导致不一致。然后，这些元数据过滤器被用于向量数据库以检索相关文档。
- en: 'When you want to fetch books from a specific author, you’re directing the `retriever`
    to pinpoint books authored by `J.K. Rowling`. The `Comparison` function with the
    `EQ` (equals) comparator ensures the retrieved documents have their `''author''`
    attribute precisely matching `''J.K. Rowling''`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 当您想要从特定作者那里获取书籍时，您正在指示`retriever`定位由`J.K. Rowling`所著的书籍。使用`Comparison`函数和`EQ`（等于）比较器确保检索到的文档具有与`'J.K.
    Rowling'`精确匹配的`'author'`属性：
- en: '[PRE33]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Initializing the `SelfQueryRetriever` with an added `enable_limit` flag set
    to `True` allows you to dictate the number of results returned. Then, you craft
    a query to obtain precisely `2 Fantasy` books. By using the `Comparison` function
    with the `EQ` (equals) comparator on the `''genre''` attribute, the retriever
    zeros in on `''Fantasy''` titles. The `limit` parameter ensures you get only two
    results, optimizing your output for precision and brevity:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设置了`enable_limit`标志为`True`的`SelfQueryRetriever`初始化，您可以指定返回的结果数量。然后，您构建一个查询以精确地获取`2
    Fantasy`书籍。通过在`'genre'`属性上使用`Comparison`函数和`EQ`（等于）比较器，检索器聚焦于`'Fantasy'`标题。`limit`参数确保您只获得两个结果，优化了输出的精确性和简洁性：
- en: '[PRE34]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Alternative Retrieval Mechanisms
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代检索机制
- en: 'When it comes to retrieval implementations, various intriguing methods each
    demonstrate their distinct approaches, advantages, and limitations:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索实现方面，各种有趣的方法各自展示了它们独特的途径、优势和局限性：
- en: MultiQueryRetriever
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: MultiQueryRetriever
- en: The [MultiQueryRetriever](https://oreil.ly/uuzpG) aims to overcome the limitations
    of distance-based retrieval by generating multiple queries from different perspectives
    for a given user input query. This leads to the generation of a larger set of
    potentially relevant documents, offering broader insights. However, challenges
    may arise if the different queries produce contradicting results or overlap.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[MultiQueryRetriever](https://oreil.ly/uuzpG)旨在克服基于距离的检索的限制，通过为给定的用户输入查询生成多个不同角度的查询。这导致生成一组更大的潜在相关文档，提供更广泛的见解。然而，如果不同的查询产生矛盾的结果或重叠，可能会出现挑战。'
- en: Contextual Compression
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文压缩
- en: The [Contextual Compression Retriever](https://oreil.ly/wzqVg) handles long
    documents by compressing irrelevant parts, ensuring relevance to context. The
    challenge with this method is the expertise needed to determine the relevance
    and importance of information.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '[上下文压缩检索器](https://oreil.ly/wzqVg)通过压缩无关部分来处理长文档，确保与上下文的相关性。这种方法面临的挑战是需要专业知识来判定信息的关联性和重要性。'
- en: Ensemble Retriever
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 集成检索器
- en: The [Ensemble Retriever](https://oreil.ly/jIuJh) uses a list of retrievers and
    combines their results. It’s essentially a “hybrid” search methodology that leverages
    the strengths of various algorithms. However, the Ensemble Retriever implies more
    computational workload due to the use of multiple retrieval algorithms, potentially
    affecting retrieval speed.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成检索器](https://oreil.ly/jIuJh)使用一系列检索器并将它们的结果结合起来。它本质上是一种“混合”搜索方法，利用了各种算法的优势。然而，由于使用了多个检索算法，集成检索器意味着更多的计算工作量，可能会影响检索速度。'
- en: Parent Document Retriever
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 父文档检索器
- en: The [Parent Document Retriever](https://oreil.ly/jXSXQ) ensures the maintenance
    of rich document backgrounds by retrieving original source documents from which
    smaller chunks are derived. But it might increase computational requirements due
    to the retrieval of larger parent documents.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[父文档检索器](https://oreil.ly/jXSXQ)通过检索原始源文档（从中提取较小的片段）来确保丰富文档背景的维护。但它可能会因为检索较大的父文档而增加计算需求。'
- en: Time-Weighted Vector Store Retriever
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 时间加权向量存储检索器
- en: The [Time-Weighted Vector Store Retriever](https://oreil.ly/9JbTt) incorporates
    *time decay* into document retrieval. Despite its advantages, the time decay factor
    might cause overlooking of relevant older documents, risking the loss of historical
    context.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '[时间加权向量存储检索器](https://oreil.ly/9JbTt)将*时间衰减*纳入文档检索中。尽管它有优势，但时间衰减因子可能会导致忽略相关较旧文档，从而风险失去历史背景。'
- en: The key to effective retrieval is understanding the trade-offs and selecting
    the method, or combination of methods, that best address your specific use case.
    Vector search adds additional cost and latency to your application, so ensure
    in testing you find that the additional context is worth it. For heavy workloads,
    paying the up-front cost of fine-tuning a custom model may be beneficial compared
    to the ongoing additional cost of prompts plus embeddings plus vector storage.
    In other scenarios, providing static examples of correct work in the prompt may
    work fine. However, when you need to pull in context to a prompt dynamically,
    based on similarity rather than a direct keyword search, there’s no real substitute
    for RAG using a vector database.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 有效检索的关键在于理解权衡并选择最适合你特定用例的方法，或方法组合。向量搜索会给你的应用增加额外的成本和延迟，因此在测试中确保你发现额外的上下文是值得的。对于重负载，与持续增加的提示、嵌入和向量存储成本相比，支付定制模型微调的前期成本可能是有益的。在其他情况下，提供正确的静态工作示例作为提示可能效果良好。然而，当你需要根据相似性而不是直接关键词搜索动态地将上下文拉入提示时，使用向量数据库的RAG（Retrieval-Augmented
    Generation）实际上没有真正的替代品。
- en: Summary
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about the power of vector databases for storing
    and querying text based on similarity. By searching for the most similar records,
    vector databases can retrieve relevant information to provide context in your
    prompts, helping AI models stay within token limits and avoid unnecessary costs
    or irrelevant data. You also discovered that the accuracy of vectors depends on
    the underlying model and saw examples where they may fail.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了向量数据库在基于相似性存储和查询文本方面的强大功能。通过搜索最相似的记录，向量数据库可以检索相关信息，为你的提示提供上下文，帮助AI模型保持在令牌限制内，避免不必要的成本或不相关数据。你还发现向量的准确性取决于底层模型，并看到了它们可能失败的一些示例。
- en: Furthermore, you explored the process of indexing documents in a vector database,
    searching for similar records using vectors, and inserting records into prompts
    as context, called RAG. In this chapter, you went through code examples for retrieving
    embeddings from both the OpenAI API and open source models like the Sentence Transformers
    library. You also learned the cost and benefits associated with retrieving embeddings
    from OpenAI relative to alternative options.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你探讨了在向量数据库中索引文档的过程，使用向量搜索相似记录，以及将记录作为上下文插入提示中，这被称为RAG。在这一章中，你学习了从OpenAI API和开源模型如Sentence
    Transformers库中检索嵌入的代码示例。你还了解了与替代选项相比，从OpenAI检索嵌入的成本和好处。
- en: In the next chapter on autonomous agents, you will enter the futuristic world
    of AI agents that can make decisions and take actions on their own. You will learn
    about the different types of autonomous agents, their capabilities, and how they
    can be trained to perform specific tasks. Additionally, you will explore the challenges
    and unreliability issues associated with agents.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章关于自主代理的内容中，你将进入一个充满未来感的AI代理世界，这些代理能够自主做出决策并采取行动。你将了解不同类型的自主代理，它们的技能，以及如何训练它们执行特定任务。此外，你还将探讨与代理相关联的挑战和不可靠性问题。
