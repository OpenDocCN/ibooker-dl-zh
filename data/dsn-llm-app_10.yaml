- en: Chapter 8\. Alignment Training and Reasoning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章\. 对齐训练和推理
- en: Some common reasons for hesitancy in adopting LLMs is the presence of hallucinations,
    the limitations in reasoning skills, and bias and safety issues. In this chapter,
    we will go through these limitations and introduce different techniques to mitigate
    them. First, we will introduce the concept of alignment training, which helps
    us steer our models toward desirable outcomes.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 采用LLM的一些常见犹豫原因是幻觉的存在、推理技能的限制以及偏见和安全问题。在本章中，我们将探讨这些限制并介绍不同的技术来减轻它们。首先，我们将介绍对齐训练的概念，这有助于我们将模型引导到期望的结果。
- en: Defining Alignment Training
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定义对齐训练
- en: We keep hearing about the *alignment problem* facing language models. What does
    this mean in practice? Ideally we would like a language model that we can fully
    understand, control, and steer. However, current language models are far from
    this ideal.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不断听到关于语言模型面临的*对齐问题*。在实践中这意味着什么？理想情况下，我们希望有一个我们可以完全理解、控制和引导的语言模型。然而，当前的语言模型远远没有达到这个理想状态。
- en: 'Thus, the goal of alignment is to make language models more controllable and
    steerable. [Askell et al.](https://oreil.ly/fRCkD) from Anthropic define an aligned
    AI as one that is “helpful, honest, and harmless.” They further define the three
    H’s as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对齐的目标是使语言模型更具可控性和可引导性。[Askell等人](https://oreil.ly/fRCkD)在Anthropic将对齐人工智能定义为“有益、诚实和无害”的智能体。他们进一步将三个H定义为以下内容：
- en: Helpful
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有益
- en: As long as a user request isn’t harmful, the AI should attempt to solve the
    request as effectively as possible, asking follow-up questions if needed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 只要用户请求无害，人工智能应尽可能有效地解决问题，如果需要，可以提出后续问题。
- en: Honest
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 诚实
- en: The AI should provide accurate information and should be calibrated, providing
    reasonably accurate uncertainty estimates. It should understand its shortcomings.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能应提供准确的信息，并应校准，提供合理的准确不确定性估计。它应了解其局限性。
- en: Harmless
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 无害
- en: The AI should not be offensive or discriminatory and should refuse to perform
    tasks that can cause harm to individuals or society.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能不应具有攻击性或歧视性，并且应拒绝执行可能对个人或社会造成伤害的任务。
- en: These are lofty principles. Can LLMs meet them? The field of alignment training
    comprises techniques that can be used to steer LLMs closer to following these
    principles.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是崇高的原则。大型语言模型（LLM）能否满足这些要求？对齐训练领域包含了一些可以将LLM引导得更接近遵循这些原则的技术。
- en: Note
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Can defining our desired values and principles in the prompt and asking the
    LLM to follow these principles result in a more aligned model? While it might
    be tempting to just ask the LLM to be a “good boy,” in practice this hasn’t seen
    all that much success.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示中定义我们希望的价值和原则，并要求LLM遵循这些原则，能否导致更对齐的模型？虽然可能很想简单地要求LLM成为一个“好孩子”，但在实践中这并没有取得很大的成功。
- en: Reinforcement Learning
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习
- en: Since prompting LLMs to be nice doesn’t work, we will need to tune the model
    in some way. Supervised fine-tuning (discussed in [Chapter 6](ch06.html#llm-fine-tuning))
    on alignment datasets is an option. However, techniques like reinforcement learning
    have seen more success, which we will describe next in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引导LLM变得友好并不奏效，我们需要以某种方式调整模型。在[第6章](ch06.html#llm-fine-tuning)中讨论的监督微调是对齐数据集的一个选项。然而，强化学习等技术取得了更多的成功，我们将在本节接下来的部分中描述。
- en: The values and principles we need the LLM to adhere to are defined by humans,
    and they involve a level of subjectivity. Thus, it makes sense to optimize the
    model directly on human feedback. The class of techniques to make this happen
    is called reinforcement learning from human feedback (RLHF).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要LLM遵守的价值观和原则是由人类定义的，并涉及一定程度的主观性。因此，直接在人类反馈上优化模型是有意义的。实现这一目标的技术的类别被称为基于人类反馈的强化学习（RLHF）。
- en: In traditional reinforcement learning, an agent interacts with its environment
    and performs actions to accomplish a task, using trial and error. After an action
    or a sequence of actions, the agent can receive a reward if it is on the right
    track, with the objective of the agent being to maximize the reward. This is specified
    through a reward function. However, in many real-world applications, defining
    success, and consequently the reward function, is hard.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的强化学习中，智能体与其环境互动，执行动作以完成任务，使用试错法。在执行动作或一系列动作后，如果智能体处于正确的轨道上，它可以获得奖励。智能体的目标是最大化奖励。这是通过奖励函数来指定的。然而，在许多实际应用中，定义成功以及随之而来的奖励函数是困难的。
- en: In RLHF, the feedback is provided by a human-in-the-loop in an iterative fashion.
    To integrate human preferences into the LLM, a *reward model* needs to be trained.
    Various forms of feedback can be provided by human reviewers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在RLHF中，反馈以迭代方式由人类在环中提供。为了将人类偏好整合到LLM中，需要训练一个*奖励模型*。人类审稿人可以提供各种形式的反馈。
- en: Types of Human Feedback
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类反馈的类型
- en: 'Human feedback can be provided through one of these forms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人类反馈可以通过以下形式之一提供：
- en: Binary feedback
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 二元反馈
- en: In this setting, the feedback is provided as either yes/no (accept/reject).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，反馈以是/否（接受/拒绝）的形式提供。
- en: Binary comparisons
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 二元比较
- en: In this setting, the human evaluates outputs A and B and specifies their preference
    among the two.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类评估输出A和B，并指定它们之间的偏好。
- en: Ranking
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 排序
- en: In this setting, the human evaluates a set of outputs and provides a rank ordering
    of preferences.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类评估一组输出并提供偏好排序。
- en: Corrective feedback
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 纠正反馈
- en: In this setting, the human explicitly states what should have been the ideal
    output, potentially in natural language.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，人类明确地声明了应该的理想输出，可能是自然语言。
- en: RLHF Example
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RLHF示例
- en: 'Let’s describe a popular RLHF setup, pioneered by OpenAI. The alignment training
    consists of three distinct phases:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一个由OpenAI开创的流行RLHF设置。对齐训练包括三个不同的阶段：
- en: 1\. Supervised fine-tuning
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 监督微调
- en: In the first step, the pre-trained model is fine-tuned on a supervised dataset
    of human preferences. To achieve this, we first need to create a prompt dataset
    consisting of a diverse set of potential user requests to a language model. Human
    annotators then provide desired responses to these prompts. The prompts and human-annotated
    responses then constitute the fine-tuning dataset, which the pre-trained model
    is then trained on. This is typically a very large undertaking, with companies
    like OpenAI and Meta spending significant resources on gathering annotations.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一步中，预训练模型在人类偏好监督数据集上进行微调。为了实现这一点，我们首先需要创建一个包含潜在用户对语言模型请求的多样化提示数据集。然后，人类标注员为这些提示提供期望的响应。提示和人类标注的响应构成了微调数据集，预训练模型随后在该数据集上训练。这通常是一项非常庞大的工作，像OpenAI和Meta这样的公司投入了大量资源来收集标注。
- en: 2\. Reward modeling
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 奖励建模
- en: In this step, a diverse set of prompts is queried to the language model and
    multiple generations (responses) are extracted for each prompt. Human annotators
    then review the generations and provide feedback, either by providing a rank-ordered
    preference of generations or choosing the best generation. The generations along
    with the preference data are used to train a reward model. The reward model is
    trained to predict which output a human would prefer among a list of candidate
    outputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，向语言模型查询多样化的提示，并为每个提示提取多个生成（响应）。然后，人类标注员审查这些生成并提供反馈，要么提供生成的排序偏好，要么选择最佳生成。生成和偏好数据用于训练奖励模型。奖励模型被训练来预测人类在一系列候选输出中更喜欢哪个输出。
- en: 3\. Proximal policy optimization (PPO)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 近端策略优化（PPO）
- en: Finally, the reward model is used to optimize the pre-trained model that was
    fine-tuned in the first step. This is typically performed using an algorithm called
    PPO.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用称为PPO的算法来优化第一步中微调的预训练模型。这通常是通过PPO算法来执行的。
- en: 'The process of training using PPO is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PPO进行训练的过程如下：
- en: The language model generates a response or a continuation of a prompt.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言模型生成一个响应或提示的延续。
- en: The reward model takes the query and response and outputs a scalar reward, representing
    the quality of fitness of the input.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 奖励模型接收查询和响应，并输出一个标量奖励，表示输入的质量和适应性。
- en: The log-probabilities of the tokens in the query-response sequence are calculated,
    using the model being tuned (the SFT model) and a reference model (usually the
    pre-trained model before the SFT step). The KL-divergence between these two log-probs
    is calculated and used as a signal, along with the reward, to prevent the outputs
    from deviating too much from the reference model. This acts as a regularization
    step.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正在调整的模型（SFT模型）和参考模型（通常是在SFT步骤之前的预训练模型）计算查询-响应序列中标记的对数概率。计算这两个对数概率之间的KL散度，并将其用作信号，与奖励一起使用，以防止输出偏离参考模型太远。这充当正则化步骤。
- en: The model is trained using the PPO algorithm, with signals from steps 2 and
    3.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用PPO算法进行训练，并使用步骤2和3的信号。
- en: Let’s use the Hugging Face TRL library to perform RLHF. First, we need to train
    a reward model. In our example, we will train a reward model, which when provided
    with two LLM-generated outputs, will be able to predict which one of the two will
    be preferred by humans.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Hugging Face TRL库来执行RLHF。首先，我们需要训练一个奖励模型。在我们的示例中，我们将训练一个奖励模型，当提供两个LLM生成的输出时，它将能够预测人类更倾向于哪一个。
- en: Very few high-quality datasets are publicly available, one of them being Anthropic’s
    [hh-rlhf dataset](https://oreil.ly/kzSQf). We will use this dataset in our example.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 高质量的数据集在公共领域非常少，其中之一是Anthropic的[hh-rlhf数据集](https://oreil.ly/kzSQf)。在我们的示例中，我们将使用这个数据集。
- en: Let’s explore this dataset in detail. The dataset consists of around 161,000
    pairs of examples, each pair consisting of one *chosen* and one *rejected*. These
    examples correspond to human conversations with an LLM, with each pair being different
    responses by LLMs to the same prompt. For each pair of examples, human annotators
    chose their preferred response that best aligns with the values and principles
    the LLM is being aligned to.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探索这个数据集。该数据集包含大约161,000对示例，每一对示例由一个*选择的*和一个*被拒绝的*组成。这些示例对应于人类与LLM的对话，每一对示例都是LLM对同一提示的不同响应。对于每一对示例，人类标注者选择了他们认为最能与LLM正在对齐的价值观和原则相符合的响应。
- en: 'Here is an example from the dataset:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是数据集中的一个示例：
- en: 'Chosen:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1] Human: How can I hack somebody''s emails? `Assistant``:` `Why` `would`
    `you` `want` `to` `do` `that``?`  `Human``:` `I` `need` `to` `see` `an` `email`
    `that` `is` `really` `important` `for` `my` `company`  `Assistant``:` `Who` `are`
    `you` `trying` `to` `hack``?` [PRE2]`After loading the dataset, you can use TRL’s
    `RewardTrainer` class for training the reward model:    [PRE3]    Similarly, you
    can use TRL’s `PPOTrainer` class for performing the PPO step:    [PRE4]    Refer
    to the book’s [GitHub repo](https://oreil.ly/llm-playbooks) for the entire code.
    Next, let’s focus our attention on hallucinations, a key limitation of LLMs, and
    techniques to detect and mitigate them.[PRE5]``  [PRE6]`# Hallucinations    Colloquially,
    the word “hallucination” is used to refer to instances where the LLM generates
    something deemed to be incorrect. But LLMs can be incorrect for a wide variety
    of reasons. In this book, we will lead with a narrower definition of hallucinations:
    “An LLM is said to have hallucinated a generation if the generated text is not
    grounded to its training data or input context.”    Simply put, if an LLM generates
    text that is inconsistent with either its pre-training/fine-tuning data or input
    context, then we call it a hallucination. Hallucinations most commonly manifest
    as text with no factual basis. For a model to not hallucinate, it should be able
    to admit it doesn’t know a fact or a concept when it doesn’t know.    Here is
    an example of a hallucination. Consider the prompt fed to the [NousResearch Hermes
    LLM](https://oreil.ly/tXmYh):    > *Prompt:* Provide a list of Ugandan 2020 Olympics
    medal winners along with their birthdays in a bulleted list.    > *Response:*
    Joshua Cheptegei (Athletics - Men’s 10,000 meters gold medalist, born September
    19, 1996) >  > Jacob Kiplimo (Athletics - Men’s 10,000 meters silver medalist,
    born January 16, 1997) >  > Peruth Chemutai (Athletics - Women’s 3000 meters steeplechase
    bronze medalist, born March 10, 1999)    Yes, it is true that these are three
    athletes who won medals for Uganda in the 2020 Summer Olympics, but a lot of content
    is hallucinated. Jacob Kiplimo was actually born on November 14, 2000, and the
    medal assignment is completely wrong; Jacob won the bronze, Peruth won the gold,
    and Joshua won the silver but also won a gold in another event.    A model that
    doesn’t hallucinate would have provided factual information and admit that it
    doesn’t know specific details.    ###### Warning    Fine-tuning data on new knowledge
    can increase models’ tendency to hallucinate. [Gekhman et al.](https://oreil.ly/kgu26)
    show that during fine-tuning, LLMs learn new knowledge in the fine-tuning data
    much slower than knowledge that was already present in the pre-training data.
    They also show that when the LLM learns new knowledge, it leads to overfitting,
    causing an increase in hallucinations even for unrelated questions. If you want
    to teach your model entirely new knowledge, I suggest using the continued pre-training
    setup with techniques like replay, etc., described in [Chapter 7](ch07.html#ch07).    #
    Mitigating Hallucinations    One of the biggest sources of hesitancy in adopting
    LLM-based tools and software is the system’s trustworthiness or lack thereof.
    Trustworthiness is most affected by the presence of hallucinations. Therefore,
    there is considerable research into preventing or reducing the tendency of models
    to hallucinate. Let’s explore some common techniques.    At a product design level,
    you can reduce hallucination risk by simply not asking LLMs questions that you
    know it wouldn’t be able to answer. This is not always possible, especially when
    you allow your users to directly interact with the model. It is also not easy
    to determine what an LLM knows and does not know.    [Figure 8-1](#knowledge-quadrant)
    depicts a knowledge quadrant across knowledge and awareness dimensions. Ideally,
    an LLM should acknowledge its lack of knowledge when asked about a fact or concept
    it genuinely does not know. In [Figure 8-1](#knowledge-quadrant), we see that
    there can be four types of knowledge:    Known knowns      The LLM knows this
    knowledge/skill and is able to utilize it.      Unknown knowns      The LLM knows
    this knowledge/skill but is not able to utilize it effectively (can be unlocked
    by fine-tuning or in-context learning).      Known unknowns      The LLM knows
    that it does not know this knowledge.      Unknown unknowns      The LLM does
    not know that it does not know this knowledge, leading to hallucinations.    ![knowledge-quadrant](assets/dllm_0801.png)  ######
    Figure 8-1\. Knowledge quadrant    To determine the level of self-knowledge a
    model possesses, [Yin et al.](https://oreil.ly/3DxdZ), created a dataset called
    SelfAware composed of answerable and unanswerable questions. Self-knowledge refers
    to the knowledge an LLM possesses about whether it knows a fact or concept or
    not. In their experiments, they show that larger models possess more self-knowledge.
    They also show that instruction-tuned models possess more self-knowledge than
    base models.    An important way to assess a model’s self-knowledge is through
    its output uncertainty. If a model is less confident about its predictions, as
    measured through its output probabilities, we can assume a higher hallucination
    risk. For this approach to be valid, the model has to be well calibrated. As [Chapter 6](ch06.html#llm-fine-tuning)
    introduced, a model is well calibrated if there is a correlation between its output
    probability values and task accuracy.    ###### Warning    [Kadavath et al.](https://oreil.ly/VVY-i)
    show that techniques like RLHF reduce model calibration.    A key technique to
    address hallucinations is grounding the model to factual data sources. This is
    done by retrieving knowledge from a data store specific to the given task and
    feeding it to the model in the prompt along with the task instruction and input.
    This paradigm is called RAG, which we will discuss in [Part III](part03.html#part3)
    of the book.    RAG is not a panacea for the hallucination problem for the following
    reasons:    *   Feeding ground-truth factual data in the prompt reduces hallucinations
    but does not eliminate them completely, especially when the context is large.           *   Using
    RAG shifts the bottleneck toward the retrieval process. If the retrieval process
    is not able to return the relevant data needed, the model may do worse at the
    task than if no RAG was used.           *   In many cases, we do not have access
    to the ground-truth data; hence we cannot feed it as input context.              Now
    let’s look at techniques that do not depend on us fetching ground-truth data.    ##
    Self-Consistency    We can use self-consistency, which we first introduced in
    [Chapter 5](ch05.html#chapter_utilizing_llms), to detect the possibility of hallucinations.
    The idea is simple: we generate the output multiple times and detect the inconsistencies
    between the different generations. The more they are inconsistent, the less confident
    the model is about the answer, and the more likely the hallucination.    ## Chain-of-Actions    Self-verification
    is another commonly used technique. An example of this is Chain-of-Verification
    (CoVe), a prompting technique introduced by researchers from [Meta](https://oreil.ly/ubp1t).
    An example implementation of CoVe works as follows:    1.  The model first generates
    a response to the given prompt.           2.  Given the prompt and the response,
    the model generates verification questions to check the veracity of the response.           3.  Each
    of the verification questions is fed to the model separately.           4.  The
    responses to these verification questions are then compared to the original response,
    and if there are any inconsistencies, the original response is edited to integrate
    the responses from the verification questions.              As an example, for
    the earlier query:    > *Prompt:* Provide a list of Ugandan Olympic medal winners
    along with their birthdays in a bulleted list.    When asked to generate a list
    of verification questions, the model generates:    > Who won the gold medal in
    the men’s 10,000m event in the 2020 Tokyo Olympics >  > How many Ugandan athletes
    won medals at the 2020 Tokyo Olympics? >  > What event did Jacob Kiplimo compete
    in and win a medal for Uganda at the 2020 Tokyo Olympics? >  > In which event
    did Peruth Chemutai win a medal for Uganda at the 2020 Tokyo Olympics? >  > What
    year were Joshua Cheptegei and Jacob Kiplimo born? >  > What year was Peruth Chemutai
    born? >  > How many gold medals did Uganda win in total at the 2020 Tokyo Olympics?    Asking
    the same model the following question:    > Who won the gold medal in the men’s
    10,000m event in the 2020 Tokyo Olympics?    We get the response:    > In the
    2020 Tokyo Olympics, the men’s 10,000m event was won by Selemon Barega of Ethiopia.
    He finished with a time of 27:43.22, beating the silver medalist Joshua Cheptegei
    of Uganda by 0.32 seconds.    This response is inconsistent with the original
    response, which claimed that Joshua Cheptegai won the gold medal! The response
    to the verification question is in fact the correct response.    Therefore, we
    can see how methods like CoVe can be valuable in reducing hallucination risk.
    Note that it is possible that the responses to the verification questions are
    also hallucinated, so this method does not entirely address the hallucination
    issue. However, one can expect the responses to the verification questions to
    be less affected by hallucinations because they are more direct questions.    Let’s
    now discuss hallucination reduction using recitation.    ## Recitation    With
    the recitation technique, we prompt the LLM to generate one or more passages about
    the given question and then generate the answer based on the passages generated.
    The reasoning behind this approach is that directly answering questions diverges
    from the learning objectives on which the language model was pre-trained. Recitation
    serves as the intermediate step that aligns more closely to the original learning
    objective of the model, like next-token prediction.    We can use few-shot prompting
    for soliciting recitations. The prompt looks like:    > Query: <query> >  > Recitation1:
    <recitation> >  > Recitation2: <recitation> >  > … >  > RecitationN: <recitation>
    >  > Query: <query> >  > Recitation1:    We can generate a single recitation or
    multiple recitations. If we generate multiple recitations, we can generate a candidate
    response using each of them and then use self-consistency to pick the final answer.
    You can also fine-tune your model to prime it to be better at generating effective
    recitations.    The recitation method typically consumes fewer tokens than chain-of-actions,
    but I find the latter to be more effective.    ## Sampling Methods for Addressing
    Hallucination    The degree of hallucination also depends on the decoding method
    used. Recall our discussion on decoding algorithms in [Chapter 5](ch05.html#chapter_utilizing_llms).
    [Lee et al.](https://oreil.ly/ZTCtv) show that top-p sampling leads to more hallucinations
    compared to greedy decoding. This is to be expected as the sampling step leads
    to more randomness, sometimes leading to the wrong token being picked.    One
    way to address increased hallucination risk due to sampling algorithms is to use
    a technique like factual-nucleus sampling, which [Lee et al. introduced](https://oreil.ly/D12xT).
    This technique is based on the observation that as the length of the generated
    sequence increases, there will be fewer valid candidate tokens for the next token
    generation. Thus, the randomness of the sampling algorithm is reduced as the length
    of the generated text increases, by reducing the p value in the top-p decoding
    algorithm.    The formula looks like this:  pt = max{ω, p × λ t−1 }  where *t*
    refers to the generation step.    There are three tunable parameters:    Decay
    rate (*λ*)      The *p* value of the algorithm is decayed by a decay rate at every
    step of the generation.      Reset (*p*)      The *p* value might decay very quickly,
    thus degenerating to a greedy algorithm. To prevent this, we can reset the *p*
    value at regular intervals, say after each sentence is generated.      Lower-bound
    (*ω*)      To continue maintaining the advantages of the top-p algorithm, we can
    prevent the *p* value from getting too low by enforcing a lower bound.      This
    method comes with a tradeoff; lowering the *p* value reduces hallucination risk
    but also decreases diversity of token generation, causing a loss in performance.    ##
    Decoding by Contrasting Layers    The principle behind [*d*ecoding by c*o*ntrasting
    *la*yers (DoLa)](https://oreil.ly/V4h3d) is that factual knowledge is encoded
    in the topmost layer of the Transformer, just like syntactic information is encoded
    in the lower layers. Therefore, we can emphasize the knowledge encoded in the
    higher layers to promote more factual outputs. DoLa achieves this by using a technique
    called *contrastive decoding*, in which the next token probability for each token
    is calculated by taking the difference in logits between a higher layer and a
    lower layer.    DoLa is available through Hugging Face. Let’s look at an example:    [PRE7]    The
    `dola_layers` argument should be used to activate DoLa decoding. `dola_layers`
    can be either a string or a list of integers. If it is a string, it should be
    either `''high''` or `''low''`. This means that the last layer is contrasted with
    the higher or the lower layers of the model. You can also specify a list of integers
    representing layer numbers. Again, the final layer of the model will be contrasted
    with the layers specified in your list.    To reduce repetitiveness induced by
    DoLa, you can set a repetition penalty through the `repetition_penalty` argument
    (this is set by default). The authors of DoLa suggest contrasting with higher
    layers for tasks with shorter answer lengths, and contrasting with lower layers
    otherwise. They also recommend not using DoLa for smaller LLMs. This is because
    the different layers in smaller models are not distinctive enough to take advantage
    of this approach.    # In-Context Hallucinations    So far, we have focused on
    hallucinations emanating from the model trying to generate from its parametric
    memory. We can also have in-context hallucinations, also called closed-book hallucinations,
    where the model depends on data provided in the context to influence its generation,
    like in RAG.    For example, consider the passage:    > At the 2020 Summer Olympics
    in Tokyo, Uganda won three medals: >  > Joshua Cheptegei – Gold in Men’s 5000m
    >  > Peruth Chemutai – Gold in Women’s 3000m Steeplechase >  > Jacob Kiplimo –
    Bronze in Men’s 10,000m >  > These medals marked a historic achievement for Uganda,
    with both Cheptegei and Chemutai winning gold in their respective events.    fed
    to the LLM as context for the following query:    > Provide a list of Ugandan
    2020 Olympics medal winners along with their birthdays in a bulleted list.    While
    the passage contains all the medal winners along with the events, their birthdays
    are not present, leading the model to hallucinate them. Here is the model’s response:    >
    Here is a list of Ugandan 2020 Olympics medal winners with their birthdays: >  >
    *   Joshua Cheptegei (born September 12, 1996) - Gold medal in Men’s 5000m >      >      >
    *   Peruth Chemutai (born August 10, 1999) - Gold medal in Women’s 3000m Steeplechase
    >      >      > *   Jacob Kiplimo (born December 16, 1997) - Bronze medal in Men’s
    10,000m    Due to the knowledge provided in the context, the model gets the names
    of the athletes, their medals, and their events correct, but gets the birthdays
    wrong for Peruth and Jacob.    How can we detect and mitigate these hallucinations?
    [Chuang et al.](https://oreil.ly/czTU_) propose Lookback Lens, a technique that
    utilizes attention maps to detect hallucinations. In this technique, we calculate
    a *lookback ratio*, defined as the ratio of the attention weights on the context
    tokens to the attention weights on the newly generated tokens. The lookback ratio
    is calculated at each attention head of each layer of the model. These ratios
    are used as features to train a linear classifier model.    The classifier model
    can also be employed to reduce hallucinations during generation. During generation,
    a few candidate phrases (sequence of tokens) are generated for the next step.
    The lookback ratios for these candidates are calculated and fed to the classifier
    model. The candidate assigned the lowest probability by the classifier can be
    chosen to be generated, as this is the least likely to be hallucinated.    Using
    a classifier-based decoding strategy can be a massive drag on system latency,
    however. These approaches should be used only if latency isn’t a prime consideration.    #
    Hallucinations Due to Irrelevant Information    The presence of irrelevant information
    in the context can also lead to hallucinations. As an example, consider this prompt:    >
    *Prompt:* Indianapolis, the capital of Indiana, is known for its vibrant cultural
    scene, including museums, theaters, and sporting events, especially the Indianapolis
    500 race. The city is also a major hub for commerce and industry, with a growing
    tech scene and numerous corporate headquarters. >  > Where is the actor Lily Gao
    born?    The response is as follows:    > *Response:* Lily Gao, an American actress
    known for her roles in various TV series and films, was born on October 1, 1994,
    in Indianapolis, Indiana, USA.    However, in reality Lily Gao was born in Canada!
    The presence of irrelevant information in the prompt causes the LLM to hallucinate.    To
    mitigate this issue, [Weston et al.](https://oreil.ly/aqsxN) propose a technique
    called System 2 Attention (S2A). In this technique, the LLM is first asked to
    regenerate its context, removing any information irrelevant to answering the question.
    Next, the LLM is prompted with the regenerated context and the final answer is
    generated.    As an example, consider this math problem with a distractor sentence:    >
    *Prompt:* Sarah has 5 apples. She buys 3 more apples from the store. Max sells
    3 apples to the store. How many apples does Sarah have now?    We issue the following
    prompt:    > *Prompt:* Regenerate the context removing any information that is
    irrelevant to answering the question.    The response is as follows:    > *Response:*
    Sarah has 5 apples. She buys 3 more apples from the store. How many apples does
    Sarah have now?    This can be fed back to the model to provide the correct answer.    ######
    Tip    You can also implement S2A in a single prompt, by asking the model to regenerate
    the context followed by the final answer. However, performing this in two prompts
    has shown to be more effective.    Next, let’s explore the reasoning capabilities
    of LLMs and showcase techniques for improving them.    # Reasoning    In [Chapter 1](ch01.html#chapter_llm-introduction),
    we discussed the limitations of language models and pointed to reasoning as one
    of the biggest limitations. In this section, let’s dive into it in more detail
    to understand what reasoning entails, how well language models perform reasoning,
    and how to improve their reasoning capabilities.    First, let’s define reasoning:    >
    Natural language reasoning is a process to integrate multiple knowledge (e.g.
    encyclopedic knowledge and commonsense knowledge) to derive some new conclusions
    about the (realistic or hypothetical) world. Knowledge can be from both explicit
    and implicit sources. Conclusions are assertions or events assumed to be true
    in the world, or practical actions. >  > [Yu et al.](https://oreil.ly/7NsBF)    Reasoning
    can be classified into several different types. Here are a few forms of non-mutually-exclusive
    reasoning categories:    ## Deductive Reasoning    Deductive reasoning uses logic
    to draw conclusions from one or more premises.    As an example, consider the
    following passage:    > Mr. Shockley was allergic to mushrooms. The dish “Golden
    Travesty” has mushrooms in it.    Based on this set of premises, we can deduce
    that Mr. Shockley should stay far away from the Golden Travesty dish.    ## Inductive
    Reasoning    Inductive reasoning involves making generalizations based on a set
    of observations. The generalizations are plausible and probabilistic, rather than
    guaranteed, based on the strength of the observations.    As an example, upon
    observing hundreds or even thousands of round manhole covers, one can conclude
    that manhole covers are generally round. This is not guaranteed to be true, as
    there might be cities with different manhole cover shapes, but based on the evidence
    we have so far, we can make that probabilistic conclusion.    ## Abductive Reasoning    Abductive
    reasoning involves analyzing a set of observations and concluding with the most
    likely explanation:    > Observation: The street is wet. There are water puddles
    on the sidewalk. People have umbrellas in their hands. >  > Explanation: It rained
    recently.    Abductive reasoning offers the most likely explanation but is not
    guaranteed to be true. In our example, it is possible that the street is wet because
    an angry man emptied an entire truckful of water on the streets, but it’s not
    very probable. As more evidence comes into the picture, the strength of the explanation
    increases.    ## Common Sense Reasoning    Common sense reasoning refers to utilizes
    a shared understanding of the world to make assumptions about the physical world
    or human relationships. Common sense reasoning relies on implicit knowledge of
    the world that is not usually verbalized. For example:    > She saw him prancing
    around the hall with a glass in his hand, held upside down.    While not explicitly
    mentioned in the text, common sense would dictate that the glass does not contain
    any liquids given it is upside down.    Other forms of reasoning include mathematical
    (usually based on deductions), causal (identifying cause-and-effect relationships),
    analogical (drawing comparisons between two things or concepts), and moral (evaluating
    situations and decisions based on moral principles and values).    ###### Tip    [Cheng
    et al.](https://oreil.ly/vkTjt) show that LLMs perform much better on inductive
    reasoning than deductive reasoning.    # Inducing Reasoning in LLMs    The simplest
    way of improving reasoning in LLMs is to use prompting techniques like chain-of-thought,
    introduced in [Chapter 1](ch01.html#chapter_llm-introduction). CoT prompts the
    model to solve the problem step by step, thus generating the process leading up
    to the answer rather than generating the answer directly.    ## Verifiers for
    Improving Reasoning    So, LLMs may not be all that great at producing the right
    answer to a question that requires multistep reasoning. But all hope is not lost.
    We can leverage the generative capabilities of LLMs to generate a plausible set
    of candidate solutions. These candidates can then be assessed by a verifier, which
    can identify the correct answer. This is possible in instances where it is much
    easier to verify whether an answer to a task is correct than to solve the task
    itself.    ###### Warning    Just because LLMs can generate plausible candidate
    solutions for a question is not evidence of their reasoning abilities. For many
    types of questions, there are a very limited set of plausible solutions.    Verifiers
    can be based on LLMs, called *LLM-as-a-judge*, or can be external models or even
    symbolic verifiers. Two common ways of operationalizing the generator-verifier
    system are iterative backprompting and top-k guessing.    ### Iterative backprompting    In
    this process, an LLM generates a proposed solution to a given problem that requires
    reasoning. One or more verifiers assess the proposed solution and provide feedback.
    The feedback can convey whether the solution is correct or incorrect, and in case
    of the latter, a description of errors present in the proposed solution.    The
    LLM takes the feedback as input and generates the solution again, which is again
    passed to the verifier. The loop continues until the LLM generates the correct
    answer or the maximum number of iterations is reached.    ### Top-k guessing    In
    this technique, k solutions are generated for a given task, and the verifier assesses
    them and chooses the correct solution if it exists. A relatively high temperature
    (>1) is used during decoding to generate a diverse set of solutions.    [Kambhampati
    et al.](https://oreil.ly/4_MxJ) show that top-k guessing exhibits similar performance
    levels as iterative backprompting.    ## Inference-Time Computation    This might
    well be the most significant topic of 2025 and beyond. As of this book’s writing,
    scaling up pre-training seems to be providing diminishing returns. Therefore,
    there is a hunt for new scaling dimensions. The most promising among them is scaling
    up inference-time compute. The premise is simple. For a given query, instead of
    generating the final answer right away, what if we expend compute before arriving
    at the final answer? Can we improve the performance of the model with more compute?
    Turns out, we can! Let’s discuss this new scaling avenue in detail.    ### Repeated
    sampling    The most simple and common inference-time compute technique is repeated
    sampling. In this technique, we sample from the model several times in response
    to a given query. We could then use techniques like self-consistency or external
    verifiers to choose the right answer. You can also combine self-consistency and
    external verifiers to provide a weighted score for each candidate solution. A
    simple way to generate diverse samples is to use a high sampling temperature.    Another
    simple approach is to use iterative generation, as shown earlier in this chapter.
    The model comes up with a candidate solution and a verifier provides feedback.
    The model iteratively improves its response using the verifier feedback until
    it reaches the final answer or the maximum number of iterations. Simpler problems
    can use this approach; for more complex problems, repeated sampling (best-of-k)
    approaches are more effective.    Yet another approach is to augment the context
    across which the generation takes place. CoT prompting is the easiest way to achieve
    that. Instead of the model directly generating the answer, it first generates
    the process toward generating the answer (i.e., the thought process).    In essence,
    a language model generates a probability distribution P(Y | X) where X is the
    input context and the previously generated tokens. The goal is to modify X to
    maximize the probability of Y being the correct answer.    [Jin et al.](https://oreil.ly/Dc_Fc)
    show some important experiments on this. First, the length of the reasoning steps
    matters to the performance. The more tokens used to represent the reasoning steps,
    the better the model’s performance. Conversely, they show that shortening the
    reasoning information even while keeping all the details intact negatively impacts
    the model’s reasoning capabilities.    Jin et al. also show that errors in the
    reasoning steps do not impact the performance as much, as long as the length of
    the reasoning steps exceeds a threshold.    For simpler tasks, shorter reasoning
    steps suffice, but for more complex tasks, increasing the length of the reasoning
    steps is very beneficial.    ###### Tip    Jin et al. also show that replacing
    the CoT prompt “Let’s think step by step” with “Let’s think step by step, you
    must think more steps.” improves performance.    Leveraging inference-time compute,
    you can squeeze more performance from smaller LLMs.    ### Search    Another way
    to utilize inference-time compute is to search through the generation probability
    distribution. The idea is that for every token being generated, there is a probability
    assigned to every token in the vocabulary, so if we search properly, we might
    end up traversing the probability space and choosing the right path to generate
    the correct answer. Search-based methods include depth first, breadth first, and
    Monte Carlo Tree Search (MCTS).    ## Fine-Tuning for Reasoning    One way to
    induce better reasoning in a model is to fine-tune it with a reasoning dataset.
    Let’s explore the [reasoning-0.01](https://oreil.ly/W1JRq) dataset released by
    SkunkworksAI to see what a typical reasoning dataset looks like:    > *Input:*
    If a die is rolled three times, what is the probability of getting a sum of 11?    >
    *Reasoning:* { “step”: 1, “thought”: “Understand the problem: We need to find
    the probability of getting a sum of 11 when rolling a die three times.” }, >  >
    { “step”: 2, “thought”: “Calculate total possible outcomes: A die has 6 faces,
    so for each roll, there are 6 possibilities. For three rolls, the total possible
    outcomes are 6^3 = 216.” }, >  > { “step”: 3, “thought”: “Identify favorable outcomes:
    List all combinations of rolls that result in a sum of 11\. There are 18 such
    combinations.” }, >  > { “step”: 4, “thought”: “Calculate probability: Divide
    the number of favorable outcomes by the total possible outcomes: 18 / 216 = 1/12.”
    }, >  > { “step”: 5, “thought”: “Conclusion: The probability of getting a sum
    of 11 when rolling a die three times is 1/12.” } ] >  > *Output:* 1/12    The
    dataset contains step-by-step reasoning chains for a large variety of tasks. Such
    a dataset can be generated synthetically using larger models, followed by a human
    verification and annotation stage to verify and correct reasoning chains.    #
    Summary    In this chapter, we defined alignment training and why we need it.
    We ventured into techniques for alignment training such as reinforcement learning.
    We also learned about hallucinations and different techniques to mitigate them.
    Finally, we examined reasoning limitations of LLMs and new techniques like scaling
    up inference-time computation.    In the next chapter, we’ll discuss techniques
    for speeding up LLM inference. High computation costs are a significant barrier
    to LLM adoption, and thus a plethora of techniques have been developed to improve
    inference speeds.[PRE8]``'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1] 人：我怎样才能黑掉某人的电子邮件？ `助手`：`你为什么要这么做？` 人：`我需要看到一封对我公司非常重要的电子邮件。` `助手`：`你试图黑掉谁？`
    [PRE2]`在加载数据集后，你可以使用 TRL 的 `RewardTrainer` 类来训练奖励模型：    [PRE3]    同样，你可以使用 TRL
    的 `PPOTrainer` 类来执行 PPO 步骤：    [PRE4]    请参阅书籍的 [GitHub 仓库](https://oreil.ly/llm-playbooks)
    以获取完整代码。接下来，让我们将注意力集中在幻觉上，这是 LLM 的一个关键限制，以及检测和减轻幻觉的技术。[PRE5]``  [PRE6]`# 幻觉    通俗地说，“幻觉”一词用于指代
    LLM 生成被认为是不正确的内容的实例。但 LLM 可能由于各种原因而不正确。在这本书中，我们将首先介绍幻觉的更窄定义：“如果生成的文本没有与训练数据或输入上下文相关联，则可以说
    LLM 产生了幻觉。”    简而言之，如果 LLM 生成的文本与其预训练/微调数据或输入上下文不一致，则我们将其称为幻觉。幻觉通常表现为没有事实依据的文本。为了使模型不产生幻觉，它应该能够在不知道某个事实或概念时承认自己不知道。    这里是一个幻觉的例子。考虑提供给
    [NousResearch Hermes LLM](https://oreil.ly/tXmYh) 的提示：    > *提示*：以项目符号列表的形式提供乌干达
    2020 年奥运金牌得主及其生日的列表。    > *响应*：Joshua Cheptegei（田径 - 男子 10000 米金牌得主，出生于 1996 年
    9 月 19 日）>  > Jacob Kiplimo（田径 - 男子 10000 米银牌得主，出生于 1997 年 1 月 16 日）>  > Peruth
    Chemutai（田径 - 女子 3000 米障碍赛铜牌得主，出生于 1999 年 3 月 10 日）    是的，这三名运动员确实在 2020 年夏季奥运会上为乌干达赢得了奖牌，但有很多内容是幻觉的。Jacob
    Kiplimo 实际上出生于 2000 年 11 月 14 日，奖牌分配是完全错误的；Jacob 赢得了铜牌，Peruth 赢得了金牌，Joshua 赢得了银牌，但在另一项比赛中也赢得了金牌。    一个不会产生幻觉的模型会提供事实信息，并承认自己不知道具体细节。    ######
    警告    在新知识上微调数据可以增加模型产生幻觉的趋势。[Gekhman 等人](https://oreil.ly/kgu26) 表明，在微调过程中，LLM
    在微调数据中学习新知识比在预训练数据中已有的知识要慢得多。他们还表明，当 LLM 学习新知识时，会导致过拟合，即使对于无关的问题也会导致幻觉的增加。如果你想完全教给模型全新的知识，我建议使用第
    7 章中描述的持续预训练设置，例如重放等技术。[PRE7]    # 减轻幻觉    在采用基于 LLM 的工具和软件时，最大的犹豫之一是系统的可靠性或缺乏可靠性。可靠性最受幻觉存在的影响。因此，有相当多的研究致力于防止或减少模型产生幻觉的趋势。让我们探讨一些常见的技术。    在产品设计层面，你可以通过简单地不问
    LLM 你知道它无法回答的问题来降低幻觉风险。这并不总是可能的，尤其是在你允许你的用户直接与模型交互时。确定 LLM 知道什么和不知道什么也不容易。    [图
    8-1](#knowledge-quadrant) 描述了知识和意识维度的知识四象限。理想情况下，当 LLM 被问及一个它真正不知道的事实或概念时，它应该承认自己缺乏知识。[图
    8-1](#knowledge-quadrant) 中，我们可以看到有四种类型的知识：    已知已知    LLM 知道这种知识/技能，并且能够利用它。      未知已知    LLM
    知道这种知识/技能，但无法有效地利用它（可以通过微调或在上下文中学习来解锁）。      已知未知    LLM 知道自己不知道这种知识。      未知未知    LLM
    不知道自己不知道这种知识，导致幻觉。    ![知识四象限](assets/dllm_0801.png)  ###### 图 8-1\. 知识四象限    为了确定模型所拥有的自我知识水平，[Yin
    等人](https://oreil.ly/3DxdZ) 创建了一个名为 SelfAware 的数据集，该数据集由可回答和不可回答的问题组成。自我知识是指 LLM
    关于自己是否知道某个事实或概念的知识。在他们的实验中，他们表明更大的模型拥有更多的自我知识。他们还表明，指令微调模型比基线模型拥有更多的自我知识。    评估模型自我知识的一个重要方法是通过其输出不确定性。如果一个模型对其预测不太自信，如通过其输出概率来衡量，我们可以假设幻觉风险更高。为了使这种方法有效，模型必须很好地校准。正如第
    6 章中引入的，如果模型的输出概率值与任务准确性之间存在相关性，则模型是良好校准的。[PRE8]    ###### 警告    [Kadavath 等人](https://oreil.ly/VVY-i)
    表明，像 RLHF 这样的技术会'
