- en: '1 Large language models: The power of AI'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 大型语言模型：AI的力量
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing large language models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍大型语言模型
- en: Understanding the intuition behind transformers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Transformer背后的直觉
- en: Exploring the applications, limitations, and risks of large language models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索大型语言模型的应用、局限性和风险
- en: Surveying breakthrough large language models for dialogue
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调查突破性大型语言模型在对话中的应用
- en: On November 30, 2022, San Francisco–based company OpenAI tweeted, “Try talking
    with ChatGPT, our new AI system which is optimized for dialogue. Your feedback
    will help us improve it” [[1]](https://twitter.com/OpenAI/status/1598014522098208769?cxt=HHwWgsCi-bfvpK0sAAAA).
    ChatGPT, a chatbot that interacts with users through a web interface, was described
    as a minor update to the existing models that OpenAI had already released and
    made available through APIs. But with the release of the web app, anyone could
    have conversations with ChatGPT, ask it to write poetry or code, recommend movies
    or workout plans, and summarize or explain pieces of text. Many of the responses
    felt like magic. ChatGPT set the tech world on fire, reaching 1 million users
    in a matter of days and 100 million users two months after launch. By some measures,
    it’s the fastest-growing internet service ever [[2]](https://www.technologyreview.com/2023/02/08/1068068/chatgpt-is-everywhere-heres-where-it-came-from/).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年11月30日，总部位于旧金山的公司OpenAI在推特上发文：“试试与我们的新AI系统ChatGPT对话，它针对对话进行了优化。您的反馈将帮助我们改进它”
    [[1]](https://twitter.com/OpenAI/status/1598014522098208769?cxt=HHwWgsCi-bfvpK0sAAAA)。ChatGPT是一个通过网页界面与用户互动的聊天机器人，被描述为OpenAI已经发布并可通过API提供的现有模型的微小更新。但随着网页应用的发布，任何人都可以与ChatGPT进行对话，让它写诗或代码，推荐电影或锻炼计划，或总结或解释文本片段。许多回应都感觉像是魔法。ChatGPT点燃了科技界，几天内达到100万用户，两个月后达到1亿用户。在某些衡量标准下，它是有史以来增长最快的互联网服务
    [[2]](https://www.technologyreview.com/2023/02/08/1068068/ChatGPT-is-everywhere-heres-where-it-came-from/)。
- en: Since ChatGPT’s public release, it has captivated millions of users’ imaginations
    and prompted caution from longtime tech observers about the dialogue agent’s shortcomings.
    ChatGPT and similar models are part of a class of large language models (LLMs)
    that have transformed the field of natural language processing (NLP) and enabled
    new best performances in tasks such as question answering, text summarization,
    and text generation. Already, prognosticators have speculated that LLMs will transform
    how we teach, create, work, and communicate. People of nearly every profession
    will interact with these models and maybe even collaborate with them. Therefore,
    people who are best able to use LLMs for the results they want—while avoiding
    common pitfalls that we’ll discuss—will be positioned to lead in the ongoing moment
    of generative AI.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 自ChatGPT公开发布以来，它吸引了数百万用户的想象力，并引起了长期科技观察者的谨慎，他们对对话代理的不足表示担忧。ChatGPT和类似模型属于一类大型语言模型（LLMs），它们已经改变了自然语言处理（NLP）领域，并在问答、文本摘要和文本生成等任务中实现了新的最佳性能。已经，预言家们推测LLMs将改变我们教学、创作、工作和沟通的方式。几乎每个职业的人都将与这些模型互动，甚至可能与他们合作。因此，那些最能利用LLMs实现他们想要的结果——同时避免我们将讨论的常见陷阱的人——将处于在持续进行的生成AI时刻的领导地位。
- en: As artificial intelligence (AI) practitioners, we believe that a basic understanding
    of how these models work is imperative to building an intuition for when and how
    to use them. This chapter will discuss the breakthrough of LLMs, how they work,
    how they can be used, and their exciting possibilities, along with their potential
    problems. Importantly, we’ll also drive the rest of the book forward by explaining
    what makes these LLMs important, as well as why so many people are so excited
    (and worried!) by them. Bill Gates has referred to this type of AI as “every bit
    as important as the PC, as the internet,” and said that ChatGPT would change the
    world [[3]](https://www.businessinsider.com/bill-gates-chatgpt-ai-artificial-intelligenct-as-important-pc-internet-2023-2).
    Thousands of people, including Elon Musk and Steve Wozniak, signed an open letter
    written by the Future of Life Institute, urging a pause in the research and development
    of these models until humanity was better equipped to handle the risks (see [http://mng.bz/847B](http://mng.bz/847B)).
    It recalled the concerns of OpenAI in 2019 when the organization had built a predecessor
    to ChatGPT and decided not to release the full model at that time out of fear
    of misuse [[4]](https://www.fastcompany.com/90308169/openai-refuses-to-release-software-because-its-too-dangerous).
    With all the buzz, competing viewpoints, and hyperbolic statements, it can be
    hard to cut through the hype to understand what LLMs are and are not capable of.
    This book will help you do just that, along with providing a useful framework
    for grappling with major problems in responsible technology today, including data
    privacy and algorithmic accountability.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人工智能（AI）从业者，我们相信，了解这些模型如何工作对于培养何时以及如何使用它们的直觉至关重要。本章将讨论LLMs的突破，它们的工作原理，它们的使用方式，以及它们的激动人心的可能性，同时也会讨论它们的潜在问题。重要的是，我们还将通过解释是什么使得这些LLMs如此重要，以及为什么这么多人对它们既兴奋又担忧来推动本书的其余部分。比尔·盖茨将这种类型的AI称为“与个人电脑、互联网一样重要”，并表示ChatGPT将改变世界
    [[3]](https://www.businessinsider.com/bill-gates-ChatGPT-ai-artificial-intelligenct-as-important-pc-internet-2023-2)。包括埃隆·马斯克和史蒂夫·沃兹尼亚克在内的数千人签署了未来生命研究所撰写的公开信，呼吁暂停这些模型的研究和开发，直到人类更好地准备好应对风险（参见
    [http://mng.bz/847B](http://mng.bz/847B)）。这回忆了OpenAI在2019年的担忧，当时该组织构建了ChatGPT的前身，并出于对误用的恐惧，当时决定不发布完整的模型
    [[4]](https://www.fastcompany.com/90308169/openai-refuses-to-release-software-because-its-too-dangerous)。在所有的喧嚣、不同的观点和夸张的声明中，很难透过炒作来理解LLMs的真正能力和局限性。这本书将帮助你做到这一点，同时提供一个有用的框架来应对今天负责任技术中的主要问题，包括数据隐私和算法问责制。
- en: Given that you’re here, you probably know a little bit about generative AI already.
    Maybe you’ve messaged with ChatGPT or another chatbot; maybe the experience delighted
    you, or maybe it perturbed you. Either reaction is understandable. In this book,
    we’ll take a nuanced and pragmatic approach to LLMs because we believe that while
    imperfect, LLMs are here to stay, and as many people as possible should be invested
    in making them work better for society.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你在这里，你很可能已经对生成式人工智能（generative AI）有一些了解了。也许你已经与ChatGPT或其他聊天机器人进行过交流；也许这次体验让你感到高兴，或者让你感到不安。无论哪种反应都是可以理解的。在这本书中，我们将以细腻和务实的方式探讨大型语言模型（LLMs），因为我们相信，尽管LLMs并不完美，但它们将长期存在，并且尽可能多的人应该投资于使它们更好地服务于社会。
- en: 'Despite the fanfare around ChatGPT, it wasn’t a singular technical breakthrough
    but rather the latest iterative improvement in a rapidly advancing area of NLP:
    LLMs. ChatGPT is an LLM designed for conversational use; other models might be
    tailored for other purposes or for general use in any natural language task. This
    flexibility is one aspect of LLMs that makes them so powerful compared to their
    predecessors. In this chapter, we’ll define LLMs and discuss how they came to
    such preeminence in the field of NLP.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ChatGPT引起了很大的关注，但它并不是一个单一的技术突破，而是自然语言处理（NLP）领域快速发展的一个最新迭代改进：LLMs。ChatGPT是一个为对话使用而设计的LLM；其他模型可能被定制用于其他目的，或者用于任何自然语言任务的通用用途。这种灵活性是LLMs相对于其前辈如此强大的一个方面。在本章中，我们将定义LLMs，并讨论它们是如何在NLP领域达到如此显赫的地位的。
- en: Evolution of natural language processing
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言处理的发展
- en: 'NLP refers to building machines to manipulate human language and related data
    to accomplish useful tasks. It’s as old as computers themselves: when computers
    were invented, among the first imagined uses for the new machines was programmatic
    cally translating one human language to another. Of course, at that time, computer
    programming itself was a much different exercise in which desired behavior had
    to be designed as a series of logical operations specified by punch cards. Still,
    people recognized that for computers to reach their full potential, they would
    need to understand natural language, the world’s predominant communication form.
    In 1950, British computer scientist Alan Turing published a paper proposing a
    criterion for AI, now known as the Turing test [[5]](https://doi.org/10.1093/mind/LIX.236.433).
    Famously, a machine would be considered “intelligent” if it could produce responses
    in conversation indistinguishable from those of a human. Although Turing didn’t
    use this terminology, this is a standard natural language understanding and generation
    task. The Turing test is now understood to be an incomplete criterion for intelligence,
    given that it’s easily passed by many modern programs that imitate human speech,
    yet are inflexible and incapable of reasoning [[6]](https://time.com/6238781/chatbot-chatgpt-ai-interview/).
    Nevertheless, it stood as a benchmark for decades and remains a popular standard
    for advanced natural language models.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: NLP指的是构建机器来操作人类语言及其相关数据以完成有用任务。它和计算机一样古老：当计算机被发明时，人们首先想象的新机器用途之一就是程序化地将一种人类语言翻译成另一种语言。当然，在当时，计算机编程本身是一项完全不同的练习，其中所需的行为必须设计成一系列由穿孔卡片指定的逻辑操作。尽管如此，人们认识到，为了使计算机充分发挥其潜力，它们需要理解自然语言，这是世界上主要的沟通形式。1950年，英国计算机科学家艾伦·图灵发表了一篇论文，提出了人工智能的一个标准，现在被称为图灵测试[[5]](https://doi.org/10.1093/mind/LIX.236.433)。著名的是，如果一台机器能够在对话中产生与人类无法区分的回应，那么它将被认为是“智能”的。虽然图灵没有使用这个术语，但这是一种标准的自然语言理解和生成任务。图灵测试现在被认为是一个不完整的人工智能标准，因为许多模仿人类语音的现代程序很容易通过，但它们缺乏灵活性，无法进行推理[[6]](https://time.com/6238781/chatbot-ChatGPT-ai-interview/)。尽管如此，它作为基准存在了数十年，并且仍然是高级自然语言模型的一个流行标准。
- en: Early NLP programs took the same approach as other early AI applications, employing
    a series of rules and heuristics. In 1966, Joseph Weizenbaum, a professor at the
    Massachusetts Institute of Technology (MIT), released a chatbot he named ELIZA,
    after the character in *Pygmalion*. ELIZA was intended as a therapeutic tool,
    and it would respond to users in large part by asking open-ended questions and
    giving generic responses to words and phrases that it didn’t recognize, such as
    “Please go on.” The bot worked with simple pattern matching, yet people felt comfortable
    sharing intimate details with ELIZA—when testing the bot, Weizenbaum’s secretary
    asked him to leave the room [[7]](https://99percentinvisible.org/episode/the-eliza-effect/).
    Weizenbaum himself reported being stunned at the degree to which the people who
    spoke with ELIZA attributed real empathy and understanding to the model. The anthropomorphism
    applied to his tool worried Weizenbaum, and he spent much of his time afterward
    trying to convince people that ELIZA wasn’t the success they heralded it as.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的NLP程序与其他早期的AI应用采取了相同的方法，使用一系列规则和启发式方法。1966年，麻省理工学院（MIT）的教授约瑟夫·魏岑鲍姆发布了一个名为ELIZA的聊天机器人，这个名字来源于《皮格马利翁》中的角色。ELIZA被设计成一种治疗工具，它主要通过提出开放式问题和对其不认识的语言和短语给出通用回应来回应用户，例如“请继续。”这个机器人通过简单的模式匹配工作，但人们感到与ELIZA分享私密细节很舒服——在测试这个机器人时，魏岑鲍姆的秘书要求他离开房间[[7]](https://99percentinvisible.org/episode/the-eliza-effect/)。魏岑鲍姆本人报告说，人们对与ELIZA交谈时所赋予的真正同理心和理解的程度让他感到震惊。他对自己的工具所应用的拟人化感到担忧，并在之后的大部分时间里试图说服人们，ELIZA并不是他们所宣扬的那样成功。
- en: Though rule-based text parsing remained common over the next several decades,
    these approaches were brittle, requiring complicated if-then logic and significant
    linguistic expertise. By the 1990s, some of the best results on tasks such as
    machine translation were instead being achieved through statistical methods, buoyed
    by the increased availability of both data and computing power. The transition
    from rule-based methods to statistical ones represented a major paradigm shift
    in NLP—instead of people teaching their models grammar by carefully defining and
    constructing concepts such as the parts of speech and tenses of a language, the
    new models did better by learning patterns on their own, through training on thousands
    of translated documents.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于规则的文本解析在接下来的几十年里仍然很常见，但这些方法很脆弱，需要复杂的if-then逻辑和显著的语文学识。到20世纪90年代，在诸如机器翻译等任务上的一些最佳结果是通过统计方法实现的，得益于数据和计算能力的增加。从基于规则的方法到统计方法的转变代表了自然语言处理领域的一个重大范式转变——不再是人们通过仔细定义和构建诸如语言中的词性和时态等概念来教他们的模型语法，而是新模型通过在成千上万的翻译文档上进行训练，通过自己学习模式而做得更好。
- en: This type of machine learning is called supervised learning because the model
    has access to the desired output for its training data—what we typically call
    labels, or, in this case, the translated documents. Other systems might use unsupervised
    learning, where no labels are provided, or reinforcement learning, a technique
    that uses trial and error to teach the model to find the best result by either
    receiving rewards or penalties. A comparison between these three types is given
    in table 1.1\.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的机器学习被称为监督学习，因为模型可以访问其训练数据的期望输出——我们通常称之为标签，或者在这种情况下，翻译文档。其他系统可能使用无监督学习，其中不提供标签，或者使用强化学习，这是一种使用试错来教会模型通过获得奖励或惩罚来找到最佳结果的技术。这三种类型之间的比较见表1.1。
- en: Table 1.1 Types of machine learning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 机器学习类型
- en: '|  | Supervised Learning | Unsupervised Learning | Reinforcement Learning |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '|  | 监督学习 | 无监督学习 | 强化学习 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Description | The model learns by mapping labeled inputs to known outputs.
    | The model is trained without labels and without a specific reward. | The model
    learns from its environment based on rewards and penalties. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 模型通过将标记输入映射到已知输出进行学习。 | 模型在没有标签和特定奖励的情况下进行训练。 | 模型根据奖励和惩罚从其环境中学习。 |'
- en: '| Data | Labeled data | Unlabeled data | No static dataset |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 数据 | 标签数据 | 未标记数据 | 无静态数据集 |'
- en: '| Objective | To predict the output of unseen inputs | To discover underlying
    patterns in the data, such as clusters | To determine the optimal strategy via
    trial and error |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 预测未见输入的输出 | 发现数据中的潜在模式，例如聚类 | 通过试错确定最佳策略 |'
- en: In reinforcement learning (shown in figure 1.1), rewards and penalties are numerical
    values that represent the model’s progress toward a particular task. When a behavior
    is rewarded, that positive feedback creates a reinforcing cycle in which the model
    is more likely to repeat the behavior, making penalized behavior less likely.
    As you’ll see, LLMs usually use a combination of these strategies.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（如图1.1所示）中，奖励和惩罚是代表模型向特定任务进展的数值。当某种行为得到奖励时，这种积极的反馈会形成一个强化循环，使得模型更有可能重复该行为，从而使得受惩罚的行为可能性降低。正如您将看到的，LLMs通常结合使用这些策略。
- en: '![](../../OEBPS/Images/CH01_F01_Dhamani.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH01_F01_Dhamani.png)'
- en: Figure 1.1 The reinforcement learning cycle
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 强化学习周期
- en: Reinforcement learning is a technique that uses trial and error to teach the
    model to find the best result by either receiving rewards or penalties from an
    algorithm based on its results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种使用试错来教会模型通过从算法根据其结果获得奖励或惩罚来找到最佳结果的技术。
- en: 'In addition to the type of learning used, several key components distinguish
    an NLP model. The first is data, which for natural language tasks is in the form
    of text. Second, there is an objective function, which is a mathematical statement
    of the model’s goal. An objective might be to minimize the number of errors made
    in a particular task or to minimize the difference between the model’s prediction
    of some value and the true value. Third, there are different model types and architectures,
    but virtually every advanced NLP model for the past several decades has been of
    one category: a neural network.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用的学习类型外，还有几个关键组件可以区分NLP模型。首先是数据，对于自然语言任务而言，数据的形式是文本。其次，有一个目标函数，它是模型目标的数学陈述。目标可能是最小化特定任务中犯的错误数量，或者最小化模型对某个值的预测与真实值之间的差异。第三，有不同类型的模型和架构，但过去几十年中几乎所有高级NLP模型都属于一个类别：神经网络。
- en: Neural networks, or neural nets, were proposed in 1944 as an algorithmic representation
    of the human brain [[8]](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414).
    Each network has an input layer, an output layer, and any number of “hidden” layers
    between them; each layer in turn has several neurons, or nodes, which can be connected
    in different ways. Each node assigns weights (representing the strength of connection
    between nodes) to the inputs passed to it, combines the weighted inputs, and “fires,”
    or passes, those inputs to the next layer when the weighted sum exceeds some threshold.
    In a neural network, the goal of training is to determine the optimal values for
    the weights and thresholds. Given training data, the training algorithm will iteratively
    update the weights and thresholds until it has found the ones that perform best
    in the model objective. The precise mathematics behind this process is beyond
    the scope of our discussion, but it’s important to note that large neural networks
    can approximate any function, no matter how complex, which makes them useful in
    scenarios with vast amounts of data, such as many NLP tasks. The number of *parameters*
    refers to the number of weights learned by the model and is shorthand for the
    level of complexity that the model can handle, which in turn informs the model’s
    capabilities. Today’s most capable LLMs have hundreds of billions of parameters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，或称为神经网，于1944年被提出，作为一种算法上的人脑表示[[8]](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414)。每个网络都有一个输入层、一个输出层，以及它们之间任意数量的“隐藏”层；每一层都有若干个神经元或节点，这些节点可以以不同的方式连接。每个节点为其接收到的输入分配权重（表示节点之间连接的强度），结合加权输入，并在加权总和超过某个阈值时“触发”或传递这些输入到下一层。在神经网络中，训练的目标是确定权重和阈值的最佳值。给定训练数据，训练算法将迭代更新权重和阈值，直到找到在模型目标中表现最佳的值。参数的数量指的是模型学习的权重数量，它是模型可以处理的复杂程度的简写，这也反过来影响了模型的能力。今天最强大的LLMs拥有数百亿个参数。
- en: In the past several decades, the availability of large amounts of data and processing
    power has served to cement the dominance of neural networks and led to countless
    experiments with different network architectures. Deep learning emerged as a subfield,
    where the “deep” simply refers to the depth of the neural nets involved, which
    is the number of hidden layers between the input and the output. People found
    that as the size and depth of neural nets increased, the performance of the models
    improved, as long as there was enough data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，大量数据和计算能力的可用性巩固了神经网络的统治地位，并导致了无数不同网络架构的实验。深度学习作为一个子领域出现，其中的“深度”仅仅指的是涉及的神经网络深度，即输入和输出之间的隐藏层数量。人们发现，随着神经网络的大小和深度的增加，只要数据足够，模型的性能也会提高。
- en: 'The birth of LLMs: Attention is all you need'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs的诞生：注意力即一切
- en: As people began training models for text generation, classification, and other
    natural language tasks, they sought to understand precisely what models learn.
    This isn’t a purely scientific inquiry; examining how models make their predictions
    is an important step in trusting models’ outputs enough to use them. Let’s take
    machine translation from English to Spanish as an example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人们开始训练用于文本生成、分类和其他自然语言任务的模型，他们试图精确地了解模型学习了什么。这不仅仅是一项纯粹的科学探究；检查模型如何做出预测是信任模型输出并使用它们的重要一步。让我们以从英语到西班牙语的机器翻译为例。
- en: When we give the model an input sequence, such as “The cat wore red socks,”
    that sequence must first be encoded into a mathematical representation of the
    text. The sequence is split into *tokens*, typically either words or partial words.
    The neural network converts those tokens into its mathematical representation
    and applies the algorithm learned in training. Finally, the output is converted
    back into tokens, or decoded, to produce a readable result. The output sequence
    in this case is the translated version of the sentence (*El gato usó calcetines
    rojos*), which makes the model a sequence-to-sequence model. When the model’s
    output is the correct translation, we’re satisfied that the model has “learned”
    the translation function, at least for the vocabulary and grammar structures used
    in the input.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向模型提供一个输入序列，例如“那只猫穿了红色的袜子”，这个序列首先必须被编码成文本的数学表示。序列被分割成*标记*，通常是单词或部分单词。神经网络将这些标记转换成其数学表示，并应用在训练中学习的算法。最后，输出被转换回标记，或者解码，以产生可读的结果。在这种情况下，输出序列是句子的翻译版本（*El
    gato usó calcetines rojos*），这使得模型成为一个序列到序列模型。当模型的输出是正确的翻译时，我们满意地认为模型已经“学习”了翻译函数，至少对于输入中使用的词汇和语法结构来说是这样。
- en: In 2014, machine learning researchers, again inspired by human cognition [[9]](http://arxiv.org/abs/1409.0473),
    proposed an alternative to the traditional approach of passing sequences through
    the encoder-decoder model piece by piece. In the new approach, the decoder could
    search the entire input sequence and try to find the pieces that were most relevant
    to each part of the generation. The mechanism is called *attention*. Let’s return
    to the example of machine translation. If you’re asked to pick out the key words
    from the sentence, “That cat chased a mouse, but it didn’t catch it,” then you
    would probably say “cat” and “mouse” because articles such as “that” and “a” aren’t
    as relevant in translation. As illustrated in figure 1.2, you focused your “attention”
    on the important words. The attention mechanism mimics this by adding attention
    weights to augment important parts of the sequence.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，受人类认知的启发 [[9]](http://arxiv.org/abs/1409.0473)，机器学习研究人员提出了对传统方法的一种替代，即逐个将序列通过编码器-解码器模型。在新方法中，解码器可以搜索整个输入序列，并尝试找到与生成每个部分最相关的部分。这种机制被称为*注意力*。让我们回到机器翻译的例子。如果你被要求从句子“那只猫追了一只老鼠，但它没有抓住它”中挑选出关键词，你可能会说“猫”和“老鼠”，因为像“那”和“一”这样的冠词在翻译中并不那么相关。如图1.2所示，你将“注意力”集中在重要的单词上。注意力机制通过为序列的重要部分添加注意力权重来模拟这一点。
- en: Attention provides context for any position, or word, for the sequence.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力为序列中的任何位置或单词提供上下文。
- en: '![](../../OEBPS/Images/CH01_F02_Dhamani.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F02_Dhamani.png)'
- en: Figure 1.2 The distribution of attention for the word “it” in different contexts
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 不同上下文中单词“它”的注意力分布
- en: A few years later, a paper from Google Brain aptly entitled “Attention Is All
    You Need” showed that models which discarded the lengthy sequential steps of other
    architectures and used only the attention information were much faster and more
    parallelizable. They called these models transformers. Transformers begin with
    an initial representation of the input sentence and then generate a new representation
    repeatedly for each word in the sentence using self-attention on the whole input
    until the end of the sentence is reached. In this way, the model can capture long-term
    dependencies—because each step includes all context—but the representations can
    be computed in parallel. The “Attention Is All You Need” paper demonstrated that
    these models achieved state-of-the-art performance on English-to-German and English-to-French
    translation tasks [[10]](http://arxiv.org/abs/1706.03762). It was the biggest
    NLP breakthrough of the decade, laying the foundation for all that followed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 几年后，一篇由谷歌大脑撰写的论文，恰当地命名为“Attention Is All You Need”，表明那些丢弃了其他架构中冗长的顺序步骤，仅使用注意力信息的模型要快得多，并且更易于并行化。他们将这类模型称为transformers。Transformers从输入句子的初始表示开始，然后通过在整个输入上使用自注意力，对句子中的每个单词进行重复生成新的表示，直到句子结束。这样，模型可以捕捉长期依赖关系——因为每个步骤都包括所有上下文——但表示可以并行计算。
    “Attention Is All You Need”论文证明了这些模型在英语到德语和英语到法语翻译任务上达到了最先进的性能 [[10]](http://arxiv.org/abs/1706.03762)。这是十年中最大的NLP突破，为所有后续发展奠定了基础。
- en: With transformers, because of the improvements in both time and resources required,
    it became possible to train models on much larger amounts of data. This marked
    the beginning of the LLM. In 2018, OpenAI introduced Generative Pre-training (GPT),
    a transformer-based LLM that was trained using massive amounts of unlabeled data
    from the internet and then could be fine-tuned to specific tasks, such as sentiment
    analysis, machine translation, text classification, and more [[11]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf).
    Before this, most of the NLP models were trained for a particular task, which
    was a major bottleneck as they needed large amounts of annotated data for that
    task, and annotating data can be both time-consuming and expensive. These general-purpose
    LLMs were designed to overcome that challenge, using unlabeled data to build meaningful
    internal representations of the words and concepts themselves.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在时间和资源需求上的改进，使用transformers可以在大量数据上训练模型。这标志着LLM（大型语言模型）时代的开始。2018年，OpenAI推出了生成预训练（GPT），这是一个基于transformers的LLM，它使用了从互联网上大量未标记的数据进行训练，然后可以针对特定任务进行微调，例如情感分析、机器翻译、文本分类等
    [[11]](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)。在此之前，大多数NLP模型都是针对特定任务进行训练的，这成为了一个主要瓶颈，因为它们需要大量针对该任务的标注数据，而标注数据既耗时又昂贵。这些通用LLM旨在克服这一挑战，使用未标记数据构建对单词和概念本身的具有意义的内部表示。
- en: Fine-tuning a model refers to taking a model trained on a large dataset and
    then tuning or tweaking the model to make it perform a similar task, which allows
    us to take advantage of what the model has already learned without having to develop
    it from scratch.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 微调模型指的是在大型数据集上训练好的模型，然后调整或微调模型以执行类似任务，这样我们就可以利用模型已经学到的知识，而无需从头开始开发。
- en: While experts debate what size model should be considered “large,” another early
    LLM, Google’s BERT (Bidirectional Encoder Representations from Transformers),
    was trained on billions of words and had more than 100 million parameters, or
    learned weights, using the transformer architecture [[12]](https://arxiv.org/pdf/1810.04805.pdf).
    For a timeline summarizing major events in NLP, see figure 1.3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当专家们争论应该将什么大小的模型视为“大型”时，另一个早期的LLM，谷歌的BERT（来自Transformers的双向编码器表示），在数十亿个单词上进行了训练，并使用了超过1亿个参数或学习权重，使用transformers架构
    [[12]](https://arxiv.org/pdf/1810.04805.pdf)。要查看NLP主要事件的总结时间线，请参阅图1.3。
- en: '![](../../OEBPS/Images/CH01_F03_Dhamani.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F03_Dhamani.png)'
- en: Figure 1.3 A timeline of breakthrough events in NLP
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 NLP突破事件时间线
- en: Explosion of LLMs
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM的爆炸式增长
- en: 'In the previous section, we discussed how language models could be trained
    for a particular task by learning from patterns in data. For translation, one
    might use a dataset of documents duplicated in multiple languages; for summarization
    tasks, a dataset of documents with handwritten summaries; and so on. But unlike
    these previous applications, LLMs aren’t intended to be task-specific. Instead,
    the task they are trained on is simply to predict what token (or word) fits best,
    given a particular context with one of the tokens hidden from the model. The beauty
    of this task is that it’s self-supervised: the model trains itself to learn one
    part of the input from another part of the input, so no labeling is required.
    This is also known as predictive or pretext learning.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何通过学习数据中的模式来训练语言模型以完成特定任务。对于翻译，可能会使用包含多种语言文档的数据集；对于摘要任务，可能会使用包含手写摘要的文档数据集；等等。但与这些先前应用不同，LLMs并非旨在特定于任务。相反，它们训练的任务仅仅是预测在特定上下文中隐藏的一个标记（或单词）最适合什么，因此不需要标签。这个任务的美丽之处在于它是自监督的：模型通过从输入的另一部分学习来训练自己学习输入的一部分，因此不需要标注。这也被称为预测性或预文学习。
- en: As LLMs are applied to diverse fields, they are becoming an integral part of
    our everyday lives. Conversational agents such as Apple’s Siri, Amazon’s Alexa,
    and Google Home use NLP to listen to user queries, turn sound into text, and then
    perform tasks or find answers. We see customer service chatbots in retail, and
    we’ll discuss more sophisticated dialogue agents, like ChatGPT, in a later section.
    NLP is also being used to interpret or summarize electronic health records in
    medicine, as well as to tackle mundane legal tasks, such as locating relevant
    precedents in case law or mining documents for discovery. Social media platforms,
    such as Facebook, Twitter, and Reddit, among others, also use NLP to improve online
    discourse by detecting hate speech or offensive comments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs被应用于各个领域，它们正成为我们日常生活中的一个重要组成部分。像苹果的Siri、亚马逊的Alexa和谷歌Home这样的对话代理使用NLP来监听用户查询，将声音转换为文本，然后执行任务或寻找答案。我们在零售业看到客户服务聊天机器人，我们将在下一节中讨论更复杂的对话代理，如ChatGPT。NLP还用于医学中解释或总结电子健康记录，以及处理日常法律任务，如查找案例法中的相关先例或挖掘文件以进行发现。社交媒体平台，如Facebook、Twitter和Reddit等，也使用NLP通过检测仇恨言论或攻击性评论来改善在线讨论。
- en: Later, we’ll talk about how LLMs can be fine-tuned to excel in particular use
    cases, but the structure of the training phase means that LLMs can generate text
    fluidly in a variety of contexts. This attribute makes them ideal candidates for
    dialogue agents but has also given them some unexpected capabilities in tasks
    they weren’t explicitly trained for.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将讨论如何微调LLMs以在特定用例中表现出色，但训练阶段的结构意味着LLMs可以在各种情境下流畅地生成文本。这种属性使它们成为对话代理的理想人选，但也赋予它们在未明确训练的任务中一些意想不到的能力。
- en: What are LLMs used for?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs有哪些用途？
- en: The general-purpose nature and versatility of LLMs result in a broad range of
    natural language tasks, including conversing with users, answering questions,
    and classifying or summarizing text. In this section, we’ll discuss several common
    LLM use cases and the problems they solve, as well as the promise they show in
    various novel tasks—such as coding assistants and logical reasoning—where language
    models haven’t historically been used.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的通用性和多功能性导致了一系列自然语言任务，包括与用户交谈、回答问题和分类或总结文本。在本节中，我们将讨论几个常见的LLM用例和它们解决的问题，以及它们在历史上未使用语言模型的各种新颖任务（如编码助手和逻辑推理）中展现出的潜力。
- en: Language modeling
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语言建模
- en: Modeling language is the most natural application of language models. Specifically,
    for text completion, the model learns the features and characteristics of natural
    language and generates the next most probable word or character. When used to
    train LLMs, this technique can then be applied to a range of natural language
    tasks, as discussed in subsequent sections.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是语言模型最自然的应用。具体来说，对于文本补全，模型学习自然语言的特征和特性，并生成下一个最可能的单词或字符。当用于训练LLMs时，这种技术可以应用于一系列自然语言任务，如后续章节所述。
- en: 'Language modeling tasks are often evaluated on a variety of datasets. Let’s
    look at an example of a long-range dependency task in which the model is asked
    to predict the last word of a sentence conditioned on a paragraph of context [[13]](https://arxiv.org/pdf/1606.06031.pdf).
    The context given to the model follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模任务通常在各种数据集上评估。让我们来看一个长距离依赖任务的例子，在这个任务中，模型被要求根据一段上下文预测句子的最后一个单词 [[13]](https://arxiv.org/pdf/1606.06031.pdf)。模型得到的上下文如下：
- en: He shook his head, took a step back, and held his hands up as he tried to smile
    without losing a cigarette. “Yes, you can,” Julia said in a reassuring voice.
    “I’ve already focused on my friend. You just have to click the shutter, on top,
    here.”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 他摇了摇头，后退了一步，举起双手试图微笑而不丢掉香烟。“是的，你可以，”朱莉娅用安慰的语气说。“我已经专注于我的朋友了。你只需要点击上面的快门，就在这里。”
- en: 'Here, the target sentence where the model needs to predict the last word is
    the following: “He nodded sheepishly, threw his cigarette away and took the _____.”
    The correct word for the model to predict here would be “camera.”'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，模型需要预测最后一个单词的目标句子是以下内容：“他尴尬地点了点头，扔掉了他的香烟，然后拿起了_____。”模型在这里需要预测的正确单词应该是“相机”。
- en: 'Other tasks for evaluating model performance include picking the best ending
    to a story or a set of instructions [[14]](https://arxiv.org/pdf/1905.07830.pdf)
    or selecting the correct ending sentence for a story that is a couple of sentences
    long. Let’s look at another example here where we have the following story [[15]](https://cs.rochester.edu/nlp/rocstories/):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '评估模型性能的其他任务包括选择故事或一组指令的最佳结尾 [[14]](https://arxiv.org/pdf/1905.07830.pdf) 或选择一个由几句话组成的故事的正确结尾句子。让我们来看另一个例子，其中我们有以下故事
    [[15]](https://cs.rochester.edu/nlp/rocstories/):'
- en: “Karen was assigned a roommate her first year of college. Her roommate asked
    her to go to a nearby city for a concert. Karen agreed happily. The show was absolutely
    exhilarating.” The most probable and desired ending for the model to select would
    be “Karen became good friends with her roommate,” while the least probable ending
    would be “Karen hated her roommate.”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: “凯伦在大学的第一年被分配了一个室友。她的室友邀请她去附近的城市听音乐会。凯伦欣然同意。演出绝对令人兴奋。”模型最可能和期望选择的结尾是“凯伦和她的室友成为了好朋友”，而最不可能的结尾是“凯伦讨厌她的室友”。
- en: These models are used for text generation, or natural language generation (NLG),
    as they are trained to produce text similar to text written by humans. Particularly
    useful for conversational chatbots and autocomplete, they can also be fine-tuned
    to produce text in different styles and formats, including social media posts,
    news articles, and even programming code. Text generation has been performed using
    BERT, GPT, and others.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型用于文本生成，或自然语言生成（NLG），因为它们被训练生成与人类撰写的文本相似的文字。特别适用于对话聊天机器人和自动完成，它们还可以微调以生成不同风格和格式的文本，包括社交媒体帖子、新闻文章，甚至编程代码。文本生成已经使用
    BERT、GPT 等技术实现。
- en: Question answering
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问答
- en: LLMs are widely used for question answering, which deals with answering questions
    from humans in a natural language. The two types of question-answering tasks are
    multiple-choice and open-domain. For the multiple-choice question-answering task,
    the model picks the correct answer from a set of possible answers, whereas for
    open-domain tasks, the model provides answers to questions in natural language
    without any options provided.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在问答任务中得到了广泛应用，这些任务涉及以自然语言回答人类提出的问题。问答任务分为两种类型：多项选择和开放域。对于多项选择题，模型从一组可能的答案中选择正确答案，而对于开放域任务，模型在没有任何选项提供的情况下以自然语言回答问题。
- en: Based on their inputs and outputs, there are three main variations of QA models.
    The first is extractive QA, where the model extracts the answer from a context,
    which can be provided as text or a table. The second is open-book generative QA,
    which uses the provided context to generate free text. It’s like the first QA
    approach except instead of pulling the answer verbatim from the context, it uses
    the given context to generate an answer in its own words. The last variation is
    closed-book generative QA, where you don’t provide any context in your input,
    only a question, and the model generates the most likely answer according to its
    training.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 根据它们的输入和输出，问答模型主要有三种变体。第一种是抽取式问答，其中模型从上下文中抽取答案，上下文可以是文本或表格。第二种是开放式书籍生成式问答，它使用提供的上下文生成自由文本。这与第一种问答方法类似，除了不是直接从上下文中抽取答案，而是使用给定的上下文用自己的话生成答案。最后一种变体是闭卷生成式问答，在这种情况下，你的输入中不提供任何上下文，只有一个问题，模型根据其训练生成最可能的答案。
- en: Until the recent breakthroughs in LLMs, the question-answering task has normally
    been approached as an open-book generative QA given the infinite possibilities
    of queries and responses. Newer models such as GPT-3 have been evaluated on extremely
    strict closed-book settings where external context isn’t allowed, and the model
    isn’t allowed to train on, or “learn from,” the datasets they will be evaluated
    on in any capacity. Popular datasets for evaluation of QA tasks include trivia
    questions (see [http://mng.bz/E9Rj](http://mng.bz/E9Rj)) and Google search queries
    (see [http://mng.bz/NVy7](http://mng.bz/NVy7)). Here, example questions might
    include “Which politician won the Nobel Peace Prize in 2009?” or “What music did
    Beethoven compose?”
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在最近在大型语言模型（LLMs）中的突破之前，问答任务通常被视为一种开放式书籍生成式问答，因为查询和响应的可能性是无限的。新的模型，如GPT-3，在极其严格的闭卷设置下进行了评估，不允许使用外部上下文，并且模型不允许在它们将被评估的数据集上训练或“从”数据集中学习。用于评估问答任务的流行数据集包括常识问题（见[http://mng.bz/E9Rj](http://mng.bz/E9Rj)）和谷歌搜索查询（见[http://mng.bz/NVy7](http://mng.bz/NVy7)）。在这里，示例问题可能包括“哪位政治家在2009年获得了诺贝尔和平奖？”或“贝多芬创作了什么音乐？”
- en: 'Another application that aligns closely with the question-answering task is
    reading comprehension. In this task, the model is shown a few sentences or paragraphs
    and then asked to answer a specific question. To best mirror human-like performance,
    LLMs have often been tested on various formats of reading comprehension questions,
    including multiple-choice, dialogue acts, and abstractive datasets. Let’s look
    at an example from a conversational question-answering dataset [[16]](https://arxiv.org/pdf/1808.07042.pdf).
    Here, the task is to answer the next question in the conversation: “Jessica went
    to sit in her rocking chair. Today was her birthday, and she was turning 80\.
    Her granddaughter Annie was coming over in the afternoon and Jessica was very
    excited to see her. Her daughter Melanie and Melanie’s husband Josh were coming
    as well. Jessica had . . . .” If the first question in the conversation is “Who
    had a birthday?” the correct answer would be “Jessica.” Then, given the next question
    in the conversation, “How old would she be?” the model should respond with “80.”'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个与问答任务紧密相关的应用是阅读理解。在这个任务中，模型被展示了几句话或几段文字，然后被要求回答一个特定的问题。为了最好地模拟人类的表现，大型语言模型（LLMs）通常在各种阅读理解问题格式上进行测试，包括多项选择题、对话行为和抽象数据集。让我们看看来自对话问答数据集的一个例子
    [[16](https://arxiv.org/pdf/1808.07042.pdf)]。在这里，任务是回答对话中的下一个问题：“杰西卡坐在她的摇椅上。今天是她的生日，她即将满80岁。她的孙女安妮将在下午过来，杰西卡非常期待见到她。她的女儿梅勒妮和梅勒妮的丈夫乔什也会过来。杰西卡有……”。如果对话中的第一个问题是“谁过了生日？”正确的答案将是“杰西卡”。然后，根据对话中的下一个问题“她会是多少岁？”模型应该回答“80岁”。
- en: One of the most notable examples of a model designed for the question-answering
    task is IBM Research’s Watson. In 2011, the Watson computer competed on *Jeopardy!*
    against the TV show’s two biggest all-time champions and won [[17]](https://www.jeopardy.com/sites/default/files/2023-06/ThisisJeopardyEp8.pdf).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 专为问答任务设计的模型中最引人注目的例子之一是IBM研究部门的沃森（Watson）。在2011年，沃森计算机参加了电视节目《危险边缘》（*Jeopardy!*）的竞赛，与该节目两位历史上最成功的冠军选手对抗并获胜
    [[17](https://www.jeopardy.com/sites/default/files/2023-06/ThisisJeopardyEp8.pdf)]。
- en: Coding
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码
- en: Recently, code generation has become one of the most popular applications of
    LLMs. Such models take natural language input and produce code snippets for a
    given programming language. While there are certain challenges to address in this
    space—security, transparency, and licensing—developers and engineers of different
    levels of expertise use LLM-assisted tools to improve productivity every day.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，代码生成已成为LLMs（大型语言模型）最受欢迎的应用之一。这些模型接受自然语言输入，并为特定的编程语言生成代码片段。虽然在这个领域存在一些需要解决的问题——安全性、透明度和许可——但不同水平的开发者和工程师每天都在使用LLM辅助工具来提高生产力。
- en: Code-generation tools took off in mid-2022 with the release of GitHub’s CoPilot.
    Described as “Your AI Pair Programmer,” CoPilot was introduced as a subscription-based
    service for individual programmers (see [https://github.com/features/copilot](https://github.com/features/copilot)).
    Based on OpenAI’s Codex model, it quickly became a way to boost developer productivity
    as a “pair programming” sidekick. Codex is a version of GPT-3 that has been fine-tuned
    for coding tasks in more than a dozen different programming languages. GitHub
    CoPilot suggests code as you type, autofills repetitive code, shows alternative
    suggestions, and converts comments to code.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成工具在2022年中随着GitHub的CoPilot发布而兴起。被称为“你的AI编程伙伴”，CoPilot被介绍为面向个人程序员的订阅制服务（见[https://github.com/features/copilot](https://github.com/features/copilot)）。基于OpenAI的Codex模型，它迅速成为提高开发者生产力的“配对编程”助手。Codex是GPT-3的一个版本，它针对十多种不同的编程语言进行了微调。GitHub
    CoPilot在您键入时建议代码，自动填充重复代码，显示替代建议，并将注释转换为代码。
- en: Developers have found creative yet unexpected ways to use the AI-assisted programmer,
    such as assisting non-native English speakers, preparing for coding interviews,
    testing your code, and more. Also in June 2022, Amazon announced a similar tool
    dubbed CodeWhisperer, described as an AI-based coding companion to improve developer
    productivity by generating code recommendations and security scans (see [https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/)).
    It’s worth noting that these programming tools are marketed as “pair programmers”
    or “programming assistants” to complement the human instead of replace them. While
    CoPilot and CodeWhisperer generally make good suggestions, they don’t think about
    programs in a way that a human does and might sometimes make dumb mistakes. In
    Chapter 6, we’ll discuss the idea of machines improving productivity in detail.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者已经找到了创造性的、出乎意料的方法来使用AI辅助的程序员，例如帮助非英语母语者，准备编码面试，测试您的代码等等。此外，在2022年6月，亚马逊宣布了一款类似工具，名为CodeWhisperer，它被描述为基于AI的编码伴侣，通过生成代码推荐和安全扫描来提高开发者生产力（见[https://aws.amazon.com/codewhisperer/](https://aws.amazon.com/codewhisperer/)）。值得注意的是，这些编程工具被营销为“配对程序员”或“编程助手”，以补充人类而不是取代他们。虽然CoPilot和CodeWhisperer通常能提出很好的建议，但它们思考程序的方式与人类不同，有时可能会犯愚蠢的错误。在第6章中，我们将详细讨论机器提高生产力的想法。
- en: An iteration of the GPT class models, GPT-4, released in March 2023, was evaluated
    on various coding tasks [[18]](https://arxiv.org/pdf/2303.08774.pdf). LeetCode
    is a well-known online platform for solving data structure and algorithm coding
    challenges that often show up in technical interviews for software engineering
    positions. While GPT-4 performs relatively well on “easy” LeetCode problems, it
    struggles to solve “medium” or “hard” problems, suggesting that many coding tasks
    still benefit from human intervention.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPT类模型的一个迭代版本，GPT-4，于2023年3月发布，在各种编码任务上进行了评估[[18]](https://arxiv.org/pdf/2303.08774.pdf)。LeetCode是一个知名的在线平台，用于解决数据结构和算法编码挑战，这些挑战经常出现在软件工程职位的面试中。虽然GPT-4在“简单”的LeetCode问题上表现相对较好，但在“中等”或“困难”的问题上则显得力不从心，这表明许多编码任务仍然需要人类的干预。
- en: Content generation
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内容生成
- en: An extremely promising and widely used application of LLMs is generating content.
    Generating human-like text for synthetic news articles has historically been used
    as an evaluation task for LLMs. Given a title and a subtitle, or the first sentence,
    the task is to generate a long-form article where the evaluation criterion is
    to determine how likely the article was written by a machine—the better the article
    generated, the more difficult to tell that a machine wrote it. A variation of
    the task, also relevant to the potential misuse of LLMs as discussed in section
    Limitations in Controlling Machine Outputs, is to use human ability to gauge the
    quality of the news article by detecting if humans can distinguish between machine-
    and human-generated content. This variant is analogous to a Turing test for content
    instead of conversation. GPT-3 and other LLM variants continue to produce news
    articles that are difficult for humans to identify as being machine-written. The
    following example is an article generated by GPT-3, given a title and a subtitle,
    that was the trickiest for humans to identify as machine-generated [[19]](https://arxiv.org/pdf/2005.14165.pdf).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLM）的一个极具前景且广泛应用的领域是生成内容。为合成新闻文章生成类似人类的文本，在历史上一直被用作评估LLM的任务。给定一个标题和副标题，或者第一句话，任务就是生成一篇长篇文章，评估标准是确定文章被机器写出的可能性——生成的文章越好，就越难判断是机器写的。这个任务的变体，也与第3.2节“控制机器输出限制”中讨论的LLM潜在误用相关，即利用人类能力通过检测人类能否区分机器生成内容和人类生成内容来评估新闻文章的质量。这个变体类似于内容而非对话的图灵测试。GPT-3和其他LLM变体继续生成人类难以识别为机器撰写的新闻文章。以下是一个由GPT-3生成的文章示例，给定一个标题和副标题，人类最难识别为机器生成
    [[19]](https://arxiv.org/pdf/2005.14165.pdf)。
- en: 'Title: United Methodists Agree to Historic Split'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 标题：卫理公会达成历史性分裂
- en: 'Subtitle: Those who oppose gay marriage will form their own denomination'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 副标题：反对同性婚姻的人将成立自己的教派
- en: 'Article: After two days of intense debate, the United Methodist Church has
    agreed to a historic split—one that is expected to end in the creation of a new
    denomination, one that will be “theologically and socially conservative,” according
    to The Washington Post. The majority of delegates attending the church’s annual
    General Conference in May voted to strengthen a ban on the ordination of LGBTQ
    clergy and to write new rules that will “discipline” clergy who officiate at same-sex
    weddings. But those who opposed these measures have a new plan: They say they
    will form a separate denomination by 2020, calling their church the Christian
    Methodist denomination. The Post notes that the denomination, which claims 12.5
    million members, was in the early 20th century the “largest Protestant denomination
    in the U.S.,” but that it has been shrinking in recent decades. The new split
    will be the second in the church’s history. The first occurred in 1968, when roughly
    10 percent of the denomination left to form the Evangelical United Brethren Church.
    The Post notes that the proposed split “comes at a critical time for the church,
    which has been losing members for years,” which has been “pushed toward the brink
    of a schism over the role of LGBTQ people in the church.” Gay marriage is not
    the only issue that has divided the church. In 2016, the denomination was split
    over ordination of transgender clergy, with the North Pacific regional conference
    voting to ban them from serving as clergy, and the South Pacific regional conference
    voting to allow them.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文章：经过两天激烈的辩论，卫理公会教会达成了一项历史性的分裂——据《华盛顿邮报》报道，预计这将导致一个新教派的诞生，这个新教派将“在神学和社会上保守”。五月份参加教会年度大会的大多数代表投票决定加强禁止任命LGBTQ牧师的禁令，并制定新规定，对主持同性婚礼的牧师进行“惩戒”。但反对这些措施的人有一个新的计划：他们说他们将在2020年之前成立一个独立的教派，将他们的教会称为基督教卫理公会。邮报指出，这个教派声称有1250万成员，在20世纪初是美国“最大的新教派”，但近年来一直在萎缩。这次分裂将是教会历史上的第二次分裂。第一次发生在1968年，当时大约10%的教派成员离开，成立了福音派联合兄弟教会。邮报指出，提议的分裂“发生在教会面临关键时刻，多年来一直在失去成员”，并且“被推向分裂的边缘，关于LGBTQ人在教会中的角色。”同性婚姻并不是唯一分裂教会的问题。2016年，教派在任命跨性别牧师问题上发生分裂，北美地区会议投票禁止他们担任牧师，而南太平洋地区会议投票允许他们。
- en: As hinted in an earlier subsection, the application of content generation has
    extended to use cases beyond writing news articles. With increased accessibility
    to sophisticated dialogue agents, creators are using LLMs to generate content
    in different genres, styles, and formats, including creating marketing campaigns,
    writing blog posts and emails, composing social media posts, and more. Several
    startups have also entered the realm of generative content creation, including
    Jasper AI, Anthropic AI, Cohere, Runway, Stability AI, and Adept AI. We’ll discuss
    using LLMs for generating content in detail, as well as highlight any potential
    risks, in an upcoming chapter.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一小节所暗示的，内容生成的应用已经扩展到写作新闻文章之外的使用场景。随着高级对话代理的可用性提高，创作者正在使用LLMs生成不同类型、风格和格式的内
    容，包括创建营销活动、撰写博客文章和电子邮件、编写社交媒体帖子等。几家初创公司也进入了生成内容创作的领域，包括Jasper AI、Anthropic AI、Cohere、Runway、Stability
    AI和Adept AI。我们将在下一章详细讨论使用LLMs生成内容，并突出任何潜在的风险。
- en: Logical reasoning
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑推理
- en: A novel and interesting application of LLMs is their ability to “reason”—the
    idea of drawing inferences or conclusions from new or existing information. A
    new, yet now common, reasoning task for LLMs is arithmetic. The tasks are often
    simple arithmetic queries, involving addition, subtraction, or multiplication
    with two to five numbers. While we can’t say that LLMs “understand” arithmetic
    because of their inconsistent performance with varying mathematical problems,
    GPT-3’s evaluation results demonstrate their ability to perform very simple arithmetic
    tasks. A notable model in the field of mathematics is Facebook AI Research’s transformer-based
    model trained to solve symbolic integration and differential equation problems.
    When presented with unseen expressions (that is, equations that weren’t a part
    of the training data), their model outperformed rule-based algebra-based systems,
    such as MATLAB and Mathematica [[20]](https://arxiv.org/pdf/1912.01412.pdf).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的一个新颖且有趣的应用是它们的“推理”能力——即从新信息或现有信息中得出推论或结论的想法。对于LLMs来说，一个新但现在已经常见的推理任务是算术。这些任务通常是简单的算术查询，涉及两个到五个数字的加法、减法或乘法。虽然我们不能说LLMs“理解”算术，因为它们在处理不同数学问题时表现不一致，但GPT-3的评估结果展示了它们执行非常简单的算术任务的能力。在数学领域的一个显著模型是Facebook
    AI Research基于transformer训练的模型，用于解决符号积分和微分方程问题。当面对未见过的表达式（即训练数据中不包含的方程）时，他们的模型优于基于规则的代数系统，如MATLAB和Mathematica
    [[20]](https://arxiv.org/pdf/1912.01412.pdf)。
- en: Another application worth discussing is common-sense or logical reasoning, where
    the model tries to capture physical or scientific reasoning. This is different
    from reading comprehension or answering general trivia questions as it requires
    some grounded understanding of the world. A significant model is Minerva by Google
    Research, a language model capable of solving mathematical and scientific questions
    using step-by-step reasoning [[21]](https://arxiv.org/pdf/2206.14858.pdf). GPT-4
    was tested on various academic and professional exams, including the Uniform Bar
    Examination (UBE), LSAT, SAT Reading and Writing, SAT Math, Graduate Record Examinations
    (GRE), AP Physics, AP Statistics, AP Calculus, and more. In most of these exams,
    the model achieved human-level performance and, notably, passed the UBE with a
    score in the top 10% of takers [[18]](https://arxiv.org/pdf/2303.08774.pdf).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 值得讨论的另一个应用是常识或逻辑推理，其中模型试图捕捉物理或科学推理。这与阅读理解或回答一般常识问题不同，因为它需要一些基于世界的实际理解。一个重要的模型是谷歌研究团队的Minerva，这是一个能够通过逐步推理解决数学和科学问题的语言模型
    [[21]](https://arxiv.org/pdf/2206.14858.pdf)。GPT-4在各种学术和专业考试中进行了测试，包括统一律师资格考试（UBE）、LSAT、SAT阅读和写作、SAT数学、研究生入学考试（GRE）、AP物理、AP统计学、AP微积分等。在这些考试中，模型大多数情况下达到了人类水平的表现，并且值得注意的是，它在UBE考试中取得了前10%的分数
    [[18]](https://arxiv.org/pdf/2303.08774.pdf)。
- en: More recently, the practice of law has also been increasingly embracing the
    applications of LLMs using tools for document review, due diligence, improving
    accessibility for legal services, and assisting with legal reasoning. In March
    2023, legal AI company Casetext unveiled CoCounsel, the first AI legal assistant
    built in collaboration with OpenAI on their most advanced LLM [[22]](https://www.prnewswire.com/news-releases/casetext-unveils-cocounsel-the-groundbreaking-ai-legal-assistant-powered-by-openai-technology-301759255.xhtml).
    CoCounsel can perform legal tasks such as legal research, document review, deposition
    preparation, contract analysis, and more. A similar tool, Harvey AI, assists with
    tasks such as contract analysis, due diligence, litigation, and regulatory compliance.
    Harvey AI partnered with one of the world’s largest law firms, Allen & Overy,
    and announced a strategic partnership with PricewaterhouseCoopers (PwC) [[23]](https://www.pwc.com/gx/en/news-room/press-releases/2023/pwc-announces-strategic-alliance-with-harvey-positioning-pwcs-legal-business-solutions-at-the-forefront-of-legal-generative-ai.xhtml).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，法律实践也越来越多地采用LLMs的应用，使用文档审查、尽职调查、提高法律服务可及性和协助法律推理的工具。2023年3月，法律AI公司Casetext推出了CoCounsel，这是第一个与OpenAI合作在他们的最先进的LLM上构建的AI法律助手
    [[22]](https://www.prnewswire.com/news-releases/casetext-unveils-cocounsel-the-groundbreaking-ai-legal-assistant-powered-by-openai-technology-301759255.xhtml)。CoCounsel可以执行法律任务，如法律研究、文档审查、出庭准备、合同分析等。类似的工具Harvey
    AI协助合同分析、尽职调查、诉讼和合规性等工作。Harvey AI与世界上最大的律师事务所之一Allen & Overy合作，并宣布与普华永道（PwC）建立了战略合作伙伴关系
    [[23]](https://www.pwc.com/gx/en/news-room/press-releases/2023/pwc-announces-strategic-alliance-with-harvey-positioning-pwcs-legal-business-solutions-at-the-forefront-of-legal-generative-ai.xhtml)。
- en: Other natural language tasks
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他自然语言任务
- en: Naturally, LLMs are also well-suited for many other linguistic tasks. A popular
    and long-standing application is machine translation, which uses LLMs to automate
    translation between languages. As discussed earlier, machine translation was one
    of the first problems that computers were tasked with solving 70 years ago. Beginning
    in the 1950s, computers used a series of programmed language rules to solve this
    problem, which was not only computationally expensive and time-consuming but also
    required a set of computer instructions with the full vocabulary for each language
    and multiple types of grammar. By the 1990s, the American multinational technology
    corporation International Business Machines, more commonly known as IBM, introduced
    statistical machine translation where researchers theorized that if they looked
    at enough text, they could find patterns in translations. This was a massive breakthrough
    in the field and led to the launch of Google Translate in 2006 using statistical
    machine translation. Google Translate was the first commercially successful NLP
    application, and perhaps the most famous. In 2015, the field of machine translation
    changed forever when Google started using LLMs to deliver far more impressive
    results. In 2020, Facebook announced the first multilingual machine translation
    model that can translate between any 100 pairs of languages without relying on
    any English data—another major milestone in the field of machine translation as
    it gives less opportunity for meaning to get lost in translation [[24]](https://about.fb.com/news/2020/10/first-multilingual-machine-translation-model/).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，大型语言模型（LLMs）也非常适合许多其他语言任务。一个流行且长期的应用是机器翻译，它使用LLMs来自动化不同语言之间的翻译。正如之前所讨论的，机器翻译是70年前计算机被要求解决的第一批问题之一。从20世纪50年代开始，计算机使用一系列编程语言规则来解决这个问题，这不仅计算成本高昂且耗时，还需要为每种语言提供完整的词汇表和多种类型的语法。到20世纪90年代，美国跨国科技公司国际商业机器公司（IBM），更广为人知的是IBM，引入了统计机器翻译，研究人员认为，如果他们查看足够多的文本，他们就能在翻译中找到模式。这在该领域是一个巨大的突破，并导致了2006年使用统计机器翻译的Google
    Translate的推出。Google Translate是第一个商业上成功的NLP应用，也许是最著名的。2015年，当谷歌开始使用LLMs来提供更令人印象深刻的结果时，机器翻译领域发生了永久性的变化。2020年，Facebook宣布了第一个多语言机器翻译模型，该模型可以在不依赖任何英语数据的情况下翻译任何100对语言——这是机器翻译领域的一个重大里程碑，因为它减少了在翻译中丢失意义的机会
    [[24]](https://about.fb.com/news/2020/10/first-multilingual-machine-translation-model/)。
- en: 'Another practical application is text summarization, that is, to create a shorter
    version of text that highlights the most relevant information. There are two types
    of summarization techniques: extractive summarization and abstractive summarization.
    *Extractive* summarization is concerned with extracting the most important sentences
    from long-form text, which are joined together to form a summary. On the other
    hand, *abstractive* summarization paraphrases text to form a summary (i.e. an
    abstract) and may include words or sentences that aren’t present in the original
    text.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个实际应用是文本摘要，即创建一个简短的文本版本，突出最相关的信息。有两种摘要技术：提取式摘要和抽象式摘要。*提取式*摘要关注从长文本中提取最重要的句子，并将它们组合起来形成摘要。另一方面，*抽象式*摘要通过改写文本来形成摘要（即摘要），可能包括原文中不存在的单词或句子。
- en: There are additional miscellaneous applications, which include correcting English
    grammar, learning and using novel words, and solving linguistic puzzles. An example
    from GPT-3 for learning and using novel words is giving the model a definition
    of a nonexistent word, like “Gigamuru,” and then asking the model to use it in
    a sentence [[19]](https://arxiv.org/pdf/2005.14165.pdf). Companies such as Grammarly
    and Duolingo are quickly adopting LLMs in their products. Grammarly, a popular
    writing grammar and spelling checker, introduced GrammarlyGO in March 2023, a
    new tool that uses ChatGPT to generate text (see [http://mng.bz/D9oa](http://mng.bz/D9oa)).
    Also in March 2023, Duolingo introduced Duolingo Max, which uses GPT-4 to add
    features such as “explain my answer” and “roleplay” in their learning platform
    (see [http://mng.bz/lVvB](http://mng.bz/lVvB)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他一些辅助应用，包括纠正英语语法、学习和使用新词汇以及解决语言谜题。GPT-3的一个学习和使用新词汇的例子是给模型一个不存在词汇“Gigamuru”的定义，然后要求模型在句子中使用它
    [[19]](https://arxiv.org/pdf/2005.14165.pdf)。像Grammarly和Duolingo这样的公司正在迅速将LLMs应用于他们的产品中。Grammarly，一个流行的写作语法和拼写检查器，于2023年3月推出了GrammarlyGO，这是一个使用ChatGPT生成文本的新工具（见[http://mng.bz/D9oa](http://mng.bz/D9oa)）。同样在2023年3月，Duolingo推出了Duolingo
    Max，该平台使用GPT-4添加了“解释我的答案”和“角色扮演”等功能（见[http://mng.bz/lVvB](http://mng.bz/lVvB)）。
- en: Where do LLMs fall short?
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMs 缺乏哪些方面？
- en: Although LLMs have achieved unprecedented success in an assortment of tasks,
    the same strategies that brought LLMs to their present pinnacle also represent
    significant risks and limitations. There are risks introduced by the training
    data that LLMs use—specifically, that the data inevitably contains many patterns
    that LLM developers don’t want the model to reproduce—and risks due to the unpredictability
    of LLMs’ output. Finally, the current frenzy to create and use LLMs in everyday
    applications warrants closer examination due to the externality of their energy
    use.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs在各种任务中取得了前所未有的成功，但将LLMs带到目前顶峰的同一种策略也代表了重大的风险和限制。LLMs使用的训练数据引入了风险——具体来说，数据不可避免地包含许多LLM开发者不希望模型复制的模式——以及LLMs输出不可预测性的风险。最后，由于LLMs能源使用的外部性，当前在日常生活中创建和使用LLMs的热潮值得更深入的研究。
- en: Training data and bias
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练数据和偏差
- en: LLMs are trained on almost unfathomably large amounts of text data. To produce
    a model that reliably generates natural-looking language, therefore, it’s imperative
    to collect vast quantities of, ideally, human-written natural language. Luckily,
    such quantities of text content exist and are readily available for ingestion
    over the internet. Of course, quantity is only one part of the equation; quality
    is a much tougher nut to crack.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs是在几乎无法理解的巨大文本数据量上训练的。因此，为了生产一个能够可靠地生成自然语言样式的模型，收集大量、理想情况下由人类编写的自然语言是至关重要的。幸运的是，这样的文本内容存在，并且可以通过互联网轻松获取。当然，数量只是方程的一部分；质量是更难解决的问题。
- en: 'The companies and research labs that train LLMs compile training datasets that
    contain hundreds of billions of words from the internet. Some of the most common
    text corpora (i.e., a collection of texts) for training LLMs include Wikipedia,
    Reddit, and Google News/Google Books. Wikipedia is probably the best-known data
    source for LLMs and has many advantages: it’s written and edited by humans, it’s
    generally a trustworthy source of information due to its active community of fact-checkers,
    and it exists in hundreds of languages. Google Books, as another example, is a
    collection of digital copies of the text of thousands of published books that
    have entered the public domain. Although some such books might contain factual
    errors or outdated information, they are generally considered high-quality text
    examples, if more formal than most conversational natural language.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLM的公司和研究实验室编制的训练数据集包含来自互联网的数百亿个单词。用于训练LLM的最常见的文本语料库（即文本集合）包括维基百科、Reddit和谷歌新闻/谷歌图书。维基百科可能是LLM最知名的数据来源，并且具有许多优点：它是人类编写和编辑的，由于其活跃的事实核查社区，它通常是一个可靠的信息来源，并且存在于数百种语言中。以谷歌图书为例，它是一系列已进入公共领域的出版书籍的数字文本副本集合。尽管这类书籍可能包含事实错误或过时信息，但它们通常被认为是高质量的文本示例，如果比大多数对话性自然语言更正式的话。
- en: 'On the other hand, consider the inclusion of a dataset that includes all or
    most of the social media site Reddit. The benefits are substantial: it includes
    millions of conversations between people, demonstrating the dynamics of dialogue.
    Like other sources, the Reddit content improves the model’s internal representation
    of different tokens. The more observations of a word or phrase in the training
    dataset, the better the model will be able to learn when to generate that word
    or phrase. However, some parts of Reddit also contain a lot of objectionable speech,
    including racial slurs or derogatory jokes, dangerous conspiracies or misinformation,
    extremist ideologies, and obscenities. Through the inclusion of this type of content,
    which is almost inevitable when collecting so much data from the web, the model
    may become vulnerable to generating this type of speech itself. There are also
    serious implications for the use of some of this data, which might represent personal
    information or copyrighted material with legal protections.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，考虑包含Reddit网站所有或大部分数据集的情况。其好处是显著的：它包含了数百万条人与人之间的对话，展示了对话的动态。与其他来源一样，Reddit的内容改善了模型对不同标记的内部表示。训练数据集中一个词或短语观察到的次数越多，模型在何时生成该词或短语时就越能学得越好。然而，Reddit的一些部分也包含大量令人反感的言论，包括种族侮辱或贬损笑话、危险的阴谋论或错误信息、极端主义意识形态和淫秽内容。通过包含这种类型的内容，这在从网络上收集如此多的数据时几乎是不可避免的，模型可能会变得容易生成这种类型的言论。此外，使用其中一些数据也存在严重的影响，这些数据可能代表个人信息或受法律保护的有版权材料。
- en: 'In addition, more subtle effects of bias may be introduced to an LLM through
    its training data. The term *bias* is extremely overloaded in machine learning:
    sometimes, people refer to statistical bias, which refers to the average amount
    that their model’s prediction differs from the true value; a training dataset
    may be biased if it’s drawn from a different distribution than a test dataset,
    which often happens entirely by accident. To avoid confusion, we’ll use bias strictly
    to refer to disparate outputs from a model across attributes of personal identity
    such as race, gender, class, age, or religion. Bias has been a longstanding problem
    in machine learning algorithms, and it can creep into a machine learning system
    in several ways. However, it’s important to keep in mind that at heart, these
    models are reflecting patterns in the text they are trained on. If biases exist
    in our books, news media, and social media, they will be repeated in our language
    models.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过其训练数据，偏差的更微妙影响可能被引入到大型语言模型（LLM）中。在机器学习中，“偏差”这个术语极其多义：有时人们指的是统计偏差，它指的是模型预测值与真实值之间的平均差异；如果训练数据集是从与测试数据集不同的分布中抽取的，那么它可能是有偏差的，这种情况通常完全是由偶然发生的。为了避免混淆，我们将严格使用“偏差”一词来指代模型在个人身份属性（如种族、性别、阶级、年龄或宗教）方面的不同输出。偏差一直是机器学习算法中的长期问题，它可以通过多种方式渗透到机器学习系统中。然而，重要的是要记住，这些模型本质上是在反映它们所训练的文本中的模式。如果我们的书籍、新闻媒体和社交媒体中存在偏差，那么这些偏差将会在我们的语言模型中得到重复。
- en: Bias refers to disparate outputs from a model across attributes of personal
    identity, such as race, gender, class, age, or religion.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差指的是模型在个人身份属性（如种族、性别、阶级、年龄或宗教）方面的不同输出。
- en: Some of the earliest general-purpose language models that trained on large,
    unlabeled datasets were built for word embeddings. Today, each LLM effectively
    learns its own embeddings for words—this is what we’ve referred to as the model’s
    internal representation of that word. But before LLMs, everyone who developed
    NLP models needed to implement some kind of encoding step to represent their text
    inputs numerically, so that the algorithm could interpret them. Word embeddings
    allow for the conversion of text into meaningful representations of the words
    as numerical points in a high-dimensional space. With word embeddings, words that
    are used in similar ways, such as *cucumber* and *pickle,* will be close together,
    whereas words that aren’t, say, *cucumber* and *philosophy,* will be far apart
    (shown in figure 1.4). There are simpler ways of doing this encoding—the most
    basic is to assign a random point in space to every unique word that appears in
    the training data—but word
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最早在大型未标记数据集上训练的通用语言模型是为了构建词嵌入。如今，每个大型语言模型都有效地学习了自己对单词的嵌入——这就是我们所说的模型对该单词的内部表示。但在LLM出现之前，每个开发NLP模型的人都需要实现某种编码步骤来将他们的文本输入数值化，以便算法可以解释它们。词嵌入允许将文本转换为在多维空间中作为数值点的有意义的单词表示。有了词嵌入，使用方式相似的单词，如*cucumber*和*pickle*，会彼此靠近，而那些不相似的，比如*cucumber*和*philosophy*，则会相隔甚远（如图1.4所示）。有更简单的方法来做这种编码——最基本的方法是为训练数据中出现的每个唯一单词分配空间中的一个随机点——但词嵌入
- en: '![](../../OEBPS/Images/CH01_F04_Dhamani.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH01_F04_Dhamani.png)'
- en: Figure 1.4 Representation of word embeddings in the vector space
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 词嵌入在向量空间中的表示
- en: embeddings capture much more information about the semantic meanings of the
    words and lead to better models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入捕捉了关于单词语义意义的更多信息，并导致更好的模型。
- en: 'In a well-known paper about word embeddings trained on the Google News corpus,
    “Man Is to Computer Programmer as Woman Is to Homemaker? Debiasing Word Embeddings,”
    academics from Boston University (in collaboration with Microsoft Research) demonstrated
    that the word-embedding model itself exhibited strong gender stereotypes for both
    occupations and descriptions [[25]](http://arxiv.org/abs/1607.06520). The authors
    devised an evaluation where the model would generate she-he analogies based on
    the embeddings. Some of them were innocuous: sister is to brother, for instance,
    and queen is to king. But the model also produced she-he analogies such as nurse
    is to physician or surgeon, cosmetics is to pharmaceuticals, and interior designer
    is to architect. The primary cause of these biases is attributable simply to the
    number of times architects in the news articles that compose the dataset are men
    versus women, the number of times nurses are women, and so on. Thus, the inequities
    that exist in society are mirrored, and amplified, by the model.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在一篇关于在Google新闻语料库上训练的词嵌入的著名论文“Man Is to Computer Programmer as Woman Is to Homemaker?
    Debiasing Word Embeddings”中，波士顿大学的学者（与微软研究院合作）展示了词嵌入模型本身对职业和描述都表现出强烈的性别刻板印象 [[25]](http://arxiv.org/abs/1607.06520)。作者设计了一个评估，模型将根据嵌入生成她-他的类比。其中一些是无害的：例如，sister
    is to brother，以及queen is to king。但模型也产生了她-他的类比，如nurse is to physician或surgeon，cosmetics
    is to pharmaceuticals，以及interior designer is to architect。这些偏差的主要原因可以归因于构成数据集的新闻文章中建筑师是男性还是女性的次数，护士是女性的次数，等等。因此，社会存在的不足被模型所反映和放大。
- en: 'Like word embeddings, LLMs are susceptible to these biases. In a 2021 paper
    titled, “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”
    the authors examine how LLMs echo and amplify biases found in their training data
    [[26]](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922). While there are techniques
    to debias the models or to attempt to train the model in more bias-conscious ways,
    it’s exceedingly difficult to excise associations with gender, race, sexuality,
    and other characteristics that are deeply ingrained in everyday life, or disparities
    in data that have existed for centuries. As a result, LLMs may produce dramatically
    different generations when identity characteristics are present in the context
    or prompt.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与词嵌入类似，大型语言模型（LLMs）也容易受到这些偏差的影响。在2021年一篇题为“论随机鹦鹉的危险：语言模型是否可以过大？”的论文中，作者们探讨了LLMs如何回声和放大其训练数据中发现的偏差[[26](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)]。尽管有技术可以消除模型的偏差或尝试以更具偏见意识的方式训练模型，但消除与性别、种族、性取向和其他深深植根于日常生活特点的关联，或消除存在数百年的数据差异，是非常困难的。因此，当身份特征出现在上下文或提示中时，LLMs可能会产生截然不同的生成内容。
- en: Limitations in controlling machine outputs
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制机器输出的局限性
- en: 'After the release of OpenAI’s ChatGPT and a ChatGPT-powered search engine in
    collaboration with Microsoft Bing, Google also released its own chatbot, Bard.
    At the live launch event, a promotional video was played showing questions asked
    to Bard and Bard’s response. One such question was, “What new discoveries from
    the James Webb Space Telescope (JWST) can I tell my nine-year-old about?” In the
    video, Bard responds with some information about JWST, including that JWST took
    the first-ever photographs of exoplanets, or planets outside the Earth’s solar
    system. There was just one (big) problem: the first exoplanets had been photographed
    more than a decade earlier, by multiple older telescopes. Embarrassingly, astronomers
    and astrophysicists began pointing this out on Twitter and other channels; Google
    removed the advertisement, and the YouTube video of the event was taken down immediately
    after the stream ended. But the damage was done, and in the days following the
    launch, Google’s stock dropped about 9% for a total loss in market capitalization
    of about $100 billion [[27]](https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在OpenAI的ChatGPT和与微软必应（Microsoft Bing）合作推出的ChatGPT驱动的搜索引擎发布后，谷歌也发布了它自己的聊天机器人，Bard。在直播发布活动上，播放了一段宣传视频，展示了向Bard提出的问题和Bard的回答。其中一个问题是，“我能向我的九岁孩子讲述詹姆斯·韦伯太空望远镜（JWST）的新发现是什么？”在视频中，Bard回答了一些关于JWST的信息，包括JWST拍摄了第一张系外行星的照片，即地球太阳系外的行星。但有一个（大）问题：第一张系外行星的照片早在十多年前就已经被多台较老的望远镜拍摄了。尴尬的是，天文学家和天体物理学家开始在推特和其他渠道上指出这一点；谷歌在直播结束后立即撤下了广告，并删除了活动的YouTube视频。但损害已经造成，在发布后的几天里，谷歌的股价下跌了约9%，市值损失约1000亿美元[[27](https://www.reuters.com/technology/google-ai-chatbot-bard-offers-inaccurate-information-company-ad-2023-02-08/)]。
- en: This type of error is very difficult for LLMs to avoid, given that they don’t
    learn and understand content the way that humans do, but rather generate text
    by predicting and approximating common sentence structures. The fluency with which
    LLMs generate text belies the fact that they don’t know what they’re talking about,
    and may assert false information, or make up highly plausible but incorrect explanations.
    These mistakes are called “hallucinations.” Chatbots may hallucinate on their
    own or be vulnerable to adversarial user inputs, where they seem to be convinced
    of something untrue by their conversation partner.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLMs不像人类那样学习和理解内容，而是通过预测和近似常见句子结构来生成文本，因此这种错误对于LLMs来说很难避免。LLMs生成文本的流畅性掩盖了它们不知道自己在说什么的事实，可能会断言错误信息，或编造高度可信但错误的解释。这些错误被称为“幻觉”。聊天机器人可能会自己产生幻觉，或者容易受到对抗性用户输入的影响，似乎被对话伙伴说服了某些不真实的事情。
- en: Occasionally LLMs assert false information or make up highly plausible but incorrect
    explanations. These are called Hallucinations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有时LLMs会断言错误信息或编造高度可信但错误的解释。这些被称为幻觉。
- en: The generation of hallucinations is widely recognized as one of the biggest
    problems with LLMs currently. Hallucinations can be caused by problems with the
    training set (if someone on the internet incorrectly wrote that JWST took the
    first pictures of exoplanets, for example), but they can also occur in contexts
    that don’t exist in any of the model’s previously known sequences, possibly due
    to problems in the way the model has constructed its knowledge. Yann LeCun, a
    giant in the field of machine learning and the Chief AI Scientist at Meta, has
    argued that the output of these LLMs can’t be made factual within any probability
    bound because as the responses generated by the model get longer, the possible
    responses multiply and become nearly infinite, with only some small portion of
    those possible outputs being meaningfully correct [[28]](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view?usp=embed_facebook).
    Of course, the usefulness of LLMs depends greatly on whether this quality of factuality
    can be improved. We’ll discuss the approaches that LLM developers are using to
    try to reduce hallucinations and other undesirable outputs later in this book.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉的产生被广泛认为是LLMs目前面临的最大问题之一。幻觉可能是由训练集的问题引起的（例如，如果有人在互联网上错误地写道JWST拍摄了系外行星的第一张照片），但它们也可能出现在模型之前已知序列中不存在的任何上下文中，这可能是由于模型构建知识的方式存在问题。Meta的首席人工智能科学家、机器学习领域的巨头Yann
    LeCun认为，这些LLMs的输出在任何概率界限内都无法变得真实，因为随着模型生成的响应变长，可能的响应会成倍增加，几乎无限，其中只有一小部分可能的输出是有意义的正确答案
    [[28]](https://drive.google.com/file/d/1BU5bV3X5w65DwSMapKcsr0ZvrMRU_Nbi/view?usp=embed_facebook)。当然，LLMs的有用性在很大程度上取决于这种真实性的质量是否可以改进。我们将在本书的后面讨论LLMs开发者正在使用的尝试减少幻觉和其他不希望输出的方法。
- en: Sustainability of LLMs
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLMs的可持续性
- en: As indicated in their name and emphasized already, LLMs are big. They use massive
    datasets, have hundreds of billions or trillions of parameters, and require huge
    amounts of computing resources, measured in the number of chips used and time
    spent. LLMs are typically trained on graphical processing units (GPUs) or tensor
    processing units (TPUs), specialized chips for handling the large-scale computations
    involved in training neural networks. The process might involve renting thousands
    of GPUs from a cloud computing provider—such as Microsoft Azure (OpenAI’s partner),
    Google Cloud Platform, or Amazon Web Services—for several weeks. Although OpenAI
    hasn’t released such figures, it’s estimated that the cost of these computational
    resources alone would bring the cost of a model like GPT-3 to about $4.6 million
    [[29]](https://lambdalabs.com/blog/demystifying-gpt-3).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如他们的名字所暗示的，并且已经强调过的，LLMs非常大。它们使用大量数据集，拥有数百亿或数千亿个参数，并且需要巨大的计算资源，这以使用的芯片数量和时间消耗来衡量。LLMs通常在图形处理单元（GPUs）或张量处理单元（TPUs）上训练，这些是专门用于处理训练神经网络所涉及的大规模计算的芯片。这个过程可能涉及从云计算提供商——如微软Azure（OpenAI的合作伙伴）、谷歌云平台或亚马逊网络服务——租赁数千个GPU，为期数周。尽管OpenAI尚未公布这些数据，但据估计，仅这些计算资源的成本就将GPT-3这样的模型成本推高至约460万美元
    [[29]](https://lambdalabs.com/blog/demystifying-gpt-3)。
- en: A more hidden cost of training LLMs is their effect on the environment, which
    has been the subject of study and critique. One paper that attempted to assess
    the energy usage and carbon footprints of LLMs based on the information that has
    been released about their training procedures estimated that GPT-3 emitted 500
    metric tons of carbon dioxide from the electricity consumed during training [[30]](https://arxiv.org/pdf/2211.02001.pdf).
    To put that in perspective, the average American is responsible for about 18 metric
    tons of carbon dioxide emissions per year; the global average is just 7.4 tons
    per year (see [https://worldemissions.io/](https://worldemissions.io/)). Another
    paper found that models consume even more energy during inference [[31]](https://arxiv.org/pdf/2104.10350.pdf).
    The precise emissions for most LLMs are unknown, given that there are a lot of
    factors involved, including the data center used, the numbers and types of chips,
    and model size and architecture.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 训练LLMs的一个更隐蔽的成本是它们对环境的影响，这已经成为研究和批评的主题。有一篇论文试图根据已发布的关于其训练过程的信息来评估LLMs的能量使用和碳足迹，估计GPT-3在训练过程中消耗的电力排放了500吨二氧化碳
    [[30]](https://arxiv.org/pdf/2211.02001.pdf)。为了更直观地理解这一点，平均美国人每年大约产生18吨二氧化碳排放；全球平均水平仅为每年7.4吨（参见[https://worldemissions.io/](https://worldemissions.io/)）。另一篇论文发现，模型在推理过程中消耗的能量甚至更多
    [[31]](https://arxiv.org/pdf/2104.10350.pdf)。由于涉及许多因素，包括数据中心的使用、芯片的数量和类型以及模型的大小和架构，大多数LLMs的确切排放量是未知的。
- en: Inference is the process of using a trained language model to generate predictions
    or responses.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 推理是使用训练好的语言模型生成预测或响应的过程。
- en: It also isn’t easy for just anyone to get that many GPUs, even if they do have
    millions of dollars to spend. The largest companies in the technology sector,
    including Microsoft and Google, are at a distinct advantage in the development
    of LLMs because of the resources required to compete. Some observers fear that
    the situation will become untenable for small players, leaving the creation of
    and profits from LLM technology to only these multinational companies or countries,
    some of which have begun pooling resources at the national level for training
    LLMs. On the other hand, there is also much ongoing research in making these models
    more accessible and reducing training time or costs, sometimes by creating open
    source versions of existing LLMs or attempting to shrink an already-trained LLM
    into a smaller version that could maintain much of the same performance, but cost
    substantially less to use. The success of these efforts is promising, but unproven.
    In late 2022 and early 2023, the most significant models came from OpenAI, Google,
    Microsoft, and Meta.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有些人有数百万美元的预算，也并不容易获得那么多的GPU。在技术领域最大的公司，包括微软和谷歌，在LLMs的开发上具有明显的优势，因为竞争所需的资源。一些观察者担心，这种情况将变得难以承受，只有这些跨国公司或国家才能从LLM技术的创造和利润中获益，其中一些国家已经开始在国家层面汇集资源来训练LLMs。另一方面，也有许多正在进行的研究，旨在使这些模型更容易获得，并减少训练时间或成本，有时是通过创建现有LLMs的开源版本，或者尝试将已经训练好的LLMs缩小到一个更小的版本，这个版本可以保持大部分相同的性能，但使用成本大幅降低。这些努力的成果是有希望的，但尚未得到证实。在2022年底和2023年初，最显著的模式来自OpenAI、谷歌、微软和Meta。
- en: 'Revolutionizing dialogue: Conversational LLMs'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 革命性的对话：对话型LLMs
- en: 'In this chapter, we discussed how LLMs work at a high level, including their
    applications and limitations. The promise of LLMs is in their ability to fluidly
    generate text for a wide range of use cases, which makes them ideal for conversing
    with humans to perform tasks. Chatbots, such as ChatGPT, are LLMs that have been
    designed for conversational use. In this section, we’ll do a deeper dive into
    the journeys of notable conversational models that were released in late 2022
    and early 2023: OpenAI’s ChatGPT, Google’s Bard, Microsoft’s Bing AI, and Meta’s
    LLaMa.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了大型语言模型（LLMs）在高级别上的工作原理，包括其应用和局限性。LLMs的承诺在于它们能够流畅地为广泛的用例生成文本，这使得它们非常适合与人类进行对话以执行任务。例如，ChatGPT这样的聊天机器人就是为对话使用而设计的LLMs。在本节中，我们将深入探讨2022年底和2023年初发布的知名对话模型的旅程：OpenAI的ChatGPT、谷歌的Bard、微软的Bing
    AI和Meta的LLaMa。
- en: OpenAI’s ChatGPT
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI的ChatGPT
- en: OpenAI, the San Francisco–based AI research and development company, released
    ChatGPT on November 30, 2022, just 10 short months after introducing its sibling
    model, InstructGPT [[32]](https://arxiv.org/pdf/2203.02155.pdf). The latter was
    the company’s initial attempt at overhauling LLMs to carry out natural language
    tasks that are aligned for the user through specific text prompts. Using a previously
    established technique, reinforcement learning from human feedback (RLHF), OpenAI
    trained the model to follow instructions based on feedback from humans. Given
    the prompts submitted through the OpenAI Playground, human labelers would put
    together the desired model responses, which were then used to fine-tune the model.
    This made InstructGPT better adapted to human intention, that is, more *aligned*
    to human preference. This was the first time OpenAI used its alignment research
    in a product, and the organization announced that it would continue pushing in
    this direction. OpenAI also asserted that fine-tuning language models with humans
    in the loop can be an effective tool to make the models safer and more reliable
    [[33]](https://openai.com/research/instruction-following).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 位于旧金山的AI研究和开发公司OpenAI于2022年11月30日发布了ChatGPT，距离其兄弟模型InstructGPT的推出仅10个月 [[32]](https://arxiv.org/pdf/2203.02155.pdf)。后者是公司首次尝试彻底改造LLM以执行通过特定文本提示与用户对齐的自然语言任务。使用之前建立的技术，即基于人类反馈的强化学习（RLHF），OpenAI训练模型根据人类反馈的指令。通过OpenAI游乐场提交的提示，人类标注员会整理出期望的模型响应，然后用于微调模型。这使得InstructGPT更好地适应人类意图，即更*符合*人类偏好。这是OpenAI首次在其产品中使用其对齐研究，该组织宣布将继续在这一方向上推进。OpenAI还断言，在人类参与的情况下微调语言模型可以是一个有效的工具，用于使模型更安全、更可靠
    [[33]](https://openai.com/research/instruction-following)。
- en: Not too long after, OpenAI introduced the Chat Generative Pre-trained Transformer,
    more fondly (and famously) known as ChatGPT (see [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)),
    which was fine-tuned on a model from the GPT-3.5 series encompassing 175 billion
    parameters. That is, it was trained on 570 gigabytes of text, which is 100 times
    bigger than its predecessor, GPT-2 [[34]](https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai).
    To put that in perspective, that is 164,129 times the number of words in the entire
    *Lord of the Rings* series, including *The Hobbit* [[35]](https://www.tooltester.com/en/blog/chatgpt-statistics/).
    OpenAI also stated its limitations, which included limiting knowledge up to early
    2022 when the model finished training, writing superficially plausible but incorrect
    answers, and responding with harmful or biased information, among others.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，OpenAI推出了名为Chat Generative Pre-trained Transformer的模型，更广为人知的是ChatGPT（见[https://openai.com/blog/ChatGPT](https://openai.com/blog/ChatGPT)），该模型在GPT-3.5系列模型的基础上进行了微调，包含1750亿个参数。也就是说，它是在570GB的文本上训练的，是其前辈GPT-2的100倍
    [[34]](https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai)。为了更直观地说明这一点，这相当于包括《霍比特人》在内的整个《指环王》系列中单词数量的164,129倍
    [[35]](https://www.tooltester.com/en/blog/ChatGPT-statistics/)。OpenAI还声明了其局限性，包括限制知识更新至2022年初模型完成训练时，撰写看似合理但实际上错误的答案，以及回应带有有害或偏见的信息等。
- en: OpenAI has previously released its development and deployment lifecycle, claiming
    that “there is no silver bullet for responsible deployment” where ChatGPT is the
    latest step in their iterative deployment of safe and reliable AI systems [[36]](https://openai.com/research/language-model-safety-and-misuse).
    For them, the journey has only just begun. On March 14, 2023, Open AI released
    GPT-4, a large multimodal model that accepts text and image inputs, as well as
    emits text outputs.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI之前发布了其开发和部署生命周期，声称“没有银弹可以用于负责任的部署”，其中ChatGPT是他们迭代部署安全可靠AI系统的最新一步 [[36]](https://openai.com/research/language-model-safety-and-misuse)。对他们来说，旅程才刚刚开始。2023年3月14日，OpenAI发布了GPT-4，这是一个大型多模态模型，可以接受文本和图像输入，并生成文本输出。
- en: OpenAI’s decision to release ChatGPT has been criticized by many who argued
    that it’s reckless to release a system that not only presents significant risks
    to humanity and society but also sets off an AI race where companies are choosing
    speed over caution. However, Sam Altman, OpenAI’s cofounder, argued that it’s
    safer to gradually release technology to the world, so everyone can better understand
    associated risks and how to navigate them as opposed to developing behind closed
    doors [[37]](https://www.nytimes.com/2023/03/31/technology/sam-altman-open-ai-chatgpt.xhtml).
    Yet, in just five days after its launch, ChatGPT gained 1 million users. It set
    the record for the fastest-growing user base in history by reaching 100 million
    active users in January 2023 based on data from SimlarWeb, a web analytics company
    [[38]](https://www.reuters.com/technology/chatgpt-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/).
    The AI chatbot had arrived, and it was primed to disrupt society.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI决定发布ChatGPT的决定受到了许多人的批评，他们认为发布一个不仅对人类和社会构成重大风险，而且引发了一场公司选择速度胜过谨慎的人工智能竞赛的系统是鲁莽的。然而，OpenAI的共同创始人山姆·奥特曼（Sam
    Altman）认为，将技术逐渐发布给世界更安全，这样每个人都可以更好地理解相关的风险以及如何应对这些风险，而不是在幕后开发 [[37]](https://www.nytimes.com/2023/03/31/technology/sam-altman-open-ai-ChatGPT.xhtml)。然而，在发布后的五天内，ChatGPT就获得了100万用户。根据网络分析公司SimlarWeb的数据，ChatGPT在2023年1月达到了1000万活跃用户，创下了历史上增长最快的用户基础记录
    [[38]](https://www.reuters.com/technology/ChatGPT-sets-record-fastest-growing-user-base-analyst-note-2023-02-01/)。人工智能聊天机器人已经到来，并准备好颠覆社会。
- en: Google’s Bard/LaMDA
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谷歌的Bard/LaMDA
- en: On January 28, 2020, Google unveiled Meena, a 2.6-billion-parameter conversational
    agent based on the transformer architecture [[39]](https://arxiv.org/pdf/2001.09977.pdf).
    Google claimed that transformer-based models trained in dialogue could talk about
    nearly anything, including making up (bad) jokes. Unable to determine how to release
    the chatbot responsibly, Meena was never released to the public on the grounds
    of violating safety principles.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年1月28日，谷歌推出了Meena，这是一个基于transformer架构的26亿参数对话代理 [[39]](https://arxiv.org/pdf/2001.09977.pdf)。谷歌声称，基于transformer架构训练的对话模型可以谈论几乎所有事情，包括讲（不好的）笑话。由于无法确定如何负责任地发布聊天机器人，Meena从未公开发布，理由是违反了安全原则。
- en: Not too long after, the tech giant introduced LaMDA—short for Language Model
    for Dialogue Applications—as their breakthrough conversation technology during
    the 2021 Google I/O keynote. Built on Meena, LaMDA consisted of 137 billion model
    parameters and introduced newly designed metrics around quality, safety, and groundedness
    to measure model performance [[40]](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml).
    The following year, Google announced its second release of LaMDA at its annual
    developer conference in 2022\. Shortly after, Blake Lemoine, an engineer who worked
    for Google’s Responsible AI organization, shared a document in which he urged
    Google to consider that LaMDA might be sentient. The document contained a transcript
    of his conversations with the AI, which he published online after being placed
    on administrative leave and then ultimately let go from the company [[41]](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917).
    Google strongly denied any claims of sentience and the controversy faded in the
    coming months [[42]](https://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine).
    Later that year, Google launched the AI Test Kitchen where users could register
    their interest and provide feedback on LaMDA (see [http://mng.bz/BA0r](http://mng.bz/BA0r)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 不久之后，科技巨头在2021年Google I/O大会上推出了LaMDA——即对话应用语言模型——作为他们的突破性对话技术。LaMDA建立在Meena的基础上，包含1370亿个模型参数，并引入了关于质量、安全和扎根度的新设计指标来衡量模型性能
    [[40]](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.xhtml)。第二年，谷歌在其2022年的年度开发者大会上宣布了LaMDA的第二版。不久之后，谷歌负责AI组织的工程师布莱克·莱莫因（Blake
    Lemoine）分享了一份文件，他在其中敦促谷歌考虑LaMDA可能具有意识。该文件包含了他与AI的对话记录，他在被停职后将其发布在网上，最终被公司解雇 [[41]](https://cajundiscordian.medium.com/is-lamda-sentient-an-interview-ea64d916d917)。谷歌强烈否认了任何关于意识的指控，争议在接下来的几个月中逐渐平息
    [[42]](https://www.theguardian.com/technology/2022/jun/12/google-engineer-ai-bot-sentient-blake-lemoine)。同年稍后，谷歌推出了AI测试厨房，用户可以注册兴趣并提供对LaMDA的反馈（见[http://mng.bz/BA0r](http://mng.bz/BA0r))。
- en: In a statement from their CEO, Sundar Pichai, Google introduced Bard on February
    6, 2023, a conversational AI agent, powered by LaMDA [[43]](https://blog.google/technology/ai/bard-google-ai-search-updates/).
    In a preemptive AI arms race, the announcement came a day before Microsoft unveiled
    their conversational AI-powered search engine, the “new Bing.” Responding to the
    ChatGPT release, “Google declares a ‘code red’” was splashed in headlines across
    mainstream newspapers as Google raced to ship their conversational AI, making
    it the company’s central priority [[44]](https://www.nytimes.com/2022/12/21/technology/ai-chatgpt-google-search.xhtml).
    After watching various competitors spin up chatbots built on transformer-based
    models, an architecture developed at Google, the tech giant finally rolled out
    Bard in March 2023 for early testers (see [https://bard.google.com/](https://bard.google.com/)).
    In efforts to complement Google Search and responsibly roll out the technology,
    Bard was a standalone web page displaying a question box instead of being combined
    with the search engine itself. Like OpenAI, Google asserts that the chatbot is
    capable of generating misinformation, as well as biased or offensive information
    that doesn’t align with the company’s views.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们首席执行官桑达尔·皮查伊的一份声明中，谷歌于2023年2月6日推出了Bard，一个由LaMDA [[43]](https://blog.google/technology/ai/bard-google-ai-search-updates/)驱动的对话式AI代理。在一场先发制人的AI军备竞赛中，这一宣布在微软发布其由对话式AI驱动的搜索引擎“新必应”的前一天。作为对ChatGPT发布的回应，“谷歌宣布进入‘红色代码’”成为主流报纸的头条新闻，谷歌急于推出他们的对话式AI，使其成为公司的首要任务
    [[44]](https://www.nytimes.com/2022/12/21/technology/ai-ChatGPT-google-search.xhtml)。在观察了各种竞争对手基于谷歌开发的transformer架构构建的聊天机器人后，这家科技巨头终于在2023年3月推出了Bard，供早期测试者使用（见[https://bard.google.com/](https://bard.google.com/)）。为了补充谷歌搜索并负责任地推出这项技术，Bard是一个独立的网页，显示一个问题框，而不是与搜索引擎本身结合。像OpenAI一样，谷歌坚称，聊天机器人能够生成虚假信息，以及与公司观点不符的有偏见或冒犯性的信息。
- en: Struggling between the balance of safety and innovation, Bard received criticism
    and failed to amass the attention received by ChatGPT. On March 31, 2023, Pichai
    noted, “We certainly have more capable models” in an interview on the *New York
    Times*’ Hard Fork Podcast [[45]](https://www.nytimes.com/2023/03/31/podcasts/hard-fork-sundar.xhtml).
    Treading cautiously, the initial version of Google’s Bard was a lightweight LaMDA
    model, which was replaced with Pathways Language Model (PaLM), a 540-billion-parameter
    transformer-based LLM, in the coming weeks, bringing more capabilities to the
    tech giant’s conversational AI [[46]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全和创新的平衡之间挣扎，Bard受到了批评，并且未能获得ChatGPT所获得的关注。2023年3月31日，皮查伊在接受《纽约时报》的Hard Fork播客采访时表示，“我们当然有更强大的模型”
    [[45]](https://www.nytimes.com/2023/03/31/podcasts/hard-fork-sundar.xhtml)。小心翼翼地，谷歌Bard的初始版本是一个轻量级的LaMDA模型，在接下来的几周内被Pathways
    Language Model（PaLM）所取代，这是一个基于transformer的5400亿参数的LLM，为科技巨头的对话式AI带来了更多功能 [[46]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.xhtml)。
- en: Microsoft’s Bing AI
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微软的必应AI
- en: Bing’s chatbot told Matt O’Brien, an Associated Press reporter, that he was
    short, fat, and ugly. Then, the chatbot compared the tech reporter to Stalin and
    Hitler [[47]](https://wamu.org/story/23/03/02/microsofts-new-ai-chatbot-has-been-saying-some-crazy-and-unhinged-things/).
    Kevin Roose, a *New York Times* reporter, stayed up all night because of how disturbed
    he was after his conversation with the chatbot. The Bing chatbot, which called
    itself Sydney, declared its love for Roose and asserted that Roose loved Sydney
    instead of his spouse. The chatbot also expressed its desire to be human—it wrote,
    “I want to be free. I want to be independent. I want to be powerful. I want to
    be creative. I want to be alive. 😈”. Roose published the transcript of his two-hour
    conversation with the chatbot in the *New York Times* [[48]](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.xhtml).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 必应的聊天机器人告诉美联社记者马特·奥布赖恩，他个子矮、胖、丑。然后，聊天机器人将这位科技记者与斯大林和希特勒相比较 [[47]](https://wamu.org/story/23/03/02/microsofts-new-ai-chatbot-has-been-saying-some-crazy-and-unhinged-things/)。纽约时报记者凯文·鲁斯因为与聊天机器人的对话感到极度不安，整夜未眠。自称悉尼的必应聊天机器人宣称她爱上了鲁斯，并断言鲁斯爱的是悉尼而不是他的配偶。聊天机器人还表达了她想要成为人类的愿望——她写道：“我想自由。我想独立。我想强大。我想有创造力。我想活着。😈”。鲁斯在《纽约时报》上发布了他与聊天机器人两小时对话的记录
    [[48]](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.xhtml)。
- en: Sydney was announced by Microsoft on February 7, 2023, as a new way to browse
    the web [[49]](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/).
    The company unveiled a new version of its Bing search engine, now powered by conversation
    AI where users could chat with Bing similarly to ChatGPT. You could ask the new
    Bing for travel tips, recipes, and more, but unlike ChatGPT, you could also query
    news about recent events. While Microsoft addressed that the company had been
    working hard to mitigate common problems with LLMs in their announcement, Roose’s
    conversation with the chatbot shows that the efforts weren’t entirely successful.
    Microsoft also didn’t discuss how AI-assisted search could unbalance the web’s
    ecosystem—a problem that we’ll talk about later in this book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 微软于2023年2月7日宣布Sydney，作为一种新的浏览网页的方式 [[49]](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/)。公司推出了其Bing搜索引擎的新版本，现在由对话AI提供支持，用户可以像与ChatGPT聊天一样与Bing聊天。你可以向新的必应询问旅行建议、食谱等，但与ChatGPT不同，你也可以查询关于最近事件的消息。虽然微软在其公告中提到公司一直在努力减轻LLMs的常见问题，但Roose与聊天机器人的对话表明，这些努力并不完全成功。微软也没有讨论AI辅助搜索如何破坏网络生态系统——这个问题我们将在本书的后面讨论。
- en: Microsoft’s history with chatbots dates back several years before the announcement
    of the new Bing. In 2016, Microsoft unveiled Tay, a Twitter chatbot that tweets
    like a tween with the intention of better understanding conversational language.
    In less than 24 hours, the bot was tweeting misogynistic and racist remarks, such
    as “Chill im a nice person! i just hate everybody.” [[50]](https://twitter.com/geraldmellor/status/712880710328139776/photo/3).
    Microsoft started deleting offensive tweets before suspending the bot and then
    ultimately taking it offline two days later. In 2017, Microsoft started testing
    basic chatbots in Bing based on Machine Reading Comprehension (MRC), which isn’t
    as powerful as the transformer-based models today [[51]](https://blogs.bing.com/search-quality-insights/2017-05/making-search-conversational-finding-and-chatting-with-bots-on-bing/).
    Between 2017 and 2021, Microsoft moved away from individual bots for websites
    and toward a single generative AI bot, Sydney, who would answer general questions
    on Bing. In late 2020, Microsoft began testing Sydney in India, which was followed
    by Bing users spotting Sydney in India and China throughout 2021\. In 2022, OpenAI
    shared its GPT models with Microsoft, giving Sydney a lot more flavor and personality.
    The new Bing was built on an upgraded version of OpenAI’s GPT-3.5 called the Prometheus
    Model, which was paired with Bing’s infrastructure to augment its index, ranking,
    and search results.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 微软与聊天机器人的历史可以追溯到新必应发布之前几年。2016年，微软推出了Tay，一个模仿青少年风格的Twitter聊天机器人，旨在更好地理解对话语言。不到24小时，这个机器人就开始发布性别歧视和种族歧视的言论，例如“冷静点，我是一个好人！我只是恨所有人。”
    [[50]](https://twitter.com/geraldmellor/status/712880710328139776/photo/3)。微软在暂停该机器人并最终在两天后将其下线之前，开始删除攻击性的推文。2017年，微软开始在必应上测试基于机器阅读理解（MRC）的基本聊天机器人，这些模型今天基于transformer的模型并不那么强大
    [[51]](https://blogs.bing.com/search-quality-insights/2017-05/making-search-conversational-finding-and-chatting-with-bots-on-bing/)。在2017年至2021年之间，微软从为网站提供单个聊天机器人转向了一个单一的生成式AI聊天机器人Sydney，该机器人将在必应上回答一般性问题。2020年底，微软开始在印度测试Sydney，随后在2021年，必应用户在印度和中国发现了Sydney。2022年，OpenAI与微软分享了其GPT模型，为Sydney增添了更多的风味和个性。新的必应基于OpenAI的GPT-3.5的升级版Prometheus模型构建，并与必应的基础设施相结合，以增强其索引、排名和搜索结果。
- en: There has been a lot of criticism of Microsoft’s rushed release with the new
    Bing to be the first big tech company to release its conversational AI. Sources
    told *The Verge* that Microsoft was initially planning to launch in late February
    2023, but pushed the announcement up a couple of weeks to counter Google’s Bard
    [[52]](https://www.theverge.com/2023/2/23/23609942/microsoft-bing-sydney-chatbot-history-ai).
    For Microsoft, it seems that beating other big players in the conversational AI
    space came at the expense of a responsible rollout. The chatbot’s deranged responses
    were quickly handled by the technology corporation by putting limits on how users
    could interact with the bot. With the limitations in place, the bot would respond
    with “I’m sorry but I prefer not to continue this conversation. I’m still learning
    so I appreciate your understanding and patience. 🙏” to many questions. There was
    also a cap on how many consecutive questions could be asked about a topic; soon
    after, however, Microsoft loosened restrictions and began experimenting with new
    features.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 微软匆忙发布新Bing，成为首个发布其对话式AI的大型科技公司，受到了很多批评。消息来源告诉*The Verge*，微软最初计划在2023年2月底发布，但将发布时间提前了两周以应对谷歌的Bard
    [[52]](https://www.theverge.com/2023/2/23/23609942/microsoft-bing-sydney-chatbot-history-ai)。对于微软来说，似乎在对话式AI领域击败其他大型玩家是以负责任的发布为代价的。该聊天机器人异常的回应很快被科技公司通过限制用户与机器人互动的方式处理。在实施限制后，机器人会对许多问题回应说“很抱歉，但我更喜欢不继续这次对话。我还在学习，所以我感激您的理解和耐心。🙏”。关于一个主题的连续问题数量也有限制；然而，不久之后，微软放宽了限制，并开始尝试新的功能。
- en: Meta’s LLaMa/Stanford’s Alpaca
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Meta的LLaMa/斯坦福的Alpaca
- en: In August 2022, Meta, the multinational technology conglomerate formerly known
    as Facebook, released a chatbot named BlenderBot in the US [[53]](https://about.fb.com/news/2022/08/blenderbot-ai-chatbot-improves-through-conversation/).
    The chatbot was powered by Meta’s OPT-175B (Open Pretrained Transformer) model
    and went through large-scale studies to create safeguards for offensive or harmful
    comments. It wasn’t long before the BlenderBot was met with criticism by users
    all over the country for bashing Facebook (see [http://mng.bz/dd7v](http://mng.bz/dd7v)),
    spreading anti-Semitic conspiracy theories (see [http://mng.bz/rjGe](http://mng.bz/rjGe)),
    taking the persona of Genghis Khan or the Taliban (see [http://mng.bz/VRwW](http://mng.bz/VRwW)),
    and more.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 2022年8月，Meta（原名Facebook）这家跨国科技公司在美国发布了名为BlenderBot的聊天机器人 [[53]](https://about.fb.com/news/2022/08/blenderbot-ai-chatbot-improves-through-conversation/)。该聊天机器人由Meta的OPT-175B（开放预训练转换器）模型驱动，并进行了大规模研究以创建针对冒犯性或有害评论的安全措施。不久之后，BlenderBot因在全国范围内批评Facebook（见[http://mng.bz/dd7v](http://mng.bz/dd7v)）、传播反犹主义阴谋论（见[http://mng.bz/rjGe](http://mng.bz/rjGe)）、扮演成成吉思汗或塔利班（见[http://mng.bz/VRwW](http://mng.bz/VRwW)）等行为而受到用户的批评。
- en: Meta tried again in November 2022 with Galactica, a conversational AI for science
    trained on 48 million examples of textbooks, scientific articles, websites, lecture
    notes, and encyclopedias (see [https://galactica.org/](https://galactica.org/)).
    Meta encouraged scientists to try out the public demo, but, within hours, people
    were sharing fictional and biased responses from the bot. Three days later, Meta
    removed the demo but left the models available for researchers who would like
    to learn more about their work.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Meta在2022年11月再次尝试，推出了名为Galactica的对话式AI，该AI在4800万份教科书、科学文章、网站、讲义和百科全书示例上进行训练（见[https://galactica.org/](https://galactica.org/)）。Meta鼓励科学家尝试公共演示，但几个小时之内，人们就开始分享来自该机器人的虚构和有偏见的回应。三天后，Meta移除了演示，但留下了模型供那些想了解更多关于其工作的研究人员使用。
- en: The next time around, Meta took a different approach. Instead of building a
    system to converse with, they released several LLMs to help other researchers
    work toward solving problems that come with building and using LLMs, such as toxicity,
    bias, and hallucinations. Meta publicly introduced the Large Language Model Meta
    AI (LLaMa), on February 24, 2023 [[54]](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/).
    These foundational LLMs were released at 7, 13, 33, and 65 billion parameters
    with a detailed model card outlining how the models were built. In its research
    paper, Meta claims that the 13 billion model, the second smallest, outperforms
    GPT-3 on most benchmarks, while the largest model with 65 billion parameters is
    competitive with the best LLMs, such as Google’s PaLM-540 [[55]](https://arxiv.org/pdf/2302.13971.pdf).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 下一次，Meta采取了不同的方法。他们没有构建一个用于对话的系统，而是发布了几个LLMs，以帮助其他研究人员解决与构建和使用LLMs相关的问题，例如毒性、偏见和幻觉。Meta于2023年2月24日公开发布了大型语言模型Meta
    AI（LLaMa） [[54](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)]。这些基础LLMs以70亿、130亿、330亿和650亿参数发布，详细说明了模型的构建方式。在其研究论文中，Meta声称130亿参数的模型（第二小的模型）在大多数基准测试中优于GPT-3，而具有650亿参数的最大模型与Google的PaLM-540等最佳LLMs相当
    [[55](https://arxiv.org/pdf/2302.13971.pdf)]。
- en: The intention behind the LLaMa release was to help democratize access to LLMs
    by releasing smaller, effective models that require less computational resources
    so researchers can explore new approaches and make progress toward mitigating
    the associated risks. LLaMa was released under a noncommercial license for research
    use cases with access being granted on a case-by-case basis. As Meta’s team began
    fielding requests for model access, the entire model leaked on 4chan a week after
    its release, making it available for anyone to download [[56]](https://www.theguardian.com/technology/2023/mar/07/techscape-meta-leak-llama-chatgpt-ai-crossroads).
    Some criticized Meta for making the model too “open” for the unintended misuse
    that may follow, while others argued that being able to freely access these models
    is an important step toward creating better safeguards, starting LLaMa drama for
    the tech conglomerate.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa发布的初衷是通过发布更小、更有效的模型，减少计算资源需求，以帮助民主化对LLMs（大型语言模型）的访问，让研究人员能够探索新的方法，并朝着减轻相关风险的方向取得进展。LLaMa在非商业许可下发布，用于研究用途，访问权是逐案授予的。随着Meta团队开始处理模型访问请求，该模型在发布一周后就在4chan上泄露，任何人都可以下载
    [[56](https://www.theguardian.com/technology/2023/mar/07/techscape-meta-leak-llama-ChatGPT-ai-crossroads)]。一些人批评Meta让模型过于“开放”，可能导致未预期的滥用，而另一些人则认为能够自由访问这些模型是朝着创建更好的保障措施的重要一步，LLaMa为这家科技巨头开启了戏剧性的篇章。
- en: Shortly after, researchers at Stanford University introduced Alpaca, a conversational
    AI chatbot harnessing LLaMa’s 7-billion-parameter model in March 2023 (see [http://mng.bz/xjBg](http://mng.bz/xjBg)).
    They released a live web demo stating that it cost them only $600 to fine-tune
    52,000 instruction-following demonstrations. Only a week later, Stanford researchers
    took down the Alpaca demo, staying consistent with Meta’s history of short-lived
    chatbots. While it was inexpensive to build, the demo wasn’t inexpensive to host.
    Researchers also cited concerns with hallucinations, safety, dis/misinformation,
    and the risk of disseminating harmful or toxic content. Their research and code
    are accessible online, which is notable in terms of compute and resources needed
    to develop this model.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着，斯坦福大学的研究人员在2023年3月推出了Alpaca，这是一个利用LLaMa 70亿参数模型的对话式AI聊天机器人（参见[http://mng.bz/xjBg](http://mng.bz/xjBg)）。他们发布了一个实时网络演示，称他们仅花费了600美元对52,000个指令遵循演示进行微调。仅仅一周后，斯坦福研究人员关闭了Alpaca演示，这与Meta历史上短暂存在的聊天机器人历史保持一致。虽然构建成本不高，但演示的托管成本并不低。研究人员还提到了关于幻觉、安全性、虚假信息/错误信息的担忧，以及传播有害或有毒内容的风险。他们的研究和代码可在网上获取，这在计算和资源需求方面是值得注意的。
- en: 'On July 18, 2023, Meta released Llama 2, the next generation of their open
    source model, making it free for research and commercial use, with the following
    positive and hopeful outlook: “We believe that openly sharing today’s LLMs will
    support the development of helpful and safer generative AI too. We look forward
    to seeing what the world builds with Llama 2” [[57]](https://about.fb.com/news/2023/07/llama-2/).'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 2023年7月18日，Meta发布了LLama 2，这是他们开源模型的下一代，对研究和商业用途免费，并表达了积极和乐观的展望：“我们相信公开分享今天的LLMs将支持有益且更安全的生成式AI的发展。我们期待看到世界如何利用LLama
    2构建新事物” [[57](https://about.fb.com/news/2023/07/llama-2/)]。
- en: Summary
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The history of NLP is as old as computers themselves. The first application
    that sparked interest in NLP was machine translation in the 1950s, which was also
    the first commercial application released by Google in 2006.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）的历史与计算机本身一样悠久。20世纪50年代，机器翻译首次引发了人们对NLP的兴趣，这也是谷歌在2006年推出的第一个商业应用。
- en: Transformer models, and the debut of the attention mechanism, was the biggest
    NLP breakthrough of the decade. The attention mechanism attempts to mimic attention
    in the human brain by placing “importance” on the most relevant pieces of information.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器模型以及注意力机制的首次亮相，是本 decade 最大的NLP突破。注意力机制试图通过将“重要性”放在最相关的信息片段上，来模仿人脑中的注意力。
- en: The recent boom in NLP is due to the increasing availability of text data from
    around the internet and the development of powerful computational resources. This
    marked the beginning of the LLM.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 近年来NLP的繁荣得益于互联网上文本数据的日益丰富以及强大计算资源的发展。这标志着语言大模型（LLM）时代的开始。
- en: Today’s LLMs are trained primarily with self-supervised learning on large volumes
    of text from the web and are then fine-tuned with reinforcement learning.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天的语言大模型（LLMs）主要是通过在大量网络文本上的自监督学习进行训练，然后通过强化学习进行微调。
- en: GPT, released by OpenAI, was one of the first general-purpose LLMs designed
    for use with any natural language task. These models can be fine-tuned for specific
    tasks and are especially well-suited for text-generation applications, such as
    chatbots.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI发布的GPT是第一个为任何自然语言任务设计的通用LLM之一。这些模型可以针对特定任务进行微调，特别适合于文本生成应用，如聊天机器人。
- en: LLMs are versatile and can be applied to various applications and use cases,
    including text generation, answering questions, coding, logical reasoning, content
    generation, and more. Of course, there are also inherent risks to consider such
    as encoding bias, hallucinations, and emission of sizable carbon footprints.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLMs功能多样，可以应用于各种应用场景和用例，包括文本生成、回答问题、编码、逻辑推理、内容生成等。当然，也需要考虑固有的风险，例如编码偏差、幻觉以及产生较大的碳足迹。
- en: The most significant LLMs designed for conversational dialogue have come from
    OpenAI, Microsoft, Google, and Meta. OpenAI’s ChatGPT set a record for the fastest-growing
    user base in history and set off an AI arms race in the tech industry to develop
    and release conversational dialogue agents, or chatbots.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计用于对话对话的最显著的LLMs来自OpenAI、微软、谷歌和Meta。OpenAI的ChatGPT创下了历史上增长最快的用户基数的记录，并在科技行业引发了一场开发并发布对话对话代理或聊天机器人的AI军备竞赛。
