- en: 11 Deep learning for text
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '-   11 文本的深度学习'
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '-   *本章涵盖*'
- en: Preprocessing text data for machine learning applications
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   为机器学习应用预处理文本数据'
- en: Bag-of-words approaches and sequence-modeling approaches for text processing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   文本处理的词袋方法和序列建模方法'
- en: The Transformer architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   Transformer 架构'
- en: Sequence-to-sequence learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '-   序列到序列学习'
- en: '11.1 Natural language processing: The bird’s-eye view'
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '-   11.1 自然语言处理：鸟瞰视角'
- en: 'In computer science, we refer to human languages, like English or Mandarin,
    as “natural” languages, to distinguish them from languages that were designed
    for machines, like Assembly, LISP, or XML. Every machine language was *designed*:
    its starting point was a human engineer writing down a set of formal rules to
    describe what statements you could make in that language and what they meant.
    Rules came first, and people started using the language only once the rule set
    was complete. With human language, it’s the reverse: usage comes first, and rules
    arise later. Natural language was shaped by an evolution process, much like biological
    organisms— that’s what makes it “natural.” Its “rules,” like the grammar of English,
    were formalized after the fact and are often ignored or broken by its users. As
    a result, although'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在计算机科学中，我们将人类语言，如英语或普通话，称为“自然”语言，以区别于为机器设计的语言，如汇编、LISP 或 XML。每种机器语言都是*设计*出来的：其起点是人类工程师书写一组形式规则，描述该语言中可以做出的语句以及它们的含义。规则先行，人们只有在规则集合完成后才开始使用该语言。而对于人类语言，情况恰恰相反：使用先行，规则随后产生。自然语言像生物体一样经历了演化过程，这就是使其“自然”的原因。其“规则”，如英语的语法，在事后才被正式化，而且常常被其使用者忽视或打破。因此，尽管'
- en: machine-readable language is highly structured and rigorous, using precise syntactic
    rules to weave together exactly defined concepts from a fixed vocabulary, natural
    language is messy—ambiguous, chaotic, sprawling, and constantly in flux.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '-   可机器读取的语言具有高度结构化和严格的特性，使用精确的语法规则将来自固定词汇表的确切定义的概念编织在一起，而自然语言则杂乱无章——含糊、混乱、庞大且不断变化。'
- en: 'Creating algorithms that can make sense of natural language is a big deal:
    language—and in particular, text—underpins most of our communications and our
    cultural production. The internet is mostly text. Language is how we store almost
    all of our knowledge. Our very thoughts are largely built upon language. However,
    the ability to understand natural language has long eluded machines. Some people
    once naively thought that you could simply write down the “rule set of English,”
    much like one can write down the rule set of LISP. Early attempts to build natural
    language processing (NLP) systems were thus made through the lens of “applied
    linguistics.” Engineers and linguists would handcraft complex sets of rules to
    perform basic machine translation or create simple chatbots, like the famous ELIZA
    program from the 1960s, which used pattern matching to sustain very basic conversation.
    But language is a rebellious thing: it’s not easily pliable to formalization.
    After several decades of effort, the capabilities of these systems remained disappointing.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '-   创造能够理解自然语言的算法是一件大事：语言——尤其是文本——是我们大部分交流和文化生产的基础。互联网主要是文本。语言是我们几乎所有知识的存储方式。我们的思维基本上是建立在语言之上的。然而，机器长期以来一直无法理解自然语言。有些人曾天真地认为，你可以简单地书写“英语的规则集”，就像可以书写
    LISP 的规则集一样。早期尝试构建自然语言处理（NLP）系统因此是通过“应用语言学”的视角进行的。工程师和语言学家将手工制作复杂的规则集来执行基本的机器翻译或创建简单的聊天机器人，如1960年代著名的
    ELIZA 程序，它使用模式匹配来维持非常基本的对话。但语言是一种叛逆的东西：它不容易被形式化。经过几十年的努力，这些系统的能力仍然令人失望。'
- en: 'Handcrafted rules held out as the dominant approach well into the 1990s. But
    starting in the late 1980s, faster computers and greater data availability started
    making a better alternative viable. When you find yourself building systems that
    are big piles of ad hoc rules, as a clever engineer, you’re likely to start asking:
    “Could I use a corpus of data to automate the process of finding these rules?
    Could I search for the rules within some kind of rule space, instead of having
    to come up with them myself?” And just like that, you’ve graduated to doing machine
    learning. And so, in the late 1980s, we started seeing machine learning approaches
    to natural language processing. The earliest ones were based on decision trees—the
    intent was literally to automate the development of the kind of if/then/else rules
    of previous systems. Then statistical approaches started gaining speed, starting
    with logistic regression. Over time, learned parametric models fully took over,
    and linguistics came to be seen as more of a hindrance than a useful tool. Frederick
    Jelinek, an early speech recognition researcher, joked in the 1990s: “Every time
    I fire a linguist, the performance of the speech recognizer goes up.”'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪90年代初，手工制定的规则被视为主流方法。但是从上世纪80年代末开始，更快的计算机和更多的数据可用性开始使得更好的替代方案成为可能。当你发现自己正在构建一堆堆临时规则的系统时，作为一个聪明的工程师，你很可能会开始问：“我能否使用一组数据来自动化找到这些规则的过程？我能否在某种规则空间内搜索规则，而不是自己想出来？”就这样，你就开始进行机器学习了。因此，从上世纪80年代末开始，我们开始看到机器学习方法应用于自然语言处理。最早的方法基于决策树——其目的实际上是自动化之前系统中的if/then/else规则的发展。然后，统计方法开始加速发展，从逻辑回归开始。随着时间的推移，学习到的参数模型完全接管了，语言学开始被视为更多的阻碍而不是有用的工具。早期语音识别研究人员Frederick
    Jelinek在1990年代开玩笑说：“每次我解雇一个语言学家，语音识别器的性能都会提高。”
- en: 'That’s what modern NLP is about: using machine learning and large datasets
    to give computers the ability not to *understand* language, which is a more lofty
    goal, but to ingest a piece of language as input and return something useful,
    like predicting the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是现代NLP的内容：利用机器学习和大型数据集，使计算机不仅*理解*语言（这是一个更为高远的目标），而是将语言片段作为输入并返回一些有用的东西，比如预测以下内容：
- en: “What’s the topic of this text?” (text classification)
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这段文字的主题是什么？”（文本分类）
- en: “Does this text contain abuse?” (content filtering)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这段文字中是否包含滥用内容？”（内容过滤）
- en: “Does this text sound positive or negative?” (sentiment analysis
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这段文字听起来是积极的还是消极的？”（情感分析）
- en: “What should be the next word in this incomplete sentence?” (language modeling)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “这不完整句子中的下一个词应该是什么？”（语言建模）
- en: “How would you say this in German?” (translation)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你会如何用德语表达这句话？”（翻译）
- en: “How would you summarize this article in one paragraph?” (summarization)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “你会如何用一段话总结这篇文章？”（总结）
- en: And so on.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诸如此类。
- en: Of course, keep in mind throughout this chapter that the text-processing models
    you will train won’t possess a human-like understanding of language; rather, they
    simply look for statistical regularities in their input data, which turns out
    to be sufficient to perform well on many simple tasks. In much the same way that
    computer vision is pattern recognition applied to pixels, NLP is pattern recognition
    applied to words, sentences, and paragraphs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在整个本章中请记住，你将训练的文本处理模型不会具有人类般的语言理解能力；相反，它们只是在其输入数据中寻找统计规律，这足以在许多简单任务中表现良好。就像计算机视觉是应用于像素的模式识别一样，NLP是应用于单词、句子和段落的模式识别。
- en: The toolset of NLP—decision trees and logistic regression—saw only slow evolution
    from the 1990s to the early 2010s. Most of the research focus was on feature engineering.
    When I (François) won my first NLP competition on Kaggle in 2013, my model was,
    you guessed it, based on decision trees and logistic regression. However, around
    2014–2015, things started changing at last. Multiple researchers began to investigate
    the language-understanding capabilities of recurrent neural networks, in particular
    LSTM—a sequence-processing algorithm from the late 1990s that had stayed under
    the radar until then.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从1990年代到2010年代初，NLP的工具集——决策树和逻辑回归——进化缓慢。大部分研究重点放在特征工程上。当我（François）在2013年赢得了我的第一个Kaggle
    NLP比赛时，你猜对了，我的模型就是基于决策树和逻辑回归的。然而，大约在2014年至2015年左右，事情开始改变。多位研究人员开始调查循环神经网络的语言理解能力，特别是LSTM——一种来自上世纪90年代末的序列处理算法，直到那时才悄悄地被关注。
- en: In early 2015, Keras made available the first open source, easy-to-use implementation
    of LSTM, just at the start of a massive wave of renewed interest in recurrent
    neural networks. Until then, there had only been “research code” that couldn’t
    be readily reused. Then from 2015 to 2017, recurrent neural networks dominated
    the booming NLP scene. Bidirectional LSTM models, in particular, set the state
    of the art on many important tasks, from summarization to question-answering to
    machine translation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年初，Keras发布了第一个开源的、易于使用的LSTM实现，刚好在重新激发对循环神经网络的兴趣的浪潮开始时。此前，只有无法被方便地重用的“研究代码”。从2015年到2017年，循环神经网络在蓬勃发展的自然语言处理领域占据主导地位。特别是双向LSTM模型，在许多重要任务（从摘要到问答再到机器翻译）中都达到了最先进的水平。
- en: 'Finally, around 2017–2018, a new architecture rose to replace RNNs: the Transformer,
    which you will learn about in the second half of this chapter. Transformers unlocked
    considerable progress across the field in a short period of time, and today most
    NLP systems are based on them.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在2017年至2018年左右，一种新的架构崛起取代了RNN：Transformer，您将在本章的下半部分学习有关它的知识。Transformer在短时间内实现了领域内的重大进展，如今大多数NLP系统都基于它们。
- en: Let’s dive into the details. This chapter will take you from the very basics
    to doing machine translation with a Transformer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解细节。本章将带你从基础知识到使用Transformer进行机器翻译。
- en: 11.2 Preparing text data
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 准备文本数据
- en: 'Deep learning models, being differentiable functions, can process only numeric
    tensors: they can’t take raw text as input. *Vectorizing* text is the process
    of transforming text into numeric tensors. Text vectorization processes come in
    many shapes and forms, but they all follow the same template (see [figure 11.1](#fig11-1)):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型作为可微分函数，只能处理数值张量：它们无法将原始文本作为输入。对文本进行向量化是将文本转换为数值张量的过程。文本向量化过程有很多形式和方式，但它们都遵循相同的模板（参见[图11.1](#fig11-1)）：
- en: First, you *standardize* the text to make it easier to process, such as by converting
    it to lowercase or removing punctuation.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，您需要对文本进行*标准化*以便更容易处理，例如转换为小写或去除标点符号。
- en: You split the text into units (called *tokens*), such as characters, words,
    or groups of words. This is called *tokenization*.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将文本拆分为单位（称为*令牌*），如字符、单词或一组单词。这称为*分词*。
- en: You convert each such token into a numerical vector. This will usually involve
    first *indexing* all tokens present in the data.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要将每个标记转换为数值向量。通常，这将首先涉及*索引*数据中存在的所有令牌。
- en: Let’s review each of these steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下每个步骤。
- en: '![Image](../images/f0337-01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0337-01.jpg)'
- en: '**Figure 11.1 From raw text to vectors**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.1 从原始文本到向量的转换**'
- en: 11.2.1 Text standardization
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 文本标准化
- en: 'Consider these two sentences:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下这两个句子：
- en: “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “sunset came. i was staring at the Mexico sky. Isnt nature splendid??”
- en: “Sunset came; I stared at the México sky. Isn’t nature splendid?”
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “Sunset came; I stared at the México sky. Isn’t nature splendid?”
- en: They’re very similar—in fact, they’re almost identical. Yet, if you were to
    convert them to byte strings, they would end up with very different representations,
    because “i” and “I” are two different characters, “Mexico” and “México” are two
    different words, “isnt” isn’t “isn’t,” and so on. A machine learning model doesn’t
    know a priori that “i” and “I” are the same letter, that “é” is an “e” with an
    accent, or that “staring” and “stared” are two forms of the same verb.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 它们非常相似 - 实际上，它们几乎相同。但是，如果您将它们转换为字节字符串，它们的表示将非常不同，因为“i”和“I”是两个不同的字符，“Mexico”和“México”是两个不同的词，“isnt”不是“isn’t”，等等。机器学习模型事先不知道“i”和“I”是同一个字母，“é”是带重音的“e”，“staring”和“stared”是同一个动词的两种形式。
- en: Text standardization is a basic form of feature engineering that aims to erase
    encoding differences that you don’t want your model to have to deal with. It’s
    not exclusive to machine learning, either—you’d have to do the same thing if you
    were building a search engine.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 文本标准化是一种基本的特征工程形式，旨在消除您不希望模型处理的编码差异。这不仅适用于机器学习，如果您构建一个搜索引擎，您也需要做同样的处理。
- en: One of the simplest and most widespread standardization schemes is “convert
    to lowercase and remove punctuation characters.” Our two sentences would become
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单和最广泛使用的标准化方案之一是“转为小写并去除标点符号”。我们的两个句子变为：
- en: “sunset came i was staring at the mexico sky isnt nature splendid”
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “sunset came i was staring at the mexico sky isnt nature splendid”
- en: “sunset came i stared at the méxico sky isnt nature splendid”
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “sunset came i stared at the méxico sky isnt nature splendid”
- en: Much closer already. Another common transformation is to convert special characters
    to a standard form, such as replacing “é” with “e,” “æ” with “ae,” and so on.
    Our token “méxico” would then become “mexico”.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 进展已经非常接近了。另一个常见的转换是将特殊字符转换为标准形式，例如用“e”替换“é”，用“ae”替换“æ”等等。我们的标记“méxico”然后会变成“mexico”。
- en: 'Lastly, a much more advanced standardization pattern that is more rarely used
    in a machine learning context is *stemming*: converting variations of a term (such
    as different conjugated forms of a verb) into a single shared representation,
    like turning “caught” and “been catching” into “[catch]” or “cats” into “[cat]”.
    With stemming, “was staring” and “stared” would become something like “[stare]”,
    and our two similar sentences would finally end up with an identical encoding:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个更加高级的标准化模式在机器学习的背景下更少见，那就是*词干提取*：将一个词的变体（例如动词的不同变形形式）转换为一个共享的单一表示，比如将“caught”和“been
    catching”变为“[catch]”，或者将“cats”变为“[cat]”。通过词干提取，“was staring”和“stared”会变成类似“[stare]”，而我们的两个相似的句子最终将以相同的编码结束：
- en: “sunset came i [stare] at the mexico sky isnt nature splendid”
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “日落时我盯着墨西哥的天空，大自然真是壮观啊”
- en: 'With these standardization techniques, your model will require less training
    data and will generalize better—it won’t need abundant examples of both “Sunset”
    and “sunset” to learn that they mean the same thing, and it will be able to make
    sense of “México”, even if it has only seen “mexico” in its training set. Of course,
    standardization may also erase some amount of information, so always keep the
    context in mind: for instance, if you’re writing a model that extracts questions
    from interview articles, it should definitely treat “?” as a separate token instead
    of dropping it, because it’s a useful signal for this specific task.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些标准化技术，您的模型将需要更少的训练数据，并且将更好地概括——它不需要丰富的“Sunset”和“sunset”示例来学习它们意味着相同的事情，它将能够理解“México”，即使它只在训练集中看到过“mexico”。当然，标准化也可能会擦除一定量的信息，所以始终牢记上下文：例如，如果您正在编写一个从面试文章中提取问题的模型，它应该绝对将“?”视为一个单独的标记，而不是删除它，因为对于这个特定任务来说，它是一个有用的信号。
- en: 11.2.2 Text splitting (tokenization)
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 文本分割（标记化）
- en: 'Once your text is standardized, you need to break it up into units to be vectorized
    (tokens), a step called *tokenization*. You could do this in three different ways:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的文本标准化，您需要将其分割成单元以进行向量化（标记化），这一步被称为*标记化*。您可以通过三种不同的方式来实现这一点：
- en: '*Word-level tokenization*—Where tokens are space-separated (or punctuation-separated)
    substrings. A variant of this is to further split words into subwords when applicable,
    for instance, treating “staring” as “star+ing” or “called” as “call+ed.”'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*单词级标记化*—其中标记是以空格（或标点符号）分隔的子字符串。在适用时，将单词进一步分割为子单词的变体，例如将“staring”视为“star+ing”或将“called”视为“call+ed”。'
- en: '*N-gram tokenization*—Where tokens are groups of *N* consecutive words. For
    instance, “the cat” or “he was” would be 2-gram tokens (also called bigrams).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N-gram标记化*—其中标记是*N*个连续单词的组合。例如，“the cat”或“he was”将是2-gram标记（也称为bigrams）。'
- en: '*Character-level tokenization*—Where each character is its own token. In practice,
    this scheme is rarely used, and you only really see it in specialized contexts,
    like text generation or speech recognition.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字符级标记化*—其中每个字符都是其自己的标记。实际上，这种方案很少使用，您只会在特定的上下文中真正看到它，例如文本生成或语音识别。'
- en: 'In general, you’ll always use either word-level or *N*-gram tokenization. There
    are two kinds of text-processing models: those that care about word order, called
    *sequence models*, and those that treat input words as a set, discarding their
    original order, called *bag-of-words models*. If you’re building a sequence model,
    you’ll use word-level tokenization, and if you’re building a bag-of-words model,
    you’ll use *N*-gram tokenization. *N*-grams are a way to artificially inject a
    small amount of local word-order information into the model. Throughout this chapter,
    you’ll learn more about each type of model and when to use them.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，您将始终使用单词级别或*N*-gram标记化。有两种文本处理模型：那些关心单词顺序的模型称为*序列模型*，而那些将输入单词视为一组并丢弃它们的原始顺序的模型称为*词袋模型*。如果您正在构建一个序列模型，您将使用单词级标记化，如果您正在构建一个词袋模型，您将使用*N*-gram标记化。*N*-grams是一种将一小部分局部单词顺序信息注入模型的方法。在本章中，您将学习更多关于每种类型的模型以及何时使用它们的信息。
- en: '**Understanding *N*-grams and bag-of-words**'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解 *N*-grams 和词袋**'
- en: Word *N*-grams are groups of *N* (or fewer) consecutive words that you can extract
    from a sentence. The same concept may also be applied to characters instead of
    words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 词* N * 元组是一组 * N *（或更少）连续的单词，您可以从句子中提取出来。相同的概念也可以应用于字符而不是单词。
- en: 'Here’s a simple example. Consider the sentence “the cat sat on the mat.” It
    may be decomposed into the following set of 2-grams:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的例子。考虑一下句子“the cat sat on the mat。”它可以分解为以下一组二元组：
- en: c("the", "the cat", "cat", "cat sat", "sat",
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: c("the", "the cat", "cat", "cat sat", "sat",
- en: '"sat on", "on", "on the", "the mat", "mat")'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '"sat on", "on", "on the", "the mat", "mat")'
- en: 'It may also be decomposed into the following set of 3-grams:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它也可以分解为以下一组三元组：
- en: c("the", "the cat", "cat", "cat sat", "the cat sat",
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: c("the", "the cat", "cat", "cat sat", "the cat sat",
- en: '"sat", "sat on", "on", "cat sat on", "on the",'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '"sat", "sat on", "on", "cat sat on", "on the",'
- en: '"sat on the", "the mat", "mat", "on the mat")'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '"sat on the", "the mat", "mat", "on the mat")'
- en: 'Such a set is called a *bag-of-2-grams or bag-of-3-grams*, respectively. The
    term “bag” here refers to the fact that you’re dealing with a set of tokens rather
    than a list or sequence: the tokens have no specific order. This family of tokenization
    methods is called *bag-of-words* (or *bag-of-N-grams*).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的一组被称为*二元组袋或三元组袋*。这里的“袋”一词是指你处理的是一组标记而不是列表或序列：标记没有特定的顺序。这系列标记方法被称为*词袋*（或*词袋-N-gram*）。
- en: Because bag-of-words isn’t an order-preserving tokenization method (the tokens
    generated are understood as a set, not a sequence, and the general structure of
    the sentences is lost), it tends to be used in shallow language-processing models
    rather than in deep learning models. Extracting *N*-grams is a form of feature
    engineering, and deep learning sequence models do away with this manual approach,
    replacing it with hierarchical feature learning. One-dimensional convnets, recurrent
    neural networks, and Transformers are capable of learning representations for
    groups of words and characters without being explicitly told about the existence
    of such groups, by looking at continuous word or character sequences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因为词袋不是一个保留顺序的标记方法（生成的标记被理解为一组，而不是一个序列，并且句子的一般结构丢失了），所以它倾向于在浅层语言处理模型中使用而不是在深度学习模型中使用。提取*
    N * 元组是一种特征工程，深度学习序列模型放弃了这种手动方法，用分层特征学习替换了它。一维卷积神经网络，循环神经网络和Transformer能够学习表示单词和字符组合，而不需要明确告诉它们这些组合的存在，通过查看连续的单词或字符序列。
- en: 11.2.3 Vocabulary indexing
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.3 词汇表索引
- en: 'Once your text is split into tokens, you need to encode each token into a numerical
    representation. You could potentially do this in a stateless way, such as by hashing
    each token into a fixed binary vector, but in practice, the way you’d go about
    it is to build an index of all terms found in the training data (the “vocabulary”),
    and assign a unique integer to each entry in the vocabulary, something like this:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的文本被分割成标记，您需要将每个标记编码为数字表示。你可能会以无状态的方式做这个，比如将每个标记哈希成一个固定的二进制向量，但在实践中，你会构建一个包含在训练数据中找到的所有术语（“词汇表”）的索引，并为词汇表中的每个条目分配一个唯一的整数，类似这样：
- en: vocabulary <- character()
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表 <- character()
- en: for (string in text_dataset) {
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: for (string in text_dataset) {
- en: tokens <- string %>%
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: tokens <- string %>%
- en: standardize() %>%
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: standardize() %>%
- en: tokenize()
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: tokenize()
- en: vocabulary <- unique(c(vocabulary, tokens))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇表 <- unique(c(词汇表, tokens))
- en: '}'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: 'You can then convert the integer index position into a vector encoding that
    can be processed by a neural network, like a one-hot vector:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你随后可以将整数索引位置转换为向量编码，该编码可以由神经网络处理，比如说一个独热向量：
- en: one_hot_encode_token <- function(token) {
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: one_hot_encode_token <- function(token) {
- en: vector <- array(0, dim = length(vocabulary))
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: vector <- array(0, dim = length(vocabulary))
- en: token_index <- match(token, vocabulary)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: token_index <- match(token, vocabulary)
- en: vector[token_index] <- 1
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: vector[token_index] <- 1
- en: vector
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 向量
- en: '}'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Note that at this step it’s common to restrict the vocabulary to only the top
    20,000 or 30,000 most common words found in the training data. Any text dataset
    tends to feature an extremely large number of unique terms, most of which show
    up only once or twice. Indexing those rare terms would result in an excessively
    large feature space, where most features would have almost no information content.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这一步，将词汇表限制为训练数据中最常见的前20,000或30,000个单词是很常见的。任何文本数据集通常都包含大量的唯一术语，其中大多数只出现一两次。索引这些罕见的术语将导致一个特征空间过大，其中大多数特征几乎没有信息内容。
- en: Remember when you were training your first deep learning models on the IMDB
    dataset in chapters 4 and 5? The data you were using from dataset_imdb() was already
    preprocessed into sequences of integers, where each integer stood for a given
    word. Back then, we used the setting num_words = 10000, to restrict our vocabulary
    to the top 10,000 most common words found in the training data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得你在第4章和第5章在IMDB数据集上训练你的第一个深度学习模型时吗？你当时使用的来自dataset_imdb()的数据已经预处理成整数序列，其中每个整数代表一个给定的词。那时，我们使用了num_words
    = 10000的设置，将我们的词汇限制为训练数据中前10000个最常见的单词。
- en: 'Now, there’s an important detail here that we shouldn’t overlook: when we look
    up a new token in our vocabulary index, it may not necessarily exist. Your training
    data may not have contained any instance of the word “cherimoya” (or maybe you
    excluded it from your index because it was too rare), so doing token_index = match(“cherimoya”,
    vocabulary) may return NA. To handle this, you should use an “out of vocabulary”
    index (abbreviated as *OOV index*)—a catch-all for any token that wasn’t in the
    index. It’s usually index 1: you’re actually doing token_index = match(“cherimoya”,
    vocabulary, nomatch = 1). When decoding a sequence of integers back into words,
    you’ll replace 1 with something like “[UNK]” (which you’d call an “OOV token”).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个重要的细节我们不能忽视：当我们在词汇表索引中查找一个新标记时，它可能并不存在。你的训练数据可能不包含任何“cherimoya”一词（或者你可能将其从索引中排除，因为它太稀有了），所以执行token_index
    = match(“cherimoya”, vocabulary)可能返回NA。为了处理这种情况，你应该使用一个“未知词汇”索引（缩写为*OOV索引*）——用于任何不在索引中的标记的通配符。通常是索引1：实际上你正在执行token_index
    = match(“cherimoya”, vocabulary, nomatch = 1)。当将一系列整数解码回单词时，你会用类似“[UNK]”的东西来替换1（你会称其为“未知标记”）。
- en: '“Why use 1 and not 0?” you may ask. That’s because 0 is already taken. There
    are two special tokens that you will commonly use: the OOV token (index 1), and
    the *mask token* (index 0). Although the OOV token means “here was a word we did
    not recognize,” the mask token tells us “ignore me, I’m not a word.” You’d use
    it in particular to pad sequence data: because data batches need to be contiguous,
    all sequences in a batch of sequence data must have the same length, so shorter
    sequences should be padded to the length of the longest sequence. If you want
    to make a batch of data with the sequences c(5, 7, 124, 4, 89) and c(8, 34, 21),
    it would have to look like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: “为什么使用1而不是0？” 你可能会问。那是因为0已经被使用了。你通常会使用两个特殊的标记：未知标记（索引1）和*掩码标记*（索引0）。虽然未知标记表示“这里有一个我们不认识的词”，但掩码标记告诉我们“忽略我，我不是一个词”。你会特别用它来填充序列数据：因为数据批次需要是连续的，序列数据批次中的所有序列必须具有相同的长度，所以较短的序列应该填充到最长序列的长度。如果你想要创建一个数据批次，其中包含序列c(5,
    7, 124, 4, 89)和c(8, 34, 21)，它看起来应该是这样的：
- en: rbind(c(5,  7, 124, 4, 89),
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: rbind(c(5,  7, 124, 4, 89),
- en: c(8, 34,  21, 0,  0))
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: c(8, 34,  21, 0,  0))
- en: The batches of integer sequences for the IMDB dataset that you worked with in
    chapters 4 and 5 were padded with zeros in this way.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第4章和第5章中使用的IMDB数据集的整数序列批次是这样填充的。
- en: 11.2.4 Using layer_text_vectorization
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.4 使用 layer_text_vectorization
- en: 'Every step I’ve introduced so far would be very easy to implement in pure R.
    Maybe you could write something like this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我介绍的每个步骤都很容易在纯R中实现。也许你可以写出类似这样的代码：
- en: new_vectorizer <- function() {
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: new_vectorizer <- function() {
- en: self <- new.env(parent = emptyenv())
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: self <- new.env(parent = emptyenv())
- en: attr(self, "class") <- "Vectorizer"
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: attr(self, "class") <- "Vectorizer"
- en: self$vocabulary <- c("[UNK]")
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: self$vocabulary <- c("[UNK]")
- en: self$standardize <- function(text) {
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: self$standardize <- function(text) {
- en: text <- tolower(text)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: text <- tolower(text)
- en: gsub("[[:punct:]]", "", text)➊
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: gsub("[[:punct:]]", "", text)➊
- en: '}'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$tokenize <- function(text) {
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: self$tokenize <- function(text) {
- en: unlist(strsplit(text, "[[:space:]]+"))➋
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: unlist(strsplit(text, "[[:space:]]+"))➋
- en: '}'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$make_vocabulary <- function(text_dataset) {➌
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: self$make_vocabulary <- function(text_dataset) {➌
- en: tokens <- text_dataset %>%
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: tokens <- text_dataset %>%
- en: self$standardize() %>%
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: self$standardize() %>%
- en: self$tokenize()
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: self$tokenize()
- en: self$vocabulary <- unique(c(self$vocabulary, tokens))
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: self$vocabulary <- unique(c(self$vocabulary, tokens))
- en: '}'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$encode <- function(text) {
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: self$encode <- function(text) {
- en: tokens <- text %>%
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: tokens <- text %>%
- en: self$standardize() %>%
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: self$standardize() %>%
- en: self$tokenize()
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: self$tokenize()
- en: match(tokens, table = self$vocabulary, nomatch = 1)➍
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: match(tokens, table = self$vocabulary, nomatch = 1)➍
- en: '}'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self$decode <- function(int_sequence) {
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: self$decode <- function(int_sequence) {
- en: vocab_w_mask_token <- c("", self$vocabulary)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_w_mask_token <- c("", self$vocabulary)
- en: vocab_w_mask_token[int_sequence + 1]➎
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_w_mask_token[int_sequence + 1]➎
- en: '}'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: self
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: self
- en: '}'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: vectorizer <- new_vectorizer()
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: vectorizer <- new_vectorizer()
- en: dataset <- c("I write, erase, rewrite",➏
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <- c("我写，擦除，重写",➏
- en: '"Erase again, and then",'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '"再次擦除，然后",'
- en: '"A poppy blooms.")'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '"虞美人开花了。")'
- en: vectorizer$make_vocabulary(dataset)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: vectorizer$make_vocabulary(dataset)
- en: ➊ **Remove punctuation.**
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **删除标点符号。**
- en: ➋ **Split on whitespace and return a flattened character vector.**
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **按空格分割并返回一个扁平化的字符向量。**
- en: ➌ **text_dataset will be a vector of strings, that is, an R character vector.**
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **text_dataset 将是一个字符串向量，即 R 字符向量。**
- en: ➍ **nomatch matches to "[UNK]".**
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **nomatch 匹配 "[UNK]"。**
- en: ➎ **The mask token is typically encoded as a 0 integer, and decoded as an empty
    string:"".**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **掩码令牌通常被编码为 0 整数，并解码为空字符串：" "。**
- en: ➏ **Haiku by poet Hokushi**
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **诗人北诗的俳句**
- en: 'It does the job:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 它完成了任务：
- en: test_sentence <- "I write, rewrite, and still rewrite again"
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: test_sentence <- "我写，重写，仍然再次重写"
- en: encoded_sentence <- vectorizer$encode(test_sentence)
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: encoded_sentence <- vectorizer$encode(test_sentence)
- en: print(encoded_sentence)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: print(encoded_sentence)
- en: '[1] 2 3 5 7 1 5 6'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] 2 3 5 7 1 5 6'
- en: decoded_sentence <- vectorizer$decode(encoded_sentence)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- vectorizer$decode(encoded_sentence)
- en: print(decoded_sentence)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: print(decoded_sentence)
- en: '[1] "i"      "write"   "rewrite" "and"      "[UNK]"   "rewrite" "again"'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "i"      "write"   "rewrite" "and"      "[UNK]"   "rewrite" "again"'
- en: 'However, using something like this wouldn’t be very performant. In practice,
    you’ll work with the Keras layer_text_vectorization(), which is fast and efficient
    and can be dropped directly into a TF Dataset pipeline or a Keras model. This
    is what layer_ text_vectorization() looks like:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，使用这样的内容效率不会很高。在实践中，您将使用 Keras layer_text_vectorization()，它快速高效，并且可以直接放入 TF
    Dataset 流水线或 Keras 模型中。这是 layer_text_vectorization() 的外观：
- en: text_vectorization <
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <
- en: layer_text_vectorization(output_mode = "int")➊
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization(output_mode = "int")➊
- en: ➊ **Configure the layer to return sequences of words encoded as integer indices.
    There are several other output modes available, which you will see in action in
    a bit.**
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **配置该层以返回以整数索引编码的单词序列。还有其他几种可用的输出模式，您很快就会看到它们的作用。**
- en: 'By default, layer_text_vectorization() will use the setting “convert to lowercase
    and remove punctuation” for text standardization, and “split on whitespace” for
    tokenization. But importantly, you can provide custom functions for standardization
    and tokenization, which means the layer is flexible enough to handle any use case.
    Note that such custom functions should operate on tf.string dtype tensors, not
    regular R character vectors! For instance, the default layer behavior is equivalent
    to the following:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，layer_text_vectorization()将使用“转换为小写并删除标点符号”的设置进行文本标准化，并使用“按空格分割”进行标记化。但是重要的是，您可以提供自定义函数进行标准化和标记化，这意味着该层足够灵活，可以处理任何用例。请注意，此类自定义函数应该对
    tf.string 类型的张量进行操作，而不是常规的 R 字符向量！例如，默认层行为相当于以下内容：
- en: library(tensorflow)
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: custom_standardization_fn <- function(string_tensor) {
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: custom_standardization_fn <- function(string_tensor) {
- en: string_tensor %>%
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: string_tensor %>%
- en: tf$strings$lower() %>% ➊
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$lower() %>% ➊
- en: tf$strings$regex_replace("[[:punct:]]", "")➋
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$regex_replace("[[:punct:]]", "")➋
- en: '}'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: custom_split_fn <- function(string_tensor) {
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: custom_split_fn <- function(string_tensor) {
- en: tf$strings$split(string_tensor)➌
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$split(string_tensor)➌
- en: '}'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: text_vectorization <- layer_text_vectorization(
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <- layer_text_vectorization(
- en: output_mode = "int",
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int",
- en: standardize = custom_standardization_fn,
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: standardize = custom_standardization_fn,
- en: split = custom_split_fn
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: split = custom_split_fn
- en: )
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Convert strings to lowercase.**
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将字符串转换为小写。**
- en: ➋ **Replace punctuation characters with the empty string.**
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **用空字符串替换标点符号字符。**
- en: ➌ **Split strings on whitespace.**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **按空格分割字符串。**
- en: 'To index the vocabulary of a text corpus, just call the adapt() method of the
    layer with a TF Dataset object that yields strings, or just with an R character
    vector:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要对文本语料库的词汇进行索引，只需调用该层的adapt()方法，传入一个 TF Dataset 对象，该对象产生字符串，或者只需传入一个 R 字符向量：
- en: dataset <- c("I write, erase, rewrite",
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: dataset <- c("我写，擦除，重写",
- en: '"Erase again, and then",'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '"再次擦除，然后",'
- en: '"A poppy blooms.")'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '"虞美人开花了。")'
- en: adapt(text_vectorization, dataset)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(text_vectorization, dataset)
- en: Note that you can retrieve the computed vocabulary via get_vocabulary(). This
    can be useful if you need to convert text encoded as integer sequences back into
    words. The first two entries in the vocabulary are the mask token (index 0) and
    the OOV token (index 1). Entries in the vocabulary list are sorted by frequency,
    so with a real-world dataset, very common words like “the” or “a” would come first.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，您可以通过 get_vocabulary() 检索计算出的词汇表。如果您需要将编码为整数序列的文本转换回单词，这可能很有用。词汇表中的前两个条目是掩码令牌（索引
    0）和 OOV 令牌（索引 1）。词汇表中的条目按频率排序，因此在真实世界的数据集中，像“the”或“a”这样非常常见的词将首先出现。
- en: '**Listing 11.1 Displaying the vocabulary**'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.1 显示词汇表**'
- en: get_vocabulary(text_vectorization)
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: get_vocabulary(text_vectorization)
- en: '![Image](../images/f0343-01.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0343-01.jpg)'
- en: 'For a demonstration, let’s try to encode and then decode an example sentence:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示，让我们尝试对一个示例句子进行编码然后解码：
- en: vocabulary <- text_vectorization %>% get_vocabulary()
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: vocabulary <- text_vectorization %>% get_vocabulary()
- en: test_sentence <- "I write, rewrite, and still rewrite again"
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: test_sentence <- "我写，改写，还在不断改写"
- en: encoded_sentence <- text_vectorization(test_sentence)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: encoded_sentence <- text_vectorization(test_sentence)
- en: decoded_sentence <- paste(vocabulary[as.integer(encoded_sentence) + 1],
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- paste(vocabulary[as.integer(encoded_sentence) + 1],
- en: collapse = " ")
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: collapse = " ")
- en: encoded_sentence
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: encoded_sentence
- en: tf.Tensor([ 7  3  5  9  1  5  10], shape=(7), dtype=int64)
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([ 7  3  5  9  1  5  10], shape=(7), dtype=int64)
- en: decoded_sentence
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence
- en: '[1] "i write rewrite and [UNK] rewrite again"'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "我写改写和[UNK]再次改写"'
- en: '**Using layer_text_vectorization() in a TF Dataset pipeline or as part of a
    model**'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**在TF Dataset管道中使用layer_text_vectorization()或作为模型的一部分**'
- en: Because layer_text_vectorization() is mostly a dictionary lookup operation that
    converts tokens to integers, it can’t be executed on a GPU (or TPU)—only on a
    CPU. So, if you’re training your model on a GPU, your layer_text_vectorization()
    will run on the CPU before sending its output to the GPU. This has important performance
    implications.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 因为layer_text_vectorization()主要是一个字典查找操作，将标记转换为整数，它不能在GPU（或TPU）上执行——只能在CPU上执行。所以，如果你在GPU上训练模型，你的layer_text_vectorization()将在CPU上运行，然后将其输出发送到GPU。这对性能有重要的影响。
- en: 'There are two ways we could use our layer_text_vectorization(). The first option
    is to put it in the TF Dataset pipeline, like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以使用我们的layer_text_vectorization()。第一种选择是将其放入TF Dataset管道中，就像这样：
- en: int_sequence_dataset <- string_dataset %>%➊
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: int_sequence_dataset <- string_dataset %>%➊
- en: dataset_map(text_vectorization,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(text_vectorization,
- en: num_parallel_calls = 4)➋
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: num_parallel_calls = 4)➋
- en: ➊ **string_dataset would be a TF Dataset that yields string tensors.**
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **string_dataset将是一个产生字符串张量的TF Dataset。**
- en: ➋ **The num_parallel_calls argument is used to parallelize the dataset_map()
    call across multiple CPU cores.**
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **num_parallel_calls参数用于在多个CPU核心上并行化dataset_map()调用。**
- en: 'The second option is to make it part of the model (after all, it’s a Keras
    layer), like this (in pseudocode):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是将其作为模型的一部分（毕竟，它是一个Keras层），就像这样（伪代码中）：
- en: text_input <- layer_input(shape = shape(), dtype = "string")➊
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: text_input <- layer_input(shape = shape(), dtype = "string")➊
- en: vectorized_text <- text_vectorization(text_input)➋
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: vectorized_text <- text_vectorization(text_input)➋
- en: embedded_input <- vectorized_text %>% layer_embedding(…)
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: embedded_input <- vectorized_text %>% layer_embedding(…)
- en: output <- embedded_input %>% …➌
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: output <- embedded_input %>% …
- en: model <- keras_model(text_input, output)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(text_input, output)
- en: ➊ **Create a symbolic input that expects strings.**
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **创建一个期望字符串的符号输入。**
- en: ➋ **Apply the text vectorization layer to it.**
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将文本向量化层应用于它。**
- en: ➌ **You can keep chaining new layers on top— just your regular Functional API
    model.**
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **你可以继续在上面链接新的层——就像你的常规Functional API模型一样。**
- en: 'There’s an important difference between the two: if the vectorization step
    is part of the model, it will happen synchronously with the rest of the model.
    This means that at each training step, the rest of the model (placed on the GPU)
    will have to wait for the output of the layer_text_vectorization() (placed on
    the CPU) to be ready before it can get to work. Meanwhile, putting the layer in
    the TF Dataset pipeline enables you to do asynchronous preprocessing of your data
    on CPU: while the GPU runs the model on one batch of vectorized data, the CPU
    stays busy by vectorizing the next batch of raw strings.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者之间有一个重要的区别：如果向量化步骤是模型的一部分，它将与模型的其余部分同步进行。这意味着在每个训练步骤中，模型的其余部分（放置在GPU上）将不得不等待layer_text_vectorization()的输出（放置在CPU上）准备好才能开始工作。与此同时，将层放入TF
    Dataset管道中使您能够在CPU上对数据进行异步预处理：当GPU在一批向量化数据上运行模型时，CPU通过向量化下一批原始字符串来保持忙碌。
- en: 'If you’re training the model on GPU or TPU, you’ll probably want to go with
    the first option to get the best performance. This is what we will do in all practical
    examples throughout this chapter. When training on a CPU, though, synchronous
    processing is fine: you will get 100% utilization of your cores, regardless of
    which option you go with.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在GPU或TPU上训练模型，你可能会选择第一种选项以获得最佳性能。这是我们将在本章的所有实际示例中所做的。不过，在CPU上进行训练时，同步处理是可以接受的：无论选择哪个选项，你都将获得100%的核心利用率。
- en: Now, if you were to export our model to a production environment, you would
    want to ship a model that accepts raw strings as input, like in the code snippet
    for the second option above; otherwise, you would have to reimplement text standardization
    and tokenization in your production environment (maybe in JavaScript?), and you
    would face the risk of introducing small preprocessing discrepancies that would
    hurt the model’s accuracy. Thankfully, the layer_text_vectorization() enables
    you to include text preprocessing right into your model, making it easier to deploy,
    even if you were originally using the layer as part of a TF Dataset pipeline.
    In the sidebar box later in the chapter, “Exporting a model that processes raw
    strings,” you’ll learn how to export an inference-only trained model that incorporates
    text preprocessing.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你要将我们的模型导出到生产环境中，你会希望发布一个接受原始字符串作为输入的模型，就像上述第二个选项的代码片段中一样；否则，你将不得不在生产环境中重新实现文本标准化和标记化（也许是在JavaScript中？），并且你将面临引入小的预处理差异可能会影响模型准确性的风险。幸运的是，layer_text_vectorization()
    让你可以将文本预处理直接包含到你的模型中，使得部署变得更容易，即使你最初将该层作为 TF Dataset 管道的一部分使用。在本章后面的侧边栏中，“导出处理原始字符串的模型”，你将学习如何导出一个仅进行推断的训练模型，其中包含了文本预处理。
- en: You’ve now learned everything you need to know about text preprocessing. Let’s
    move on to the modeling stage.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经学会了关于文本预处理的所有知识。让我们进入建模阶段。
- en: '11.3 Two approaches for representing groups of words: Sets and sequences'
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 表示词组的两种方法：集合和序列
- en: 'How a machine learning model should represent *individual words* is a relatively
    uncontroversial question: they’re categorical features (values from a predefined
    set), and we know how to handle those. They should be encoded as dimensions in
    a feature space, or as category vectors (word vectors in this case). A much more
    problematic question, however, is how to encode *the way words are woven into
    sentences*: word order.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型应该如何表示*单个单词*是一个相对没有争议的问题：它们是分类特征（来自预定义集合的值），我们知道如何处理这些特征。它们应该被编码为特征空间中的维度，或者作为类别向量（在这种情况下是单词向量）。然而，一个更为棘手的问题是如何编码*单词被编织到句子中的方式*：词序。
- en: 'The problem of order in natural language is an interesting one: unlike the
    steps of a time series, words in a sentence don’t have a natural, canonical order.
    Different languages order similar words in very different ways. For instance,
    the sentence structure of English is quite different from that of Japanese. Even
    within a given language, you can typically say the same thing in different ways
    by reshuffling the words a bit. Even further, if you fully randomize the words
    in a short sentence, you can still largely figure out what it was saying, though
    in many cases, significant ambiguity seems to arise. Order is clearly important,
    but its relationship to meaning isn’t straightforward.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言中的顺序问题是一个有趣的问题：与时间序列的步骤不同，句子中的单词没有自然的、规范的顺序。不同的语言以非常不同的方式排序相似的单词。例如，英语的句子结构与日语大不相同。甚至在同一种语言中，你通常可以通过稍微重排单词来用不同的方式表达同样的事情。更进一步，如果你完全随机排列一个短句中的单词，你仍然可以大致理解它的意思，尽管在许多情况下，会出现相当大的歧义。顺序显然很重要，但它与意义的关系并不简单。
- en: 'How to represent word order is the pivotal question from which different kinds
    of NLP architectures spring. The simplest thing you could do is just discard order
    and  treat text as an unordered set of words—this gives you bag-of-words models.
    You could also decide that words should be processed strictly in the order in
    which they appear, one at a time, like steps in a time series—you could then leverage
    the recurrent models from the last chapter. Finally, a hybrid approach is also
    possible: the Transformer architecture is technically order agnostic, yet it injects
    word-position information into the representations it processes, which enables
    it to simultaneously look at different parts of a sentence (unlike RNNs) while
    still being order aware. Because they take into account word order, both RNNs
    and Transformers are called *sequence models*.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如何表示词序是不同类型的自然语言处理架构产生的关键问题。你可以做的最简单的事情就是丢弃顺序，将文本视为无序的单词集合——这给你带来了词袋模型。你也可以决定严格按照单词出现的顺序逐个处理单词，就像时间序列中的步骤一样——然后你可以利用上一章的递归模型。最后，还可以采用混合方法：transformers架构在技术上是无关顺序的，但它将单词位置信息注入到它处理的表示中，这使得它可以同时查看句子的不同部分（不像递归神经网络），同时又具有顺序感知。因为它们考虑了单词顺序，所以递归神经网络和transformers被称为*序列模型*。
- en: Historically, most early applications of machine learning to NLP just involved
    bagof-words models. Interest in sequence models started rising only in 2015, with
    the rebirth of recurrent neural networks. Today, both approaches remain relevant.
    Let’s see how they work and when to leverage which.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 历史上，大多数早期应用于NLP的机器学习都只涉及词袋模型。 对序列模型的兴趣直到2015年才开始上升，随着递归神经网络的复兴。 如今，这两种方法都仍然相关。
    让我们看看它们是如何工作的以及何时利用它们。
- en: 'We’ll demonstrate each approach on a well-known text classification benchmark:
    the IMDB movie review sentiment-classification dataset. In chapters 4 and 5, you
    worked with a prevectorized version of the IMDB dataset; now let’s process the
    raw IMDB text data, just like you would do when approaching a new text-classification
    problem in the real world.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在IMDB电影评论情感分类数据集上演示每种方法。 在第4章和第5章中，您使用了IMDB数据集的预矢量化版本； 现在让我们处理原始IMDB文本数据，就像您在真实世界中处理新的文本分类问题时所做的那样。
- en: 11.3.1 Preparing the IMDB movie reviews data
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 准备IMDB电影评论数据
- en: 'Let’s start by downloading the dataset from the Stanford page of Andrew Maas
    and uncompressing it:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Andrew Maas的斯坦福页面下载数据集并解压缩：
- en: url <- "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: url <- "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
- en: filename <- basename(url)
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: filename <- basename(url)
- en: options(timeout = 60 * 10)➊
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: options(timeout = 60 * 10)
- en: download.file(url, destfile = filename)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: download.file(url, destfile = filename)
- en: untar(filename)
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: untar(filename)
- en: ➊ **10-minute timeout**
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**10分钟超时**'
- en: 'You’re left with a directory named aclImdb, with the following structure:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到一个名为aclImdb的目录，其结构如下：
- en: fs::dir_tree("aclImdb", recurse = 1, type = "directory")
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: fs::dir_tree("aclImdb", recurse = 1, type = "directory")
- en: '![Image](../images/f0345-01.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0345-01.jpg)'
- en: For instance, the train/pos/ directory contains a set of 12,500 text files,
    each of which contains the text body of a positive-sentiment movie review to be
    used as training data. The negative-sentiment reviews live in the “neg” directories.
    In total, there are 25,000 text files for training and another 25,000 for testing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，train/pos/目录包含一组12500个文本文件，每个文件都包含一个用作训练数据的积极情感电影评论的文本正文。 负面情绪的评论存储在“neg”目录中。
    总共，有25000个文本文件用于训练，另外25000个用于测试。
- en: 'There’s also a train/unsup subdirectory in there, which we don’t need. Let’s
    delete it:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 里面还有一个名为train/unsup的子目录，我们不需要。 让我们删除它：
- en: fs::dir_delete("aclImdb/train/unsup/")
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: fs::dir_delete("aclImdb/train/unsup/")
- en: 'Take a look at the content of a few of these text files. Whether you’re working
    with text data or image data, remember to always inspect what your data looks
    like before you dive into modeling it. It will ground your intuition about what
    your model is actually doing:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 查看一下其中一些文本文件的内容。 无论您处理的是文本数据还是图像数据，请务必在开始对其进行建模之前始终检查数据的外观。 这将为您对模型实际正在做什么有基础的直觉：
- en: writeLines(readLines("aclImdb/train/pos/4077_10.txt", warn = FALSE))
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: writeLines(readLines("aclImdb/train/pos/4077_10.txt", warn = FALSE))
- en: I first saw this back in the early 90s on UK TV, i did like it then but i missed
    the chance to tape it, many years passed but the film always stuck with me and
    i lost hope of seeing it TV again, the main thing that stuck with me was the end,
    the hole castle part really touched me, its easy to watch, has a great story,
    great music, the list goes on and on, its OK me saying how good it is but everyone
    will take there own best bits away with them once they have seen it, yes the animation
    is top notch and beautiful to watch, it does show its age in a very few parts
    but that has now become part of it beauty, i am so glad it has came out on DVD
    as it is one of my top 10 films of all time. Buy it or rent it just see it, best
    viewing is at night alone with drink and food in reach so you don’t have to stop
    the film.<br /><br />Enjoy
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我在90年代初在英国电视上第一次看到这部电影，当时我很喜欢，但我错过了录制的机会，许多年过去了，但这部电影一直留在我心中，我不再希望能再次在电视上看到它，最让我印象深刻的是结尾，整个城堡部分真的触动了我，它很容易观看，有一个很棒的故事，优美的音乐，等等，我可以说它有多么好，但每个人看完后都会带走自己的最爱，是的，动画效果非常出色，非常美丽，但在很少的几个部分显示出它的年龄，但这现在已经成为它美丽的一部分，我很高兴它已经在DVD上发行，因为它是我有史以来最喜欢的十大电影之一。
    买下它或者租下它，只是看看它，最好是在夜晚独自一人，旁边备有饮料和食物，这样你就不必停止电影了。<br /><br />享受吧
- en: 'Next, let’s prepare a validation set by setting apart 20% of the training text
    files in a new directory, aclImdb/val. As before, we’ll use the fs R package:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将20％的训练文本文件分离出来，放在一个新目录aclImdb/val中以准备验证集。 与之前一样，我们将使用fs R软件包：
- en: library(fs)
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: library(fs)
- en: set.seed(1337)➊
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: set.seed(1337)
- en: base_dir <- path("aclImdb")
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: base_dir <- path("aclImdb")
- en: for (category in c("neg", "pos")) {
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: for (category in c("neg", "pos")) {
- en: filepaths <- dir_ls(base_dir / "train" / category)
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: filepaths <- dir_ls(base_dir / "train" / category)
- en: num_val_samples <- round(0.2 * length(filepaths))➋
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: num_val_samples <- round(0.2 * length(filepaths))➋
- en: val_files <- sample(filepaths, num_val_samples)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: val_files <- sample(filepaths, num_val_samples)
- en: dir_create(base_dir / "val" / category)
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: dir_create(base_dir / "val" / category)
- en: file_move(val_files, ➌
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: file_move(val_files, ➌
- en: base_dir / "val" / category)
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: base_dir / "val" / category)
- en: '}'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Set a seed, to ensure we get the same validation set from the sample() call
    every time we run the code.**
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **设置一个种子，以确保我们每次运行代码时都从sample()调用中获得相同的验证集。**
- en: ➋ **Take 20% of the training files to use for validation.**
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将训练文件的20%用于验证。**
- en: ➌ **Move the files to aclImdb/val/neg and aclImdb/val/pos.**
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将文件移动到aclImdb/val/neg和aclImdb/val/pos。**
- en: 'Remember how, in chapter 8, we used the image_dataset_from_directory() utility
    to create a batched TF Dataset of images and their labels for a directory structure?
    You can do the exact same thing for text files using the text_dataset_from_directory()
    utility. Let’s create three TF Dataset objects for training, validation, and testing:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得吗，在第8章中，我们使用了image_dataset_from_directory()工具来为目录结构创建图像和标签的批处理TF Dataset吗？你可以使用text_dataset_from_directory()工具来为文本文件做同样的事情。让我们为训练、验证和测试创建三个TF
    Dataset对象：
- en: library(keras)
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库(keras)
- en: library(tfdatasets)
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 导入库(tfdatasets)
- en: train_ds <- text_dataset_from_directory("aclImdb/train")➊
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds <- text_dataset_from_directory("aclImdb/train")➊
- en: val_ds <- text_dataset_from_directory("aclImdb/val")
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: val_ds <- text_dataset_from_directory("aclImdb/val")
- en: test_ds <- text_dataset_from_directory("aclImdb/test")➋
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: test_ds <- text_dataset_from_directory("aclImdb/test")➋
- en: ➊ **Running this line should output "Found 20000 files belonging to 2 classes";
    if you see "Found 70000 files belonging to 3 classes," it means you forgot to
    delete the aclImdb/train/unsup directory.**
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **运行此行应输出“找到 20000 个文件，属于 2 个类”; 如果看到“找到 70000 个文件，属于 3 个类”，这意味着您忘记删除aclImdb/train/unsup目录了。**
- en: '➋ **The default batch_size is 32\. If you encounter out-of-memory errors when
    training models on your machine, you can try a smaller batch_size: text_dataset_from_directory("aclImdb/train",
    batch_size = 8).**'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **默认的批量大小是32。如果在您的机器上训练模型时遇到内存不足的错误，可以尝试较小的批量大小：text_dataset_from_directory("aclImdb/train",
    batch_size = 8)。**
- en: These datasets yield inputs that are TensorFlow tf.string tensors and targets
    that are int32 tensors encoding the value “0” or “1.”
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据集产生的输入是 TensorFlow tf.string 张量，目标是 int32 张量，编码值为“0”或“1”。
- en: '**Listing 11.2 Displaying the shapes and dtypes of the first batch**'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**列出 11.2 显示第一批的形状和dtype**'
- en: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
- en: str(inputs)
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: str(inputs)
- en: 'tf.Tensor: shape=(32), dtype=string, numpy=…>'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 'tf.Tensor: shape=(32), dtype=string, numpy=…>'
- en: str(targets)
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: str(targets)
- en: 'tf.Tensor: shape=(32), dtype=int32, numpy=…>'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 'tf.Tensor: shape=(32), dtype=int32, numpy=…>'
- en: inputs[1]
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: inputs[1]
- en: tf.Tensor(b’Let me start by saying that I\’d read a number of reviews before
    renting this film and kind of knew what to expect. Still, I was surprised by just
    how bad it was. <br /><br />I am a big werewolf fan, and have grown …
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(b’让我先说，我在租借这部电影之前看过一些评论，有点知道会发生什么。但我还是被它的糟糕程度吓了一跳。<br /><br />我是个大狼人迷，一直都很喜欢…，
    dtype=string, numpy=…>
- en: Otherwise, give this one a miss.’, shape=(), dtype=string)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，就算了吧。’， shape=(), dtype=string)
- en: targets[1]
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: targets[1]
- en: tf.Tensor(0, shape=(), dtype=int32)
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(0, shape=(), dtype=int32)
- en: All set. Now let’s try learning something from this data.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好了。现在让我们尝试从这些数据中学习一些东西。
- en: '11.3.2 Processing words as a set: The bag-of-words approach'
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.2 将单词处理为集合：词袋法
- en: The simplest way to encode a piece of text for processing by a machine learning
    model is to discard order and treat it as a set (a “bag”) of tokens. You could
    either look at individual words (unigrams) or try to recover some local order
    information by looking at groups of consecutive tokens (*N*-grams).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 处理机器学习模型的文本的最简单方法是丢弃顺序，并将其视为令牌的集合（“袋”）。你可以查看单个词（unigrams），或者尝试通过查看一组连续令牌（*N*-grams）来恢复一些局部顺序信息。
- en: SINGLE WORDS (UNIGRAMS) WITH BINARY ENCODING
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词（UNIGRAMS）与二进制编码
- en: 'If you use a bag of single words, the sentence “the cat sat on the mat” becomes
    a character vector where we ignore order:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用单词袋，那么句子“the cat sat on the mat”就成为一个字符向量，我们忽略顺序：
- en: c("cat", "mat", "on", "sat", "the")
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: c("cat", "mat", "on", "sat", "the")
- en: The main advantage of this encoding is that you can represent an entire text
    as a single vector, where each entry is a presence indicator for a given word.
    For instance, using binary encoding (multi-hot), you’d encode a text as a vector
    with as many dimensions as there are words in your vocabulary, with 0s almost
    everywhere and some 1s for dimensions that encode words present in the text. This
    is what we did when we worked with text data in chapters 4 and 5\. Let’s try this
    on our task.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这种编码的主要优点是，您可以将整个文本表示为单个向量，其中每个条目都是给定单词的存在指示器。例如，使用二进制编码（多热编码），您可以将文本编码为一个向量，具有与词汇表中的单词数量一样多的维度，其中几乎每个地方都是
    0，并且对于编码文本中存在的单词的一些维度为 1。这就是我们在第 4 和 5 章中处理文本数据时所做的。让我们在我们的任务上尝试一下。
- en: First, let’s process our raw text datasets with a layer_text_vectorization()
    layer so that they yield multi-hot-encoded binary word vectors. Our layer will
    look only at single words (that is to say, *unigrams*).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 layer_text_vectorization() 层处理我们的原始文本数据集，以便它们产生多热编码的二进制单词向量。我们的层将只查看单词（也就是说，*unigrams*）。
- en: '**Listing 11.3 Preprocessing our datasets with layer_text_vectorization()**'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.3 使用 layer_text_vectorization() 预处理我们的数据集**'
- en: text_vectorization <-
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <-
- en: layer_text_vectorization(max_tokens = 20000,➊
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization(max_tokens = 20000,➊
- en: output_mode = "multi_hot")➋
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "multi_hot")➋
- en: text_only_train_ds <- train_ds %>%➌
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: text_only_train_ds <- train_ds %>%➌
- en: dataset_map(function(x, y) x)
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(function(x, y) x)
- en: adapt(text_vectorization, text_only_train_ds)➍
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(text_vectorization, text_only_train_ds)➍
- en: binary_1gram_train_ds <- train_ds %>%➎
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: binary_1gram_train_ds <- train_ds %>%➎
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map( ~ list(text_vectorization(.x), .y),
- en: num_parallel_calls = 4)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: num_parallel_calls = 4)
- en: binary_1gram_val_ds <- val_ds %>%
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: binary_1gram_val_ds <- val_ds %>%
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map( ~ list(text_vectorization(.x), .y),
- en: num_parallel_calls = 4)
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: num_parallel_calls = 4)
- en: binary_1gram_test_ds <- test_ds %>%
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: binary_1gram_test_ds <- test_ds %>%
- en: dataset_map( ~ list(text_vectorization(.x), .y),
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map( ~ list(text_vectorization(.x), .y),
- en: num_parallel_calls = 4)
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: num_parallel_calls = 4)
- en: ➊ **Limit the vocabulary to the 20,000 most frequent words. Otherwise we'd be
    indexing every word in the training data—potentially tens of thousands of terms
    that occur only once or twice and thus aren't informative. In general, 20,000
    is the right vocabulary size for text classification.**
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **将词汇表限制为 20,000 个最常见的单词。否则，我们将索引��练数据中的每个单词 —— 可能是几十万个仅出现一两次的条目，因此并没有信息量。通常，20,000
    是文本分类的正确词汇表大小。**
- en: ➋ **Encode the output tokens as multi-hot binary vectors.**
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将输出标记编码为多热二进制向量。**
- en: ➌ **Prepare a dataset that yields only raw text inputs (no labels).**
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **准备一个仅生成原始文本输入（无标签）的数据集。**
- en: ➍ **Use that dataset to index the dataset vocabulary via the adapt() method.**
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **使用该数据集通过 adapt() 方法对数据集词汇进行索引。**
- en: ➎ **Prepare processed versions of our training, validation, and test dataset.
    Make sure to specify num_parallel_calls to leverage multiple CPU cores.**
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **准备我们的训练、验证和测试数据集的处理版本。确保指定 num_parallel_calls 以利用多个 CPU 内核。**
- en: '**~ formula function definition**'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '**~ 公式函数定义**'
- en: 'For the map_func argument to dataset_map(), we passed a formula defined witĥ,
    not a function. If the map_func argument is a formula, e.g. ~ .x + 2, it is converted
    to a function. There are three ways to refer to the arguments:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 dataset_map() 的 map_func 参数，我们传递了一个用定义定义的公式，而不是函数。如果 map_func 参数是一个公式，例如
    ~ .x + 2，它将被转换为函数。有三种方法可以引用参数：
- en: For a single argument function, use .x.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于单参数函数，请使用 .x。
- en: For a two argument function, use .x and .y.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于两个参数函数，请使用 .x 和 .y。
- en: For more arguments, use ..1, ..2, ..3 and so on.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于更多的参数，使用 ..1, ..2, ..3 等等。
- en: This syntax allows you to create very compact anonymous functions. For more
    details and examples, see the ?purrr::map() help page in R.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这种语法允许您创建非常紧凑的匿名函数。有关更多详细信息和示例，请参阅 R 中的 ?purrr::map() 帮助页面。
- en: You can try to inspect the output of one of these datasets.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以尝试检查这些数据集中的一个的输出。
- en: '**Listing 11.4 Inspecting the output of our binary unigram dataset**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.4 检查我们的二元一元组数据集的输出**'
- en: c(inputs, targets) %<-% iter_next(as_iterator(binary_1gram_train_ds))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% iter_next(as_iterator(binary_1gram_train_ds))
- en: str(inputs)
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: str(inputs)
- en: '<tf.Tensor: shape=(32, 20000), dtype=float32, numpy=…>'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(32, 20000), dtype=float32, numpy=…>'
- en: str(targets)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: str(targets)
- en: '<tf.Tensor: shape=(32), dtype=int32, numpy=…>'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(32), dtype=int32, numpy=…>'
- en: inputs[1, ]➊
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: inputs[1, ]➊
- en: tf.Tensor([1\. 1\. 1\. … 0\. 0\. 0.], shape=(20000), dtype=float32)
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([1\. 1\. 1\. … 0\. 0\. 0.], shape=(20000), dtype=float32)
- en: targets[1]
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: targets[1]
- en: tf.Tensor(1, shape=(), dtype=int32)
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(1, shape=(), dtype=int32)
- en: ➊ **Inputs are batches of 20,000-dimensional vectors. These vectors consist
    entirely of ones and zeros.**
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入是 20,000 维向量的批处理。这些向量完全由 1 和 0 组成。**
- en: Next, let’s write a reusable model-building function that we’ll use in all of
    our experiments in this section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们编写一个可重复使用的模型构建函数，我们将在本节中的所有实验中使用。
- en: '**Listing 11.5 Our model-building utility**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.5 我们的模型构建实用工具**'
- en: get_model <- function(max_tokens = 20000, hidden_dim = 16) {
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: get_model <- function(max_tokens = 20000, hidden_dim = 16) {
- en: inputs <- layer_input(shape = c(max_tokens))
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(max_tokens))
- en: outputs <- inputs %>%
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- inputs %>%
- en: layer_dense(hidden_dim, activation = "relu") %>%
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(hidden_dim, activation = "relu") %>%
- en: layer_dropout(0.5) %>%
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 编译(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确率")
- en: model
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '}'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: Finally, let’s train and test our model.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们训练和测试我们的模型。
- en: '**Listing 11.6 Training and testing the binary unigram model**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.6 训练和测试二进制单字母模型**'
- en: model <- get_model()
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: model <- get_model()
- en: model
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0349-01.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0349-01.jpg)'
- en: callbacks <- list(
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 <- list(
- en: callback_model_checkpoint("binary_1gram.keras", save_best_only = TRUE)
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("binary_1gram.keras", save_best_only = TRUE)
- en: )
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>% fit(
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 拟合(
- en: dataset_cache(binary_1gram_train_ds),
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_cache(binary_1gram_train_ds),
- en: validation_data = dataset_cache(binary_1gram_val_ds),➊
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = dataset_cache(binary_1gram_val_ds),➊
- en: epochs = 10,
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 = callbacks
- en: )
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("binary_1gram.keras")
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("binary_1gram.keras")
- en: cat(sprintf(
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf(
- en: '"Test acc: %.3f\n", evaluate(model, binary_1gram_test_ds)["accuracy"]))'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '"测试准确率: %.3f\n", evaluate(model, binary_1gram_test_ds)["accuracy"]))'
- en: 'Test acc: 0.887'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率: 0.887'
- en: '➊ **We call dataset_cache() on the datasets to cache them in memory: this way,
    we will do the preprocessing only once, during the first epoch, and we''ll reuse
    the preprocessed texts for the following epochs. This can only be done if the
    data is small enough to fit in memory.**'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们在数据集上调用 dataset_cache() 来将它们缓存到内存中：这样，我们就只在第一个 epoch 进行预处理一次，并在后续的 epoch
    中重新使用预处理的文本。这只适用于数据足够小以适应内存。**
- en: 'This gets us to a test accuracy of 88.7%: not bad! Note that in this case,
    because the dataset is a balanced two-class classification dataset (there are
    as many positive samples as negative samples), the “naive baseline” we could reach
    without training an actual model would only be 50%. Meanwhile, the best score
    that can be achieved on this dataset without leveraging external data is around
    95% test accuracy.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们达到了 88.7% 的测试准确率：不错！请注意，在本例中，因为数据集是一个平衡的两类分类数据集（正样本和负样本数量相同），如果不训练实际模型，我们所能达到的“简单基线”只能达到
    50%。同时，在这个数据集上不利用外部数据可实现的最佳分数约为 95% 的测试准确率。
- en: BIGRAMS WITH BINARY ENCODING
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元组使用二进制编码
- en: 'Of course, discarding word order is very reductive, because even atomic concepts
    can be expressed via multiple words: the term “United States” conveys a concept
    that is quite distinct from the meaning of the words “states” and “united” taken
    separately. For this reason, you will usually end up re-injecting local order
    information into your bag-of-words representation by looking at *N*-grams rather
    than single words (most commonly, bigrams).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，舍弃单词顺序非常简化，因为即使是原子概念也可以通过多个词表达：术语“美国”传达的概念与单独取词“states”和“united”的含义截然不同。因此，通常你会通过查看*N*元组而不是单词（最常见的是二元组）向你的词袋表示中重新注入局部顺序信息。
- en: 'With bigrams, our sentence becomes:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 有了二元组，我们的句子变成了：
- en: c("the", "the cat", "cat", "cat sat", "sat",
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: c("the", "the cat", "cat", "cat sat", "sat",
- en: '"sat on", "on", "on the", "the mat", "mat")'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '"坐在", "在", "在地上", "地上的", "垫子")'
- en: 'The layer_text_vectorization() layer can be configured to return arbitrary
    *N-*grams: bigrams, trigrams, and so on. Just pass an ngrams = N argument as in
    the following listing.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization() 层可以配置为返回任意*N-*元组：二元组、三元组等等。只需像以下清单中那样传递一个 ngrams
    = N 的参数。
- en: '****Listing 11.7 Configuring layer_text_vectorization() to return bigrams****'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '****清单 11.7 配置 layer_text_vectorization() 返回二元组****'
- en: text_vectorization <
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <
- en: layer_text_vectorization(ngrams = 2,
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization(ngrams = 2,
- en: max_tokens = 20000,
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = 20000,
- en: output_mode = "multi_hot")
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 输出模式 = "multi_hot")
- en: Let’s test how our model performs when trained on such binary-encoded bags of
    bigrams (listing 11.8).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试一下当以这种二进制编码的二元组袋训练时模型的表现（清单 11.8）。
- en: '**Listing 11.8 Training and testing the binary bigram model**'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.8 训练和测试二进制双字母模型**'
- en: adapt(text_vectorization, text_only_train_ds)
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 适应(文本向量化, 仅文本训练数据集)
- en: dataset_vectorize <- function(dataset) {➊
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集向量化 <- function(数据集) {➊
- en: dataset %>%
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 %>%
- en: dataset_map(~ list(text_vectorization(.x), .y),
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集映射(~ 列表(文本向量化(.x), .y),
- en: num_parallel_calls = 4)
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: num_parallel_calls = 4)
- en: '}'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: binary_2gram_train_ds <- train_ds %>% dataset_vectorize()
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: binary_2gram_train_ds <- 训练数据集 %>% 数据集向量化()
- en: binary_2gram_val_ds <- val_ds %>% dataset_vectorize()
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: binary_2gram_val_ds <- 验证数据集 %>% 数据集向量化()
- en: binary_2gram_test_ds <- test_ds %>% dataset_vectorize()
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: binary_2gram_test_ds <- 测试数据集 %>% 数据集向量化()
- en: model <- get_model()
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: model <- 获取模型()
- en: model
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0351-01.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0351-01.jpg)'
- en: callbacks <- list(callback_model_checkpoint("binary_2gram.keras",
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- 列表(回调模型检查点("二元二元组.keras",
- en: save_best_only = TRUE))
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: model %>% fit(
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 拟合(
- en: dataset_cache(binary_2gram_train_ds),
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集缓存(二元二元组训练数据集),
- en: validation_data = dataset_cache(binary_2gram_val_ds),
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = 数据集缓存(二元二元组验证数据集),
- en: epochs = 10,
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = 回调函数
- en: )
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("binary_2gram.keras")
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: model <- 加载模型_tf("二元二元组.keras")
- en: evaluate(model, binary_2gram_test_ds)["accuracy"] %>%
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 评估(模型, 二元二元组测试数据集)["准确率"] %>%
- en: 'sprintf("Test acc: %.3f\n", .) %>% cat()'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("测试准确率: %.3f\n", .) %>% cat()'
- en: 'Test acc: 0.895'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率: 0.895'
- en: ➊ **Define a helper function for applying the text_vectorization layer to a
    text TF Dataset because we'll be doing this multiple times (with different text_vectorization
    layers) throughout the chapter.**
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **定义一个辅助函数，用于将 text_vectorization 层应用于文本 TF 数据集，因为我们将在本章中多次执行此操作（使用不同的 text_vectorization
    层）。**
- en: We’re now getting 89.5% test accuracy, a marked improvement! Turns out local
    order is pretty important.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的测试准确率达到了 89.5%，这是一个明显的提升！原来局部顺序非常重要。
- en: BIGRAMS WITH TF-IDF ENCODING
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TF-IDF 编码的二元组
- en: 'You can also add a bit more information to this representation by counting
    how many times each word or *N*-gram occurs, that is to say, by taking the histogram
    of the words over the text:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过计算每个单词或*N*-gram出现的次数来为这个表示添加更多信息，也就是说，通过对文本中的单词进行直方图处理：
- en: c("the" = 2, "the cat" = 1, "cat" = 1, "cat sat" = 1, "sat" = 1,
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: c("the" = 2, "the cat" = 1, "cat" = 1, "cat sat" = 1, "sat" = 1,
- en: '"sat on" = 1, "on" = 1, "on the" = 1, "the mat" = 1, "mat" = 1)'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '"坐在" = 1, "在" = 1, "在地毯上" = 1, "地毯上" = 1, "地毯" = 1)'
- en: 'If you’re doing text classification, knowing how many times a word occurs in
    a sample is critical: any sufficiently long movie review may contain the word
    “terrible” regardless of sentiment, but a review that contains many instances
    of the word “terrible” is likely a negative one. Here’s how you’d count bigram
    occurrences with layer_text_ vectorization():'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在进行文本分类，知道一个单词在样本中出现的次数至关重要：任何足够长的电影评论都可能包含“terrible”这个词，无论情感如何，但是包含“terrible”一词多次的评论很可能是负面的。这里是如何使用
    layer_text_ vectorization() 计算二元组出现次数的：
- en: '**Listing 11.9 Configuring layer_text_vectorization() to return token counts**'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.9 配置 layer_text_vectorization() 以返回令牌计数**'
- en: text_vectorization <
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <
- en: layer_text_vectorization(ngrams = 2,
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization(ngrams = 2,
- en: max_tokens = 20000,
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = 20000,
- en: output_mode = "count")
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "count")
- en: Now, of course, some words are bound to occur more often than others no matter
    what the text is about. The words “the,” “a,” “is,” and “are” will always dominate
    your word count histograms, drowning out other words, despite being pretty much
    useless features in a classification context. How could we address this?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，无论文本内容如何，某些词汇都会比其他词汇出现得更频繁。像“the”、“a”、“is”和“are”这样的词汇将始终主导您的词频直方图，淹没其他词汇，尽管它们在分类上几乎是无用的特征。我们该如何解决这个问题呢？
- en: 'You already guessed it: via normalization. We could just normalize word counts
    by subtracting the mean and dividing by the variance (computed across the entire
    training dataset). That would make sense. Except most vectorized sentences consist
    almost entirely of zeros (our previous example features 12 nonzero entries and
    19,988 zero entries), a property called “sparsity.” That’s a great property to
    have, because it dramatically reduces compute load and reduces the risk of overfitting.
    If we subtracted the mean from each feature, we’d wreck sparsity. Thus, whatever
    normalization scheme we use should be divide-only. What, then, should we use as
    the denominator? The best practice is to go with something called *TF-IDF normalization*—TF-IDF
    stands for “term frequency, inverse document frequency.”'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经猜到了：通过归一化。我们可以通过减去均值并除以方差（计算在整个训练数据集上）来简单地规范化词频。那是有道理的。除了大多数向量化的句子几乎完全由零组成（我们之前的示例特征有
    12 个非零条目和 19,988 个零条目），这种性质称为“稀疏性”。这是一个很好的属性，因为它大大减少了计算负载并减少了过拟合的风险。如果我们从每个特征中减去平均值，我们将破坏稀疏性。因此，我们使用的任何规范化方案都应该是仅除法。那么，我们应该用什么作为分母呢？最佳做法是采用一种称为
    *TF-IDF 规范化* 的东西 —— TF-IDF 代表“词频，逆文档频率”。
- en: '**Understanding TF-IDF normalization**'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解 TF-IDF 规范化**'
- en: 'The more a given term appears in a document, the more important that term is
    for understanding what the document is about. At the same time, the frequency
    at which the term appears across all documents in your dataset matters, too: terms
    that appear in almost every document (like “the” or “a”) aren’t particularly informative,
    while terms that appear only in a small subset of all texts (like “Herzog”) are
    very distinctive and, thus, important. TF-IDF is a metric that fuses these two
    ideas. It weights a given term by taking “term frequency,” how many times the
    term appears in the current document, and dividing it by a measure of “document
    frequency,” which estimates how often the term comes up across the dataset. You’d
    compute it as follows:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 一个给定术语在文档中出现的次数越多，该术语对于理解文档内容的重要性就越大。同时，该术语在数据集中出现的频率也很重要：几乎在每个文档中出现的术语（如“the”或“a”）并不特别信息丰富，而在所有文本的一个小子集中出现的术语（如“Herzog”）非常具有特色和重要性。TF-IDF
    是将这两个想法融合起来的一个指标。它通过采用“词频”，即术语在当前文档中出现的次数，除以“文档频率”的度量来加权给定术语，后者估计了术语在数据集中出现的频率。您可以如下计算它：
- en: tf_idf <- function(term, document, dataset) {
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: tf_idf <- function(term, document, dataset) {
- en: term_freq <- sum(document == term)➊
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: term_freq <- sum(document == term)➊
- en: doc_freqs <- sapply(dataset, function(doc) sum(doc == term))➋
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: doc_freqs <- sapply(dataset, function(doc) sum(doc == term))➋
- en: doc_freq <- log(1 + sum(doc_freqs))
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: doc_freq <- log(1 + sum(doc_freqs))
- en: term_freq / doc_freq
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: term_freq / doc_freq
- en: '}'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Count the number times 'term' appears in the document.**
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **计算文档中 'term' 出现的次数。**
- en: ➋ **Count the number times 'term' appears across the full dataset.**
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算 'term' 在整个数据集中出现的次数。**
- en: TF-IDF is so common that it’s built into layer_text_vectorization(). All you
    need to do to start using it is to switch the output_mode argument to “tf_idf”.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF 是如此常见，以至于它已经内置到 layer_text_vectorization() 中。你所需要做的就是将 output_mode 参数切换到
    “tf_idf”。
- en: '**Listing 11.10 Configuring layer_text_vectorization to return TF-IDF outputs**'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.10 配置 layer_text_vectorization 以返回 TF-IDF 输出**'
- en: text_vectorization <
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <
- en: layer_text_vectorization(ngrams = 2,
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: layer_text_vectorization(ngrams = 2,
- en: max_tokens = 20000,
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = 20000,
- en: output_mode = "tf_idf")
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "tf_idf")
- en: Let’s train a new model with this scheme.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用这个方案训练一个新模型。
- en: '**Listing 11.11 Training and testing the TF-IDF bigram model**'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.11 训练和测试 TF-IDF 二元模型**'
- en: '[with(tf$device("CPU"), {➊'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[with(tf$device("CPU"), {➊'
- en: adapt(text_vectorization, text_only_train_ds) ➋
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 适应(text_vectorization, text_only_train_ds) ➋
- en: '})'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: tfidf_2gram_train_ds <- train_ds %>% dataset_vectorize()
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_2gram_train_ds <- train_ds %>% 数据集向量化()
- en: tfidf_2gram_val_ds <- val_ds %>% dataset_vectorize()
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_2gram_val_ds <- val_ds %>% 数据集向量化()
- en: tfidf_2gram_test_ds <- test_ds %>% dataset_vectorize()
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: tfidf_2gram_test_ds <- test_ds %>% 数据集向量化()
- en: model <- get_model()
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: model <- 获取模型()
- en: model
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0353-01.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0353-01.jpg)'
- en: callbacks <- list(callback_model_checkpoint("tfidf_2gram.keras",
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 <- list(callback_model_checkpoint("tfidf_2gram.keras",
- en: save_best_only = TRUE))
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: model %>% fit(
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% 拟合(
- en: dataset_cache(tfidf_2gram_train_ds),
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_cache(tfidf_2gram_train_ds),
- en: validation_data = dataset_cache(tfidf_2gram_val_ds),
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = dataset_cache(tfidf_2gram_val_ds),
- en: epochs = 10,
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 回调 = 回调
- en: )
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("tfidf_2gram.keras")
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("tfidf_2gram.keras")
- en: evaluate(model, tfidf_2gram_test_ds)["accuracy"] %>%
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 评估(model, tfidf_2gram_test_ds)["accuracy"] %>%
- en: 'sprintf("Test acc: %.3f", .) %>% cat("\n")'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 'sprintf("Test acc: %.3f", .) %>% cat("\n")'
- en: 'Test acc: 0.896'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 'Test acc: 0.896'
- en: ➊ **We pin this operation to a CPU only, because it uses operations that a GPU
    device doesn't support yet.**
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **我们将此操作固定在 CPU 上，因为它使用了 GPU 设备尚不支持的操作。**
- en: ➋ **The adapt() call will learn the TF-IDF weights in addition to the vocabulary.**
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **adapt() 调用将学习 TF-IDF 权重以及词汇表。**
- en: 'This gets us an 89.6% test accuracy on the IMDB classification task: it doesn’t
    seem to be particularly helpful in this case. However, for many text-classification
    datasets, it would be typical to see a one-percentage-point increase when using
    TF-IDF compared to plain binary encoding.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们在 IMDB 分类任务上的测试准确率为 89.6%：在这种情况下似乎并不特别有帮助。然而，对于许多文本分类数据集，使用 TF-IDF 相对于纯二进制编码，会看到增加一个百分点是很典型的。
- en: '**Exporting a model that processes raw strings**'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**导出一个处理原始字符串的模型**'
- en: In the preceding examples, we did our text standardization, splitting, and indexing
    as part of the TF Dataset pipeline. But if we want to export a standalone model
    independent of this pipeline, we should make sure that it incorporates its own
    text preprocessing (otherwise, you’d have to reimplement in the production environment,
    which can be challenging or can lead to subtle discrepancies between the training
    data and the production data). Thankfully, this is easy.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们在 TF Dataset 管道的一部分中进行了文本标准化、拆分和索引。但是，如果我们想要导出一个独立于此管道的单独模型，我们应该确保它包含自己的文本预处理（否则，你将不得不在生产环境中重新实现，这可能具有挑战性，或者可能导致训练数据和生产数据之间的细微差异）。幸运的是，这很容易。
- en: 'Just create a new model that reuses your text_vectorization layer and adds
    to it the model you just trained:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 只需创建一个新模型，该模型重用你的 text_vectorization 层，并将刚训练的模型添加到其中：
- en: inputs <- layer_input(shape = c(1), dtype = "string") ➊
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(1), dtype = "string") ➊
- en: outputs <- inputs %>%
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: text_vectorization() %>%➋
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization() %>%➋
- en: model() ➌
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: model() ➌
- en: inference_model <- keras_model(inputs, outputs)➍
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: inference_model <- keras_model(inputs, outputs)➍
- en: ➊ **One input sample would be one string.**
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **一个输入样本将是一个字符串。**
- en: ➋ **Apply text preprocessing.**
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **应用文本预处理。**
- en: ➌ **Apply the previously trained model.**
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **应用先前训练过的模型。**
- en: ➍ **Instantiate the end-to-end model.**
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **实例化端到端模型。**
- en: 'The resulting model can process batches of raw strings:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 结果生成的模型可以处理原始字符串的批次：
- en: raw_text_data <- "That was an excellent movie, I loved it." %>%
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: raw_text_data <- "那是一部很棒的电影，我喜欢它。" %>%
- en: as_tensor(shape = c(-1, 1))➊
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(shape = c(-1, 1))➊
- en: predictions <- inference_model(raw_text_data)
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: predictions <- inference_model(raw_text_data)
- en: str(predictions)
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: str(predictions)
- en: '<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.93249124]],'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.93249124]],'
- en: '![Image](../images/common01bg.jpg) dtype=float32)>'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/common01bg.jpg) dtype=float32)>'
- en: cat(sprintf("%.2f percent positive\n",
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf("%.2f percent positive\n",
- en: as.numeric(predictions) * 100))
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: as.numeric(predictions) * 100))
- en: 93.25 percent positive
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 93.25 percent positive
- en: ➊ **The model expects inputs to be a batch of samples, that is, a one-column
    matrix.**
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **该模型期望输入为样本批次，即一列矩阵。**
- en: '11.3.3 Processing words as a sequence: The sequence model approach'
  id: totrans-452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.3 将单词作为序列进行处理：序列模型方法
- en: 'These past few examples clearly show that word order matters: manual engineering
    of order-based features, such as bigrams, yields a nice accuracy boost. Now remember:
    the history of deep learning is that of a move away from manual feature engineering,
    toward letting models learn their own features from exposure to data alone. What
    if, instead of manually crafting order-based features, we exposed the model to
    raw word sequences and let it figure out such features on its own? This is what
    *sequence models* are about.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 这些最近的例子清楚地表明了单词顺序的重要性：基于顺序的特征的手动工程处理，例如二元组，可以带来很好的准确率提升。现在记住：深度学习的历史是从手动特征工程向模型自动从数据中学习其自己的特征的移动。如果，而不是手动制作基于顺序的特征，我们将模型暴露于原始单词序列，并让其自行找出这样的特征？这就是*序列模型*的含义。
- en: To implement a sequence model, you’d start by representing your input samples
    as sequences of integer indices (one integer standing for one word). Then, you’d
    map each integer to a vector to obtain vector sequences. Finally, you’d feed these
    sequences of vectors into a stack of layers that could cross-correlate features
    from adjacent vectors, such as a 1D convnet, an RNN, or a Transformer.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现一个序列模型，你首先要将你的输入样本表示为整数索引序列（一个整数代表一个单词）。然后，你会将每个整数映射到一个向量以获得向量序列。最后，你将这些向量序列馈送到一堆层中，这些层可以对相邻向量的特征进行交叉相关，例如
    1D 卷积网络、RNN 或 Transformer。
- en: For some time, around 2016–2017, bidirectional RNNs (in particular, bidirectional
    LSTMs) were considered to be the state of the art for sequence modeling. Because
    you’re already familiar with this architecture, this is what we’ll use in our
    first sequence model examples. However, nowadays sequence modeling is almost universally
    done with Transformers, which we will cover shortly. Oddly, one-dimensional convnets
    were never very popular in NLP, even though, in my own experience, a residual
    stack of depthwise-separable 1D convolutions can often achieve comparable performance
    to a bidirectional LSTM, at a greatly reduced computational cost.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016年至2017年期间的一段时间里，双向 RNN（特别是双向 LSTM）被认为是序列建模的最先进技术。因为您已经熟悉了这种架构，所以我们将在我们的第一个序列模型示例中使用它。然而，现在几乎所有的序列建模都是用
    Transformers 完成的，我们很快就会介绍。奇怪的是，一维卷积网络在自然语言处理中从未很受欢迎，尽管在我的经验中，一堆深度可分离的 1D 卷积往往可以以大大降低的计算成本达到与双向
    LSTM 相媲美的性能。
- en: A FIRST PRACTICAL EXAMPLE
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第一个实际例子
- en: Let’s try out a first sequence model in practice. First, let’s prepare datasets
    that return integer sequences.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在实践中使用第一个序列模型。首先，让我们准备返回整数序列的数据集。
- en: '**Listing 11.12 Preparing integer sequence datasets**'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.12 准备整数序列数据集**'
- en: max_length <— 600 ➊
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: max_length <— 600 ➊
- en: max_tokens <— 20000
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens <— 20000
- en: text_vectorization <- layer_text_vectorization(
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: text_vectorization <- layer_text_vectorization(
- en: max_tokens = max_tokens,
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = max_tokens,
- en: output_mode = "int",
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int",
- en: output_sequence_length = max_length
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: output_sequence_length = max_length
- en: )
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: adapt(text_vectorization, text_only_train_ds)
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(text_vectorization, text_only_train_ds)
- en: int_train_ds <- train_ds %>% dataset_vectorize()
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: int_train_ds <- train_ds %>% dataset_vectorize()
- en: int_val_ds <- val_ds %>% dataset_vectorize()
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: int_val_ds <- val_ds %>% dataset_vectorize()
- en: int_test_ds <- test_ds %>% dataset_vectorize()
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: int_test_ds <- test_ds %>% dataset_vectorize()
- en: ➊ **To keep a manageable input size, we'll truncate the inputs after the first
    600 words. This is a reasonable choice, because the average review length is 233
    words, and only 5% of reviews are longer than 600 words.**
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **为了保持可管理的输入大小，我们将在前 600 个单词之后截断输入。这是一个合理的选择，因为平均评论长度为 233 个单词，只有 5% 的评论超过
    600 个单词。**
- en: Next, let’s make a model. The simplest way to convert our integer sequences
    to vector sequences is to one-hot-encode the integers (each dimension would represent
    one possible term in the vocabulary). On top of these one-hot vectors, we’ll add
    a simple bidirectional LSTM.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一个模型。将整数序列转换为向量序列的最简单方法是对整数进行 one-hot 编码（每个维度代表词汇表中的一个可能的术语）。在这些 one-hot
    向量之上，我们将添加一个简单的双向 LSTM。
- en: '**Listing 11.13 A sequence model built on one-hot-encoded vector sequences**'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.13 基于 one-hot 编码向量序列构建的序列模型**'
- en: inputs <- layer_input(shape(NULL), dtype = "int64")➊
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape(NULL), dtype = "int64")➊
- en: embedded <- inputs %>%
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: embedded <- inputs %>%
- en: tf$one_hot(depth = as.integer(max_tokens))➋
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: tf$one_hot(depth = as.integer(max_tokens))➋
- en: outputs <- embedded %>%
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- embedded %>%
- en: bidirectional(layer_lstm(units = 32)) %>%➌
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: bidirectional(layer_lstm(units = 32)) %>%➌
- en: layer_dropout(.5) %>%
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(.5) %>%
- en: layer_dense(1, activation = "sigmoid")➍
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")➍
- en: model <- keras_model(inputs, outputs)
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0356-01.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0356-01.jpg)'
- en: ➊ **One input is a sequence of integers.**
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **一个输入是整数序列。**
- en: ➋ **Encode the integers into binary 20,000-dimensional vectors.**
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **将整数编码为二进制的 20000 维向量。**
- en: ➌ **Add a bidirectional LSTM.**
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **添加一个双向 LSTM。**
- en: ➍ **Finally, add a classification layer.**
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **最后，添加一个分类层。**
- en: Now, let’s train our model.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练我们的模型。
- en: '**Listing 11.14 Training a first basic sequence model**'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.14 训练一个基本的序列模型**'
- en: callbacks <- list(
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint("one_hot_bidir_lstm.keras",
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("one_hot_bidir_lstm.keras",
- en: save_best_only = TRUE))
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: 'A first observation: this model trains very slowly, especially compared to
    the lightweight model of the previous section. This is because our inputs are
    quite large: each input sample is encoded as a matrix of size (600, 20000) (600
    words per sample, 20,000 possible words). That’s 12,000,000 floats for a single
    movie review. Our bidirectional LSTM has a lot of work to do. Second, the model
    gets only to 87% test accuracy— it doesn’t perform nearly as well as our (very
    fast) binary unigram model.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 首先观察到：这个模型训练非常缓慢，特别是与上一节的轻量级模型相比。这是因为我们的输入相当大：每个输入样本都被编码为大小为（600，20000）的矩阵（每个样本600个单词，20000个可能的单词）。这对于单个电影评论来说是1200万个浮点数。我们的双向LSTM需要做很多工作。其次，模型只能达到87%的测试准确率——它的表现远不如我们的（非常快速的）二元unigram模型。
- en: 'Clearly, using one-hot encoding to turn words into vectors, which was the simplest
    thing we could do, wasn’t a great idea. There’s a better way: *word embeddings*.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，使用one-hot编码将单词转换为向量，这是我们可以做的最简单的事情，不是一个好主意。有一个更好的方法：*词嵌入*。
- en: '**Reduce batch_size to avoid out-of-memory errors**'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '**减小批处理大小以避免内存错误**'
- en: 'Depending on your machine and the available RAM your GPU has, you may encounter
    out-of-memory errors trying to train this larger, bidirectional model. If that
    happens, try training with a smaller batch size. You can pass a smaller batch_size
    argument to text_dataset_from_directory(batch_size = ), or you can rebatch an
    existing TF Dataset like this:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的机器和GPU的可用RAM，您可能会遇到内存不足错误，尝试训练更大的双向模型。如果发生这种情况，请尝试使用较小的批处理大小进行训练。您可以将较小的batch_size参数传递给text_dataset_from_directory(batch_size
    = )，或者您可以重新对现有的TF Dataset进行重新分批，如下所示：
- en: int_train_ds_smaller <- int_train_ds %>%
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: int_train_ds_smaller <- int_train_ds %>%
- en: dataset_unbatch() %>%
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_unbatch() %>%
- en: dataset_batch(16)
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(16)
- en: model %>% fit(int_train_ds_smaller, validation_data = int_val_ds,
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 使用费曼技巧，你只需花上`20 min`就能深入理解知识点，而且记忆深刻，*难以遗忘*。
- en: epochs = 10, callbacks = callbacks)
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10, callbacks = callbacks)
- en: model <- load_model_tf("one_hot_bidir_lstm.keras")
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("one_hot_bidir_lstm.keras")
- en: 'sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: sprintf("测试准确率：%.3f", evaluate(model, int_test_ds)["accuracy"])
- en: '[1] "Test acc: 0.873"'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试准确率：0.873"'
- en: UNDERSTANDING WORD EMBEDDINGS
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解词嵌入
- en: 'Crucially, when you encode something via one-hot encoding, you’re making a
    feature-engineering decision. You’re injecting into your model a fundamental assumption
    about the structure of your feature space. That assumption is that *the different
    tokens you’re encoding are all independent from each other*: indeed, one-hot vectors
    are all orthogonal to one another. And in the case of words, that assumption is
    clearly wrong. Words form a structured space: they share information with each
    other. The words “movie” and “film” are interchangeable in most sentences, so
    the vector that represents “movie” should not be orthogonal to the vector that
    represents “film”—they should be the same vector, or close enough.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，当你通过one-hot编码对某物进行编码时，你正在做出一个特征工程的决定。你在你的模型中注入了一个关于特征空间结构的基本假设。这个假设是*你正在编码的不同标记彼此独立*：确实，one-hot向量彼此正交。在单词的情况下，这个假设显然是错误的。单词形成了一个结构化的空间：它们彼此共享信息。单词“电影”和“影片”在大多数句子中是可以互换的，所以表示“电影”的向量不应该正交于表示“影片”的向量——它们应该是相同的向量，或者足够接近。
- en: To get a bit more abstract, the *geometric relationship* between two word vectors
    should reflect the *semantic relationship* between these words. For instance,
    in a reasonable word vector space, you would expect synonyms to be embedded into
    similar word vectors, and in general, you would expect the geometric distance
    (such as the cosine distance or L2 distance) between any two word vectors to relate
    to the “semantic distance” between the associated words. Words that mean different
    things should lie far away from each other, whereas related words should be closer.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 为了变得更加抽象，两个词向量之间的*几何关系*应该反映出这些词之间的*语义关系*。例如，在一个合理的词向量空间中，你期望同义词被嵌入到相似的词向量中，而且通常，你期望任意两个词向量之间的几何距离（如余弦距离或L2距离）与相关词之间的“语义距离”相关联。意思不同的词应该相距较远，而相关的词应该更接近。
- en: '*Word embeddings* are vector representations of words that achieve exactly
    this: they map human language into a structured geometric space. Whereas the vectors
    obtained through one-hot encoding are binary, sparse (mostly made of zeros), and
    very high-dimensional (the same dimensionality as the number of words in the vocabulary),
    word embeddings are low-dimensional floating-point vectors (i.e., dense vectors,
    as opposed to sparse vectors); see [figure 11.2](#fig11-2). It’s common to see
    word embeddings that are 256-dimensional, 512-dimensional, or 1,024-dimensional
    when dealing with very large vocabularies. On the other hand, one-hot-encoding
    words generally leads to vectors that are 20,000-dimensional or greater (capturing
    a vocabulary of 20,000 tokens, in this case). So, word embeddings pack more information
    into far fewer dimensions.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '*词嵌入*是单词的向量表示，它恰好实现了这一点：它们将人类语言映射到结构化几何空间。虽然通过一位有效编码获得的向量是二进制的、稀疏的（主要由零组成）和非常高维的（与词汇量中的单词数量相同的维度），但是词嵌入是低维的浮点向量（即稠密向量，与稀疏向量相对）；请参见[图11.2](#fig11-2)。当处理非常大的词汇表时，常见的词嵌入是256维、512维或1,024维。另一方面，一位有效编码单词通常会导致20,000维或更高维的向量（在这种情况下，捕捉一个20,000令牌的词汇表）。因此，词嵌入将更多的信息压缩到更少的维度中。'
- en: '![Image](../images/f0358-01.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0358-01.jpg)'
- en: '**Figure 11.2 Word representations obtained from one-hot encoding or hashing
    are sparse, high-dimensional, and hardcoded. Word embeddings are dense, relatively
    low-dimensional, and learned from data.**'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.2 通过一位有效编码或散列获取的单词表示是稀疏、高维和硬编码的。而单词嵌入是密集、相对低维并且从数据中学习到的。**'
- en: Besides being *dense* representations, word embeddings are also *structured*
    representations, and their structure is learned from data. Similar words are embedded
    in close locations, and further, specific *directions* in the embedding space
    are meaningful. To make this clearer, let’s look at a concrete example.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 在是密集向量之外，词向量也是结构化表示，它们的结构是从数据中学习的。相似的单词嵌入紧密的位置，另外，嵌入空间中的特殊*方向*是有意义的。为了更清楚地了解这一点，让我们来看一个具体的例子。
- en: 'In [figure 11.3](#fig11-3), four words are embedded on a 2D plane: *cat, dog,
    wolf*, and *tiger*. With the vector representations we chose here, some semantic
    relationships between these words can be encoded as geometric transformations.
    For instance, the same vector allows us to go from *cat* to *tiger* and from *dog*
    to *wolf*: this vector could be interpreted as the “from pet to wild animal” vector.
    Similarly, another vector lets us go from *dog* to *cat* and from *wolf* to *tiger*,
    which could be interpreted as a “from canine to feline” vector.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11.3](#fig11-3)中，四个单词被嵌入在一个2D平面上：*猫，狗，狼*和*老虎*。在我们选择的向量表示中，一些语义关系可以被编码为几何变换。例如，相同的向量使我们从*猫*到*老虎*，从*狗*到*狼*：这个向量可以被解释为“从宠物到野生动物”的向量。类似地，另一个向量让我们从*狗*到*猫*，从*狼*到*老虎*，这可以被解释为“从犬科到猫科”的向量。
- en: In real-world word-embedding spaces, common examples of meaningful geometric
    transformations are “gender” vectors and “plural” vectors. For instance, by adding
    a “female” vector to the vector “king,” we obtain the vector “queen.” By adding
    a “plural” vector, we obtain “kings.” Word-embedding spaces typically feature
    thousands of such interpretable and potentially useful vectors.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实世界的词嵌入空间中，常见的有意义的几何变换有“性别”向量和“复数”向量。例如，通过向向量“国王”添加“女性”向量，我们可以得到向量“皇后”。通过添加“复数”向量，我们可以得到“国王们”。词嵌入空间通常具有数千个这样的可解释和潜在有用的向量。
- en: '![Image](../images/f0358-02.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0358-02.jpg)'
- en: '**Figure 11.3 A toy example of a word-embedding space**'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.3 一个词嵌入空间的示例**'
- en: 'Let’s look at how to use such an embedding space in practice. There are two
    ways to obtain word embeddings:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在实践中使用这样的嵌入空间。有两种方法可以获得词嵌入：
- en: Learn word embeddings jointly with the main task you care about (such as document
    classification or sentiment prediction). In this setup, you start with random
    word vectors and then learn word vectors in the same way you learn the weights
    of a neural network.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与您关心的主要任务（如文档分类或情感预测）一起学习单词嵌入。在这种设置中，您从随机单
- en: Load into your model word embeddings that were precomputed using a different
    machine learning task than the one you’re trying to solve. These are called *pretrained
    word embeddings*.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将预先使用不同的机器学习任务计算得到的单词嵌入加载到您的模型中。这些被称为*预训练的词嵌入*。
- en: Let’s review each of these approaches.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个审查这些方法。
- en: LEARNING WORD EMBEDDINGS WITH THE EMBEDDING LAYER
  id: totrans-522
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用嵌入层学习单词嵌入
- en: 'Is there some ideal word-embedding space that would perfectly map human language
    and could be used for any natural language processing task? Possibly, but we have
    yet to compute anything of the sort. Also, there is no such a thing as *human
    language*— there are many different languages, and they aren’t isomorphic to one
    another, because a language is the reflection of a specific culture and a specific
    context. But more pragmatically, what makes a good word-embedding space depends
    heavily on your task: the perfect word-embedding space for an English-language
    movie-review sentiment-analysis model may look different from the perfect embedding
    space for an English-language legal-document classification model, because the
    importance of certain semantic relationships varies from task to task.'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 是否存在一种理想的单词嵌入空间，能够完美地映射人类语言，并可用于任何自然语言处理任务？可能有，但我们尚未计算出这样的东西。此外，没有*人类语言*这样的东西——有许多不同的语言，它们并不是同构的，因为语言是特定文化和特定上下文的反映。但更实际的是，一个好的单词嵌入空间的特征取决于你的任务：用于英语电影评论情感分析模型的完美单词嵌入空间可能与用于英语法律文件分类模型的完美嵌入空间不同，因为某些语义关系的重要性因任务而异。
- en: 'It’s thus reasonable to *learn* a new embedding space with every new task.
    Fortunately, backpropagation makes this easy, and Keras makes it even easier.
    It’s about learning the weights of a layer: layer_embedding().'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过每个新任务*学习*一个新的嵌入空间是合理的。幸运的是，反向传播使这变得容易，而 Keras 使其变得更容易。这是关于学习一层的权重：layer_embedding()。
- en: '**Listing 11.15 Instantiating a layer_embedding**'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.15 实例化一个 layer_embedding**'
- en: embedding_layer <- layer_embedding(input_dim = max_tokens,
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: embedding_layer <- layer_embedding(input_dim = max_tokens,
- en: output_dim = 256)➊
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim = 256)➊
- en: '➊layer_embedding() takes at least two arguments: the number of possible tokens
    and the dimensionality of the embeddings (here, 256).'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: ➊layer_embedding() 至少需要两个参数：可能的标记数量和嵌入的维度（这里是 256）。
- en: layer_embedding() is best understood as a dictionary that maps integer indices
    (which stand for specific words) to dense vectors. It takes integers as input,
    looks up these integers in an internal dictionary, and returns the associated
    vectors. It’s effectively a dictionary lookup (see [figure 11.4](#fig11-4)).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding() 最好理解为一个将整数索引（代表特定单词）映射到密集向量的字典。它接受整数作为输入，查找这些整数在内部字典中的位置，并返回相关的向量。它实际上是一个字典查找（见[图11.4](#fig11-4)）。
- en: '![Image](../images/f0359-01.jpg)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0359-01.jpg)'
- en: '**Figure 11.4 The Embedding layer**'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.4 嵌入层**'
- en: The embedding layer takes as input a rank 2 tensor of integers, of shape (batch_size,
    sequence_length), where each entry is a sequence of integers. The layer then returns
    a 3D floating-point tensor of shape (batch_size, sequence_length, embedding_ dimensionality).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入层的输入是一个整数的秩为 2 的张量，形状为 (batch_size, sequence_length)，其中每个条目是一个整数序列。然后，该层返回一个形状为
    (batch_size, sequence_length, 嵌入维度) 的 3D 浮点张量。
- en: When you instantiate a layer_embedding(), its weights (its internal dictionary
    of token vectors) are initially random, just as with any other layer. During training,
    these word vectors are gradually adjusted via backpropagation, structuring the
    space into something the downstream model can exploit. Once fully trained, the
    embedding space will show a lot of structure—a kind of structure specialized for
    the specific problem for which you’re training your model.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 当实例化一个 layer_embedding() 时，它的权重（内部字典中的标记向量）最初是随机的，就像任何其他层一样。在训练期间，这些词向量会通过反向传播逐渐调整，将空间结构化为下游模型可以利用的东西。一旦完全训练完成，嵌入空间将展现出很多结构——一种针对你训练模型的特定问题的结构。
- en: Let’s build a model that includes a layer_embedding() and benchmark it on our
    task.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个包含 layer_embedding() 的模型，并在我们的任务上进行基准测试。
- en: '**Listing 11.16 Model that uses a layer_embedding trained from scratch**'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.16 从头开始训练一个 layer_embedding 的模型**'
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape(NA), dtype = "int64")
- en: embedded <- inputs %>%
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: embedded <- inputs %>%
- en: layer_embedding(input_dim = max_tokens, output_dim = 256)
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(input_dim = max_tokens, output_dim = 256)
- en: outputs <- embedded %>%
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- embedded %>%
- en: bidirectional(layer_lstm(units = 32)) %>%
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: bidirectional(layer_lstm(units = 32)) %>%
- en: layer_dropout(0.5) %>%
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>%
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0360-01.jpg)'
  id: totrans-549
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0360-01.jpg)'
- en: callbacks <- list(callback_model_checkpoint("embeddings_bidir_lstm.keras",
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(callback_model_checkpoint("embeddings_bidir_lstm.keras",
- en: save_best_only = TRUE))
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: model %>%
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%
- en: fit(int_train_ds,
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: fit(int_train_ds,
- en: validation_data = int_val_ds,
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = int_val_ds,
- en: epochs = 10,
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks)
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks)
- en: model <- load_model_tf("embeddings_bidir_lstm.keras")
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("embeddings_bidir_lstm.keras")
- en: evaluate(model, int_test_ds)["accuracy"] %>%
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate(model, int_test_ds)["accuracy"] %>%
- en: 'sprintf("Test acc: %.3f\n", .) %>% cat()'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: sprintf("测试准确度：%.3f\n", .) %>% cat()
- en: 'Test acc: 0.842'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确度：0.842
- en: 'It trains much faster than the one-hot model (because the LSTM has to process
    only 256-dimensional vectors instead of 20,000-dimensional ones), and its test
    accuracy is comparable (84%). However, we’re still some way off from the results
    of our basic bigram model. Part of the reason is simply that the model is looking
    at slightly less data: the bigram model processed full reviews, whereas our sequence
    model truncates sequences after 600 words.'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 它的训练速度比独热模型快得多（因为 LSTM 只需处理 256 维向量，而不是 20,000 维向量），其测试准确度可比拟（84%）。然而，我们离基本双字模型的结果还有一段距离。部分原因只是因为模型查看的数据稍微少一些：双字模型处理完整评论，而我们的序列模型在
    600 个单词后截断序列。
- en: UNDERSTANDING PADDING AND MASKING
  id: totrans-562
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解填充和掩码
- en: 'One thing that’s slightly hurting model performance here is that our input
    sequences are full of zeros. This comes from our use of the output_sequence_length
    = max_ length option in layer_text_vectorization() (with max_length equal to 600):
    sentences longer than 600 tokens are truncated to a length of 600 tokens, and
    sentences shorter than 600 tokens are padded with zeros at the end so that they
    can be concatenated with other sequences to form contiguous batches.'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 这里稍微影响模型性能的一件事情是，我们的输入序列中充满了零。这来自我们在 layer_text_vectorization() 中使用 output_sequence_length
    = max_ length 选项（max_length 等于 600）：长于 600 个标记的句子将被截断为 600 个标记的长度，并且短于 600 个标记的句子将在末尾填充零，以便它们可以与其他序列连接以形成连续批次。
- en: 'We’re using a bidirectional RNN: two RNN layers running in parallel, with one
    processing the tokens in their natural order, and the other processing the same
    tokens in reverse. The RNN that looks at the tokens in their natural order will
    spend its last iterations seeing only vectors that encode padding—possibly for
    several hundreds of iterations if the original sentence was short. The information
    stored in the internal state of the RNN will gradually fade out as it is exposed
    to these meaningless inputs.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了双向 RNN：两个 RNN 层并行运行，其中一个按照它们的自然顺序处理标记，另一个按照相同的标记逆序处理。以自然顺序查看标记的 RNN 将在最后的迭代中仅看到编码填充的向量
    —— 如果原始句子很短，可能会连续数百次迭代。随着暴露于这些无意义的输入，RNN 内部状态中存储的信息将逐渐消失。
- en: 'We need some way to tell the RNN that it should skip these iterations. There’s
    an API for that: *masking*. layer_embedding() is capable of generating a “mask”
    that corresponds to its input data. This mask is a tensor of ones and zeros (or
    TRUE/FALSE Booleans), of shape (batch_size, sequence_length), where the entry
    mask[i, t] indicates whether time step t of sample i should be skipped or not
    (the time step will be skipped if mask[i, t] is 0 or FALSE, and processed otherwise).'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方式告诉 RNN 它应该跳过这些迭代。有一个 API 可以做到这一点：*掩码*。layer_embedding() 能够生成与其输入数据相对应的“掩码”。这个掩码是一个由
    1 和 0（或 TRUE/FALSE 布尔值）组成的张量，形状为（batch_size，sequence_length），其中条目 mask[i, t] 表示样本
    i 的时间步 t 是否应该跳过（如果 mask[i, t] 为 0 或 FALSE，则会跳过时间步，否则会处理）。
- en: 'By default, this option isn’t active—you can turn it on by passing mask_zero
    = TRUE to your layer_embedding(). You can retrieve the mask with the compute_mask()
    method:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，此选项未激活 —— 您可以通过将 mask_zero = TRUE 传递给您的 layer_embedding() 来打开它。您可以使用 compute_mask()
    方法检索掩码：
- en: embedding_layer <- layer_embedding(input_dim = 10, output_dim = 256,
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: embedding_layer <- layer_embedding(input_dim = 10, output_dim = 256,
- en: mask_zero = TRUE)
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: mask_zero = TRUE)
- en: some_input <- rbind(c(4, 3, 2, 1, 0, 0, 0),
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: some_input <- rbind(c(4, 3, 2, 1, 0, 0, 0),
- en: c(5, 4, 3, 2, 1, 0, 0),
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: c(5, 4, 3, 2, 1, 0, 0),
- en: c(2, 1, 0, 0, 0, 0, 0))
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: c(2, 1, 0, 0, 0, 0, 0))
- en: mask <- embedding_layer$compute_mask(some_input)
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: mask <- embedding_layer$compute_mask(some_input)
- en: mask
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: mask
- en: tf.Tensor(
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(
- en: '[[ True  True  True  True False False False]'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '[[ True  True  True  True False False False]'
- en: '[ True  True  True  True  True False False]'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '[ True  True  True  True  True False False]'
- en: '[ True  True False False False False False]], shape=(3, 7), dtype=bool)'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '[ True  True False False False False False]], shape=(3, 7), dtype=bool)'
- en: In practice, you will almost never have to manage masks by hand. Instead, Keras
    will automatically pass on the mask to every layer that is able to process it
    (as a piece of metadata attached to the sequence it represents). This mask will
    be used by RNN layers to skip masked steps. If your model returns an entire sequence,
    the mask will also be used by the loss function to skip masked steps in the output
    sequence. Let’s try retraining our model with masking enabled.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你几乎永远不需要手动管理遮蔽。相反，Keras会自动将遮蔽传递给每个能够处理它的层（作为附加到它表示的序列的元数据）。这个遮蔽将被RNN层用于跳过遮蔽的步骤。如果你的模型返回了整个序列，遮蔽也将被损失函数用于跳过输出序列中的遮蔽步骤。让我们尝试重新训练我们的模型，并启用遮蔽。
- en: '**Listing 11.17 Using an embedding layer with masking enabled**'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.17 使用启用了遮蔽的嵌入层**'
- en: inputs <- layer_input(c(NA), dtype = "int64")
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(c(NA), dtype = "int64")
- en: embedded <- inputs %>%
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: embedded <- inputs %>%
- en: layer_embedding(input_dim = max_tokens,
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(input_dim = max_tokens,
- en: output_dim = 256,
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim = 256,
- en: mask_zero = TRUE)
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: mask_zero = TRUE)
- en: outputs <- embedded %>%
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- embedded %>%
- en: bidirectional(layer_lstm(units = 32)) %>%
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: bidirectional(layer_lstm(units = 32)) %>%
- en: layer_dropout(0.5) %>%
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: '![Image](../images/f0362-01.jpg)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0362-01.jpg)'
- en: callbacks <- list(
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint("embeddings_bidir_lstm_with_masking.keras",
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("embeddings_bidir_lstm_with_masking.keras",
- en: save_best_only = TRUE)
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE)
- en: )
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>%fit(
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: model %>%fit(
- en: int_train_ds,
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: int_train_ds,
- en: validation_data = int_val_ds,
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = int_val_ds,
- en: epochs = 10,
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10,
- en: callbacks = callbacks
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf("embeddings_bidir_lstm_with_masking.keras")
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf("embeddings_bidir_lstm_with_masking.keras")
- en: 'cat(sprintf("Test acc: %.3f\n",'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 'cat(sprintf("测试准确率: %.3f\n",'
- en: evaluate(model, int_test_ds)["accuracy"]))
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: evaluate(model, int_test_ds)["accuracy"]))
- en: 'Test acc: 0.880'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '测试准确率: 0.880'
- en: This time we get to 88% test accuracy—a small but noticeable improvement.
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们达到了88%的测试准确率——虽然只是一个小幅但明显的提高。
- en: USING PRETRAINED WORD EMBEDDINGS
  id: totrans-610
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的词嵌入
- en: 'Sometimes you have so little training data available that you can’t use your
    data alone to learn an appropriate task-specific embedding of your vocabulary.
    In such cases, instead of learning word embeddings jointly with the problem you
    want to solve, you can load embedding vectors from a precomputed embedding space
    that you know is highly structured and exhibits useful properties—one that captures
    generic aspects of language structure. The rationale behind using pretrained word
    embeddings in natural language processing is much the same as for using pretrained
    convnets in image classification: you don’t have enough data available to learn
    truly powerful features on your own, but you expect that the features you need
    are fairly generic—that is, common visual features or semantic features. In this
    case, it makes sense to reuse features learned on a different problem.'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你的训练数据非常少，以至于你无法单独使用数据来学习适合特定任务的词汇嵌入。在这种情况下，你可以从预先计算的嵌入空间中加载嵌入向量，这个空间是高度结构化的并具有有用的属性——它捕捉到了语言结构的通用方面。在自然语言处理中使用预训练的词嵌入的理由与在图像分类中使用预训练的卷积网络的理由基本相同：你没有足够的数据可用来自己学习真正强大的特征，但你期望你需要的特征是相当通用的——即，常见的视觉特征或语义特征。在这种情况下，重用在不同问题上学习到的特征是有意义的。
- en: 'Such word embeddings are generally computed using word-occurrence statistics
    (observations about what words co-occur in sentences or documents), using a variety
    of techniques, some involving neural networks, others not. The idea of a dense,
    low-dimensional embedding space for words, computed in an unsupervised way, was
    initially explored by Bengio et al. in the early 2000s,^([1](#Rendnote1)) but
    it started to take off in research and industry applications only after the release
    of one of the most famous and successful word-embedding schemes: the Word2Vec
    algorithm ([https://code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec)),
    developed by Tomas Mikolov at Google in 2013\. Word2Vec dimensions capture specific
    semantic properties, such as gender.'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '这种单词嵌入通常是使用单词共现统计（关于单词在句子或文档中共现的观察）计算的，使用各种技术，有些涉及神经网络，有些不涉及。将单词计算在一个密集的、低维的嵌入空间中，以无监督的方式进行，最早是由Bengio等人在2000年代初探索的^([1](#Rendnote1))，但是它在研究和工业应用中开始蓬勃发展，仅在发布了最著名和最成功的单词嵌入方案之一后才开始:
    Word2Vec算法（[https://code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec)），2013年由Google的Tomas
    Mikolov开发。Word2Vec维度捕获特定的语义属性，如性别。'
- en: You can download various precomputed databases of word embeddings and use them
    in a Keras layer_embedding(). Word2Vec is one of them. Another popular one is
    called Global Vectors for Word Representation (GloVe, [https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove)),
    which was developed by Stanford researchers in 2014\. This embedding technique
    is based on factorizing a matrix of word co-occurrence statistics. Its developers
    have made available precomputed embeddings for millions of English tokens, obtained
    from Wikipedia and Common Crawl data.
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以下载各种预先计算的单词嵌入数据库，并在Keras中使用它们作为一个层。其中之一是Word2Vec。另一个流行的是全球词向量表示（GloVe，[https://nlp.stanford.edu/projects/glove](https://nlp.stanford.edu/projects/glove)），它是由斯坦福大学的研究人员在2014年开发的。这种嵌入技术是基于分解一个单词共现统计矩阵。它的开发者已经提供了数百万个英文标记的预计算嵌入，这些标记是从维基百科和公共爬网数据中获得的。
- en: Let’s look at how you can get started using GloVe embeddings in a Keras model.
    The same method is valid for Word2Vec embeddings or any other word-embedding database.
    We’ll start by downloading the GloVe files and parsing them. We’ll then load the
    word vectors into a Keras layer_embedding() layer, which we’ll use to build a
    new model.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何开始在Keras模型中使用GloVe嵌入。相同的方法对Word2Vec嵌入或任何其他单词嵌入数据库都有效。我们将从下载GloVe文件并解析它们开始。然后我们将把单词向量加载到一个Keras
    `layer_embedding()`层中，我们将用它来构建一个新模型。
- en: 'First, let’s download the GloVe word embeddings precomputed on the 2014 English
    Wikipedia dataset. It’s an 822 MB zip file containing 100-dimensional embedding
    vectors for 400,000 words (or nonword tokens):'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们下载在2014年英文维基百科数据集上预先计算的GloVe单词嵌入。这是一个822 MB的zip文件，包含了400,000个单词（或非单词标记）的100维嵌入向量：
- en: download.file("http://nlp.stanford.edu/data/glove.6B.zip",
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`download.file("http://nlp.stanford.edu/data/glove.6B.zip",`下载文件。
- en: destfile = "glove.6B.zip")
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 目标文件 = "glove.6B.zip")
- en: zip::unzip("glove.6B.zip")
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: zip::unzip("glove.6B.zip")
- en: Let’s parse the unzipped file (a .txt file) to build an index that maps words
    (as strings) to their vector representation. Because the file structure is essentially
    a numeric matrix with row names, that’s what we’ll make in R.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解析解压后的文件（一个.txt文件）以建立一个将单词（作为字符串）映射到它们的向量表示的索引。因为文件结构本质上是一个具有行名的数值矩阵，所以我们将在R中创建这样一个结构。
- en: '**Listing 11.18 Parsing the GloVe word-embeddings file**'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单11.18 解析GloVe单词嵌入文件**'
- en: path_to_glove_file <- "glove.6B.100d.txt"
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 文件路径`path_to_glove_file` <- "glove.6B.100d.txt"
- en: embedding_dim <- 100
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入维度 <- 100
- en: df <- readr::read_table(
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: df <- readr::read_table(
- en: path_to_glove_file,
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 文件路径`path_to_glove_file`,
- en: col_names = FALSE,➊
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '`col_names = FALSE`,➊'
- en: col_types = paste0("c", strrep("n", 100))➋
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 列类型 = paste0("c", strrep("n", 100))➋
- en: )
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: embeddings_index <- as.matrix(df[, -1])➌
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入索引 <- as.matrix(df[, -1])➌
- en: rownames(embeddings_index) <- df[[1]]
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: rownames(嵌入索引) <- df[[1]]
- en: colnames(embeddings_index) <- NULL ➍
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: colnames(嵌入索引) <- NULL ➍
- en: rm(df)➎
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: rm(df)➎
- en: ➊ **read_table() returns a data.frame. col_names = FALSE tells read_table()
    the text file does not have a header line, and the data itself starts at the first
    line.**
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **`read_table()`返回一个数据框。`col_names = FALSE`告诉`read_table()`文本文件没有标题行，并且数据本身从第一行开始。**
- en: ➋ **Passing col_types is not necessary, but is a best practice and a good guard
    against surprises (e.g., if you're reading a corrupted file, or the wrong file!).
    Here we tell read_table() the first column is of type 'character', and then the
    next 100 are of type 'numeric'.**
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ 传递col_types并不是必需的，但是是一种最佳实践和对意外情况的良好保护（例如，如果你正在读取一个损坏的文件，或者错误的文件！）。在这里，我们告诉read_table()第一列是'character'类型，然后下一个100列是'numeric'类型。
- en: ➌ **The first column is the word, and the remaining 100 columns are the numeric
    embeddings.**
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ 第一列是单词，剩余的100列是数值嵌入。
- en: ➍ **Discard the column names that read_table() automatically created (R data.frames
    must have column names).**
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 丢弃read_table()自动创建的列名（R数据框必须有列名）。
- en: ➎ **Clear the temporary data.frame from memory.**
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ 清除内存中的临时数据框。
- en: 'Here is what embedding_matrix looks like:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 这是embedding_matrix的样子：
- en: str(embeddings_index)
  id: totrans-638
  prefs: []
  type: TYPE_NORMAL
  zh: str(embeddings_index)
- en: num [1:400000, 1:100] -0.0382 -0.1077 -0.3398 -0.1529 -0.1897 …
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:400000, 1:100] -0.0382 -0.1077 -0.3398 -0.1529 -0.1897 …
- en: '- attr(*, "dimnames")=List of 2'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '- attr(*, "dimnames")=List of 2'
- en: '..$ : chr [1:400000] "the" "," "." "of" …'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ : chr [1:400000] "the" "," "." "of" …'
- en: '..$ : NULL'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: '..$ : NULL'
- en: Next, let’s build an embedding matrix that you can load into a layer_embedding(),
    It must be a matrix of shape (max_words, embedding_dim), where each entry *i*
    contains the embedding_dim-dimensional vector for the word of index *i* in the
    reference word index (built during tokenization).
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一个嵌入矩阵，你可以加载到一个layer_embedding()中，它必须是一个形状为(max_words, embedding_dim)的矩阵，其中每个条目i包含索引为i的单词的embedding_dim维向量（在分词期间构建的参考词索引中）。
- en: '**Listing 11.19 Preparing the GloVe word-embeddings matrix**'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表11.19 准备GloVe单词嵌入矩阵**'
- en: vocabulary <- text_vectorization %>% get_vocabulary() ➊
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: vocabulary <- text_vectorization %>% get_vocabulary() ➊
- en: str(vocabulary)
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: str(vocabulary)
- en: chr [1:20000] "" "[UNK]" "the" "a" "and" "of" "to" "is" "in" "it" "i" …
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: chr [1:20000] "" "[UNK]" "the" "a" "and" "of" "to" "is" "in" "it" "i" …
- en: tokens <- head(vocabulary[-1], max_tokens)➋
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: tokens <- head(vocabulary[-1], max_tokens)➋
- en: i <- match(vocabulary, rownames(embeddings_index),➌
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: i <- match(vocabulary, rownames(embeddings_index),➌
- en: nomatch = 0)
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: nomatch = 0)
- en: embedding_matrix <- array(0, dim = c(max_tokens, embedding_dim))➍
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: embedding_matrix <- array(0, dim = c(max_tokens, embedding_dim))➍
- en: embedding_matrix[i != 0, ] <- embeddings_index[i, ]➎➏
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: embedding_matrix[i != 0, ] <- embeddings_index[i, ]➎➏
- en: str(embedding_matrix)
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: str(embedding_matrix)
- en: num [1:20000, 1:100] 0 0 -0.0382 -0.2709 -0.072 …
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: num [1:20000, 1:100] 0 0 -0.0382 -0.2709 -0.072 …
- en: ➊ **Retrieve the vocabulary indexed by our previous text_vectorization layer.**
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ 检索我们之前text_vectorization层索引的词汇表。
- en: ➋ **[-1] to remove the mask token "" in the first position. head(, max_tokens)
    is just a sanity check—we passed the same max_tokens to text_vectorization earlier.**
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ [-1]是为了移除第一个位置上的掩码标记""。head(, max_tokens)仅是一个健全性检查 - 我们之前将相同的max_tokens传递给了text_vectorization。
- en: ➌ **i is an integer vector of the row-number in embeddings_index that matched
    to each corresponding word in vocabulary, and 0 if there was no matching word.**
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ i是与词汇表中每个对应单词匹配的embeddings_index中的行号的整数向量，如果没有匹配的单词，则为0。
- en: ➍ **Prepare a matrix of all zeros that we'll fill with the GloVe vectors.**
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ 准备一个全零矩阵，我们将用GloVe向量填充。
- en: ➎ **Fill entries in the matrix with the corresponding word vector. Row numbers
    of rows embedding_matrix corresponds to index positions of words in vocabulary.
    Words not found in the embedding index will be all zeros.**
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ 用相应的词向量填充矩阵中的条目。嵌入矩阵的行号对应于词汇表中单词的索引位置。在嵌入索引中找不到的单词将全部为零。
- en: '➏ **0s in indexes passed to [ for R arrays are ignored. For example: (1:10)[c(1,0,2,0,3)]
    returns c(1, 2, 3).**'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ R数组中传递给[的0将被忽略。例如：（1:10）[c(1,0,2,0,3)]返回c(1, 2, 3)。
- en: 'Finally, we use a initializer_constant() to load the pretrained embeddings
    in a layer_embedding(). So as not to disrupt the pretrained representations during
    training, we freeze the layer via trainable = FALSE:'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用initializer_constant()将预训练的嵌入加载到layer_embedding()中。为了在训练过程中不破坏预训练的表示，我们通过trainable
    = FALSE来冻结该层：
- en: embedding_layer <- layer_embedding(
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: embedding_layer <- layer_embedding(
- en: input_dim = max_tokens,
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: input_dim = max_tokens，
- en: output_dim = embedding_dim,
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim = embedding_dim，
- en: embeddings_initializer = initializer_constant(embedding_matrix),
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: embeddings_initializer = initializer_constant(embedding_matrix)，
- en: trainable = FALSE,
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: trainable = FALSE，
- en: mask_zero = TRUE
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: mask_zero = TRUE
- en: )
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: We’re now ready to train a new model—identical to our previous model, but leveraging
    the 100-dimensional pretrained GloVe embeddings instead of 128-dimensional learned
    embeddings.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练一个新模型 - 与我们之前的模型相同，但利用了100维的预训练GloVe嵌入，而不是128维的学习嵌入。
- en: '**Listing 11.20 Model that uses a pretrained embedding layer**'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表11.20 使用预训练嵌入层的模型**'
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 <- layer_input(shape(NA), dtype = "int64")
- en: embedded <- embedding_layer(inputs)
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入 <- embedding_layer(inputs)
- en: outputs <- embedded %>%
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- 嵌入层 %>%
- en: bidirectional(layer_lstm(units = 32)) %>%
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: bidirectional(layer_lstm(units = 32)) %>%
- en: layer_dropout(0.5) %>%
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, 激活函数 = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数 = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 = "准确率")
- en: model
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 模型
- en: '![Image](../images/f0365-01.jpg) ![Image](../images/f0366-01.jpg)'
  id: totrans-682
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0365-01.jpg) ![Image](../images/f0366-01.jpg)'
- en: callbacks <- list(
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数列表 <- list(
- en: callback_model_checkpoint("glove_embeddings_sequence_model.keras",
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("glove_embeddings_sequence_model.keras",
- en: save_best_only = TRUE)
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE)
- en: )
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>%
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 %>%
- en: fit(int_train_ds, validation_data = int_val_ds,
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: fit(int_train_ds, validation_data = int_val_ds,
- en: epochs = 10, callbacks = callbacks)
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 10, callbacks = callbacks)
- en: model <- load_model_tf("glove_embeddings_sequence_model.keras")
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 <- load_model_tf("glove_embeddings_sequence_model.keras")
- en: cat(sprintf(
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf(
- en: '"Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '"测试准确率：%.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
- en: 'Test acc: 0.877'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率：0.877
- en: You’ll find that on this particular task, pretrained embeddings aren’t very
    helpful, because the dataset contains enough samples that it is possible to learn
    a specialized enough embedding space from scratch. However, leveraging pretrained
    embeddings can be very helpful when you’re working with a smaller dataset.
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的任务中，你会发现预训练的嵌入并不是很有用，因为数据集包含足够的样本，可以从头开始学习一个足够专业的嵌入空间。然而，当你处理较小的数据集时，利用预训练的嵌入可能会非常有帮助。
- en: 11.4 The Transformer architecture
  id: totrans-695
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 transformers架构
- en: 'Starting in 2017, a new model architecture started overtaking recurrent neural
    networks across most natural language processing tasks: the Transformer. Transformers
    were introduced in the seminal paper “Attention Is All You Need” by Vaswani et
    al.^([2](#Rendnote2)) The gist of the paper is right there in the title: as it
    turned out, a simple mechanism called “neural attention” could be used to build
    powerful sequence models that didn’t feature any recurrent layers or convolution
    layers.'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: 从2017年开始，一种新的模型架构开始在大多数自然语言处理任务中取代循环神经网络：transformers。transformers是由Vaswani等人在开创性论文“Attention
    Is All You Need”中引入的。论文的要点就在标题中：事实证明，一个简单的叫做“神经注意力”的机制可以用来构建强大的序列模型，而不需要循环层或卷积层。
- en: This finding unleashed nothing short of a revolution in natural language processing
    and beyond. Neural attention has fast become one of the most influential ideas
    in deep learning. In this section, you’ll get an in-depth explanation of how it
    works and why it has proven so effective for sequence data. We’ll then leverage
    self-attention to create a Transformer encoder, one of the basic components of
    the Transformer architecture, and we’ll apply it to the IMDB movie review classification
    task.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 这一发现引发了自然语言处理领域乃至更广泛领域的一场革命。神经注意力已经迅速成为深度学习中最具影响力的思想之一。在本节中，你将深入了解它是如何工作以及为什么它对于序列数据如此有效。然后，我们将利用自注意力来创建一个transformers编码器，这是transformers架构的基本组件之一，并将其应用于IMDB电影评论分类任务。
- en: 11.4.1 Understanding self-attention
  id: totrans-698
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.1 理解自注意力
- en: 'As you’re going through this book, you may be skimming some parts and attentively
    reading others, depending on what your goals or interests are. What if your models
    did the same? It’s a simple yet powerful idea: not all input information seen
    by a model is equally important to the task at hand, so models should “pay more
    attention” to some features and “pay less attention” to other features. Does that
    sound familiar? You’ve already encountered a similar concept twice in this book:'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 当你阅读这本书时，你可能会快速浏览某些部分，而对其他部分进行仔细阅读，这取决于你的目标或兴趣是什么。如果你的模型也是这样做呢？这是一个简单但强大的想法：模型看到的所有输入信息对于手头的任务来说并不都是同等重要的，所以模型应该“更加关注”某些特征，而“更少关注”其他特征。这听起来熟悉吗？在这本书中你已经两次遇到了类似的概念：
- en: 'Max pooling in convnets looks at a pool of features in a spatial region and
    selects just one feature to keep. That’s an “all or nothing” form of attention:
    keep the most important feature and discard the rest.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷积神经网络中的最大池化操作会查看空间区域内的一组特征，并选择保留其中的一个特征。这是一种“全有或全无”的注意形式：保留最重要的特征，丢弃其余的。
- en: TF-IDF normalization assigns importance scores to tokens based on how much information
    different tokens are likely to carry. Important tokens are boosted while irrelevant
    tokens are faded out. That’s a continuous form of attention.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-IDF 归一化根据不同单词可能携带的信息量为单词分配重要性分数。重要的单词得到增强，而不相关的单词被淡化。这是一种持续的注意形式。
- en: There are many different forms of attention you could imagine, but they all
    start by computing importance scores for a set of features, with higher scores
    for more relevant features and lower scores for less relevant ones (see [figure
    11.5](#fig11-5)). How these scores should be computed, and what you should do
    with them, will vary from approach to approach.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象许多不同形式的注意力，但它们都是从计算一组特征的重要性分数开始的，对于更相关的特征得分较高，对于不太相关的特征得分较低（参见[图 11.5](#fig11-5)）。如何计算这些分数，以及如何处理它们，将根据不同方法而异。
- en: '![Image](../images/f0367-01.jpg)'
  id: totrans-703
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0367-01.jpg)'
- en: '**Figure 11.5 The general concept of “attention” in deep learning: Input features
    are assigned “attention scores,” which can be used to inform the next representation
    of the input.**'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.5 深度学习中“注意力”的一般概念：输入特征被赋予“注意力分数”，这些分数可以用来指导输入的下一个表示。**'
- en: 'Crucially, this kind of attention mechanism can be used for more than just
    highlighting or erasing certain features. It can be used to make features *context
    aware*. You’ve just learned about word embeddings: vector spaces that capture
    the “shape” of the semantic relationships between different words. In an embedding
    space, a single word has a fixed position—a fixed set of relationships with every
    other word in the space. But that’s not quite how language works: the meaning
    of a word is usually context specific. When you mark the date, you’re not talking
    about the same “date” as when you go on a date, nor is it the kind of date you’d
    buy at the market. When you say, “I’ll see you soon,” the meaning of the word
    “see” is subtly different from the “see” in “I’ll see this project to its end”
    or “I see what you mean.” And, of course, the meaning of pronouns like “he,” “it,”
    “you,” and so on is entirely sentence specific and can even change multiple times
    within a single sentence.'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，这种注意机制不仅可以用于突出或消除某些特征，还可以用于使特征*具有上下文意识*。你刚刚了解了单词嵌入：捕捉不同单词之间“形状”的语义关系的向量空间。在嵌入空间中，一个单词有一个固定的位置——与空间中的每个其他单词的一组固定关系。但这并不完全符合语言的工作方式：单词的含义通常是上下文特定的。当你标记日期时，你说的“日期”和你约会时的“日期”不同，也不是你在市场上买到的那种日期。当你说“我很快就会见到你”时，单词“见”在“我会把这个项目进行到底”或“我明白你的意思”中的含义略有不同。当然，“他”、“它”、“你”等代词的含义完全是句子特定的，甚至可以在一个句子中多次改变。
- en: 'Clearly, a smart embedding space would provide a different vector representation
    for a word depending on the other words surrounding it. That’s where *self-attention*
    comes in. The purpose of self-attention is to modulate the representation of a
    token by using the representations of related tokens in the sequence. This produces
    context-aware token representations. Consider an example sentence: “The train
    left the station on time.” Now, consider one word in the sentence: station. What
    kind of station are we talking about? Could it be a radio station? Maybe the International
    Space Station? Let’s figure it out algorithmically via self-attention (see [figure
    11.6](#fig11-6)).'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，一个智能的嵌入空间会根据周围的其他单词为一个单词提供不同的向量表示。这就是*自注意力*的作用所在。自注意力的目的是通过使用序列中相关单词的表示来调节一个标记的表示。这产生了具有上下文意识的标记表示。考虑一个例句：“火车准时离开了车站。”现在，考虑句子中的一个词：车站。我们在谈论什么样的车站？可能是广播电台吗？也许是国际空间站？让我们通过自注意力算法来算出（参见[图
    11.6](#fig11-6)）。
- en: '![Image](../images/f0368-01.jpg)'
  id: totrans-707
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0368-01.jpg)'
- en: '**Figure 11.6 Self-attention: Attention scores are computed between “station”
    and every other word in the sequence, and they are then used to weight a sum of
    word vectors that becomes the new “station” vector.**'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.6 自注意力：计算“站”与序列中每个其他单词之间的注意力分数，然后用它们加权一组单词向量，这成为新的“站”向量。**'
- en: Step 1 is to compute relevancy scores between the vector for “station” and every
    other word in the sentence. These are our “attention scores.” We’re simply going
    to use the dot product between two word vectors as a measure of the strength of
    their relationship. It’s a very computationally efficient distance function, and
    it was already the standard way to relate two word embeddings to each other long
    before Transformers. In practice, these scores will also go through a scaling
    function and a softmax, but for now, that’s just an implementation detail.
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是计算“station”向量与句子中每个其他单词之间的相关性分数。这些是我们的“注意力分数”。我们简单地使用两个单词向量之间的点积作为衡量它们关系强度的指标。这是一种非常高效的计算距离函数，而且在
    Transformers 之前它已经是将两个词嵌入彼此相关联的标准方式。实际上，这些分数还将通过一个缩放函数和一个 softmax，但现在，这只是一个实现细节。
- en: 'Step 2 is to compute the sum of all word vectors in the sentence, weighted
    by our relevancy scores. Words closely related to “station” will contribute more
    to the sum (including the word “station” itself), whereas irrelevant words will
    contribute almost nothing. The resulting vector is our new representation for
    “station”: a representation that incorporates the surrounding context. In particular,
    it includes part of the “train” vector, clarifying that it is, in fact, a “train
    station.”'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是计算句子中所有单词向量的加权和，权重由我们的相关性分数决定。与“station”密切相关的单词将更多地 contribute to the sum（包括单词“station”本身），而不相关的单词将几乎不贡献任何内容。得到的向量是我们对“station”的新表示：一种包含周围上下文的表示。特别是，它包括“train”向量的一部分，澄清了它实际上是“火车站”。
- en: 'You’d repeat this process for every word in the sentence, producing a new sequence
    of vectors encoding the sentence. Let’s see it in R-like pseudocode:'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 对于句子中的每个单词，您需要重复此过程，生成一个编码句子的新向量序列。让我们用类似 R 的伪代码来看一下：
- en: self_attention <- function(input_sequence) {
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: self_attention <- function(input_sequence) {
- en: c(sequence_len, embedding_size) %<-% dim(input_sequence)
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: c(sequence_len, embedding_size) %<-% dim(input_sequence)
- en: output <- array(0, dim(input_sequence))
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: output <- array(0, dim(input_sequence))
- en: for (i in 1:sequence_len) {➊
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in 1:sequence_len) {➊
- en: pivot_vector <- input_sequence[i, ]
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: pivot_vector <- input_sequence[i, ]
- en: scores <- sapply(1:sequence_len, function(j) ➋
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: scores <- sapply(1:sequence_len, function(j) ➋
- en: pivot_vector %*% input_sequence[j, ])➌
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: pivot_vector %*% input_sequence[j, ])➌
- en: scores <- softmax(scores / sqrt(embedding_size))➍
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: scores <- softmax(scores / sqrt(embedding_size))➍
- en: broadcast_scores <
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: broadcast_scores <
- en: as.matrix(scores)[, rep(1, embedding_size)]➎
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: as.matrix(scores)[, rep(1, embedding_size)]➎
- en: new_pivot_representation <
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: new_pivot_representation <
- en: colSums(input_sequence * broadcast_scores)➏
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: colSums(input_sequence * broadcast_scores)➏
- en: output[i, ] <- new_pivot_representation
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: output[i, ] <- new_pivot_representation
- en: '}'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: output
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '}'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: softmax <- function(x) {
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: softmax <- function(x) {
- en: e <- exp(x - max(x))
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: e <- exp(x - max(x))
- en: e / sum(e)
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: e / sum(e)
- en: '}'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Iterate over each token in the input sequence.**
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **遍历输入序列中的每个标记。**
- en: ➋ **Compute the dot product (attention score) between the token and every other
    token.**
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **计算标记与每个其他标记之间的点积（注意力分数）。**
- en: ➌ **%*% with two 1D vectors returns a scalar, the dot product. scores has shape
    (sequence_len).**
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **%*% 用于两个 1D 向量返回一个标量，即点积。scores 的形状为 (sequence_len)。**
- en: ➍ **Scale by a normalization factor, and apply a softmax.**
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **通过一个标准化因子进行缩放，并应用 softmax。**
- en: '➎ **Broadcast the scores vector (shape: (sequence_len)) into a matrix of shape
    (sequence_len, embedding_size), the shape of input_sequence.**'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **将分数向量（形状为 (sequence_len)）广播成一个形状为 (sequence_len, embedding_size) 的矩阵，即 input_sequence
    的形状。**
- en: ➏ **Sum the score-adjusted input sequences to make a new embedding vector.**
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **将得分调整后的输入序列求和以生成一个新的嵌入向量。**
- en: 'Of course, in practice you’d use a vectorized implementation. Keras has a built-in
    layer to handle it: layer_multi_head_attention(). Here’s how you would use it:'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在实践中，您会使用矢量化实现。Keras 有一个内置层来处理它：layer_multi_head_attention()。这是您如何使用它的方法：
- en: num_heads <- 4
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads <- 4
- en: embed_dim <- 256
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: mha_layer <- layer_multi_head_attention(num_heads = num_heads,
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: mha_layer <- layer_multi_head_attention(num_heads = num_heads,
- en: key_dim = embed_dim)
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: key_dim = embed_dim)
- en: outputs <- mha_layer(inputs, inputs, inputs)➊
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- mha_layer(输入, 输入, 输入)➊
- en: ➊ **inputs has shape (batch_size, sequence_length, embed_dim).**
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入的形状为 (batch_size, sequence_length, embed_dim)。**
- en: 'Reading this, you’re probably wondering:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 读到这里，您可能会想：
- en: Why are we passing the inputs to the layer *three* times? That seems redundant.
  id: totrans-746
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们要将输入传递给该层*三*次？这似乎是多余的。
- en: What are these “multiple heads” we’re referring to? That sounds intimidating—
    do they also grow back if you cut them?
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些“多个头”是什么？听起来很吓人 —— 如果您把它们剪掉，它们也会再长出来吗？
- en: Both of these questions have simple answers. Let’s take a look.
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题都有简单的答案。让我们来看看。
- en: 'GENERALIZED SELF-ATTENTION: THE QUERY-KEY-VALUE MODEL'
  id: totrans-749
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 泛化的自注意力：查询-键-值模型
- en: 'So far, we have considered only one input sequence. However, the Transformer
    architecture was originally developed for machine translation, where you have
    to deal with two input sequences: the source sequence you’re currently translating
    (such as “How’s the weather today?”), and the target sequence you’re converting
    it to (such as “¿Qué tiempo hace hoy?”). A Transformer is a *sequence-to-sequence*
    model: it was designed to convert one sequence into another. You’ll learn about
    sequence-to-sequence models in depth later in this chapter.'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了一个输入序列。然而，Transformer 架构最初是为机器翻译开发的，在那里你必须处理两个输入序列：你当前正在翻译的源序列（如“今天天气如何？”），以及你将其转换为的目标序列（如“¿Qué
    tiempo hace hoy?”）。Transformer 是一个*序列到序列*模型：它被设计用来将一个序列转换为另一个序列。你将在本章后面深入学习有关序列到序列模型的内容。
- en: 'Now let’s take a step back. The self-attention mechanism as we’ve introduced
    it performs the following, schematically:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们退一步。就像我们介绍的那样，自注意力机制执行以下操作，概括地说：
- en: '![Image](../images/f0370-01.jpg)'
  id: totrans-752
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0370-01.jpg)'
- en: 'This means “for each token in inputs (A), compute how much the token is related
    to every token in inputs (B), and use these scores to weight a sum of tokens from
    inputs (C).” Crucially, there’s nothing that requires A, B, and C to refer to
    the same input sequence. In the general case, you could be doing this with three
    different sequences. We’ll call them “query,” “keys,” and “values.” The operation
    becomes “for each element in the query, compute how much the element is related
    to every key, and use these scores to weight a sum of values”:'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着“对于输入（A）中的每个标记，计算标记与输入（B）中的每个标记的关联程度，并使用这些分数对输入（C）中的标记进行加权求和。”关键是，没有什么需要A、B和C引用相同的输入序列。在一般情况下，你可以用三个不同的序列来完成这个操作。我们称它们为“查询”，“键”和“值”。操作变成了“对于查询中的每个元素，计算该元素与每个键的关联程度，并使用这些分数对值进行加权求和”：
- en: outputs <- sum( **values** * pairwise_scores( **query**, **keys** ))
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- sum( **值** * 两两得分（ **查询**， **键** ）)
- en: This terminology comes from search engines and recommender systems (see [figure
    11.7](#fig11-7)). Imagine that you’re typing up a query to retrieve a photo from
    your collection, “dogs on the beach.” Internally, each of your pictures in the
    database is described by a set of keywords—“cat,” “dog,” “party,” and so forth.
    We’ll call those “keys.” The search engine will start by comparing your query
    to the keys in the database. “Dog” yields a match of 1, and “cat” yields a match
    of 0\. It will then rank those keys by strength of match—relevance—and it will
    return the pictures associated with the top *N* matches, in order of relevance.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 这个术语来自搜索引擎和推荐系统（参见[图11.7](#fig11-7)）。想象一下，你正在输入一个查询，以从你的收藏中检索一张照片，“海滩上的狗”。在内部，数据库中的每张图片都由一组关键词描述——“猫”，“狗”，“派对”等等。我们将这些称为“键”。搜索引擎将首先将您的查询与数据库中的键进行比较。“狗”产生1个匹配，“猫”产生0个匹配。然后它将根据匹配的强度——相关性对这些键进行排名，并以相关性顺序返回与前*N*个匹配关联的图片。
- en: '![Image](../images/f0371-01.jpg)'
  id: totrans-756
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0371-01.jpg)'
- en: '**Figure 11.7 Retrieving images from a database: The “query” is compared to
    a set of “keys,” and the match scores are used to rank “values” (images).**'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.7 从数据库检索图像：将“查询”与一组“键”进行比较，并使用匹配得分对“值”（图像）进行排名。**'
- en: 'Conceptually, this is what Transformer-style attention is doing. You’ve got
    a reference sequence that describes something you’re looking for: the query. You’ve
    got a body of knowledge that you’re trying to extract information from: the values.
    Each value is assigned a key that describes the value in a format that can be
    readily compared to a query. You simply match the query to the keys. Then you
    return a weighted sum of values.'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 在概念上，这就是 Transformer 风格的注意力在做的事情。你有一个描述你正在寻找的东西的参考序列：查询。你有一个你想从中提取信息的知识体系：值。每个值被分配一个键，描述了值以一种可以与查询轻松比较的格式。你只需将查询与键匹配即可。然后返回值的加权总和。
- en: 'In practice, the keys and the values are often the same sequence. In machine
    translation, for instance, the query would be the target sequence, and the source
    sequence would play the roles of both keys and values: for each element of the
    target (like “tiempo”), you want to go back to the source (“How’s the weather
    today?”) and identify the different bits that are related to it (“tiempo” and
    “weather” should have a strong match). And naturally, if you’re just doing sequence
    classification, then query, keys, and values are all the same: you’re comparing
    a sequence to itself, to enrich each token with context from the whole sequence.'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，键和值通常是相同的序列。例如，在机器翻译中，查询将是目标序列，而源序列将扮演键和值的角色：对于目标的每个元素（比如“tiempo”），你希望回到源头（“今天天气如何？”）并识别与之相关的不同部分（“tiempo”和“weather”应该有很强的匹配）。自然地，如果你只是进行序列分类，那么查询、键和值都是相同的：你正在将一个序列与自身进行比较，以丰富每个标记的上下文。
- en: That explains why we needed to pass inputs three times to our layer_multi_ head_attention()
    layer. But why “multi-head” attention?
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 这解释了为什么我们需要将输入传递三次到我们的layer_multi_ head_attention()层。但为什么要“多头”注意力呢？
- en: 11.4.2 Multi-head attention
  id: totrans-761
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.2 多头注意力
- en: '“Multi-head attention” is an extra tweak to the self-attention mechanism, introduced
    in “Attention Is All You Need.” The “multi-head” moniker refers to the fact that
    the output space of the self-attention layer is factored into a set of independent
    subspaces, learned separately: the initial query, key, and value are sent through
    three independent sets of dense projections, resulting in three separate vectors.
    Each vector is processed via neural attention, and the three outputs are concatenated
    back into a single output sequence. Each such subspace is called a “head.” The
    full picture is shown in [figure 11.8](#fig11-8).'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: “多头注意力”是自注意力机制的一项额外调整，由“Attention Is All You Need”引入。 “多头”这个名字指的是自注意力层的输出空间被分解为一组独立的子空间，分别学习：初始查询、键和值通过三组独立的密集投影发送，生成三个单独的向量。每个向量通过神经注意力进行处理，三个输出被串联回一个单一的输出序列。这样的子空间被称为“头”。全貌如
    [图11.8](#fig11-8) 所示。
- en: '![Image](../images/f0372-01.jpg)'
  id: totrans-763
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0372-01.jpg)'
- en: '**Figure 11.8 The MultiHeadAttention layer**'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.8 多头注意力图层**'
- en: The presence of the learnable dense projections enables the layer to actually
    learn something, as opposed to being a purely stateless transformation that would
    require additional layers before or after it to be useful. In addition, having
    independent heads helps the layer learn different groups of features for each
    token, where features within one group are correlated with each other but are
    mostly independent from features in a different group.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 通过可学习的密集投影的存在，该层实际上可以学到一些东西，而不是成为一个纯粹的状态转换，需要额外的层在其之前或之后才能变得有用。此外，拥有独立的头部可以帮助该层学习每个标记的不同特征组，其中一个组内的特征彼此相关，但与另一个组内的特征大部分是独立的。
- en: 'This is similar in principle to what makes depthwise-separable convolutions
    work: in a depthwise-separable convolution, the output space of the convolution
    is factored into many subspaces (one per input channel) that are learned independently.
    The “Attention Is All You Need” paper was written at a time when the idea of factoring
    feature spaces into independent subspaces had been shown to provide great benefits
    for computer vision models, both in the case of depthwise-separable convolutions
    and in the case of a closely related approach, *grouped convolutions*. Multi-head
    attention is simply the application of the same idea to self-attention.'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 这与深度可分离卷积的工作原理相似：在深度可分离卷积中，卷积的输出空间被分解为许多独立的子空间（每个输入通道一对一），它们是独立学习的。《Attention
    Is All You Need》一文是在已经表明将特征空间分解为独立子空间提供了计算机视觉模型极大收益的时候编写的，无论是深度可分离卷积的情况还是与之密切相关的一种方法，即*分组卷积*。多头注意力只是将相同的想法应用到自注意力中。
- en: 11.4.3 The Transformer encoder
  id: totrans-767
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.3 Transformer编码器
- en: 'If adding extra dense projections is so useful, why don’t we also apply one
    or two to the output of the attention mechanism? Actually, that’s a great idea—let’s
    do that. And our model is starting to do a lot, so we might want to add residual
    connections to make sure we don’t destroy any valuable information along the way;
    you learned in chapter 9 that they’re a must for any sufficiently deep architecture.
    And there’s another thing you learned in chapter 9: normalization layers are supposed
    to help gradients flow better during backpropagation. Let’s add those, too.'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: 如果添加额外的密集投影如此有用，为什么我们不将其应用到注意力机制的输出上呢？实际上，这是一个绝佳的主意—让我们这样做。我们的模型开始做很多事情，所以我们可能想要添加残差连接，以确保我们不会在途中破坏任何有价值的信息；你在第9章中学到的，对于任何足够深的架构来说，这是非常必要的。还有一件事情是你在第9章中学到的：归一化层应该有助于梯度在反向传播期间更好地流动。让我们也把这些添加进来。
- en: That’s roughly the thought process that I imagine unfolded in the minds of the
    inventors of the Transformer architecture at the time. Factoring outputs into
    multiple independent spaces, adding residual connections, adding normalization
    layers—all of these are standard architecture patterns that one would be wise
    to leverage in any complex model. Together, these bells and whistles form the
    Transformer encoder—one of two critical parts that make up the Transformer architecture
    (see [figure 11.9](#fig11-9)).
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 我大致想象当时transformers架构的发明者们心中展开的思维过程。将输出因子化为多个独立的空间，添加残差连接，添加归一化层—所有这些都是一种明智之举，可以在任何复杂模型中加以利用的标准架构模式。这些花哨且实用的部件汇聚在一起形成transformers编码器—构成transformers架构的两个关键部分之一，请见[图11.9](#fig11-9)。
- en: 'The original Transformer architecture consists of two parts: a *Transformer
    encoder* that processes the source sequence, and a *Transformer decoder* that
    uses the source sequence to generate a translated version. You’ll learn about
    the decoder part in a minute.'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的transformers架构由两部分组成：一个*transformers编码��*用于处理源序列，一个*transformers解码器*利用源序列生成翻译版本。你马上就会了解解码器部分。
- en: Crucially, the encoder part can be used for text classification. It’s a very
    generic module that ingests a sequence and learns to turn it into a more useful
    representation. Let’s implement a Transformer encoder and try it on the movie
    review sentiment classification task.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 至关重要的是，编码器部分可以用于文本分类。这是一个非常通用的模块，接受一个序列并学习将其转化为更有用的表示。让我们实现一个transformers编码器，并在电影评论情感分类任务上进行尝试。
- en: '![Image](../images/f0373-01.jpg)'
  id: totrans-772
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0373-01.jpg)'
- en: '**Figure 11.9 The Transformer-Encoder chains a layer_multi_ head_attention()
    with a dense projection and adds normalization as well as residual connections.**'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.9 transformers编码器通过将一个`layer_multi_head_attention()`连接到一个密集投影，并添加归一化以及残差连接。**'
- en: '**Listing 11.21 Transformer encoder implemented as a subclassed Layer**'
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: '**清单 11.21 作为子类Layer实现的transformers编码器**'
- en: layer_transformer_encoder <- new_layer_class(
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_encoder <- new_layer_class(
- en: classname = "TransformerEncoder",
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 类名 = "TransformerEncoder",
- en: initialize = function(embed_dim, dense_dim, num_heads, …) {
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(embed_dim, dense_dim, num_heads, …) {
- en: super$initialize(…)
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize(…)
- en: self$embed_dim <- embed_dim ➊
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: self$embed_dim <- embed_dim ➊
- en: self$dense_dim <- dense_dim ➋
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense_dim <- dense_dim ➋
- en: self$num_heads <- num_heads ➌
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: self$num_heads <- num_heads ➌
- en: self$attention <
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: self$attention <
- en: layer_multi_head_attention(num_heads = num_heads,
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: layer_multi_head_attention(num_heads = num_heads,
- en: key_dim = embed_dim)
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: key_dim = embed_dim)
- en: self$dense_proj <- keras_model_sequential() %>%
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense_proj <- keras_model_sequential() %>%
- en: layer_dense(dense_dim, activation = "relu") %>%
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(dense_dim, activation = "relu") %>%
- en: layer_dense(embed_dim)
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(embed_dim)
- en: self$layernorm_1 <- layer_layer_normalization()
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_1 <- layer_layer_normalization()
- en: self$layernorm_2 <- layer_layer_normalization()
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_2 <- layer_layer_normalization()
- en: '},'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs, mask = NULL) {➍
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs, mask = NULL) {➍
- en: if (!is.null(mask))➎
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: if (!is.null(mask))➎
- en: mask <- mask[, tf$newaxis, ]➎
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: mask <- mask[, tf$newaxis, ]➎
- en: inputs %>%
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 %>%
- en: '{ self$attention(., ., attention_mask = mask) + . } %>% ➏'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '{ self$attention(., ., attention_mask = mask) + . } %>% ➏'
- en: self$layernorm_1() %>%
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_1() %>%
- en: '{ self$dense_proj(.) + . } %>% ➐'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '{ self$dense_proj(.) + . } %>% ➐'
- en: self$layernorm_2()
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_2()
- en: '},'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: get_config = function() { ➑
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: get_config = function() { ➑
- en: config <- super$get_config()
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: config <- super$get_config()
- en: for(name in c("embed_dim", "num_heads", "dense_dim"))
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: for(name in c("embed_dim", "num_heads", "dense_dim"))
- en: config[[name]] <- self[[name]]
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: config[[name]] <- self[[name]]
- en: config
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: config
- en: '}'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **Size of the input token vectors**
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入令牌向量的大小**
- en: ➋ **Size of the inner dense layer**
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **内部密集层的大小**
- en: ➌ **Number of attention heads**
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **注意力头的数量**
- en: ➍ **Computation goes in call()**
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '➍ **计算发生在call()中** '
- en: ➎ **The mask that will be generated by the embedding layer will be 2D, but the
    attention layer expects it to be 3D or 4D, so we expand its rank.**
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **由嵌入层生成的遮罩将是 2D 的，但注意力层期望它是 3D 或 4D 的，因此我们扩展其秩。**
- en: ➏ **Add residual connection to output of attention layer.**
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **在注意力层的输出中添加残差连接。**
- en: ➐ **Add residual connection to output of the dense_proj() layer.**
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **在 dense_proj() 层的输出中添加残差连接。**
- en: ➑ **Implement serialization so we can save the model.**
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **实现序列化，以便我们可以保存模型。**
- en: '**%>% and { }**'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '**%>% 和 { }**'
- en: 'In the above example, we pipe with %>% into an expression that is wrapped with
    { }. This is an advanced feature of %>%, which allows you to pipe into complex
    or compound expressions. %>% will place in the piped argument to each location
    we request with the . symbol. For example:'
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的示例中，我们使用 %>% 将其传递到用 { } 包装的表达式中。这是 %> 的高级功能，它允许您将管道传递到复杂或复合表达式中。%>% 将在我们使用
    . 符号请求的每个位置放置管道参数。例如：
- en: x %>% { fn(., .) + . }
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: x %>% { fn(., .) + . }
- en: 'is equivalent to:'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: 等同于：
- en: fn(x, x) + x
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: fn(x, x) + x
- en: 'If we were to write the call() method of layer_transformer_encoder() without
    %>%, it would look like this:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们编写 layer_transformer_encoder() 的 call() 方法而不使用 %>%，它将如下所示：
- en: call = function(inputs, mask = NULL) {
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs, mask = NULL) {
- en: if (!is.null(mask))
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 (!is.null(mask))
- en: mask <- mask[, tf$newaxis, ]
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: mask <- mask[, tf$newaxis, ]
- en: attention_output <- self$attention(inputs, inputs,
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: attention_output <- self$attention(inputs, inputs,
- en: attention_mask = mask)
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: attention_mask = mask)
- en: proj_input <- self$layernorm_1(inputs + attention_output)
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: proj_input <- self$layernorm_1(inputs + attention_output)
- en: proj_output <- self$dense_proj(proj_input)
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: proj_output <- self$dense_proj(proj_input)
- en: self$layernorm_2(proj_input + proj_output)
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_2(proj_input + proj_output)
- en: '}'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '**Saving custom layers**'
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: '**保存自定义层**'
- en: 'When you write custom layers, make sure to implement the get_config() method:
    this enables the layer to be reinstantiated from its config, which is useful during
    model saving and loading. The method should return a named R list that contains
    the values of the constructor arguments used to create the layer.'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 当您编写自定义层时，请确保实现 get_config() 方法：这使得可以从其配置重新实例化该层，在模型保存和加载过程中非常有用。该方法应返回一个命名的
    R 列表，其中包含用于创建层的构造函数参数的值。
- en: 'All Keras layers can be serialized and deserialized as follows:'
  id: totrans-832
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Keras 层都可以如下序列化和反序列化：
- en: config <- layer$get_config()
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: config <- layer$get_config()
- en: new_layer <- do.call(layer_<type>, config)
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: new_layer <- do.call(layer_<type>, config)
- en: 'where layer_<type> is the original layer constructor. For example:'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 layer_<type> 是原始层构造函数。例如：
- en: layer <- layer_dense(units = 10)
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: layer <- layer_dense(units = 10)
- en: config <- layer$get_config() ➊
  id: totrans-837
  prefs: []
  type: TYPE_NORMAL
  zh: config <- layer$get_config() ➊
- en: new_layer <- do.call(layer_dense, config)➋
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: new_layer <- do.call(layer_dense, config)➋
- en: ➊ **config is a regular named R list. You can safely save it to disk as an rds,
    then load it in a new R session.**
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **config 是一个常规的命名 R 列表。您可以将其安全地保存到磁盘上作为 rds，然后在新的 R 会话中加载它。**
- en: ➋ **The config does not contain weight values, so all weights in the layer are
    initialized from scratch.**
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **配置不包含权重值，因此层中的所有权重都将从头开始初始化。**
- en: 'You can also access the unwrapped original layer constructor from any existing
    layer directly via the special symbol __class__ (though you rarely need to do
    so):'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过特殊符号 __class__ 直接从任何现有层访问未包装的原始层构造函数（尽管您很少需要这样做）：
- en: layer$`__class__`
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: layer$`__class__`
- en: <class ‘keras.layers.core.dense.Dense’>
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: <class ‘keras.layers.core.dense.Dense’>
- en: new_layer <- layer$`__class__`$from_config(config)
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: new_layer <- layer$`__class__`$from_config(config)
- en: 'Defining the get_config() method in custom layer classes enables the same work-flow.
    For instance:'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 在自定义层类中定义 get_config() 方法会启用相同的工作流程。例如：
- en: layer <- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: layer <- layer_transformer_encoder(embed_dim = 256, dense_dim = 32,
- en: num_heads = 2)
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads = 2)
- en: config <- layer$get_config()
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: config <- layer$get_config()
- en: new_layer <- do.call(layer_transformer_encoder, config)
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: new_layer <- do.call(layer_transformer_encoder, config)
- en: -- or --
  id: totrans-850
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: -- 或 --
- en: new_layer <- layer$`__class__`$from_config(config)
  id: totrans-851
  prefs: []
  type: TYPE_NORMAL
  zh: new_layer <- layer$`__class__`$from_config(config)
- en: 'When saving a model that contains custom layers, the saved file will contain
    these configs. When loading the model from the file, you should provide the custom
    layer classes to the loading process, so that it can make sense of the config
    objects:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 当保存包含自定义层的模型时，保存的文件将包含这些配置。在从文件加载模型时，您应该向加载过程提供自定义层类，以便它可以理解配置对象：
- en: model <- save_model_tf(model, filename)
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: model <- save_model_tf(model, filename)
- en: model <- load_model_tf(filename,
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf(filename,
- en: custom_objects = list(layer_transformer_encoder))
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: custom_objects = list(layer_transformer_encoder))
- en: 'Note that if the list supplied to custom_objects is named, then names are matched
    to the classname argument that was provided when the custom object was constructed:'
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果 custom_objects 列表中提供的列表具有名称，则名称将与构建自定义对象时提供的 classname 参数进行匹配：
- en: model <- load_model_tf(
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf(
- en: filename,
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: filename,
- en: custom_objects = list(TransformerEncoder = layer_transformer_encoder))
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: custom_objects = list(TransformerEncoder = layer_transformer_encoder))
- en: 'You’ll note that the normalization layers we’re using here aren’t layer_batch_
    normalization() like those we’ve used before in image models. That’s because layer_
    batch_normalization() doesn’t work well for sequence data. Instead, we’re using
    the layer_layer_normalization(), which normalizes each sequence independently
    from other sequences in the batch. Like this, in R pseudocode:'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到，我们在这里使用的归一化层不是像之前在图像模型中使用的 layer_batch_normalization() 那样的层。那是因为 layer_batch_normalization()
    不适用于序列数据。相反，我们使用 layer_layer_normalization()，它将每个序列与批次中的其他序列独立地进行归一化。在 R 的伪代码中就像这样：
- en: layer_normalization <- function(batch_of_sequences) {
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: layer_normalization <- function(batch_of_sequences) {
- en: c(batch_size, sequence_length, embedding_dim) %<-%
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: c(batch_size, sequence_length, embedding_dim) %<-%
- en: dim(batch_of_sequences)➊
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: dim(batch_of_sequences)➊
- en: means <- variances <-
  id: totrans-864
  prefs: []
  type: TYPE_NORMAL
  zh: means <- variances <-
- en: array(0, dim = dim(batch_of_sequences))
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: array(0, dim = dim(batch_of_sequences))
- en: for (b in seq(batch_size))
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: for (b in seq(batch_size))
- en: for (s in seq(sequence_length)) {
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: for (s in seq(sequence_length)) {
- en: embedding <- batch_of_sequences[b, s, ]➋
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: embedding <- batch_of_sequences[b, s, ]➋
- en: means[b, s, ] <- mean(embedding)
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: means[b, s, ] <- mean(embedding)
- en: variances[b, s, ] <- var(embedding)
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: variances[b, s, ] <- var(embedding)
- en: '}'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: (batch_of_sequences - means) / variances
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: (batch_of_sequences - means) / variances
- en: '}'
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '➊ **Input shape: (batch_size, sequence_length, embedding_dim)**'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入形状：(batch_size, sequence_length, embedding_dim)**
- en: ➋ **To compute mean and variance, we pool data only over the last axis (axis
    -1, the embedding axis).**
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **要计算均值和方差，我们仅在最后一个轴（轴 -1，即嵌入轴）上汇总数据。**
- en: 'Compare to layer_batch_normalization() (during training):'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 与 layer_batch_normalization()（在训练期间）进行比较：
- en: batch_normalization <- function(batch_of_images) {
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: batch_normalization <- function(batch_of_images) {
- en: c(batch_size, height, width, channels) %<-%
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: c(batch_size, height, width, channels) %<-%
- en: dim(batch_of_images) ➊
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: dim(batch_of_images) ➊
- en: means <- variances <-
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: means <- variances <-
- en: array(0, dim = dim(batch_of_images))
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: array(0, dim = dim(batch_of_images))
- en: for (ch in seq(channels)) {
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: for (ch in seq(channels)) {
- en: channel <- batch_of_images[, , , ch]➋
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: channel <- batch_of_images[, , , ch]➋
- en: means[, , , ch] <- mean(channel)
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: means[, , , ch] <- mean(channel)
- en: variances[, , , ch] <- var(channel)
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: variances[, , , ch] <- var(channel)
- en: '}'
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: (batch_of_images - means) / variances
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: (batch_of_images - means) / variances
- en: '}'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '➊ **Input shape: (batch_size, height, width, channels)**'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **输入形状：(batch_size, height, width, channels)**
- en: ➋ **Pool the data over the batch axis (the first axis), which creates interactions
    between samples in a batch.**
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在批次轴（第一个轴）上汇总数据，这会在批次中的样本之间创建交互。**
- en: Although batch_normalization() collects information from many samples to obtain
    accurate statistics for the feature means and variances, layer_normalization()
    pools data within each sequence separately, which is more appropriate for sequence
    data.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 batch_normalization() 从许多样本中收集信息以获得特征均值和方差的准确统计数据，但 layer_normalization()
    在每个序列内部汇集数据，这对于序列数据更为合适。
- en: Now that we’ve implemented our TransformerEncoder, we can use it to assemble
    a text-classification model similar to the LSTM-based one you’ve seen previously.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了 TransformerEncoder，我们可以使用它来组装一个类似于您之前看到的基于 LSTM 的文本分类模型。
- en: '**Listing 11.22 Using the Transformer Encoder for text classification**'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.22 使用 Transformer 编码器进行文本分类**'
- en: vocab_size <- 20000
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_size <- 20000
- en: embed_dim <- 256
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: num_heads <- 2
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads <- 2
- en: dense_dim <- 32
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: dense_dim <- 32
- en: inputs <- layer_input(shape(NA), dtype = "int64")
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape(NA), dtype = "int64")
- en: outputs <- inputs %>%
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_embedding(vocab_size, embed_dim) %>%
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(vocab_size, embed_dim) %>%
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
- en: layer_global_average_pooling_1d() %>%➊
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: layer_global_average_pooling_1d() %>%➊
- en: layer_dropout(0.5) %>%
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <- keras_model(inputs, outputs)
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: model %>% compile(optimizer = "rmsprop",
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0377-01.jpg)'
  id: totrans-910
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0377-01.jpg)'
- en: ➊ **Because TransformerEncoder returns full sequences, we need to reduce each
    sequence to a single vector for classification via a global pooling layer.**
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **由于 TransformerEncoder 返回完整序列，因此我们需要通过全局汇集层将每个序列减少为单个向量进行分类。**
- en: Let’s train it. It gets to 88.5% test accuracy.
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行训练。它达到了 88.5% 的测试准确率。
- en: '**Listing 11.23 Training and evaluating the Transformer encoder–based model**'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 11.23 训练和评估基于 Transformer 编码器的模型**'
- en: callbacks = list(callback_model_checkpoint("transformer_encoder.keras",
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数 = 列表（callback_model_checkpoint("transformer_encoder.keras",
- en: save_best_only = TRUE))
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE))
- en: model %>% fit(
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: int_train_ds,
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: int_train_ds,
- en: validation_data = int_val_ds,
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 验证数据 = int_val_ds,
- en: epochs = 20,
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: callbacks = callbacks
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 回调函数 = 回调函数
- en: )
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf(
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf(
- en: '"transformer_encoder.keras",'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '"transformer_encoder.keras",'
- en: custom_objects = layer_transformer_encoder)➊
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: custom_objects = layer_transformer_encoder)➊
- en: 'sprintf("Test acc: %.3f", evaluate(model, int_test_ds)["accuracy"])'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: sprintf("测试准确率：%.3f", evaluate(model, int_test_ds)["accuracy"])
- en: '[1] "Test acc: 0.885"'
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "测试准确率：0.885"'
- en: ➊ **Provide the custom TransformerEncoder class to the model-loading process.**
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **为模型加载过程提供自定义 TransformerEncoder 类。**
- en: At this point, you should start to feel a bit uneasy. Something’s off here.
    Can you tell what it is?
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该开始感到有点不安。这里有点不对劲。你能说出是什么吗？
- en: This section is ostensibly about “sequence models.” I started off by highlighting
    the importance of word order. I said that Transformer was a sequence-processing
    architecture, originally developed for machine translation. And yet… the Transformer
    encoder you just saw in action wasn’t a sequence model at all. Did you notice?
    It’s composed of dense layers that process sequence tokens independently from
    each other, and an attention layer that looks at the tokens *as a set*. You could
    change the order of the tokens in a sequence, and you’d get the exact same pairwise
    attention scores and the exact same context-aware representations. If you were
    to completely scramble the words in every movie review, the model wouldn’t notice,
    and you’d still get the exact same accuracy. Self-attention is a set-processing
    mechanism, focused on the relationships between pairs of sequence elements (see
    [figure 11.10](#fig11-10))—it’s blind to whether these elements occur at the beginning,
    at the end, or in the middle of a sequence. So why do we say that Transformer
    is a sequence model? And how could it possibly be good for machine translation
    if it doesn’t look at word order?
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分表面上是关于“序列模型”的。我首先强调了词序的重要性。我说 Transformer 是一种序列处理架构，最初是为机器翻译而开发的。然而……你刚刚看到的
    Transformer 编码器根本不是序列模型。你注意到了吗？它由处理序列令牌的密集层和查看令牌 *作为集合* 的注意层组成。你可以改变序列中令牌的顺序，你会得到完全相同的成对注意分数和完全相同的上下文感知表示。如果你完全打乱每个电影评论中的单词，模型不会注意到，你仍然会得到完全相同的准确性。自注意力是一种集合处理机制，专注于序列元素对之间的关系（见[图
    11.10](#fig11-10)）—它对于这些元素是出现在序列的开始、结束还是中间是盲目的。那么我们为什么说 Transformer 是一个序列模型呢？它怎么可能适用于机器翻译，如果它不考虑词序呢？
- en: 'I hinted at the solution earlier in the chapter: I mentioned in passing that
    Transformer was a hybrid approach that is technically order agnostic but that
    manually injects order information in the representations it processes. This is
    the missing ingredient! It’s called *positional encoding*. Let’s take a look.'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: 我在本章前面已经暗示了解决方案：我顺便提到了 Transformer 是一种技术上无序的混合方法，但在处理其表示时手动注入顺序信息。这是缺失的要素！它被称为
    *位置编码*。让我们来看看。
- en: '![Image](../images/f0378-01.jpg)'
  id: totrans-931
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0378-01.jpg)'
- en: '**Figure 11.10 Features of different types of NLP models**'
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.10 不同类型 NLP 模型的特征**'
- en: USING POSITIONAL ENCODING TO REINJECT ORDER INFORMATION
  id: totrans-933
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用位置编码来重新注入顺序信息
- en: 'The idea behind positional encoding is very simple: to give the model access
    to word-order information, we’re going to add the word’s position in the sentence
    to each word embedding. Our input word embeddings will have two components: the
    usual word vector, which represents the word independently of any specific context,
    and a position vector, which represents the position of the word in the current
    sentence. Hopefully, the model will then figure out how to best leverage this
    additional information.'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码背后的想法非常简单：为了让模型访问词序信息，我们将在每个词嵌入中添加词在句子中的位置。我们的输入词嵌入将有两个组成部分：通常的词向量，表示独立于任何特定上下文的词，以及位置向量，表示词在当前句子中的位置。希望模型能够找出如何最好地利用这些额外信息。
- en: The simplest scheme you could come up with would be to concatenate the word’s
    position to its embedding vector. You’d add a “position” axis to the vector and
    fill it with 0 for the first word in the sequence, 1 for the second, and so on.
    That may not be ideal, however, because the positions can potentially be very
    large integers, which will disrupt the range of values in the embedding vector.
    As you know, neural networks don’t like very large input values, or discrete input
    distributions.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方案是将单词的位置连接到其嵌入向量中。您会为向量添加一个“位置”轴，并将其填充为 0（对应序列中的第一个单词）、1（对应序列中的第二个单词），依此类推。然而，这可能不是最理想的，因为位置可能是非常大的整数，这将扰乱嵌入向量中的值的范围。如您所知，神经网络不喜欢非常大的输入值或离散的输入分布。
- en: 'The original “Attention Is All You Need” paper used an interesting trick to
    encode word positions: it added to the word embeddings a vector containing values
    in the range [-1, 1] that varied cyclically depending on the position (it used
    cosine functions to achieve this). This trick offers a way to uniquely characterize
    any integer in a large range via a vector of small values. It’s clever, but it’s
    not what we’re going to use in our case. We’ll do something simpler and more effective:
    we’ll learn position-embedding vectors the same way we learn to embed word indices.
    We’ll then proceed to add our position embeddings to the corresponding word embeddings,
    to obtain a position-aware word embedding. This technique is called “positional
    embedding.” Let’s implement it.'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的“注意力就是你所需要的一切”论文使用了一个有趣的技巧来编码单词位置：它在单词嵌入中添加了一个向量，其中包含范围在[-1, 1]之间的值，这些值根据位置周期性地变化（它使用余弦函数来实现这一点）。这个技巧提供了一种通过一组小值的向量来唯一地表征大范围内的任何整数的方法。这很聪明，但不是我们要在这种情况下使用的。我们将做一些更简单和更有效的事情：我们将学习位置嵌入向量，就像我们学习嵌入单词索引一样。然后，我们将继续将我们的位置嵌入添加到相应的单词嵌入中，以获得一个位置感知的单词嵌入。这个技术称为“位置嵌入”。让我们来实现它。
- en: '**Listing 11.24 Implementing positional embedding as a subclassed layer**'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.24 实现位置嵌入为子类化的层**'
- en: layer_positional_embedding <- new_layer_class(
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding <- new_layer_class(
- en: classname = "PositionalEmbedding",
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "PositionalEmbedding",
- en: initialize = function(sequence_length, ➊
  id: totrans-940
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(sequence_length, ➊
- en: input_dim, output_dim, …) {
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: input_dim, output_dim, …) {
- en: super$initialize(…)
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize(…)
- en: self$token_embeddings <-➋
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: self$token_embeddings <-➋
- en: layer_embedding(input_dim = input_dim,
  id: totrans-944
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(input_dim = input_dim,
- en: output_dim = output_dim)
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim = output_dim)
- en: self$position_embeddings <-➌
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: self$position_embeddings <-➌
- en: layer_embedding(input_dim = sequence_length,
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(input_dim = sequence_length,
- en: output_dim = output_dim)
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: output_dim = output_dim)
- en: self$sequence_length <- sequence_length
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: self$sequence_length <- sequence_length
- en: self$input_dim <- input_dim
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: self$input_dim <- input_dim
- en: self$output_dim <- output_dim
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: self$output_dim <- output_dim
- en: '},'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: call = function(inputs) {
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs) {
- en: len <- tf$shape(inputs)[-1]➍
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: len <- tf$shape(inputs)[-1]➍
- en: positions <-
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: positions <-
- en: tf$range(start = 0L, limit = len, delta = 1L)➎
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: tf$range(start = 0L, limit = len, delta = 1L)➎
- en: embedded_tokens <- self$token_embeddings(inputs)
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: embedded_tokens <- self$token_embeddings(inputs)
- en: embedded_positions <- self$position_embeddings(positions)
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: embedded_positions <- self$position_embeddings(positions)
- en: embedded_tokens + embedded_positions➏
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: embedded_tokens + embedded_positions➏
- en: '},'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: compute_mask = function(inputs, mask = NULL) {➐
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: compute_mask = function(inputs, mask = NULL) {➐
- en: inputs != 0
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: inputs != 0
- en: '},'
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: get_config = function() {➑
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: get_config = function() {➑
- en: config <- super$get_config()
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: config <- super$get_config()
- en: for(name in c("output_dim", "sequence_length", "input_dim"))
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: for(name in c("output_dim", "sequence_length", "input_dim"))
- en: config[[name]] <- self[[name]]
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: config[[name]] <- self[[name]]
- en: config
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: config
- en: '}'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: )
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: ➊ **A downside of position embeddings is that the sequence length needs to be
    known in advance.**
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **位置嵌入的一个缺点是需要提前知道序列长度。**
- en: ➋ **Prepare a layer_embedding() for the token indices.**
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **为标记索引准备一个 layer_embedding()。**
- en: ➌ **Prepare another one for the token positions.**
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **为标记位置准备另一个。**
- en: ➍ **tf$shape(inputs)[-1] slices out the last element of the shape, the size
    of the embedding dimension. (tf$shape() returns the shape as a tensor.)**
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **tf$shape(inputs)[-1] 切片出形状的最后一个元素，即嵌入维度的大小。（tf$shape() 返回张量的形状。）**
- en: '➎ **tf$range() is similar to seq() in R, makes a integer sequence: [0, 1, 2,
    …, limit - 1].**'
  id: totrans-975
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **tf$range() 类似于 R 中的 seq()，生成整数序列：[0, 1, 2, …, limit - 1]。**
- en: ➏ **Add both embedding vectors together.**
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **将两个嵌入向量相加。**
- en: ➐ **Like layer_embedding(), this layer should be able to generate a mask so
    we can ignore padding 0s in the inputs. The compute_mask() method will called
    automatically by the framework, and the mask will be propagated to the next layer.**
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **像layer_embedding()一样，这个层应该能够生成一个蒙版，这样我们就可以忽略输入中的填充0。compute_mask()方法将由框架自动调用，并且蒙版将传播到下一层。**
- en: ➑ **Implement serialization so we can save the model.**
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **实现序列化以便我们可以保存模型。**
- en: You would use this layer_positional_embedding() just like a regular layer_ embedding().
    Let’s see it in action!
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 你会像使用常规的layer_embedding()一样使用这个layer_positional_embedding()。让我们看看它的作用！
- en: 'PUTTING IT ALL TOGETHER: A TEXT-CLASSIFICATION TRANSFORMER'
  id: totrans-980
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起：一个文本分类Transformer
- en: All you have to do to start taking word order into account is swap the old layer_
    embedding() with our position-aware version.
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: 只需将旧的layer_embedding()替换为我们的位置感知版本，就可以开始考虑单词顺序。
- en: Listing 11.25 Combining the Transformer encoder with positional embedding
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.25 结合Transformer编码器和位置嵌入
- en: vocab_size <- 20000
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_size <- 20000
- en: sequence_length <- 600
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length <- 600
- en: embed_dim <- 256
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: num_heads <- 2
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads <- 2
- en: dense_dim <- 32
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: dense_dim <- 32
- en: inputs <- layer_input(shape(NULL), dtype = "int64")
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape(NULL), dtype = "int64")
- en: outputs <- inputs %>%
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: outputs <- inputs %>%
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_encoder(embed_dim, dense_dim, num_heads) %>%
- en: layer_global_average_pooling_1d() %>%
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: layer_global_average_pooling_1d() %>%
- en: layer_dropout(0.5) %>%
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(1, activation = "sigmoid")
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(1, activation = "sigmoid")
- en: model <-
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: model <-
- en: keras_model(inputs, outputs) %>%
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: keras_model(inputs, outputs) %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "binary_crossentropy",
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "binary_crossentropy",
- en: metrics = "accuracy")
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: model
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: model
- en: '![Image](../images/f0380-01.jpg)'
  id: totrans-1001
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0380-01.jpg)'
- en: callbacks <- list(
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks <- list(
- en: callback_model_checkpoint("full_transformer_encoder.keras",
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: callback_model_checkpoint("full_transformer_encoder.keras",
- en: save_best_only = TRUE)
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: save_best_only = TRUE)
- en: )
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model %>% fit(
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: model %>% fit(
- en: int_train_ds,
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: int_train_ds,
- en: validation_data = int_val_ds,
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: validation_data = int_val_ds,
- en: epochs = 20,
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: epochs = 20,
- en: callbacks = callbacks
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: callbacks = callbacks
- en: )
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: model <- load_model_tf(
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: model <- load_model_tf(
- en: '"full_transformer_encoder.keras",'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: '"full_transformer_encoder.keras",'
- en: custom_objects = list(layer_transformer_encoder,
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: custom_objects = list(layer_transformer_encoder,
- en: layer_positional_embedding))
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding))
- en: cat(sprintf(
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: cat(sprintf(
- en: '"Test acc: %.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '"测试准确率：%.3f\n", evaluate(model, int_test_ds)["accuracy"]))'
- en: 'Test acc: 0.886'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: ' 测试准确率：0.886'
- en: Look here! We get to 88.6% test accuracy—an improvement that demonstrates the
    value of word-order information for text classification. This is our best sequence
    model so far! However, it’s still one notch below the bag-of-words approach.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 看这里！我们达到了88.6%的测试准确率——这种改进证明了单词顺序信息对文本分类的价值。这是我们迄今为止最好的序列模型！然而，它仍然比词袋模型差一档。
- en: 11.4.4 When to use sequence models over bag-of-words models
  id: totrans-1020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4.4 何时使用序列模型而不是词袋模型
- en: 'You may sometimes hear that bag-of-words methods are outdated and that Transformer-based
    sequence models are the way to go, no matter what task or dataset you’re looking
    at. This is definitely not the case: a small stack of dense layers on top of a
    bag-of-bigrams remains a perfectly valid and relevant approach in many cases.
    In fact, among the various techniques that we’ve tried on the IMDB dataset throughout
    this chapter, the best performing so far was the bag-of-bigrams! So, when should
    you prefer one approach over the other?'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会听说词袋模型方法已过时，而基于Transformer的序列模型是前进的道路，无论你看的是什么任务或数据集。这绝对不是这种情况：在许多情况下，在词袋模型之上放置一小叠稠密层仍然是一个完全有效和相关的方法。事实上，在本章节中我们在IMDB数据集上尝试的各种技术中，迄今为止表现最佳的是词袋模型！那么，何时您应该在另一种方法上更倾向于另一种方法？
- en: In 2017, my team and I ran a systematic analysis of the performance of various
    text-classification techniques across many different types of text datasets, and
    we discovered a remarkable and surprising rule of thumb for deciding whether to
    go with a bag-of-words model or a sequence model ([http://mng.bz/AOzK](http://mng.bz/AOzK))—a
    golden constant of sorts. It turns out that when approaching a new text-classification
    task, you should pay close attention to the ratio between the number of samples
    in your training data and the mean number of words per sample (see [figure 11.11](#fig11-11)).
    If that ratio is small— less than 1,500—then the bag-of-bigrams model will perform
    better (and as a bonus, it will be much faster to train and to iterate on, too).
    If that ratio is higher than 1,500, then you should go with a sequence model.
    In other words, sequence models work best when lots of training data is available
    and when each sample is relatively short.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年，我和我的团队对多种不同类型的文本数据集上各种文本分类技术的性能进行了系统分析，我们发现了一个惊人而令人惊讶的经验法则，用于决定是选择词袋模型还是序列模型（[http://mng.bz/AOzK](http://mng.bz/AOzK)）——一种黄金常数。事实证明，当面对一个新的文本分类任务时，你应该密切关注训练数据中样本数量与每个样本平均字数之间的比率（见[图
    11.11](#fig11-11)）。如果这个比率很小——小于 1,500——那么二元模型的表现会更好（而且作为额外奖励，它的训练和迭代速度也会更快）。如果这个比率高于
    1,500，则应选择序列模型。换句话说，当有大量训练数据可用且每个样本相对较短时，序列模型的效果最佳。
- en: '![Image](../images/f0381-01.jpg)'
  id: totrans-1023
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../images/f0381-01.jpg)'
- en: '**Figure 11.11 A simple heuristic for selecting a text-classification model:
    The ratio between the number of training samples and the mean number of words
    per sample**'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.11 选择文本分类模型的一个简单启发式方法：训练样本数与每个样本平均字数之间的比率**'
- en: So, if you’re classifying 1,000-word long documents, and you have 100,000 of
    them (a ratio of 100), you should go with a bigram model. If you’re classifying
    tweets that are 40 words long on average, and you have 50,000 of them (a ratio
    of 1,250), you should also go with a bigram model. But if you increase your dataset
    size to 500,000 tweets (a ratio of 12,500), go with a Transformer encoder. What
    about the IMDB movie-review classification task? We had 20,000 training samples
    and an average word count of 233, so our rule of thumb points toward a bigram
    model, which confirms what we found in practice.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果你要分类的是 1,000 字长的文档，而你有 100,000 个这样的文档（比例为 100），你应该选择一个二元模型。如果你要分类的是平均长度为
    40 个字的推文，而你有 50,000 条这样的推文（比例为 1,250），你也应该选择一个二元模型。但如果你的数据集大小增加到 500,000 条推文（比例为
    12,500），那就选择一个 Transformer 编码器。那 IMDB 电影评论分类任务呢？我们有 20,000 个训练样本，平均字数为 233，所以我们的经验法则指向一个二元模型，这证实了我们在实践中的发现。
- en: 'This intuitively makes sense: the input of a sequence model represents a richer
    and more complex space, and thus it takes more data to map out that space; meanwhile,
    a plain set of terms is a space so simple that you can train a logistic regression
    on top using just a few hundreds or thousands of samples. In addition, the shorter
    a sample is, the less the model can afford to discard any of the information it
    contains— in particular, word order becomes more important, and discarding it
    can create ambiguity. The sentences “this movie is the bomb” and “this movie was
    a bomb” have very close unigram representations, which could confuse a bag-of-words
    model, but a sequence model could tell which one is negative and which one is
    positive. With a longer sample, word statistics would become more reliable and
    the topic or sentiment would be more apparent from the word histogram alone.'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 这在直觉上是有道理的：序列模型的输入代表了一个更丰富、更复杂的空间，因此需要更多的数据来映射出这个空间；与此同时，一组简单的术语是一个如此简单的空间，以至于你可以只用几百或几千个样本来训练顶部的逻辑回归。此外，样本越短，模型就越不能丢弃其中包含的任何信息——特别是，单词顺序变得更加重要，丢弃它可能会产生歧义。句子“这部电影太棒了”和“这部电影是一颗炸弹”有非常接近的单字表示，这可能会让词袋模型感到困惑，但序列模型可以告诉哪一个是消极的，哪一个是积极的。对于更长的样本，单词统计将变得更可靠，而从单词直方图中就能更明显地看出主题或情感。
- en: Now, keep in mind that this heuristic rule was developed specifically for text
    classification. It may not necessarily hold for other NLP tasks—when it comes
    to machine translation, for instance, Transformer shines especially for very long
    sequences, compared to RNNs. Our heuristic is also just a rule of thumb, rather
    than a scientific law, so expect it to work most of the time, but not necessarily
    every time.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请记住，这个启发式规则是专门为文本分类而开发的。它不一定适用于其他自然语言处理任务——例如，对于机器翻译来说，相比于循环神经网络，Transformer在处理非常长的序列时表现得特别出色。我们的启发式规则也只是一个经验法则，而不是科学定律，所以请期望它大部分时间都有效，但不一定总是有效。
- en: '11.5 Beyond text classification: Sequence-to-sequence learning'
  id: totrans-1028
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 超越文本分类：序列到序列学习
- en: 'You now possess all of the tools you will need to tackle most natural language
    processing tasks. However, you’ve seen these tools in action on only a single
    problem: text classification. This is an extremely popular use case, but there’s
    a lot more to NLP than classification. In this section, you’ll deepen your expertise
    by learning about *sequence-to-sequence models*.'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在拥有了处理大多数自然语言处理任务所需的所有工具。然而，你只看到这些工具在单一问题上的应用：文本分类。这是一个极其流行的用例，但自然语言处理远不止于此。在这一部分，你将通过学习*序列到序列模型*来深化你的专业知识。
- en: 'A sequence-to-sequence model takes a sequence as input (often a sentence or
    paragraph) and translates it into a different sequence. This is the task at the
    heart of many of the most successful applications of NLP:'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型接受一个序列作为输入（通常是一个句子或段落），并将其转换成另一个序列。这是许多最成功的自然语言处理应用程序的核心任务之一：
- en: '*Machine translation*—Convert a paragraph in a source language to its equivalent
    in a target language.'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器翻译*—将源语言的段落转换成目标语言的等效段落。'
- en: '*Text summarization*—Convert a long document to a shorter version that retains
    the most important information.'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本摘要*—将长篇文档转换成保留最重要信息的较短版本。'
- en: '*Question answering*—Convert an input question into its answer.'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问答*—将输入的问题转换成答案。'
- en: '*Chatbots*—Convert a dialogue prompt into a reply to this prompt, or convert
    the history of a conversation into the next reply in the conversation.'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*聊天机器人*—将对话提示转换成对该提示的回复，或将对话历史转换成对话中的下一个回复。'
- en: '*Text generation*—Convert a text prompt into a paragraph that completes the
    prompt.'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*文本生成*—将文本提示转换成完成提示的段落。'
- en: And so forth.
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等等。
- en: 'The general template behind sequence-to-sequence models is described in [figure
    11.12](#fig11-12). During training:'
  id: totrans-1037
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型的一般模板描述在[图 11.12](#fig11-12)中。在训练过程中：
- en: An *encoder* model turns the source sequence into an intermediate representation.
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*编码器*模型将源序列转换为中间表示。'
- en: A *decoder* is trained to predict the next token i in the target sequence by
    looking at both previous tokens (1 to i - 1) and the encoded source sequence.
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*解码器*通过查看之前的标记（1到 i - 1）和编码后的源序列来训练，以预测目标序列中的下一个标记 i。'
- en: '![Image](../images/f0383-01.jpg)'
  id: totrans-1040
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../images/f0383-01.jpg)'
- en: '**Figure 11.12 Sequence-to-sequence learning: The source sequence is processed
    by the encoder and is then sent to the decoder. The decoder looks at the target
    sequence so far and predicts the target sequence offset by one step in the future.
    During inference, we generate one target token at a time and feed it back into
    the decoder.**'
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.12 序列到序列学习：源序列经过编码器处理，然后发送到解码器。解码器查看目标序列至今，并预测偏移一个步骤的目标序列。在推断过程中，我们逐个目标标记地生成并将其送回解码器。**'
- en: 'During inference, we don’t have access to the target sequence—we’re trying
    to predict it from scratch. We’ll have to generate it one token at a time:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 在推断中，我们无法访问目标序列——我们试图从头开始预测它。我们将不得不逐个标记地生成它：
- en: '**1** We obtain the encoded source sequence from the encoder.'
  id: totrans-1043
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**1** 我们从编码器中获得编码后的源序列。'
- en: '**2** The decoder starts by looking at the encoded source sequence as well
    as an initial “seed” token (such as the string “[start]”), and uses that to predict
    the first real token in the sequence.'
  id: totrans-1044
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**2** 解码器首先查看编码后的源序列以及一个初始的“种子”标记（例如字符串“[start]”），并用它来预测序列中的第一个真实标记。'
- en: '**3** The predicted sequence so far is fed back into the decoder, which generates
    the next token, and so on, until it generates a stop token (such as the string
    “[end]”).'
  id: totrans-1045
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**3** 到目前为止预测的序列被送回解码器，解码器生成下一个标记，依此类推，直到生成一个停止标记（如字符串“[end]”）。'
- en: Everything you’ve learned so far can be repurposed to build this new kind of
    model. Let’s dive in.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你学到的所有东西都可以重新用于构建这种新型模型。让我们深入了解。
- en: 11.5.1 A machine translation example
  id: totrans-1047
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 一个机器翻译示例
- en: 'We’ll demonstrate sequence-to-sequence modeling on a machine translation task.
    Machine translation is precisely what Transformer was developed for! We’ll start
    with a recurrent sequence model, and we’ll follow up with the full Transformer
    architecture. We’ll be working with an English-to-Spanish translation dataset
    available at [http://www.manythings.org/anki/.](http://www.manythings.org/anki/)
    Let’s download it:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个机器翻译任务上演示序列到序列建模。机器翻译正是Transformer开发的初衷！我们将从循环序列模型开始，并将跟进完整的Transformer架构。我们将使用[http://www.manythings.org/anki/.](http://www.manythings.org/anki/)上提供的英语到西班牙语翻译数据集。让我们下载它：
- en: download.file(
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: download.file(
- en: '"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip",'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip",'
- en: destfile = "spa-eng.zip")
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: destfile = "spa-eng.zip")
- en: zip::unzip("spa-eng.zip")
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: zip::unzip("spa-eng.zip")
- en: 'The text file contains one example per line: an English sentence, followed
    by a tab character, followed by the corresponding Spanish sentence. Let’s use
    readr::read_tsv() since we have tab-separated values:'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 文本文件每行包含一个示例：一个英语句子，后跟一个制表符，然后是相应的西班牙句子。让我们使用readr::read_tsv()，因为我们有制表符分隔的值：
- en: text_file <- "spa-eng/spa.txt"
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: text_file <- "spa-eng/spa.txt"
- en: text_pairs <- text_file %>%➊
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: text_pairs <- text_file %>%➊
- en: readr::read_tsv(col_names = c("english", "spanish"),➋
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: readr::read_tsv(col_names = c("english", "spanish"),➋
- en: col_types = c("cc")) %>%➌
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: col_types = c("cc")) %>%➌
- en: within(spanish %<>% paste("[start]", ., "[end]"))➍
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: within(spanish %<>% paste("[start]", ., "[end]"))➍
- en: ➊ **Read the file using read_tsv() (tab-separated values).**
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **使用read_tsv()读取文件（制表符分隔的值）。**
- en: ➋ **Each line contains an English phrase and its Spanish translation, tab separated.**
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **每行包含一个英语短语及其西班牙语翻译，用制表符分隔。**
- en: ➌ **Two-character columns**
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **两字符列**
- en: ➍ **We prepend "[start]" and append "[end]" to the spanish sentence, to match
    the template from [figure 11.12](#fig11-12).**
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **我们在西班牙语句子前加上“[start]”，并在后面加上“[end]”，以匹配[图11.12](#fig11-12)中的模板。**
- en: 'Our text_pairs look like this:'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的text_pairs看起来是这样的：
- en: str(text_pairs[sample(nrow(text_pairs), 1), ])
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: str(text_pairs[sample(nrow(text_pairs), 1), ])
- en: 'tibble [1 × 2] (S3: tbl_df/tbl/data.frame)'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 'tibble [1 × 2] (S3: tbl_df/tbl/data.frame)'
- en: '$ english: chr "I’m staying in Italy."'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '$ english: chr "I’m staying in Italy."'
- en: '$ spanish: chr "[start] Me estoy quedando en Italia. [end]"'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: '$ spanish: chr "[start] Me estoy quedando en Italia. [end]"'
- en: 'Let’s shuffle them and split them into the usual training, validation, and
    test sets:'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对它们进行洗牌并将它们拆分成通常的训练、验证和测试集：
- en: num_test_samples <- num_val_samples <-
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: num_test_samples <- num_val_samples <-
- en: round(0.15 * nrow(text_pairs))
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: round(0.15 * nrow(text_pairs))
- en: num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: num_train_samples <- nrow(text_pairs) - num_val_samples - num_test_samples
- en: pair_group <- sample(c(
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: pair_group <- sample(c(
- en: rep("train", num_train_samples),
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: rep("train", num_train_samples),
- en: rep("test", num_test_samples),
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: rep("test", num_test_samples),
- en: rep("val", num_val_samples)
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: rep("val", num_val_samples)
- en: ))
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: ))
- en: train_pairs <- text_pairs[pair_group == "train", ]
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: train_pairs <- text_pairs[pair_group == "train", ]
- en: test_pairs <- text_pairs[pair_group == "test", ]
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: test_pairs <- text_pairs[pair_group == "test", ]
- en: val_pairs <- text_pairs[pair_group == "val", ]
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: val_pairs <- text_pairs[pair_group == "val", ]
- en: 'Next, let’s prepare two separate TextVectorization layers: one for English
    and one for Spanish. We’re going to need to customize the way strings are preprocessed:'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们准备两个单独的TextVectorization层：一个用于英语，一个用于西班牙语。我们需要定制字符串的预处理方式：
- en: We need to preserve the “[start]” and “[end]” tokens that we’ve inserted. By
    default, the characters [ and ] would be stripped, but we want to keep them so
    we can tell apart the word “start” and the start token “[start]”.
  id: totrans-1081
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要保留我们插入的“[start]”和“[end]”标记。默认情况下，字符[和]将被剥离，但我们希望保留它们，以便我们可以区分单词“start”和起始标记“[start]”。
- en: Punctuation is different from language to language! In the Spanish Text-Vectorization
    layer, if we’re going to strip punctuation characters, we need to also strip the
    character ¿.
  id: totrans-1082
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标点符号在不同语言之间是不同的！在西班牙语Text-Vectorization层中，如果我们要剥离标点字符，我们还需要剥离字符¿。
- en: Note that for a non-toy translation model, we would treat punctuation characters
    as separate tokens rather than stripping them, because we would want to be able
    to generate correctly punctuated sentences. In our case, for simplicity, we’ll
    get rid of all punctuation.
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于非玩具翻译模型，我们将把标点字符视为单独的标记，而不是剥离它们，因为我们希望能够生成正确标点的句子。在我们的情况下，为了简单起见，我们将摆脱所有标点符号。
- en: 'We prepare a custom string standardization function for the Spanish TextVectorization
    layer: it preserves [ and ] but strips ¿, ¡, and all other characters from the
    [:punct:] class. (The double negation of the [:punct:] class cancels out, as if
    it was not negated at all. However, having the outer negated regex grouping lets
    us specifically exclude [ and ] from the [:punct:] regex class. We use | to add
    other special characters that are not in the [:punct:] character class, like ¡
    and ¿.)'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为西班牙语 TextVectorization 层准备了一个自定义字符串标准化函数：它保留了 [ 和 ]，但剥离了 ¿、¡ 和 [:punct:]
    类中的所有其他字符。（[:punct:] 类的双重否定会互相抵消，就好像根本没有否定一样。然而，外部否定正则表达式分组让我们能够明确排除 [:punct:]
    正则表达式类中的 [ 和 ]。我们使用 | 添加了其他不在 [:punct:] 字符类中的特殊字符，比如 ¡ 和 ¿。）
- en: Listing 11.26 Vectorizing the English and Spanish text pairs
  id: totrans-1085
  prefs: []
  type: TYPE_NORMAL
  zh: 第 11.26 节 将英语和西班牙语文本对转为向量
- en: punctuation_regex <- "[^[:^punct:][\\]]|[¡¿]"➊
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: punctuation_regex <- "[^[:^punct:][\\]]|[¡¿]"➊
- en: library(tensorflow)
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: library(tensorflow)
- en: custom_standardization <- function(input_string) {➋
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: custom_standardization <- function(input_string) {➋
- en: input_string %>%
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: input_string %>%
- en: tf$strings$lower() %>%
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$lower() %>%
- en: tf$strings$regex_replace(punctuation_regex, "")
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$regex_replace(punctuation_regex, "")
- en: '}'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: input_string <- as_tensor("[start] ¡corre! [end]")
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: input_string <- as_tensor("[start] ¡corre! [end]")
- en: custom_standardization(input_string)
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: custom_standardization(input_string)
- en: tf.Tensor(b’[start] corre [end]’, shape=(), dtype=string)➌
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor(b’[start] corre [end]’, shape=(), dtype=string)➌
- en: ➊ **Essentially, [[:punct:]], except it omits "[" and "]" and adds "¿" and "¡".**
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **基本上，就是 [[:punct:]]，除了省略了 "[" 和 "]"，添加了 "¿" 和 "¡"。**
- en: '➋ **Note: this time we''re using tensor operations. This allows the function
    to be traced into a TensorFlow graph.**'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **注意：这次我们使用张量操作。这允许函数被追踪到 TensorFlow 图中。**
- en: ➌ **Preserved the [] of [start] and [end], and stripped out ¡ and !.**
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **保留了 [start] 和 [end] 的 []，并去除了 ¡ 和 !。**
- en: '**WARNING** The TensorFlow regex has minor differences from the R regex engine.
    Consult the source documentation if you need advanced regular expressions: [https://github.com/google/re2/wiki/Syntax](https://www.github.com/google/re2/wiki/Syntax).'
  id: totrans-1099
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**警告** TensorFlow 正则表达式与 R 正则引擎有细微差异。如果您需要高级正则表达式，请查阅源文档：[https://github.com/google/re2/wiki/Syntax](https://www.github.com/google/re2/wiki/Syntax)。'
- en: vocab_size <- 15000➊
  id: totrans-1100
  prefs: []
  type: TYPE_NORMAL
  zh: vocab_size <- 15000➊
- en: sequence_length <- 20
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: sequence_length <- 20
- en: source_vectorization <- layer_text_vectorization(➋
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: source_vectorization <- layer_text_vectorization(➋
- en: max_tokens = vocab_size,
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = vocab_size,
- en: output_mode = "int",
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int",
- en: output_sequence_length = sequence_length
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: output_sequence_length = sequence_length
- en: )
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: target_vectorization <- layer_text_vectorization(➌
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: target_vectorization <- layer_text_vectorization(➌
- en: max_tokens = vocab_size,
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: max_tokens = vocab_size,
- en: output_mode = "int", output_sequence_length = sequence_length + 1,➍
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: output_mode = "int", output_sequence_length = sequence_length + 1,➍
- en: standardize = custom_standardization
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: standardize = custom_standardization
- en: )
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: adapt(source_vectorization, train_pairs$english)➎
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(source_vectorization, train_pairs$english)➎
- en: adapt(target_vectorization, train_pairs$spanish)
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: adapt(target_vectorization, train_pairs$spanish)
- en: ➊ **To keep things simple, we'll look at only the top 15,000 words in each language,
    and we'll restrict sentences to 20 words.**
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **为了简单起见，我们将只考虑每种语言中的前 15,000 个单词，并将句子限制在 20 个词以内。**
- en: ➋ **The English layer**
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **英语层**
- en: ➌ **The Spanish layer**
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **西班牙语层**
- en: ➍ **Generate Spanish sentences that have one extra token, because we'll need
    to offset the sentence by one step during training.**
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **生成西班牙句子，多了一个额外的标记，因为在训练过程中我们需要将句子向前偏移一步。**
- en: ➎ **Learn the vocabulary of each language.**
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **学习每种语言的词汇。**
- en: Finally, we can turn our data into a TF Dataset pipeline. We want it to return
    a pair (inputs, target) where inputs is a named list with two entries, the english
    sentence (the encoder input), and the spanish sentence (the decoder input), and
    target is the Spanish sentence offset by one step ahead.
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将我们的数据转为 TF Dataset 流水线。我们希望它返回一个对，(inputs, target)，其中 inputs 是一个带有两个条目的命名列表，英语句子（编码器输入）和西班牙句子（解码器输入），target
    是西班牙句子向前偏移一步。
- en: Listing 11.27 Preparing datasets for the translation task
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 第 11.27 节 为翻译任务准备数据集
- en: format_pair <- function(pair) {
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: format_pair <- function(pair) {
- en: eng <- source_vectorization(pair$english)➊
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: eng <- source_vectorization(pair$english)➊
- en: spa <- target_vectorization(pair$spanish)
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: spa <- target_vectorization(pair$spanish)
- en: inputs <- list(english = eng,
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- list(english = eng,
- en: spanish = spa[NA:-2])➋
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: spanish = spa[NA:-2])➋
- en: targets <- spa[2:NA]➌
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: targets <- spa[2:NA]➌
- en: list(inputs, targets)➍
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: list(inputs, targets)➍
- en: '}'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: batch_size <- 64
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: batch_size <- 64
- en: library(tfdatasets)
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: library(tfdatasets)
- en: make_dataset <- function(pairs) {
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: make_dataset <- function(pairs) {
- en: tensor_slices_dataset(pairs) %>%
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: tensor_slices_dataset(pairs) %>%
- en: dataset_map(format_pair, num_parallel_calls = 4) %>%
  id: totrans-1133
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_map(format_pair, num_parallel_calls = 4) %>%
- en: dataset_cache() %>%➎
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_cache() %>%➎
- en: dataset_shuffle(2048) %>%
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_shuffle(2048) %>%
- en: dataset_batch(batch_size) %>%
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_batch(batch_size) %>%
- en: dataset_prefetch(16)
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: dataset_prefetch(16)
- en: '}'
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: train_ds <- make_dataset(train_pairs)
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: train_ds <- make_dataset(train_pairs)
- en: val_ds <- make_dataset(val_pairs)
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: val_ds <- make_dataset(val_pairs)
- en: ➊ **The vectorization layer can be called with either batched or unbatched data.
    Here, we apply the vectorization before batching the data.**
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **矢量化层可以使用批量化或非批量化数据调用。在这里，我们在对数据进行批量化之前应用矢量化。**
- en: ➋ **Omit the last token from the Spanish sentence, so inputs and targets are
    the same length. [NA:-2] drops the last element of a tensor.**
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **省略西班牙句子的最后一个标记，这样输入和目标的长度就一样了。[NA:-2]删除了张量的最后一个元素。**
- en: ➌ **[2:NA] drops the first element of a tensor.**
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **[2:NA]删除了张量的第一个元素。**
- en: ➍ **The target Spanish sentence is one step ahead. Both are still the same length
    (20 words).**
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **目标西班牙句子比源句子提前一步。两者长度仍然相同（20个单词）。**
- en: ➎ **Use in-memory caching to speed up preprocessing.**
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **使用内存缓存来加快预处理速度。**
- en: 'Here’s what our dataset outputs look like:'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的数据集输出的样子：
- en: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: c(inputs, targets) %<-% iter_next(as_iterator(train_ds))
- en: str(inputs)
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: str(inputs)
- en: List of 2
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 2个列表
- en: '$ english:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: '$ english:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
- en: '$ spanish:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: '$ spanish:<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
- en: str(targets)
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: str(targets)
- en: '<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '<tf.Tensor: shape=(64, 20), dtype=int64, numpy=…>'
- en: The data is now ready—time to build some models. We’ll start with a recurrent
    sequence-to-sequence model before moving on to a Transformer.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 数据现在准备好了——是时候构建一些模型了。我们将先从一个递归序列到序列模型开始，然后再转向一个Transformer模型。
- en: 11.5.2 Sequence-to-sequence learning with RNNs
  id: totrans-1155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 使用RNN进行序列到序列学习
- en: Recurrent neural networks dominated sequence-to-sequence learning from 2015–
    2017 before being overtaken by Transformer. They were the basis for many real-world
    machine translation systems, as mentioned in chapter 10\. Google Translate circa
    2017 was powered by a stack of seven large LSTM layers. It’s still worth learning
    about this approach today, because it provides an easy entry point to understanding
    sequence-to-sequence models.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年至2017年期间，递归神经网络在序列到序列学习中占据主导地位，然后被Transformer超越。它们是许多实际机器翻译系统的基础，如第10章所提到的。2017年左右的谷歌翻译就是由七个大型LSTM层堆叠而成。今天仍然值得学习这种方法，因为它为理解序列到序列模型提供了一个简单的入门点。
- en: 'The simplest, naive way to use RNNs to turn a sequence into another sequence
    is to keep the output of the RNN at each time step. In Keras, it would look like
    this:'
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RNN将一个序列转换为另一个序列的最简单、天真的方式是保留RNN在每个时间步的输出。在Keras中，它看起来像这样：
- en: inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: inputs <- layer_input(shape = c(sequence_length), dtype = "int64")
- en: outputs <- inputs %>%
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 输出 <- 输入 %>%
- en: layer_embedding(input_dim = vocab_size, output_dim = 128) %>%
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(input_dim = vocab_size, output_dim = 128) %>%
- en: layer_lstm(32, return_sequences = TRUE) %>%
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: layer_lstm(32, return_sequences = TRUE) %>%
- en: layer_dense(vocab_size, activation = "softmax")
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(vocab_size, activation = "softmax")
- en: model <- keras_model(inputs, outputs)
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: model <- keras_model(inputs, outputs)
- en: 'However, this approach has two major issues:'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法存在两个主要问题：
- en: The target sequence must always be the same length as the source sequence. In
    practice, this is rarely the case. Technically, this isn’t critical, because you
    could always pad either the source sequence or the target sequence to make their
    lengths match.
  id: totrans-1165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标序列必须始终与源序列具有相同的长度。实际上，情况很少如此。从技术上讲，这并不重要，因为你始终可以在源序列或目标序列中填充其中之一，以使它们的长度匹配。
- en: Due to the step-by-step nature of RNNs, the model will be looking only at tokens
    1…*N* in the source sequence to predict token *N* in the target sequence. This
    constraint makes this setup unsuitable for most tasks, particularly translation.
    Consider translating “The weather is nice today” to French—that would be “Il fait
    beau aujourd’hui.” You’d need to be able to predict “Il” from just “The,” “Il
    fait” from just “The weather,” and so on, which is simply impossible.
  id: totrans-1166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于RNN的逐步性质，该模型只会查看源序列中的标记1…*N*来预测目标序列中的标记*N*。这种限制使得这种设置对大多数任务不适用，尤其是翻译任务。考虑将“The
    weather is nice today”翻译成法语，即“Il fait beau aujourd’hui。”你需要能够仅仅从“The”预测出“Il”，仅仅从“The
    weather”预测出“Il fait”，等等，这是不可能的。
- en: If you’re a human translator, you’d start by reading the entire source sentence
    before starting to translate it. This is especially important if you’re dealing
    with languages that have wildly different word ordering, like English and Japanese.
    And that’s exactly what standard sequence-to-sequence models do.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个人类翻译员，你会先阅读整个源句子，然后开始翻译它。这在处理词序完全不同的语言时尤其重要，比如英语和日语。而标准的序列到序列模型正是这样做的。
- en: In a proper sequence-to-sequence setup (see [figure 11.13](#fig11-13)), you
    would first use an RNN (the encoder) to turn the entire source sequence into a
    single vector (or set of vectors). This could be the last output of the RNN, or
    alternatively, its final internal state vectors. Then you would use this vector
    (or vectors) as the *initial state* of another RNN (the decoder), which would
    look at elements 1…*N* in the target sequence, and try to predict step *N*+1 in
    the target sequence.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个适当的序列到序列设置中（见 [图 11.13](#fig11-13)），你首先会使用一个 RNN（编码器）将整个源序列转换为单个向量（或一组向量）。这可以是
    RNN 的最后输出，或者是其最终的内部状态向量。然后，您将使用此向量（或向量）作为另一个 RNN（解码器）的*初始状态*，该解码器将查看目标序列的元素 1…*N*，并尝试预测目标序列中的步骤
    *N*+1。
- en: '![Image](../images/f0388-01.jpg)'
  id: totrans-1169
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0388-01.jpg)'
- en: '**Figure 11.13 A sequence-to-sequence RNN: an RNN encoder is used to produce
    a vector that encodes the entire source sequence, which is used as the initial
    state for an RNN decoder.**'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.13 一个序列到序列 RNN：一个 RNN 编码器用于产生编码整个源序列的向量，这个向量被用作另一个 RNN 解码器的初始状态。**'
- en: Let’s implement this in Keras with GRU-based encoders and decoders. The choice
    of GRU rather than LSTM makes things a bit simpler, because GRU has only a single
    state vector, whereas LSTM has multiple. Let’s start with the encoder.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 Keras 中用基于 GRU 的编码器和解码器来实现这一点。与 LSTM 相比，选择 GRU 使事情变得简单一些，因为 GRU 只有一个状态向量，而
    LSTM 有多个。让我们从编码器开始。
- en: Listing 11.28 GRU-based encoder
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.28 基于 GRU 的编码器
- en: embed_dim <- 256
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: latent_dim <- 1024
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: latent_dim <- 1024
- en: source <- layer_input(c(NA), dtype = "int64",
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: source <- layer_input(c(NA), dtype = "int64",
- en: name = "english")➊
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: name = "english")➊
- en: encoded_source <- source %>%
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: encoded_source <- source %>%
- en: layer_embedding(vocab_size, embed_dim,
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(vocab_size, embed_dim,
- en: mask_zero = TRUE) %>%➋
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: mask_zero = TRUE) %>%➋
- en: bidirectional(layer_gru(units = latent_dim),
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 双向(layer_gru(units = latent_dim),
- en: merge_mode = "sum")➌
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: merge_mode = "sum")➌
- en: ➊ **The English source sentence goes here. Specifying the name of the input
    enables us to fit() the model with a named list of inputs.**
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **英文源句子在这里。通过指定输入的名称，我们能够用一个命名的输入列表来拟合()模型。**
- en: '➋ **Don''t forget masking: it''s critical in this setup.**'
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **不要忘记掩码：在这种设置中很关键。**
- en: ➌ **Our encoded source sentence is the last output of a bidirectional GRU.**
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **我们编码的源句子是双向 GRU 的最后一个输出。**
- en: Next, let’s add the decoder—a simple GRU layer that takes as its initial state
    the encoded source sentence. On top of it, we add a layer_dense() that produces
    for each output step a probability distribution over the Spanish vocabulary.
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们添加解码器——一个简单的 GRU 层，它的初始状态是编码的源句子。在其上面，我们添加一个 layer_dense()，为每个输出步骤生成对西班牙语词汇的概率分布。
- en: Listing 11.29 GRU-based decoder and the end-to-end model
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.29 基于 GRU 的解码器和端到端模型
- en: decoder_gru <- layer_gru(units = latent_dim, return_sequences = TRUE)
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: decoder_gru <- layer_gru(units = latent_dim, return_sequences = TRUE)
- en: past_target <- layer_input(shape = c(NA), dtype = "int64", name = "spanish")➊
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: past_target <- layer_input(shape = c(NA), dtype = "int64", name = "spanish")➊
- en: target_next_step <- past_target %>%
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: target_next_step <- past_target %>%
- en: layer_embedding(vocab_size, embed_dim,
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: layer_embedding(vocab_size, embed_dim,
- en: mask_zero = TRUE) %>%➋
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: mask_zero = TRUE) %>%➋
- en: decoder_gru(initial_state = encoded_source) %>%➌
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: decoder_gru(initial_state = encoded_source) %>%➌
- en: layer_dropout(0.5) %>%
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(vocab_size, activation = "softmax")➍
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(vocab_size, activation = "softmax")➍
- en: seq2seq_rnn <-
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq_rnn <-
- en: keras_model(inputs = list(source, past_target),➎
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: keras_model(inputs = list(source, past_target),➎
- en: outputs = target_next_step)
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: outputs = target_next_step)
- en: ➊ **The Spanish target sentence goes here.**
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **西班牙目标句子放在这里。**
- en: ➋ **Don't forget masking.**
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **不要忘记掩码。**
- en: ➌ **The encoded source sentence serves as the initial state of the decoder GRU.**
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **编码的源句子作为解码器 GRU 的初始状态。**
- en: ➍ **Predict the next token.**
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **预测下一个标记。**
- en: '➎ **End-to-end model: map the source sentence and the target sentence to the
    target sentence one step in the future**'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **端到端模型：将源句子和目标句子映射到未来的目标句子中的一步**
- en: During training, the decoder takes as input the entire target sequence, but
    thanks to the step-by-step nature of RNNs, it looks only at tokens 1…*N* in the
    input to predict token *N* in the output (which corresponds to the next token
    in the sequence, because the output is intended to be offset by one step). This
    means we only use information from the past to predict the future, as we should;
    otherwise, we’d be cheating, and our model would not work at inference time.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间，解码器将整个目标序列作为输入，但由于RNN的逐步性质，它仅查看输入中的令牌1…*N*，以预测输出中的令牌*N*（它对应于序列中的下一个令牌，因为输出旨在偏移一个步骤）。这意味着我们只使用过去的信息来预测未来，正如我们应该的那样；否则，我们将作弊，我们的模型在推断时将无法工作。
- en: Let’s start training.
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始训练。
- en: Listing 11.30 Training our recurrent sequence-to-sequence model
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 11.30 训练我们的循环序列到序列模型
- en: seq2seq_rnn %>% compile(optimizer = "rmsprop",
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq_rnn %>% compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: seq2seq_rnn %>% fit(train_ds, epochs = 15, validation_data = val_ds)
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq_rnn %>% fit(train_ds, epochs = 15, validation_data = val_ds)
- en: 'We picked accuracy as a crude way to monitor validation-set performance during
    training. We get to 64% accuracy: on average, the model predicts the next word
    in the Spanish sentence correctly 64% of the time. However, in practice, next-token
    accuracy isn’t a great metric for machine translation models, in particular because
    it makes the assumption that the correct target tokens from 0 to *N* are already
    known when predicting token *N* +1\. In reality, during inference, you’re generating
    the target sentence from scratch, and you can’t rely on previously generated tokens
    being 100% correct. If you work on a real-world machine translation system, you
    will likely use “BLEU scores” to evaluate your models—a metric that looks at entire
    generated sequences and that seems to correlate well with human perception of
    translation quality.'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择准确性作为在训练过程中监视验证集性能的粗略方式。我们达到了64%的准确率：平均而言，模型在64%的时间内正确预测了西班牙语句子中的下一个单词。然而，在实践中，下一个标记准确性并不是机器翻译模型的好指标，特别是因为它假设在预测标记*N*
    +1时，已知从0到*N*的正确目标标记。实际上，在推断期间，您正在从头开始生成目标句子，而不能依赖于先前生成的标记完全正确。如果您正在开发真实世界的机器翻译系统，您可能会使用“BLEU分数”来评估您的模型——这是一个考虑整个生成序列的指标，似乎与人类对翻译质量的感知良好相关。
- en: At last, let’s use our model for inference. We’ll pick a few sentences in the
    test set and check how our model translates them. We’ll start from the seed token,
    “[start]”, and feed it into the decoder model, together with the encoded English
    source sentence. We’ll retrieve a next-token prediction, and we’ll reinject it
    into the decoder repeatedly, sampling one new target token at each iteration,
    until we get to “[end]” or reach the maximum sentence length.
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们使用我们的模型进行推断。我们将在测试集中挑选几个句子，并检查我们的模型如何翻译它们。我们将从种子标记“[start]”开始，并将其连同编码的英语源句子一起馈送到解码器模型中。我们将检索下一个标记预测，并将其反复注入解码器，每次迭代抽样一个新的目标标记，直到我们到达“[end]”或达到最大句子长度。
- en: Listing 11.31 Translating new sentences with our RNN encoder and decoder
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: Listing 11.31 使用我们的RNN编码器和解码器翻译新句子
- en: spa_vocab <- get_vocabulary(target_vectorization)➊
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: spa_vocab <- get_vocabulary(target_vectorization)➊
- en: max_decoded_sentence_length <- 20
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: max_decoded_sentence_length <- 20
- en: decode_sequence <- function(input_sentence) {
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: decode_sequence <- function(input_sentence) {
- en: tokenized_input_sentence <
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_input_sentence <
- en: source_vectorization(array(input_sentence, dim = c(1, 1)))
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: source_vectorization(array(input_sentence, dim = c(1, 1)))
- en: decoded_sentence <- "[start]"➋
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- "[start]"➋
- en: for (i in seq(max_decoded_sentence_length)) {
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(max_decoded_sentence_length)) {
- en: tokenized_target_sentence <-
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence <-
- en: target_vectorization(array(decoded_sentence, dim = c(1, 1)))
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: target_vectorization(array(decoded_sentence, dim = c(1, 1)))
- en: next_token_predictions <- seq2seq_rnn %>%
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: next_token_predictions <- seq2seq_rnn %>%
- en: predict(list(tokenized_input_sentence,➌
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: predict(list(tokenized_input_sentence,➌
- en: tokenized_target_sentence))
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence))
- en: sampled_token_index <- which.max(next_token_predictions[1, i, ])
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token_index <- which.max(next_token_predictions[1, i, ])
- en: sampled_token <- spa_vocab[sampled_token_index]➍
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token <- spa_vocab[sampled_token_index]➍
- en: decoded_sentence <- paste(decoded_sentence, sampled_token)
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- paste(decoded_sentence, sampled_token)
- en: if (sampled_token == "[end]")➎
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: if (sampled_token == "[end]")➎
- en: break
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '}'
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: decoded_sentence
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence
- en: '}'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: for (i in seq(20)) {
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(20)) {
- en: input_sentence <- sample(test_pairs$english, 1)
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: input_sentence <- sample(test_pairs$english, 1)
- en: print(input_sentence)
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: print(input_sentence)
- en: print(decode_sequence(input_sentence))
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: print(decode_sequence(input_sentence))
- en: print("-")
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: print("-")
- en: '}'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: '[1] "Does this dress look OK on me?"'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "这件裙子穿在我身上好看吗？"'
- en: '[1] "[start] este vestido me parece bien [UNK] [end]"'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "[start] este vestido me parece bien [UNK] [end]"'
- en: '[1] "-"'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] "-"'
- en: …
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: …
- en: ➊ **Prepare vocabulary to convert token index predictions to string tokens.**
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **准备词汇表，将令牌索引预测转换为字符串令牌。**
- en: ➋ **Seed token**
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **种子令牌**
- en: ➌ **Sample the next token.**
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **抽样下一个令牌。**
- en: ➍ **Convert the next token prediction to a string, and append it to the generated
    sentence.**
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **将下一个令牌预测转换为字符串，并将其附加到生成的句子中。**
- en: '➎ **Exit condition: either hit max length or sample a stop token.**'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **退出条件：达到最大长度或抽样到停止令牌。**
- en: decode_sequence() is working nicely, though perhaps a little slower than we
    would like. One easy way to speed up eager code like this is to use tf_function(),
    which we first saw in chapter 7\. Let’s rewrite decode_sentence() to be compiled
    by tf_function(). This means that instead of using eager R functions like seq(),
    predict(), and which.max(), we will use TensorFlow equivalents, like tf$range(),
    calling model() directly, and tf$argmax().
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: decode_sequence() 现在工作得很好，尽管可能比我们想象的要慢一些。加速 eager 代码的一个简单方法是使用 tf_function()，我们在第
    7 章首次见到它。让我们重写 decode_sentence()，使其由 tf_function() 编译。这意味着我们将不再使用 eager R 函数，如
    seq()、predict() 和 which.max()，而是使用 TensorFlow 的等效函数，如 tf$range()，直接调用 model()，以及
    tf$argmax()。
- en: 'Because tf$range() and tf$argmax() return a 0-based value, we’ll set a function
    local option: option(tensorflow.extract.style = “python”). This changes the behavior
    of [ for tensors to be 0-based as well.'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 tf$range() 和 tf$argmax() 返回的是基于 0 的值，我们将设置一个函数局部选项：option(tensorflow.extract.style
    = “python”)。这样一来，张量的 [ 行为也将是基于 0 的。
- en: tf_decode_sequence <- tf_function(function(input_sentence) {
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: tf_decode_sequence <- tf_function(function(input_sentence) {
- en: withr::local_options(
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: withr::local_options(
- en: tensorflow.extract.style = "python")➊
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow.extract.style = "python")➊
- en: tokenized_input_sentence <- input_sentence %>%
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_input_sentence <- input_sentence %>%
- en: as_tensor(shape = c(1, 1)) %>%
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(shape = c(1, 1)) %>%
- en: source_vectorization()
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: source_vectorization()
- en: spa_vocab <- as_tensor(spa_vocab)
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: spa_vocab <- as_tensor(spa_vocab)
- en: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
  id: totrans-1257
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
- en: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
- en: tokenized_target_sentence <- decoded_sentence %>%
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence <- decoded_sentence %>%
- en: target_vectorization()
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: target_vectorization()
- en: next_token_predictions <-
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: next_token_predictions <-
- en: seq2seq_rnn(list(tokenized_input_sentence,
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: seq2seq_rnn(list(tokenized_input_sentence,
- en: tokenized_target_sentence))
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence))
- en: sampled_token_index <-
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token_index <-
- en: tf$argmax(next_token_predictions[0, i, ])➋
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: tf$argmax(next_token_predictions[0, i, ])➋
- en: sampled_token <- spa_vocab[sampled_token_index]➌
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token <- spa_vocab[sampled_token_index]➌
- en: decoded_sentence <-
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <-
- en: tf$strings$join(c(decoded_sentence, sampled_token),
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$join(c(decoded_sentence, sampled_token),
- en: separator = " ")
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: separator = " ")
- en: if (sampled_token == "[end]")
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: if (sampled_token == "[end]")
- en: break
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '}'
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: decoded_sentence
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence
- en: '})'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: for (i in seq(20)) {
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in seq(20)) {
- en: input_sentence <- sample(test_pairs$english, 1)
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: input_sentence <- sample(test_pairs$english, 1)
- en: cat(input_sentence, "\n")
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: cat(input_sentence, "\n")
- en: cat(input_sentence %>% as_tensor() %>%➍
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: cat(input_sentence %>% as_tensor() %>%➍
- en: tf_decode_sequence() %>% as.character(), "\n")
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: tf_decode_sequence() %>% as.character(), "\n")
- en: cat("-\n")
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: cat("-\n")
- en: '}'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Now all Tensor subsetting with [ will be 0-based until this function exits.**
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **现在所有使用 [ 进行张量子集的操作都将是基于 0 的，直到此函数退出。**
- en: ➋ **i from tf$range() is 0-based.**
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **tf$range() 中的 i 是基于 0 的。**
- en: ➌ **tf$argmax() returns a 0-based index.**
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **tf$argmax() 返回的是基于 0 的索引。**
- en: ➍ **Convert to a tensor before calling tf_decode_sequence(), then convert output
    back to an R character.**
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **在调用 tf_decode_sequence() 之前转换为张量，然后将输出转换回 R 字符串。**
- en: Our tf_decode_sentence() is about 10× faster than the eager version. Not bad!
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 tf_decode_sentence() 比 eager 版本快了约 10 倍。还不错！
- en: Note that this inference setup, although very simple, is rather inefficient,
    because we reprocess the entire source sentence and the entire generated target
    sentence every time we sample a new word. In a practical application, you’d factor
    the encoder and the decoder as two separate models, and your decoder would run
    only a single step at each token-sampling iteration, reusing its previous internal
    state.
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管这种推理设置非常简单，但相当低效，因为每次抽样新单词时，我们都会重新处理整个源句子和整个生成的目标句子。在实际应用中，您应该将编码器和解码器分开为两个单独的模型，并且您的解码器每次仅在抽样迭代中运行一步，重复使用其先前的内部状态。
- en: Here are our translation results. Our model works decently well for a toy model,
    though it still makes many basic mistakes.
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的翻译结果。对于一个玩具模型来说，我们的模型效果还不错，尽管仍然会出现许多基本错误。
- en: Listing 11.32 Some sample results from the recurrent translation model
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.32 递归翻译模型的一些样本结果
- en: Who is in this room?
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: 谁在这个房间里？
- en: '[start] quién está en esta habitación [end]'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] 在这个房间里谁 [end]'
- en: '-'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: That doesn’t sound too dangerous.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 那听起来不太危险。
- en: '[start] eso no es muy difícil [end]'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] 那不太难 [end]'
- en: '-'
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: No one will stop me.
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 没人能阻止我。
- en: '[start] nadie me va a hacer [end]'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] 没有人能阻止我 [end]'
- en: '-'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: Tom is friendly.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: 汤姆很友好。
- en: '[start] tom es un buen [UNK] [end]'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] 汤姆很友好 [end]'
- en: 'There are many ways this toy model could be improved: we could use a deep stack
    of recurrent layers for both the encoder and the decoder (note that for the decoder,
    this makes state management a bit more involved). We could use an LSTM instead
    of a GRU. And so on. Beyond such tweaks, however, the RNN approach to sequence-to-sequence
    learning has a few fundamental limitations:'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: 这个玩具模型有很多改进的方法：我们可以对编码器和解码器都使用堆叠的深度循环层（请注意，对于解码器，这会使状态管理变得更加复杂）。我们可以使用 LSTM
    而不是 GRU 等等。除了这些微调之外，RNN 方法用于序列到序列学习具有一些根本上的限制：
- en: The source sequence representation has to be held entirely in the encoder state
    vector(s), which puts significant limitations on the size and complexity of the
    sentences you can translate. It’s a bit as if a human were translating a sentence
    entirely from memory, without looking twice at the source sentence while producing
    the translation.
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源序列表示必须完全保存在编码器状态向量中，这对你能够翻译的句子的大小和复杂度施加了重要的限制。这有点像一个人完全从记忆中翻译句子，而不在产生翻译时再看一眼源语句子。
- en: RNNs have trouble dealing with very long sequences, because they tend to progressively
    forget about the past—by the time you’ve reached the 100th token in either sequence,
    little information remains about the start of the sequence. That means RNN-based
    models can’t hold onto long-term context, which can be essential for translating
    long documents.
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 在处理非常长的序列时会遇到麻烦，因为它们往往会逐渐忘记过去——当你已经到达任一序列的第100个标记时，关于序列开头的信息已经几乎消失了。这意味着基于
    RNN 的模型无法保持长期的上下文，而这对于翻译长文档可能是必要的。
- en: These limitations are what has led the machine learning community to embrace
    the Transformer architecture for sequence-to-sequence problems. Let’s take a look.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制正是机器学习社区采用 Transformer 架构解决序列到序列问题的原因。让我们来看一下。
- en: 11.5.3 Sequence-to-sequence learning with Transformer
  id: totrans-1305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.3 带 Transformer 的序列到序列学习
- en: Sequence-to-sequence learning is the task where Transformer really shines. Neural
    attention enables Transformer models to successfully process sequences that are
    considerably longer and more complex than those RNNs can handle.
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列学习是 Transformer 真正发挥作用的任务。神经注意力使得 Transformer 模型能够成功处理比 RNN 更长、更复杂的序列。
- en: As a human translating English to Spanish, you’re not going to read the English
    sentence one word at a time, keep its meaning in memory, and then generate the
    Spanish sentence one word at a time. That may work for a five-word sentence, but
    it’s unlikely to work for an entire paragraph. Instead, you’ll probably want to
    go back and forth between the source sentence and your translation in progress
    and pay attention to different words in the source as you’re writing down different
    parts of your translation.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个将英语翻译成西班牙语的人，你不会逐个单词地阅读英语句子，将其意义记在脑海中，然后再逐个单词地生成西班牙语句子。这对于一个五个单词的句子可能有效，但对于整个段落而言可能很难奏效。相反，你可能需要在源语句子和正在翻译的语句之间来回切换，并在写下翻译的不同部分时注意源语句子中的不同单词。
- en: 'That’s exactly what you can achieve with neural attention and Transformers.
    You’re already familiar with the Transformer encoder, which uses self-attention
    to produce context-aware representations of each token in an input sequence. In
    a sequence-to-sequence Transformer, the Transformer encoder would naturally play
    the role of the encoder, which reads the source sequence and produces an encoded
    representation of it. Unlike our previous RNN encoder, though, the Transformer
    encoder keeps the encoded representation in a sequence format: it’s a sequence
    of context-aware embedding vectors.'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是你可以通过神经注意力和 Transformer 实现的。你已经熟悉了 Transformer 编码器，它使用自注意力来为输入序列中的每个标记产生上下文感知表示。在序列到序列
    Transformer 中，Transformer 编码器自然地扮演编码器的角色，阅读源序列并生成其编码表示。不像以前的 RNN 编码器，Transformer
    编码器将编码表示保持为序列格式：它是一系列上下文感知嵌入向量。
- en: 'The second half of the model is the *Transformer decoder*. Just like the RNN
    decoder, it reads tokens 1…*N* in the target sequence and tries to predict token
    *N* + 1\. Crucially, while doing this, it uses neural attention to identify which
    tokens in the encoded source sentence are most closely related to the target token
    it’s currently trying to predict— perhaps not unlike what a human translator would
    do. Recall the query-key-value model: in a Transformer decoder, the target sequence
    serves as an attention “query” that is used to pay closer attention to different
    parts of the source sequence (the source sequence plays the roles of both keys
    and values).'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的第二部分是 *Transformer 解码器*。就像 RNN 解码器一样，它读取目标序列中的令牌 1…*N* 并尝试预测令牌 *N* + 1。至关重要的是，在执行此操作时，它使用神经注意力来识别编码源句子中与当前正在尝试预测的目标令牌最相关的令牌——也许与人类翻译者所做的事情类似。回想一下查询-键-值模型：在
    Transformer 解码器中，目标序列充当用于更密切关注源序列不同部分的注意力“查询”的角色（源序列扮演了键和值的角色）。
- en: THE TRANSFORMER DECODER
  id: totrans-1310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TRANSFORMER 解码器
- en: '[Figure 11.14](#fig11-14) shows the full sequence-to-sequence Transformer.
    Look at the decoder internals: you’ll recognize that it looks very similar to
    the Transformer encoder, except that an extra attention block is inserted between
    the self-attention block applied to the target sequence and the dense layers of
    the exit block.'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.14](#fig11-14) 展示了完整的序列到序列 Transformer。看一下解码器的内部：你会认识到它看起来非常类似于 Transformer
    编码器，只是在应用于目标序列的自注意力块和退出块的密集层之间插入了一个额外的注意力块。'
- en: '![Image](../images/f0393-01.jpg)'
  id: totrans-1312
  prefs: []
  type: TYPE_IMG
  zh: '![Image](../images/f0393-01.jpg)'
- en: '**Figure 11.14 The TransformerDecoder is similar to the TransformerEncoder,
    except it features an additional attention block where the keys and values are
    the source sequence encoded by the TransformerEncoder. Together, the encoder and
    the decoder form an end-to-end Transformer.**'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 11.14 TransformerDecoder 类似于 TransformerEncoder，不同之处在于它具有一个额外的注意力块，其中键和值是由
    TransformerEncoder 编码的源序列。编码器和解码器共同形成一个端到端的 Transformer。**'
- en: Let’s implement it. Like for the TransformerEncoder, we’ll be creating a new
    layer class. Before we focus on the call(), method, where the action happens,
    let’s start by defining the class constructor, containing the layers we’re going
    to need.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来实现它。就像对于 TransformerEncoder 一样，我们将创建一个新的层类。在我们关注 call() 方法之前，这个方法是发生动作的地方，让我们先定义类构造函数，包含我们将需要的层。
- en: Listing 11.33 The TransformerDecoder
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: '**TransformerDecoder** 列表 11.33'
- en: layer_transformer_decoder <- new_layer_class(
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_decoder <- new_layer_class(
- en: classname = "TransformerDecoder",
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: classname = "TransformerDecoder",
- en: initialize = function(embed_dim, dense_dim, num_heads, …) {
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: initialize = function(embed_dim, dense_dim, num_heads, …) {
- en: super$initialize(…)
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: super$initialize(…)
- en: self$embed_dim <- embed_dim
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: self$embed_dim <- embed_dim
- en: self$dense_dim <- dense_dim
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense_dim <- dense_dim
- en: self$num_heads <- num_heads
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: self$num_heads <- num_heads
- en: self$attention_1 <- layer_multi_head_attention(num_heads = num_heads,
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: self$attention_1 <- layer_multi_head_attention(num_heads = num_heads,
- en: key_dim = embed_dim)
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: key_dim = embed_dim)
- en: self$attention_2 <- layer_multi_head_attention(num_heads = num_heads,
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: self$attention_2 <- layer_multi_head_attention(num_heads = num_heads,
- en: key_dim = embed_dim)
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: key_dim = embed_dim)
- en: self$dense_proj <- keras_model_sequential() %>%
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: self$dense_proj <- keras_model_sequential() %>%
- en: layer_dense(dense_dim, activation = "relu") %>%
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(dense_dim, activation = "relu") %>%
- en: layer_dense(embed_dim)
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(embed_dim)
- en: self$layernorm_1 <- layer_layer_normalization()
  id: totrans-1330
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_1 <- layer_layer_normalization()
- en: self$layernorm_2 <- layer_layer_normalization()
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_2 <- layer_layer_normalization()
- en: self$layernorm_3 <- layer_layer_normalization()
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_3 <- layer_layer_normalization()
- en: self$supports_masking <- TRUE➊
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: self$supports_masking <- TRUE➊
- en: '},'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: get_config = function() {
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: get_config = function() {
- en: config <- super$get_config()
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: config <- super$get_config()
- en: for (name in c("embed_dim", "num_heads", "dense_dim"))
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: for (name in c("embed_dim", "num_heads", "dense_dim"))
- en: config[[name]] <- self[[name]]
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: config[[name]] <- self[[name]]
- en: config
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: config
- en: '},'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: ➊ **This attribute ensures that the layer will propagate its input mask to its
    outputs; masking in Keras is explicitly opt-in. If you pass a mask to a layer
    that doesn't implement compute_mask() and that doesn't expose this supports_masking
    attribute, that's an error.**
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **这个属性确保层将其输入掩码传播到其输出；在 Keras 中，掩码必须明确地选择。如果你将一个掩码传递给一个不实现 compute_mask()
    并且不公开这个 supports_masking 属性的层，那就是一个错误。**
- en: 'The call() method is almost a straightforward rendering of the connectivity
    diagram from [figure 11.14](#fig11-14). But there’s an additional detail we need
    to take into account: *causal padding*. Causal padding is absolutely critical
    to successfully training a sequence-to-sequence Transformer. Unlike an RNN, which
    looks at its input one step at a time, and thus will only have access to steps
    1…*N* to generate output step *N* (which is token *N*+1 in the target sequence),
    the TransformerDecoder is order agnostic: it looks at the entire target sequence
    at once. If it were allowed to use its entire input, it would simply learn to
    copy input step *N*+1 to location *N* in the output. The model would thus achieve
    perfect training accuracy, but of course, when running inference, it would be
    completely useless, because input steps beyond *N* aren’t available.'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: call() 方法几乎是从 [图 11.14](#fig11-14) 的连通性图的直观渲染。但是还有一个额外的细节我们需要考虑：*因果填充*。因果填充对于成功训练序列到序列
    Transformer 是绝对关键的。与 RNN 不同，RNN 逐步查看其输入，因此在生成输出步骤 *N*（这是目标序列中的标记 *N*+1）时只能访问步骤
    1…*N* 的信息，TransformerDecoder 是无序的：它一次查看整个目标序列。如果允许它使用其整个输入，它将简单地学会将输入步骤 *N*+1
    复制到输出的位置 *N*。因此，该模型将实现完美的训练准确度，但当进行推断时，它将完全无用，因为超出 *N* 的输入步骤是不可用的。
- en: 'The fix is simple: we’ll mask the upper half of the pairwise attention matrix
    to prevent the model from paying any attention to information from the future—only
    information from tokens 1…*N* in the target sequence should be used when generating
    target token *N*+1\. To do this, we’ll add a get_causal_attention_mask(inputs)
    method to our TransformerDecoder to retrieve an attention mask that we can pass
    to our MultiHeadAttention layers.'
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方法很简单：我们将屏蔽成对注意力矩阵的上半部分，以防止模型关注未来的任何信息——只有目标序列中的标记 1…*N* 的信息应该在生成目标标记 *N*+1
    时使用。为此，我们将在我们的 TransformerDecoder 中添加一个 get_causal_attention_mask(inputs) 方法，以检索我们可以传递给我们的
    MultiHeadAttention 层的注意力屏蔽。
- en: '**Listing 11.34 TransformerDecoder method that generates a causal mask**'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: '**列出 11.34 TransformerDecoder 生成因果掩码的方法**'
- en: get_causal_attention_mask = function(inputs) {
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: get_causal_attention_mask = function(inputs) {
- en: c(batch_size, sequence_length, .) %<-%➊
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: c(batch_size, sequence_length, .) %<-%➊
- en: tf$unstack(tf$shape(inputs))
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: tf$unstack(tf$shape(inputs))
- en: x <- tf$range(sequence_length)➋
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: x <- tf$range(sequence_length)➋
- en: i <- x[, tf$newaxis]
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: i <- x[, tf$newaxis]
- en: j <- x[tf$newaxis, ]
  id: totrans-1350
  prefs: []
  type: TYPE_NORMAL
  zh: j <- x[tf$newaxis, ]
- en: mask <- tf$cast(i >= j, "int32")➌ ➍
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: mask <- tf$cast(i >= j, "int32")➌ ➍
- en: tf$tile(mask[tf$newaxis, , ],
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: tf$tile(mask[tf$newaxis, , ],
- en: tf$stack(c(batch_size, 1L, 1L)))➎
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: tf$stack(c(batch_size, 1L, 1L)))➎
- en: '},'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: '},'
- en: ➊ **The third axis is encoding_length; we do not use it here.**
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **第三个轴是 encoding_length；我们在这里不使用它。**
- en: ➋ **Integer sequence [0, 1, 2, … sequence_length-1]**
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **整数序列 [0, 1, 2, … sequence_length-1]**
- en: ➌ **Use Tensor broadcasting in our >= operation. Cast dtype bool to int32.**
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **在我们的 >= 操作中使用 Tensor 广播。将 dtype bool 转换为 int32。**
- en: ➍ **mask is a square matrix with shape (sequence_length, sequence_length), with
    1s in the lower triangle and 0s everywhere else. For example, if sequence_length
    is 4, mask is:**
  id: totrans-1358
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **掩码是一个形状为 (sequence_length, sequence_length) 的方阵，其中下三角有 1，其他地方为 0。例如，如果 sequence_length
    是 4，那么掩码是：**
- en: tf.Tensor([[1 0 0 0]
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: tf.Tensor([[1 0 0 0]
- en: '[1 1 0 0]'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 1 0 0]'
- en: '[1 1 1 0]'
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 1 1 0]'
- en: '[1 1 1 1]], shape=(4, 4), dtype=int32)'
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: '[1 1 1 1]], shape=(4, 4), dtype=int32)'
- en: ➎ **Add a batch dimension to mask, then tile (rep()) along the batch dim for
    batch_size times. The returned tensor has shape (batch_size, sequence_length,
    sequence_length).**
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **向掩码添加一个批量维度，然后沿着批量维度复制（rep()） batch_size 次。返回的张量形状为 (batch_size, sequence_length,
    sequence_length)。**
- en: Now we can write down the full call() method implementing the forward pass of
    the decoder.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以写下实现解码器前向传递的完整 call() 方法。
- en: Listing 11.35 The forward pass of the TransformerDecoder
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 11.35 列出了 TransformerDecoder 的前向传递
- en: call = function(inputs, encoder_outputs, mask = NULL) {
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: call = function(inputs, encoder_outputs, mask = NULL) {
- en: causal_mask <- self$get_causal_attention_mask(inputs)➊
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: causal_mask <- self$get_causal_attention_mask(inputs)➊
- en: if (is.null(mask))➋
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: if (is.null(mask))➋
- en: mask <- causal_mask
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: mask <- causal_mask
- en: else
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: else
- en: mask %<>% { tf$minimum(tf$cast(.[, tf$newaxis, ], "int32"),
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: mask %<>% { tf$minimum(tf$cast(.[, tf$newaxis, ], "int32"),
- en: causal_mask) }➌
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
  zh: causal_mask) }➌
- en: inputs %>%
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: inputs %>%
- en: '{ self$attention_1(query = ., value = ., key = .,'
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: '{ self$attention_1(query = ., value = ., key = .,'
- en: attention_mask = causal_mask) + . } %>%➍
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: attention_mask = causal_mask) + . } %>%➍
- en: self$layernorm_1() %>%➎
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_1() %>%➎
- en: '{ self$attention_2(query = .,'
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: '{ self$attention_2(query = .,'
- en: value = encoder_outputs,➏
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: value = encoder_outputs,➏
- en: key = encoder_outputs,➏
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: key = encoder_outputs,➏
- en: attention_mask = mask) + . } %>%➐
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: attention_mask = mask) + . } %>%➐
- en: self$layernorm_2() %>%➑
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_2() %>%➑
- en: '{ self$dense_proj(.) + . } %>%➒'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: '{ self$dense_proj(.) + . } %>%➒'
- en: self$layernorm_3()
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: self$layernorm_3()
- en: '})'
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: ➊ **Retrieve the causal mask.**
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **检索因果掩码。**
- en: ➋ **The mask supplied in the call is the padding mask (it describes padding
    locations in the target sequence).**
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **在调用中提供的掩码是填充掩码（它描述目标序列中的填充位置）**。
- en: ➌ **Combine the padding mask with the causal mask.**
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将填充掩码与因果屏蔽组合。**
- en: ➍ **Pass the causal mask to the first attention layer, which performs self-attention
    over the target sequence.**
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **将因果屏蔽传递给第一个注意力层，该注意力层在目标序列上执行自我注意力。**
- en: ➎ **The output of attention_1() with residual added is passed to layernorm_1().**
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: ➎ **将带有残差的attention_1()输出传递给layernorm_1()。**
- en: ➏ **Use encoder_outputs supplied in the call as the value and key to attention_2().**
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: ➏ **在调用中使用encoder_output作为attention_2()的value和key参数。**
- en: ➐ **Pass the combined mask to the second attention layer, which relates the
    source sequence to the target sequence.**
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: ➐ **将组合屏蔽传递给第二个注意力层，该层将源序列与目标序列相关联。**
- en: ➑ **The output of attention_2() with residual added is passed to layernorm_2().**
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: ➑ **将带有残差的attention_2()输出传递给layernorm_2()。**
- en: ➒ **The output of dense_proj() with residual is added and passed to layernorm_3().**
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: ➒ **将带有残差的dense_proj()输出加起来并传递到layernorm_3()。**
- en: 'PUTTING IT ALL TOGETHER: A TRANSFORMER FOR MACHINE TRANSLATION'
  id: totrans-1394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容组合到一起：面向机器翻译的Transformer
- en: 'The end-to-end Transformer is the model we’ll be training. It maps the source
    sequence and the target sequence to the target sequence one step in the future.
    It straightforwardly combines the pieces we’ve built so far: PositionalEmbedding
    layers, the TransformerEncoder, and the TransformerDecoder. Note that both the
    Transformer-Encoder and the TransformerDecoder are shape invariant, so you could
    be stacking many of them to create a more powerful encoder or decoder. In our
    example, we’ll stick to a single instance of each.'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端Transformer是我们将要训练的模型。它将源序列和目标序列映射到目标序列的下一个步骤。它直接组合了我们迄今为止构建的部分：PositionalEmbedding层，TransformerEncoder和TransformerDecoder。请注意，Transformer-Encoder和TransformerDecoder都形状不变，因此您可以堆叠许多它们来创建更强大的编码器或解码器。在我们的示例中，我们将坚持每个部分单独一个实例。
- en: '**Listing 11.36 End-to-end Transformer**'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.36 端到端Transformer**'
- en: embed_dim <- 256
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: embed_dim <- 256
- en: dense_dim <- 2048
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: dense_dim <- 2048
- en: num_heads <- 8
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: num_heads <- 8
- en: encoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "english")
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: encoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "english")
- en: encoder_outputs <- encoder_inputs %>%
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: encoder_outputs <- encoder_inputs %>%
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
- en: layer_transformer_encoder(embed_dim, dense_dim, num_heads)➊
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_encoder(embed_dim, dense_dim, num_heads)➊
- en: transformer_decoder <-
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: transformer_decoder <-
- en: layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)➋
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: layer_transformer_decoder(NULL, embed_dim, dense_dim, num_heads)➋
- en: decoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "spanish")
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: decoder_inputs <- layer_input(shape(NA), dtype = "int64", name = "spanish")
- en: decoder_outputs <- decoder_inputs %>%
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: decoder_outputs <- decoder_inputs %>%
- en: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: layer_positional_embedding(sequence_length, vocab_size, embed_dim) %>%
- en: transformer_decoder(., encoder_outputs) %>%➌
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: transformer_decoder(., encoder_outputs) %>%➌
- en: layer_dropout(0.5) %>%
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dropout(0.5) %>%
- en: layer_dense(vocab_size, activation = "softmax")➍
  id: totrans-1411
  prefs: []
  type: TYPE_NORMAL
  zh: layer_dense(vocab_size, activation = "softmax")➍
- en: transformer <- keras_model(list(encoder_inputs, decoder_inputs),
  id: totrans-1412
  prefs: []
  type: TYPE_NORMAL
  zh: transformer <- keras_model(list(encoder_inputs, decoder_inputs),
- en: decoder_outputs)
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: decoder_outputs)
- en: ➊ **Encode the source sentence.**
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **编码源句子。**
- en: ➋ **Pass NULL for the first argument, so a layer instance is created and returned
    directly and not composed with anything yet.**
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **对于第一个参数传递NULL，以便直接创建并返回一个层实例，而不是将其与任何内容组合。**
- en: ➌ **Encode the target sentence and combine it with the encoded source sentence.**
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **编码目标句子并将其与编码的源句子结合起来。**
- en: ➍ **Predict a word for each output position.**
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **针对每个输出位置预测一个单词。**
- en: We’re now ready to train our model—we get to 67% accuracy, a good deal above
    the GRU-based model.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备训练我们的模型，我们得到了67％的准确率，比基于GRU的模型高得多。
- en: '**Listing 11.37 Training the sequence-to-sequence Transformer**'
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: '**图11.37 训练序列到序列的Transformer**'
- en: transformer %>%
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: transformer %>%
- en: compile(optimizer = "rmsprop",
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: compile(optimizer = "rmsprop",
- en: loss = "sparse_categorical_crossentropy",
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: loss = "sparse_categorical_crossentropy",
- en: metrics = "accuracy")
  id: totrans-1423
  prefs: []
  type: TYPE_NORMAL
  zh: metrics = "accuracy")
- en: transformer %>%
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: transformer %>%
- en: fit(train_ds, epochs = 30, validation_data = val_ds)
  id: totrans-1425
  prefs: []
  type: TYPE_NORMAL
  zh: fit(train_ds, epochs = 30, validation_data = val_ds)
- en: Finally, let’s try using our model to translate never-seen-before English sentences
    from the test set. The setup is identical to what we used for the sequence-to-sequence
    RNN model; all that’s changed is we’re replaced seq2seq_rnn with transformer,
    and we drop the extra token that we configured our target_vectorization() layer
    to add.
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们尝试使用我们的模型来翻译来自测试集的从未见过的英语句子。设置与我们用于序列到序列 RNN 模型相同；唯一改变的是我们将 seq2seq_rnn
    替换为 transformer，并且删除了我们配置的 target_vectorization() 层添加的额外标记。
- en: '**Listing 11.38 Translating new sentences with our Transformer model**'
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: '**列表 11.38 使用我们的 Transformer 模型翻译新句子**'
- en: tf_decode_sequence <- tf_function(function(input_sentence) {
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: tf_decode_sequence <- tf_function(function(input_sentence) {
- en: withr::local_options(tensorflow.extract.style = "python")
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: withr::local_options(tensorflow.extract.style = "python")
- en: tokenized_input_sentence <- input_sentence %>%
  id: totrans-1430
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_input_sentence <- input_sentence %>%
- en: as_tensor(shape = c(1, 1)) %>%
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: as_tensor(shape = c(1, 1)) %>%
- en: source_vectorization()
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: source_vectorization()
- en: spa_vocab <- as_tensor(spa_vocab)
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: spa_vocab <- as_tensor(spa_vocab)
- en: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <- as_tensor("[start]", shape = c(1, 1))
- en: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
  id: totrans-1435
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in tf$range(as.integer(max_decoded_sentence_length))) {
- en: tokenized_target_sentence <-
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence <-
- en: target_vectorization(decoded_sentence)[, NA:-1]➊
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: target_vectorization(decoded_sentence)[, NA:-1]➊
- en: next_token_predictions <-➋
  id: totrans-1438
  prefs: []
  type: TYPE_NORMAL
  zh: next_token_predictions <-➋
- en: transformer(list(tokenized_input_sentence,
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: transformer(list(tokenized_input_sentence,
- en: tokenized_target_sentence))
  id: totrans-1440
  prefs: []
  type: TYPE_NORMAL
  zh: tokenized_target_sentence))
- en: sampled_token_index <- tf$argmax(next_token_predictions[0, i, ])
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token_index <- tf$argmax(next_token_predictions[0, i, ])
- en: sampled_token <- spa_vocab[sampled_token_index]➌
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: sampled_token <- spa_vocab[sampled_token_index]➌
- en: decoded_sentence <-
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence <-
- en: tf$strings$join(c(decoded_sentence, sampled_token),
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: tf$strings$join(c(decoded_sentence, sampled_token),
- en: separator = " ")
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: separator = " ")
- en: if (sampled_token == "[end]")➍
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: if (sampled_token == "[end]")➍
- en: break
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: break
- en: '}'
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: decoded_sentence
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: decoded_sentence
- en: '})'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: '})'
- en: for (i in sample.int(nrow(test_pairs), 20)) {
  id: totrans-1451
  prefs: []
  type: TYPE_NORMAL
  zh: for (i in sample.int(nrow(test_pairs), 20)) {
- en: c(input_sentence, correct_translation) %<-% test_pairs[i, ]
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: c(input_sentence, correct_translation) %<-% test_pairs[i, ]
- en: cat(input_sentence, "\n")
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: cat(input_sentence, "\n")
- en: cat(input_sentence %>% as_tensor() %>%
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: cat(input_sentence %>% as_tensor() %>%
- en: tf_decode_sequence() %>% as.character(), "\n")
  id: totrans-1455
  prefs: []
  type: TYPE_NORMAL
  zh: tf_decode_sequence() %>% as.character(), "\n")
- en: cat("-\n")
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: cat("-\n")
- en: '}'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: '}'
- en: ➊ **Drop the last token; "python" style is not inclusive of a slice end.**
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **删除最后一个标记；“python” 样式不包括切片结尾。**
- en: ➋ **Sample the next token.**
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: ➋ **随机抽取下一个标记。**
- en: ➌ **Convert the next token prediction to a string, and append it to the generated
    sentence.**
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: ➌ **将下一个标记预测转换为字符串，并附加到生成的句子中。**
- en: ➍ **Exit condition**
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: ➍ **退出条件**
- en: Subjectively, the Transformer seems to perform significantly better than the
    GRU-based translation model. It’s still a toy model, but it’s a better toy model.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 主观上，Transformer 似乎比基于 GRU 的翻译模型表现要好得多。它仍然是一个玩具模型，但是是一个更好的玩具模型。
- en: Listing 11.39 Some sample results from the Transformer translation model
  id: totrans-1463
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.39 Transformer 翻译模型的一些示例结果
- en: This is a song I learned when I was a kid.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我小的时候学会的一首歌。
- en: '[start] esta es una canción que aprendí cuando'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] esta es una canción que aprendí cuando'
- en: '![Image](../images/common01.jpg) era chico [end]➊'
  id: totrans-1466
  prefs: []
  type: TYPE_NORMAL
  zh: '![Image](../images/common01.jpg) era chico [end]➊'
- en: '-'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: She can play the piano.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 她会弹钢琴。
- en: '[start] ella puede tocar piano [end]'
  id: totrans-1469
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] ella puede tocar piano [end]'
- en: '-'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: I’m not who you think I am.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 我不是你认为的那个人。
- en: '[start] no soy la persona que tú creo que soy [end]'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] no soy la persona que tú creo que soy [end]'
- en: '-'
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: '-'
- en: It may have rained a little last night.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 昨晚可能下了一点雨。
- en: '[start] puede que llueve un poco el pasado [end]'
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: '[start] puede que llueve un poco el pasado [end]'
- en: ➊ **Although the source sentence wasn't gendered, this translation assumes a
    male speaker. Keep in mind that translation models will often make unwarranted
    assumptions about their input data, which leads to algorithmic bias. In the worst
    cases, a model might hallucinate memorized information that has nothing to do
    with the data it's currently processing.**
  id: totrans-1476
  prefs: []
  type: TYPE_NORMAL
  zh: ➊ **尽管源句子没有明确性别，但这个翻译假设了一个男性说话者。请记住，翻译模型经常会对其输入数据做出不合理的假设，这会导致算法偏见。在最糟糕的情况下，模型可能会产生与当前处理的数据无关的记忆信息。**
- en: That concludes this chapter on natural language processing—you just went from
    the very basics to a fully fledged Transformer that can translate from English
    to Spanish. Teaching machines to make sense of language is the latest superpower
    you can add to your collection.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于自然语言处理的这一章节——你刚刚从最基础的知识到了一个完全成熟的 Transformer，可以将英语翻译成西班牙语。教会机器理解语言是你可以添加到自己技能集合中的最新超能力。
- en: Summary
  id: totrans-1478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'There are two kinds of NLP models: *bag-of-words models* that process sets
    of words or *N*-grams without taking into account their order, and *sequence models*
    that process word order. A bag-of-words model is made of Dense layers, whereas
    a sequence model could be an RNN, a 1D convnet, or a Transformer.'
  id: totrans-1479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有两种NLP模型：处理单词集或*N*-gram而不考虑其顺序的*词袋模型*，以及处理单词顺序的*序列模型*。词袋模型由稠密层构成，而序列模型可以是RNN、一维卷积网络或Transformer。
- en: When it comes to text classification, the ratio between the number of samples
    in your training data and the mean number of words per sample can help you determine
    whether you should use a bag-of-words model or a sequence model.
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本分类方面，训练数据中的样本数量与每个样本的平均单词数之间的比例可以帮助您确定是使用词袋模型还是序列模型。
- en: '*Word embeddings* are vector spaces where semantic relationships between words
    are modeled as distance relationships between vectors that represent those words.'
  id: totrans-1481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*词嵌入* 是语义关系建模为代表这些词的向量之间的距离关系的向量空间。'
- en: '*Sequence-to-sequence learning* is a generic, powerful learning framework that
    can be applied to solve many NLP problems, including machine translation. A sequence-to-sequence
    model is made of an encoder, which processes a source sequence, and a decoder,
    which tries to predict future tokens in target sequence by looking at past tokens,
    with the help of the encoder-processed source sequence.'
  id: totrans-1482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*序列到序列学习* 是一种通用、强大的学习框架，可用于解决许多NLP问题，包括机器翻译。序列到序列模型由一个编码器（处理源序列）和一个解码器（通过查看编码器处理的源序列的过去标记来尝试预测目标序列中的未来标记）组成。'
- en: '*Neural attention* is a way to create context-aware word representations. It’s
    the basis for the Transformer architecture.'
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*神经注意力* 是一种创建上下文感知词表示的方法。这是Transformer架构的基础。'
- en: The *Transformer* architecture, which consists of a TransformerEncoder and a
    TransformerDecoder, yields excellent results on sequence-to-sequence tasks. The
    first half, the TransformerEncoder, can also be used for text classification or
    any sort of single-input NLP task.
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Transformer* 架构由TransformerEncoder和TransformerDecoder组成，在序列到序列的任务上产生了出色的结果。前半部分，TransformerEncoder，也可以用于文本分类或任何单输入NLP任务。'
- en: ^([1](#endnote1)) Yoshua Bengio et al., “A Neural Probabilistic Language Model,”
    *Journal of Machine Learning Research* (2003).
  id: totrans-1485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([1](#endnote1)) Yoshua Bengio等人，《神经概率语言模型》，《机器学习研究杂志》（2003）。
- en: ^([2](#endnote2)) Ashish Vaswani et al., “Attention Is All You Need” (2017),
    [https://arxiv.org/abs/1706.03762.](https://www.arxiv.org/abs/1706.03762)
  id: totrans-1486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ^([2](#endnote2)) Ashish Vaswani等人，《关注是你所需要的一切》（2017），[https://arxiv.org/abs/1706.03762](https://www.arxiv.org/abs/1706.03762)。
