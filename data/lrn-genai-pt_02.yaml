- en: 2 Deep learning with PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 使用PyTorch进行深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: PyTorch tensors and basic operations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch张量和基本操作
- en: Preparing data for deep learning in PyTorch
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为PyTorch中的深度学习准备数据
- en: Building and training deep neural networks with PyTorch
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PyTorch构建和训练深度神经网络
- en: Conducting binary and multicategory classifications with deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习进行二进制和多类别分类
- en: Creating a validation set to decide training stop points
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建验证集以决定训练停止点
- en: In this book, we’ll use deep neural networks to generate a wide range of content,
    including text, images, shapes, music, and more. I assume you already have a foundational
    understanding of machine learning (ML) and, in particular, artificial neural networks.
    In this chapter, I’ll refresh your memory on essential concepts such as loss functions,
    activation functions, optimizers, and learning rates, which are crucial for developing
    and training deep neural networks. If you find any gaps in your understanding
    of these topics, I strongly encourage you to address them before proceeding with
    the projects in this book. Appendix B provides a summary of the basic skills and
    concepts needed, including the architecture and training of artificial neural
    networks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用深度神经网络生成各种内容，包括文本、图像、形状、音乐等。我假设你已经对机器学习（ML）有一个基础的了解，特别是人工神经网络。在本章中，我将回顾一些基本概念，如损失函数、激活函数、优化器和学习率，这些对于开发和管理深度神经网络至关重要。如果你在这些主题的理解上存在任何空白，我强烈建议你在继续本书的项目之前解决这些问题。附录B提供了所需的基本技能和概念的总结，包括人工神经网络的架构和训练。
- en: NOTE There are plenty of great ML books out there for you to choose from. Examples
    include *Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow* (2019,
    O’Reilly) and *Machine Learning, Animated* (2023, CRC Press). Both books use TensorFlow
    to create neural networks. If you prefer a book that uses PyTorch, I recommend
    *Deep Learning with PyTorch* (2020, Manning Publications).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：市面上有很多优秀的机器学习书籍供你选择。例如，包括《动手学习Scikit-Learn、Keras和TensorFlow》（2019年，O'Reilly）和《机器学习，动画》（2023年，CRC出版社）。这两本书都使用TensorFlow创建神经网络。如果你更喜欢使用PyTorch的书籍，我推荐《使用PyTorch的深度学习》（2020年，Manning
    Publications）。
- en: Generative AI models are frequently confronted with the task of either binary
    or multicategory classification. For instance, in generative adversarial networks
    (GANs), the discriminator undertakes the essential role of a binary classifier,
    its purpose being to distinguish between the fake samples created by the generator
    from real samples from the training set. Similarly, in the context of text generation
    models, whether in recurrent neural networks or Transformers, the overarching
    objective is to predict the subsequent character or word from an extensive array
    of possibilities (essentially a multicategory classification task).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式AI模型经常面临二进制或多类别分类的任务。例如，在生成对抗网络（GANs）中，判别器扮演着二进制分类器的关键角色，其目的是区分生成器创建的假样本和训练集中的真实样本。同样，在文本生成模型的背景下，无论是循环神经网络还是Transformer，其总体目标都是从大量可能性中预测下一个字符或单词（本质上是一个多类别分类任务）。
- en: In this chapter, you’ll learn how to use PyTorch to create deep neural networks
    to perform binary and multicategory classifications so that you become well-versed
    in deep learning and classification tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用PyTorch创建深度神经网络以执行二进制和多类别分类，以便你精通深度学习和分类任务。
- en: Specifically, you’ll engage in an end-to-end deep learning project in PyTorch,
    on a quest to classify grayscale images of clothing items into different categories
    such as coats, bags, sneakers, shirts, and so on. The intention is to prepare
    you for the creation of deep neural networks, capable of performing both binary
    and multicategory classification tasks in PyTorch. This, in turn, will get you
    ready for the upcoming chapters, where you use deep neural networks in PyTorch
    to create various generative models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，你将参与一个端到端的PyTorch深度学习项目，目标是将服装物品的灰度图像分类到不同的类别，如外套、包、运动鞋、衬衫等。目的是为你准备创建能够执行PyTorch中二进制和多类别分类任务的深度神经网络。这将使你为即将到来的章节做好准备，在这些章节中，你将使用PyTorch中的深度神经网络来创建各种生成模型。
- en: To train generative AI models, we harness a diverse range of data formats such
    as raw text, audio files, image pixels, and arrays of numbers. Deep neural networks
    created in PyTorch cannot take these forms of data directly as inputs. Instead,
    we must first convert them into a format that the neural networks understand and
    accept. Specifically, you’ll convert various forms of raw data into PyTorch tensors
    (fundamental data structures used to represent and manipulate data) before feeding
    them to generative AI models. Therefore, in this chapter, you’ll also learn the
    basics of data types, how to create various forms of PyTorch tensors, and how
    to use them in deep learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练生成式 AI 模型，我们利用各种数据格式，如原始文本、音频文件、图像像素和数字数组。在 PyTorch 中创建的深度神经网络不能直接以这些形式的数据作为输入。相反，我们必须首先将它们转换为神经网络理解和接受的形式。具体来说，你将把各种原始数据形式转换为
    PyTorch 张量（用于表示和操作数据的基本数据结构），然后再将它们提供给生成式 AI 模型。因此，在本章中，你还将学习数据类型的基础知识、如何创建各种形式的
    PyTorch 张量以及如何在深度学习中使用它们。
- en: Knowing how to perform classification tasks has many practical applications
    in our society. Classifications are widely used in healthcare for diagnostic purposes,
    such as identifying whether a patient has a particular disease (e.g., positive
    or negative for a specific cancer based on medical imaging or test results). They
    play a vital role in many business tasks (stock recommendations, credit card fraud
    detection, and so on). Classification tasks are also integral to many systems
    and services that we use daily such as spam detection and facial recognition.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 了解如何执行分类任务在我们的社会中有许多实际应用。分类在医疗保健中广泛用于诊断目的，例如确定患者是否患有特定疾病（例如，基于医学影像或测试结果，对特定癌症的阳性或阴性）。它们在许多商业任务中扮演着至关重要的角色（如股票推荐、信用卡欺诈检测等）。分类任务也是我们日常使用的许多系统和服务的核心，例如垃圾邮件检测和面部识别。
- en: 2.1 Data types in PyTorch
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 PyTorch 中的数据类型
- en: We’ll use datasets from a wide range of sources and formats in this book, and
    the first step in deep learning is to transform the inputs into arrays of numbers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将使用来自广泛来源和格式的数据集，深度学习的第一步是将输入转换为数字数组。
- en: In this section, you’ll learn how PyTorch converts different formats of data
    into algebraic structures known as *tensors*. Tensors can be represented as multidimensional
    arrays of numbers, similar to NumPy arrays but with several key differences, chief
    among them the ability of GPU accelerated training. There are different types
    of tensors depending on their end use, and you’ll learn how to create different
    types of tensors and when to use each type. We’ll discuss the data structure in
    PyTorch in this section by using the heights of the 46 U.S. presidents as our
    running example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解 PyTorch 如何将不同格式的数据转换为称为 *张量* 的代数结构。张量可以表示为多维数字数组，类似于 NumPy 数组，但有几个关键区别，其中最重要的是能够进行
    GPU 加速训练。根据其最终用途，存在不同类型的张量，你将学习如何创建不同类型的张量以及何时使用每种类型。我们将通过使用 46 位美国总统的高度作为示例来讨论
    PyTorch 中的数据结构。
- en: 'Refer to the instructions in appendix A to create a virtual environment and
    install PyTorch and Jupyter Notebook on your computer. Open the Jupyter Notebook
    app within the virtual environment and run the following line of code in a new
    cell:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅附录 A 中的说明，在计算机上创建虚拟环境并安装 PyTorch 和 Jupyter Notebook。在虚拟环境中打开 Jupyter Notebook
    应用程序，并在新单元格中运行以下代码行：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command will install the Matplotlib library on your computer, enabling
    you to plot images in Python.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将在你的计算机上安装 Matplotlib 库，使你能够在 Python 中绘制图像。
- en: 2.1.1 Creating PyTorch tensors
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 创建 PyTorch 张量
- en: When training deep neural networks, we feed the models with arrays of numbers
    as inputs. Depending on what a generative model is trying to create, these numbers
    have different types. For example, when generating images, the inputs are raw
    pixels in the form of integers between 0 and 255, but we’ll convert them to floating-point
    numbers between –1 and 1; when generating text, there is a “vocabulary” akin to
    a dictionary, and the input is a sequence of integers telling you which entry
    in the dictionary the word corresponds to.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络时，我们以数字数组的形式向模型提供输入。根据生成模型试图创建的内容，这些数字有不同的类型。例如，在生成图像时，输入是介于 0 和 255
    之间的整数形式的原始像素，但我们将它们转换为介于 –1 和 1 之间的浮点数；在生成文本时，有一个类似于字典的“词汇表”，输入是一个整数序列，告诉你单词对应字典中的哪个条目。
- en: 'NOTE The code for this chapter, as well as other chapters in this book, is
    available at the book’s GitHub repository: [https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章代码以及本书其他章节的代码可在本书的 GitHub 仓库中找到：[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)。
- en: 'Imagine you want to use PyTorch to calculate the average height of the 46 U.S.
    presidents. We can first collect the heights of the 46 U.S. presidents in centimeters
    and store them in a Python list:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想使用 PyTorch 计算美国 46 位总统的平均身高。我们首先收集 46 位美国总统的身高（以厘米为单位），并将它们存储在一个 Python
    列表中：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The numbers are in chronological order: the first value in the list, 189, indicates
    that the first U.S. president, George Washington, was 189 centimeters tall. The
    last value shows that Joe Biden’s height is 183 centimeters. We can convert a
    Python list into a PyTorch tensor by using the `tensor()` method in PyTorch:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字按时间顺序排列：列表中的第一个值 189 表示美国第一任总统乔治·华盛顿身高 189 厘米。最后一个值显示乔·拜登的身高为 183 厘米。我们可以通过使用
    PyTorch 中的 `tensor()` 方法将 Python 列表转换为 PyTorch 张量：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Converts a Python list to a PyTorch tensor
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将 Python 列表转换为 PyTorch 张量
- en: ② Specifies the data type in the PyTorch tensor
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在 PyTorch 张量中指定数据类型
- en: We specify the data type using the `dtype` argument in the `tensor()` method.
    The default data type in PyTorch tensors is `float32`, a 32-bit floating-point
    number. In the preceding code cell, we converted the data type to `float64`, double-precision
    floating-point numbers. `float64` provides more precise results than `float32`,
    but it takes longer to compute. There is a tradeoff between precision and computational
    costs. Which data type to use depends on the task at hand.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `tensor()` 方法中使用 `dtype` 参数指定数据类型。PyTorch 张量的默认数据类型是 `float32`，即 32 位浮点数。在前面的代码单元中，我们将数据类型转换为
    `float64`，双精度浮点数。`float64` 提供比 `float32` 更精确的结果，但计算时间更长。精度和计算成本之间存在权衡。使用哪种数据类型取决于手头的任务。
- en: Table 2.1 lists different data types and the corresponding PyTorch tensor types.
    These include integers and floating-point numbers with different precisions. Integers
    can also be either signed or unsigned.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 列出了不同的数据类型以及相应的 PyTorch 张量类型。这些包括具有不同精度的整数和浮点数。整数也可以是有符号或无符号的。
- en: Table 2.1 Data and tensor types in PyTorch
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.1 PyTorch 中的数据和张量类型
- en: '| PyTorch tensor type | dtype argument in `tensor()` | Data type |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| PyTorch 张量类型 | `tensor()` 方法中的 dtype 参数 | 数据类型 |'
- en: '| --- | --- | --- |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `FloatTensor` | `torch.float32 or torch.float` | 32-bit floating point |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| `FloatTensor` | `torch.float32 or torch.float` | 32 位浮点数 |'
- en: '| `HalfTensor` | `torch.float16 or torch.half` | 16-bit floating point |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| `HalfTensor` | `torch.float16 or torch.half` | 16 位浮点数 |'
- en: '| `DoubleTensor` | `torch.float64 or torch.double` | 64-bit floating point
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleTensor` | `torch.float64 or torch.double` | 64 位浮点数 |'
- en: '| `CharTensor` | `torch.int8` | 8-bit integer (signed) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `CharTensor` | `torch.int8` | 8 位整数（有符号） |'
- en: '| `ByteTensor` | `torch.uint8` | 8-bit integer (unsigned) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `ByteTensor` | `torch.uint8` | 8 位整数（无符号） |'
- en: '| `ShortTensor` | `torch.int16 or torch.short` | 16-bit integer (signed) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `ShortTensor` | `torch.int16 or torch.short` | 16 位整数（有符号） |'
- en: '| `IntTensor` | `torch.int32 or torch.int` | 32-bit integer (signed) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `IntTensor` | `torch.int32 or torch.int` | 32 位整数（有符号） |'
- en: '| `LongTensor` | `torch.int64 or torch.long` | 64-bit integer (signed) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `LongTensor` | `torch.int64 or torch.long` | 64 位整数（有符号） |'
- en: You can create a tensor with a certain data type in one of the two ways. The
    first way is to use the PyTorch class as specified in the first column of table
    2.1\. The second way is to use the `torch.tensor()` method and specify the data
    type using the `dtype` argument (the value of the argument is listed in the second
    column of table 2.1). For example, to convert the Python list `[1, 2, 3]` into
    a PyTorch tensor with 32-bit integers in it, you can use two methods in the following
    listing.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过两种方式之一创建具有特定数据类型的张量。第一种方式是使用表 2.1 的第一列中指定的 PyTorch 类。第二种方式是使用 `torch.tensor()`
    方法，并通过 `dtype` 参数指定数据类型（该参数的值列在表 2.1 的第二列中）。例如，要将 Python 列表 `[1, 2, 3]` 转换为包含
    32 位整数的 PyTorch 张量，您可以在以下列表中的两种方法中选择一种。
- en: Listing 2.1 Two ways of specifying tensor types
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 指定张量类型的两种方法
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Uses torch.IntTensor() to specify the tensor type
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用 torch.IntTensor() 指定张量类型
- en: ② Uses dtype=torch.int to specify the tensor type
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用 dtype=torch.int 指定张量类型
- en: 'This leads to the following output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Exercise 2.1
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2.1
- en: Use two different methods to convert the Python list `[5, 8, 10]` into a PyTorch
    tensor with 64-bit floating-point numbers in it. Consult the third row in table
    2.1 for this question.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用两种不同的方法将Python列表`[5, 8, 10]`转换为包含64位浮点数的PyTorch张量。请参考表2.1的第三行来回答这个问题。
- en: 'Many times, you need to create a PyTorch tensor with values 0 everywhere. For
    example, in GANs, we create a tensor of zeros as the labels for fake samples,
    as you’ll see in chapter 3\. The `zeros()` method in PyTorch generates a tensor
    of zeros with a certain shape. In PyTorch, a tensor is an n-dimensional array,
    and its shape is a tuple representing the size along each of its dimensions. The
    following lines of code generate a tensor of zeros with two rows and three columns:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，你需要创建一个所有值都为0的PyTorch张量。例如，在生成对抗网络（GANs）中，我们创建一个全零张量作为伪造样本的标签，正如你在第3章中将会看到的。PyTorch中的`zeros()`方法可以生成具有特定形状的全零张量。在PyTorch中，张量是一个n维数组，其形状是一个元组，表示其每个维度的尺寸。以下代码行生成一个具有两行三列的全零张量：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The tensor has a shape of (2, 3), which means the tensor is a 2D array; there
    are two elements in the first dimension and three elements in the second dimension.
    Here, we didn’t specify the data type, and the output has the default data type
    of `float32`*.*
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 该张量的形状为（2，3），这意味着该张量是一个2D数组；第一维有两个元素，第二维有三个元素。在这里，我们没有指定数据类型，输出默认数据类型为`float32`*
- en: 'From time to time, you need to create a PyTorch tensor with values 1 everywhere.
    For example, in GANs, we create a tensor of ones as the labels for real samples.
    Here we use the `ones()` method to create a 3D tensor with values 1 everywhere:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 不时地，你需要创建一个所有值都为1的PyTorch张量。例如，在GANs中，我们创建一个全1张量作为真实样本的标签。在这里，我们使用`ones()`方法创建一个所有值都为1的3D张量：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output is
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We have generated a 3D PyTorch tensor. The shape of the tensor is (1, 4, 5).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了一个3D PyTorch张量。张量的形状为（1，4，5）。
- en: Exercise 2.2
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.2
- en: Create a 3D PyTorch tensor with values 0 in it. Make the shape of the tensor
    (2, 3, 4).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个所有值为0的3D PyTorch张量。张量的形状为（2，3，4）。
- en: 'You can also use a NumPy array instead of a Python list in the tensor constructor:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在张量构造函数中使用NumPy数组代替Python列表：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output is
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 2.1.2 Index and slice PyTorch tensors
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 索引和切片PyTorch张量
- en: 'We use square brackets (`[ ]`) to index and slice PyTorch tensors, as we do
    with Python lists. Indexing and slicing allow us to operate on one or more elements
    in a tensor, instead of on all elements. To continue our example of the heights
    of the 46 U.S. presidents, if we want to assess the height of the third president,
    Thomas Jefferson, we can do the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用方括号（`[ ]`）来索引和切片PyTorch张量，就像我们使用Python列表一样。索引和切片使我们能够对一个或多个张量元素进行操作，而不是对所有元素进行操作。为了继续我们关于46位美国总统身高的例子，如果我们想评估第三位总统托马斯·杰斐逊的身高，我们可以这样做：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This leads to an output of
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致输出结果如下
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The output shows that the height of Thomas Jefferson was 189 centimeters.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示，托马斯·杰斐逊的身高为189厘米。
- en: 'We can use negative indexing to count from the back of the tensor. For example,
    to find the height of Donald Trump, who is the second to last president in the
    list, we use index –2:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用负索引从张量的后端进行计数。例如，为了找到列表中倒数第二位总统唐纳德·特朗普的身高，我们使用索引-2：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output is
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output shows that Trump’s height is 191 centimeters.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果显示，特朗普的身高为191厘米。
- en: 'What if we want to know the heights of five recent presidents in the tensor
    `heights_tensor`? We can obtain a slice of the tensor:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想知道`heights_tensor`张量中最近五位总统的身高怎么办？我们可以获取张量的一部分：
- en: '[PRE15]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The colon (`:`) is used to separate the starting and end index. If no starting
    index is provided, the default is 0; if no end index is provided, you include
    the very last element in the tensor (as we did in the preceding code cell). Negative
    indexing means you count from the back. The output is
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 冒号（`:`）用于分隔起始和结束索引。如果没有提供起始索引，则默认为0；如果没有提供结束索引，则包括张量中的最后一个元素（正如我们在前面的代码单元中所做的那样）。负索引意味着从后向前计数。输出结果如下
- en: '[PRE16]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The results show that the five recent presidents in the tensor (Clinton, Bush,
    Obama, Trump, and Biden) are 188, 182, 185, 191, and 183 centimeters tall, respectively.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，张量中的五位最近总统（克林顿、布什、奥巴马、特朗普和拜登）的身高分别为188、182、185、191和183厘米。
- en: Exercise 2.3
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.3
- en: Use slicing to obtain the heights of the first five U.S. presidents in the tensor
    `heights_tensor`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用切片从`heights_tensor`张量中获取前五位美国总统的身高。
- en: 2.1.3 PyTorch tensor shapes
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 PyTorch张量形状
- en: 'PyTorch tensors have an attribute *shape*, which tells us the dimensions of
    a tensor. It’s important to know the shapes of PyTorch tensors because mismatched
    shapes will lead to errors when we operate on them. For example, if we want to
    find out the shape of the tensor `heights_tensor`, we can do this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch张量有一个名为*shape*的属性，它告诉我们张量的维度。了解PyTorch张量的形状非常重要，因为不匹配的形状会导致我们在操作它们时出错。例如，如果我们想找出张量`heights_tensor`的形状，我们可以这样做：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output is
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This tells us that `heights_tensor` is a 1D tensor with 46 values in it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们`heights_tensor`是一个包含46个值的1D张量。
- en: 'You can also change the shape of a PyTorch tensor. To learn how, let’s first
    convert the heights from centimeters to feet. Since a foot is about 30.48 centimeters,
    we can accomplish this by dividing the tensor by 30.48:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以更改PyTorch张量的形状。要了解如何操作，让我们首先将高度从厘米转换为英尺。由于一英尺大约是30.48厘米，我们可以通过将张量除以30.48来完成此操作：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This leads to the following output (I omitted some values to save space; the
    complete output is in the book’s GitHub repository):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下输出（为了节省空间，我省略了一些值；完整的输出在本书的GitHub仓库中）：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The new tensor, `heights_in_feet`, stores the heights in feet. For example,
    the last value in the tensor shows that Joe Biden is 6.0039 feet tall.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 新的张量`heights_in_feet`用于存储以英尺为单位的高度。例如，张量中的最后一个值显示乔·拜登身高为6.0039英尺。
- en: 'We can use the `cat()` method in PyTorch to concatenate the two tensors:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用PyTorch中的`cat()`方法连接两个张量：
- en: '[PRE21]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The `dim` argument is used in various tensor operations to specify the dimension
    along which the operation is to be performed. In the preceding code cell, `dim=0`
    means we concatenate the two tensors along the first dimension. This leads to
    the following output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim`参数在多种张量操作中用于指定要执行操作的维度。在前面的代码单元格中，`dim=0`表示我们沿着第一个维度连接两个张量。这导致以下输出：'
- en: '[PRE22]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The resulting tensor is 1D with 92 values, with some values in centimeters
    and others in feet. Therefore, we need to reshape it into two rows and 46 columns
    so that the first row represents heights in centimeters and the second in feet:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 结果张量是1D的，包含92个值，其中一些值以厘米为单位，其他以英尺为单位。因此，我们需要将其重塑为两行46列，以便第一行表示厘米单位的高度，第二行表示英尺单位的高度：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The new tensor, `heights_reshaped`, is 2D with a shape of (2, 46). We can index
    and slice multidimensional tensors using square brackets as well. For example,
    to print out the height of Trump in feet, we can do this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 新的张量`heights_reshaped`是2D的，形状为(2, 46)。我们可以使用方括号索引和切片多维张量。例如，为了打印出特朗普的身高（以英尺为单位），我们可以这样做：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This leads to a result of
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下结果
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The command `heights_reshaped[1,-2]` tells Python to look for the value in the
    second row and the second to last column, which returns Trump’s height in feet,
    6.2664.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 命令`heights_reshaped[1,-2]`告诉Python查找第二行和倒数第二列的值，这返回了特朗普的身高，6.2664英尺。
- en: tip The number of indexes needed to refer to scalar values within the tensor
    is the same as the dimensionality of the tensor. That’s why we used only one index
    to locate values in the 1D tensor `heights_tensor` but we used two indexes to
    locate values in the 2D tensor `heights_reshaped`.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：引用张量内标量值所需的索引数量与张量的维度性相同。这就是为什么我们在1D张量`heights_tensor`中只使用一个索引来定位值，但在2D张量`heights_reshaped`中使用了两个索引来定位值。
- en: Exercise 2.4
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.4
- en: Use indexing to obtain the height of Joe Biden in the tensor `heights_reshaped`
    in centimeters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用索引从张量`heights_reshaped`中获取乔·拜登的身高（以厘米为单位）。
- en: 2.1.4 Mathematical operations on PyTorch tensors
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 PyTorch张量上的数学运算
- en: 'We can conduct mathematical operations on PyTorch tensors by using different
    methods such as `mean()`, `median()`, `sum()`, `max()`, and so on. For example,
    to find the median height of the 46 presidents in centimeters, we can do this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用不同的方法如`mean()`、`median()`、`sum()`、`max()`等在PyTorch张量上执行数学运算。例如，为了找到46位总统的中位身高（以厘米为单位），我们可以这样做：
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The code snippet `heights_reshaped[0,:]` returns the first row and all columns
    in the tensor `heights_reshaped`. The preceding line of code returns the median
    value in the first row, and this leads to an output of
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 代码片段`heights_reshaped[0,:]`返回张量`heights_reshaped`的第一行和所有列。前面的代码行返回第一行中的中位值，这导致以下输出
- en: '[PRE27]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This means the median height of U.S. presidents is 182 centimeters.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着美国总统的中位身高为182厘米。
- en: 'To find the average height in both rows, we can use the `dim=1` argument in
    the `mean()` method:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到两行中的平均身高，我们可以在`mean()`方法中使用`dim=1`参数：
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `dim=1` argument indicates that the averages are calculated by collapsing
    columns (the dimension indexed 1), effectively obtaining averages along the dimension
    indexed 0 (rows). The output is
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`dim=1` 参数表示通过折叠列（索引为1的维度），实际上是在索引为0的维度（行）上计算平均值。输出是'
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The results show that the average values in the two rows are 180.0652 centimeters
    and 5.9077 feet.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，两行中的平均值分别是180.0652厘米和5.9077英尺。
- en: 'To find out the tallest president, we can do this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出最高的总统，我们可以这样做：
- en: '[PRE30]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The output is
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE31]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The `torch.max()` method returns two tensors: a tensor `values` with the tallest
    president’s height (in centimeters and in feet), and a tensor `indices` with the
    indexes of the president with the maximum height. The results show that the 16th
    president (Lincoln) is the tallest, at 193 centimeters, or 6.332 feet.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.max()`方法返回两个张量：一个包含最高总统身高的张量`values`（以厘米和英尺为单位），以及一个包含身高最高的总统索引的张量`indices`。结果显示，第16任总统（林肯）是最高的，身高为193厘米，或6.332英尺。'
- en: Exercise 2.5
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.5
- en: Use the `torch.min()` method to find out the index and height of the shortest
    U.S. president.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torch.min()`方法找出最矮的美国总统的索引和身高。
- en: 2.2 An end-to-end deep learning project with PyTorch
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用PyTorch的端到端深度学习项目
- en: In the next few sections, you’ll work through an example deep learning project
    with PyTorch, learning to classify grayscale images of clothing items into 1 of
    the 10 types. In this section, we’ll first provide a high-level overview of the
    steps involved. We then discuss how to obtain training data for this project and
    how to preprocess the data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，你将通过PyTorch完成一个深度学习项目的示例，学习将服装物品的灰度图像分类为10种类型中的1种。在本节中，我们将首先提供一个涉及步骤的高级概述。然后，我们将讨论如何获取这个项目的训练数据以及如何预处理数据。
- en: '2.2.1 Deep learning in PyTorch: A high-level overview'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 PyTorch中的深度学习：高级概述
- en: Our job in this project is to create and train a deep neural network in PyTorch
    to classify grayscale images of clothing items. Figure 2.1 provides a diagram
    of the steps involved.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们的任务是创建并训练一个PyTorch深度神经网络，以对服装物品的灰度图像进行分类。图2.1提供了涉及步骤的示意图。
- en: '![](../../OEBPS/Images/CH02_F01_Liu.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F01_Liu.png)'
- en: Figure 2.1 The steps involved in training a deep learning model
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 深度学习模型训练涉及的步骤
- en: First, we’ll obtain a dataset of grayscale clothing images, as shown on the
    left of figure 2.1\. The images are in raw pixels, and we’ll convert them to PyTorch
    tensors in the form of float numbers (step 1). Each image comes with a label.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将获取如图2.1左侧所示的灰度服装图像数据集。图像是原始像素，我们将它们转换为浮点数形式的PyTorch张量（步骤1）。每张图像都附有标签。
- en: We’ll then create a deep neural network in PyTorch, as shown in the center of
    figure 2.1\. Some neural networks in this book involve convolutional neural networks
    (CNNs). For this simple classification problem, we’ll use dense layers only for
    the moment.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将在PyTorch中创建一个深度神经网络，如图2.1中心的所示。本书中的一些神经网络涉及卷积神经网络（CNNs）。对于这个简单的分类问题，我们目前只使用密集层。
- en: We’ll select a loss function for multicategory classification, and cross-entropy
    loss is commonly used for this task. Cross-entropy loss measures the difference
    between the predicted probability distribution and the true distribution of the
    labels. We’ll use the Adam optimizer (a variant of the gradient descent algorithm)
    to update the network’s weights during training. We set the learning rate to 0.001\.
    The learning rate controls how much the model’s weights are adjusted with respect
    to the loss gradient during training.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为多类别分类选择一个损失函数，交叉熵损失通常用于这项任务。交叉熵损失衡量预测概率分布与标签真实分布之间的差异。我们将在训练过程中使用Adam优化器（梯度下降算法的一种变体）来更新网络的权重。我们将学习率设置为0.001。学习率控制模型权重在训练过程中相对于损失梯度的调整程度。
- en: Optimizers in ML
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的优化器
- en: Optimizers in ML are algorithms that update model parameters based on gradient
    information to minimize the loss function. Stochastic Gradient Descent (SGD) is
    the most fundamental optimizer, utilizing straightforward updates based on the
    loss gradient. Adam, the most popular optimizer, is known for its efficiency and
    out-of-the-box performance, as it combines the strengths of the Adaptive Gradient
    Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp). Despite their
    differences, all optimizers aim to iteratively adjust parameters to minimize the
    loss function, each creating a unique optimization path to reach this goal.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的优化器是算法，根据梯度信息更新模型参数以最小化损失函数。随机梯度下降（SGD）是最基本的优化器，它使用基于损失梯度的简单更新。Adam是最受欢迎的优化器，以其效率和即插即用的性能而闻名，因为它结合了自适应梯度算法（AdaGrad）和根均方传播（RMSProp）的优点。尽管它们有所不同，但所有优化器都旨在迭代调整参数以最小化损失函数，每个优化器都创建一条独特的优化路径以达到这一目标。
- en: We’ll divide the training data into a train set and a validation set. In ML,
    we usually use the validation set to provide an unbiased evaluation of the model
    and to select the best hyperparameters such as the learning rate, number of epochs
    of training, and so on. The validation set can also be used to avoid overfitting
    the model in which the model works well in the training set but poorly on unseen
    data. An epoch is when all the training data is used to train the model once and
    only once.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把训练数据分成训练集和验证集。在机器学习中，我们通常使用验证集来提供模型的无偏评估，并选择最佳超参数，如学习率、训练epoch数等。验证集还可以用来避免模型过拟合，即模型在训练集中表现良好，但在未见过的数据上表现不佳。一个epoch是指所有训练数据被用来训练模型一次且仅一次。
- en: During training, you’ll iterate through the training data. During forward passes,
    you feed images through the network to obtain predictions (step 2) and compute
    the loss by comparing the predicted labels with the actual labels (step 3; see
    the right side of figure 2.1). You’ll then backpropagate the gradient through
    the network to update the weights. This is where the learning happens (step 4),
    as shown at the bottom of figure 2.1.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您将遍历训练数据。在正向传播过程中，您将图像通过网络以获得预测（步骤2）并计算损失，通过比较预测标签与实际标签（步骤3；见图2.1右侧）。然后，您将通过网络反向传播梯度以更新权重。这就是学习发生的地方（步骤4），如图2.1底部所示。
- en: You’ll use the validation set to determine when we should stop training. We
    calculate the loss in the validation set. If the model stops improving after a
    fixed number of epochs, we consider the model trained. We then evaluate the trained
    model on the test set to assess its performance in classifying images into different
    labels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用验证集来确定何时停止训练。我们在验证集中计算损失。如果模型在固定数量的epoch后停止改进，我们认为模型已训练完成。然后，我们在测试集上评估训练好的模型，以评估其在将图像分类到不同标签方面的性能。
- en: Now that you have a high-level overview of how deep learning in PyTorch works,
    let’s dive into the end-to-end project!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经对PyTorch中的深度学习有了高级概述，让我们深入到端到端项目吧！
- en: 2.2.2 Preprocessing data
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 数据预处理
- en: We’ll be using the Fashion Modified National Institute of Standards and Technology
    (MNIST) dataset in this project. Along the way, you’ll learn how to use the `datasets`
    and `transforms` packages in the Torchvision library, as well as the `Dataloader`
    packages in PyTorch that will help you for the rest of the book. You’ll use these
    tools to preprocess data throughout the book. The Torchvision library provides
    tools for image processing, including popular datasets, model architectures, and
    common image transformations for deep learning applications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用Fashion Modified National Institute of Standards and Technology
    (MNIST)数据集。在这个过程中，您将学习如何使用Torchvision库中的`datasets`和`transforms`包，以及PyTorch中的`Dataloader`包，这些包将帮助您在本书的其余部分。您将使用这些工具在本书中预处理数据。Torchvision库提供了图像处理工具，包括流行的数据集、模型架构和深度学习应用中的常见图像转换。
- en: We first import needed libraries and instantiate a `Compose()` class in the
    `transforms` package to transform raw images to PyTorch tensors.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的库并在`transforms`包中实例化一个`Compose()`类，以将原始图像转换为PyTorch张量。
- en: Listing 2.2 Transforming raw image data to PyTorch tensors
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2 将原始图像数据转换为PyTorch张量
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ① Composes several transforms together
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将多个转换组合在一起
- en: ② Converts image pixels to PyTorch tensors
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将图像像素转换为PyTorch张量
- en: ③ Normalizes the values to the range [–1, 1]
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将值归一化到范围[–1, 1]
- en: We use the `manual_seed()` method in PyTorch to fix the random state so that
    results are reproducible. The *transforms* package in Torchvision can help create
    a series of transformations to preprocess images. The `ToTensor()` class converts
    image data (in either Python Imaging Library (PIL) image formats or NumPy arrays)
    into PyTorch tensors. In particular, the image data are integers ranging from
    0 to 255, and the `ToTensor()` class converts them to float tensors with values
    in the range of 0.0 and 1.0.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 PyTorch 中的 `manual_seed()` 方法来固定随机状态，以便结果可重现。Torchvision 中的 *transforms*
    包可以帮助创建一系列转换以预处理图像。`ToTensor()` 类将图像数据（无论是 Python Imaging Library (PIL) 图像格式还是
    NumPy 数组）转换为 PyTorch 张量。特别是，图像数据是介于 0 到 255 之间的整数，而 `ToTensor()` 类将它们转换为介于 0.0
    和 1.0 范围内的浮点张量。
- en: The `Normalize()` class normalizes tensor images with mean and standard deviation
    for *n* channels. The Fashion MNIST data are grayscale images of clothing items
    so there is only one color channel. Later in this book, we’ll deal with images
    of three different color channels (red, green, and blue). In the preceding code
    cell, `Normalize([0.5],[0.5])` means that we subtract 0.5 from the data and divide
    the difference by 0.5\. The resulting image data range from –1 to 1\. Normalizing
    the input data to the range [–1, 1] allows gradient descent to operate more efficiently
    by maintaining more uniform step sizes across dimensions. This helps in faster
    convergence during training, and you’ll do this often in this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`Normalize()` 类使用 *n* 个通道的平均值和标准差对张量图像进行归一化。Fashion MNIST 数据是服装项目的灰度图像，因此只有一个颜色通道。在本书的后面部分，我们将处理具有三个不同颜色通道（红色、绿色和蓝色）的图像。在先前的代码单元中，`Normalize([0.5],[0.5])`
    表示从数据中减去 0.5，并将差值除以 0.5。结果图像数据范围从 –1 到 1。将输入数据归一化到 [–1, 1] 范围允许梯度下降在维度上保持更均匀的步长，这有助于在训练过程中更快地收敛。您将在本书中经常这样做。'
- en: NOTE The code in listing 2.2 only defines the data transformation process. It
    doesn’t perform the actual transformation, which happens in the next code cell.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：列表 2.2 中的代码仅定义了数据转换过程。它不执行实际的转换，这将在下一个代码单元中发生。
- en: 'Next, we use the *datasets* package in Torchvision to download the dataset
    to a folder on your computer and perform the transformation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 Torchvision 中的 *datasets* 包将数据集下载到您的计算机上的一个文件夹，并执行转换：
- en: '[PRE33]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ① Which dataset to download
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ① 下载哪个数据集
- en: ② Where to savethe data
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ② 数据保存的位置
- en: ③ The training or test dataset
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 训练或测试数据集
- en: ④ Whether or not to download the data to your computer
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 是否将数据下载到您的计算机上
- en: ⑤ Performs data transformation
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 执行数据转换
- en: 'You can print out the first sample in the training set:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以打印出训练集中的第一个样本：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The first sample consists of a tensor with 784 values and a label 9\. The 784
    numbers represent a 28 by 28 grayscale image (28 × 28 = 784), and the label 9
    means it’s an ankle boot. You may be wondering: How do you know the label 9 indicates
    an ankle boot? There are 10 different types of clothing items. The labels in the
    dataset are numbered from 0 to 9\. You can search online and find the text labels
    for the 10 categories (for example, I got the text labels here [https://github.com/pranay414/Fashion-MNIST-Pytorch](https://github.com/pranay414/Fashion-MNIST-Pytorch)).
    The list `text_labels` contains the 10 text labels corresponding to the numerical
    labels 0 to 9\. For example, if an item has a numerical label of 0 in the dataset,
    the corresponding text label is “t-shirt.” The list `text_labels` is defined as
    follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个样本包含一个具有 784 个值的张量和标签 9。784 个数字代表一个 28×28 的灰度图像（28 × 28 = 784），标签 9 表示这是一双短靴。您可能想知道：您如何知道标签
    9 表示短靴？数据集中有 10 种不同的服装项目。数据集中的标签从 0 到 9 编号。您可以在网上搜索并找到 10 个类别的文本标签（例如，我在这里找到了文本标签
    [https://github.com/pranay414/Fashion-MNIST-Pytorch](https://github.com/pranay414/Fashion-MNIST-Pytorch)）。`text_labels`
    列表包含与数值标签 0 到 9 对应的 10 个文本标签。例如，如果数据集中的项目具有数值标签 0，则相应的文本标签是“T恤”。`text_labels`
    列表定义如下：
- en: '[PRE35]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can plot the data to visualize the clothing items in the dataset.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以绘制数据以可视化数据集中的服装项目。
- en: Listing 2.3 Visualizing the clothing items
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 可视化服装项目
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ① Where to place the image
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 图像放置的位置
- en: ② Obtains the i-th image from the training data
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从训练数据中获取第 i 个图像
- en: ③ Converts the values from [–1,1] to [0,1]
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将值从 [–1,1] 转换为 [0,1]
- en: ④ Reshapes the image to 28 by 28
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将图像重塑为 28×28
- en: ⑤ Adds text label to each image
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 为每张图像添加文本标签
- en: The plot in figure 2.2 shows 24 clothing items such as coats, pullovers, sandals,
    and so on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 中的图表显示了 24 种服装项目，如大衣、套头衫、凉鞋等。
- en: '![](../../OEBPS/Images/CH02_F02_Liu.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F02_Liu.png)'
- en: Figure 2.2 Grayscale images of clothing items in the Fashion MNIST dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 Fashion MNIST数据集中服装项目的灰度图像。
- en: You’ll learn how to create deep neural networks with PyTorch to perform binary
    and multicategory classification problems in the next two sections.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的两节中，你将学习如何使用PyTorch创建深度神经网络，以执行二进制和多类别分类问题。
- en: 2.3 Binary classification
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 二进制分类
- en: In this section, we’ll first create batches of data for training. We then build
    a deep neural network in PyTorch for this purpose and train the model using the
    data. Finally, we’ll use the trained model to make predictions and test how accurate
    the predictions are. The steps involved with binary and multicategory classifications
    are similar, with a few notable exceptions that I’ll highlight later.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先为训练创建数据批次。然后，我们使用PyTorch构建一个深度神经网络用于此目的，并使用数据训练模型。最后，我们将使用训练好的模型进行预测并测试预测的准确性。二进制和多类别分类涉及到的步骤相似，但有几个显著的例外，我将在后面强调。
- en: 2.3.1 Creating batches
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 创建批次
- en: 'We’ll create a training set and a test set that contain only two types of clothing
    items: t-shirts and ankle boots. (Later in this chapter when we discuss multicategory
    classification, you’ll also learn to create a validation set to determine when
    to stop training.) The following code cell accomplishes that goal:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个只包含两种服装类型（T恤和踝靴）的训练集和测试集。（在本章后面讨论多类别分类时，你还将学习如何创建验证集以确定何时停止训练。）以下代码单元实现了该目标：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We only keep samples with numerical labels 0 and 9 to create a binary classification
    problem with a balanced training set. Next, we create batches for training the
    deep neural network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只保留标签为0和9的样本，以创建一个具有平衡训练集的二进制分类问题。接下来，我们为训练深度神经网络创建批次。
- en: Listing 2.4 Creating batches for training and testing
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 创建训练和测试的批次
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ① Creates batches for the binary training set
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为二进制训练集创建批次
- en: ② Number of samples in each batch
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ② 每个批次中的样本数量
- en: ③ Shuffles the observations when batching
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在批量处理时打乱观察值
- en: ④ Creates batches for the binary test set
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 为二进制测试集创建批次
- en: 'The `DataLoader` class in the PyTorch *utils* package helps create data iterators
    in batches. We set the batch size to 64\. We created two data loaders in listing
    2.4: a training set and a test set for binary classification. We shuffle the observations
    when creating batches to avoid correlations among the original dataset: the training
    is more stable if different labels are evenly distributed in the data loader.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch *utils* 包中的 `DataLoader` 类帮助批量创建数据迭代器。我们将批大小设置为64。在列表2.4中，我们创建了两个数据加载器：用于二进制分类的训练集和测试集。在创建批次时，我们随机打乱观察值，以避免原始数据集中的相关性：如果数据加载器中的标签分布均匀，则训练将更加稳定。
- en: 2.3.2 Building and training a binary classification model
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 构建和训练二进制分类模型
- en: We’ll first create a binary classification model. We then train the model by
    using the images of t-shirts and ankle boots. Once it’s trained, we’ll see if
    the model can tell t-shirts from ankle boots. We use PyTorch to create the following
    neural network by using the Pytorch `nn.Sequential` class (in later chapters,
    you’ll also learn to use the `nn.Module` class to create PyTorch neural networks).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个二进制分类模型。然后，我们使用T恤和踝靴的图像来训练模型。一旦训练完成，我们将看看模型能否区分T恤和踝靴。我们使用PyTorch的 `nn.Sequential`
    类（在后面的章节中，你还将学习如何使用 `nn.Module` 类来创建PyTorch神经网络）创建以下神经网络。
- en: Listing 2.5 Creating a binary classification model
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 创建二进制分类模型
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ① PyTorch automatically detects if a CUDA-enabled GPU is available.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ① PyTorch自动检测是否有可用的CUDA启用GPU。
- en: ② Creates a sequential neural network in PyTorch
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ② 在PyTorch中创建一个顺序神经网络
- en: ③ Numbers of input and output neurons in a linear layer
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 线性层中的输入和输出神经元数量
- en: ④ Applies ReLU activation to outputs of the layer
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 对层的输出应用ReLU激活函数
- en: ⑤ Applies sigmoid activation and moves the model to a GPU if available
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 应用sigmoid激活函数并将模型移动到可用的GPU上
- en: 'The `Linear()` class in PyTorch creates a linear transformation of the incoming
    data. This effectively creates a dense layer in the neural network. The input
    shape is 784 because we’ll later flatten the 2D image to a 1D vector with 28 ×
    28 = 784 values in it. We flatten the 2D image into a 1D tensor because dense
    layers only take 1D inputs. In later chapters, you’ll see that you don’t need
    to flatten images when you use convolutional layers. There are three hidden layers
    in the network, with 256, 128, and 32 neurons in them, respectively. The numbers
    256, 128, and 32 are chosen somewhat arbitrarily: changing them to, say, 300,
    200, and 50 won’t affect the training process. We apply the rectified linear unit
    (ReLU) activation function on the three hidden layers. The ReLU activation function
    decides whether a neuron should be turned on based on the weighted sum. These
    functions introduce nonlinearity to the output of a neuron so that the network
    can learn nonlinear relations between inputs and outputs. ReLU is your go-to activation
    function with very few exceptions, and you’ll encounter a few other activation
    functions in later chapters.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch中的`Linear()`类创建了对传入数据的线性变换。这实际上在神经网络中创建了一个密集层。输入形状是784，因为我们稍后将2D图像展平成一个包含28
    × 28 = 784个值的1D向量。我们将2D图像展平成一个1D张量，因为密集层只接受1D输入。在后面的章节中，你会看到当你使用卷积层时，不需要展平图像。网络中有三个隐藏层，分别包含256、128和32个神经元。256、128和32这些数字的选择是相当随意的：将它们改为，比如说，300、200和50，不会影响训练过程。我们在三个隐藏层上应用了ReLU激活函数。ReLU激活函数根据加权的总和决定是否激活神经元。这些函数向神经元的输出引入非线性，从而使网络能够学习输入和输出之间的非线性关系。ReLU是除了极少数例外情况外你首选的激活函数，你将在后面的章节中遇到一些其他的激活函数。
- en: The output of the last layer of the model contains a single value, and we use
    the sigmoid activation function to squeeze the number to the range [0, 1] so that
    it can be interpreted as the probability that the object is an ankle boot. With
    the complementary probability, the object is a t-shirt.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的最后一层的输出包含一个单一值，我们使用sigmoid激活函数将数值压缩到[0, 1]的范围内，这样它可以被解释为该物体是踝靴的概率。通过互补概率，该物体是T恤。
- en: 'Here we set the learning rate and define the optimizer and the loss function:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置学习率并定义优化器和损失函数：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We set the learning rate to 0.001\. What learning rate to set is an empirical
    question, and the answer comes with experience. It can also be determined by using
    hyperparameter tuning using a validation set. Most optimizers in PyTorch use a
    default learning rate of 0.001\. The Adam optimizer is a variant of the gradient
    descent algorithm, which is used to determine how much to adjust the model parameters
    in each training step. The Adam optimizer was first introduced in 2014 by Diederik
    Kingma and Jimmy Ba.^([1](#footnote-000)) In the traditional gradient descent
    algorithm, only gradients in the current iteration are considered. The Adam optimizer,
    in contrast, takes into consideration gradients in previous iterations as well.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习率设置为0.001。设置学习率是一个经验问题，答案来自于经验。它也可以通过使用验证集进行超参数调整来确定。PyTorch中的大多数优化器使用默认的学习率0.001。Adam优化器是梯度下降算法的一种变体，用于确定在每一步训练中应调整多少模型参数。Adam优化器首次由Diederik
    Kingma和Jimmy Ba于2014年提出。[1](#footnote-000) 在传统的梯度下降算法中，只考虑当前迭代的梯度。相比之下，Adam优化器还考虑了之前迭代的梯度。
- en: We use `nn.BCELoss()`, which is the binary cross-entropy loss function. Loss
    functions measure how well an ML model performs. The training of a model involves
    adjusting parameters to minimize the loss function. The binary cross-entropy loss
    function is widely used in ML, particularly in binary classification problems.
    It measures the performance of a classification model whose output is a probability
    value between 0 and 1\. The cross-entropy loss increases as the predicted probability
    diverges from the actual label.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`nn.BCELoss()`，这是二元交叉熵损失函数。损失函数衡量ML模型的表现。模型的训练涉及调整参数以最小化损失函数。二元交叉熵损失函数在ML中广泛使用，尤其是在二元分类问题中。它衡量的是输出为0到1之间概率值的分类模型的表现。当预测概率偏离实际标签时，交叉熵损失会增加。
- en: We train the neural network we just created as shown in the following listing.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们按照以下列表训练我们刚刚创建的神经网络。
- en: Listing 2.6 Training a binary classification model
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 训练二元分类模型
- en: '[PRE41]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ① Trains for 50 epochs
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ① 训练50个epoch
- en: ② Iterates through all batches
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ② 遍历所有批次
- en: ③ Flattens the image before moving the tensor to GPU
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在将张量移动到GPU之前将图像展平
- en: ④ Converts labels to 0 and 1
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将标签转换为0和1
- en: ⑤ Calculates the loss
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算损失
- en: ⑥ Backpropagation
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 反向传播
- en: In training deep learning models in PyTorch, `loss.backward()` computes the
    gradient of the loss with respect to each model parameter, enabling backpropagation,
    while `optimizer.step()` updates the model parameters based on these computed
    gradients to minimize the loss. We train the model for 50 epochs for simplicity
    (an epoch is when the training data is used to train the model once). In the next
    section, you’ll use a validation set and an early stopping class to determine
    how many epochs to train. In binary classifications, we label the targets as 0s
    and 1s. Since we have kept only t-shirts and ankle boots with labels 0 and 9,
    respectively, we converted them to 0 and 1 in listing 2.6\. As a result, the labels
    for the two categories of clothing items are 0 and 1, respectively.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中训练深度学习模型时，`loss.backward()`计算损失相对于每个模型参数的梯度，从而实现反向传播，而`optimizer.step()`根据这些计算的梯度更新模型参数以最小化损失。为了简单起见，我们训练模型50个epoch（一个epoch是指使用训练数据训练模型一次）。在下一节中，你将使用验证集和提前停止类来确定训练的epoch数。在二分类中，我们将目标标签标记为0和1。由于我们只保留了标签为0和9的T恤和踝靴，我们在列表2.6中将它们转换为0和1。因此，两种服装类别的标签分别为0和1。
- en: This training takes a few minutes if you use GPU training. It takes longer if
    you use CPU training, but the training time should be less than an hour.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用GPU进行训练，这个过程可能需要几分钟。如果你使用CPU进行训练，则所需时间更长，但训练时间应该不到一小时。
- en: 2.3.3 Testing the binary classification model
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.3 测试二分类模型
- en: 'The prediction from the trained binary classification model is a number between
    0 and 1\. We’ll use the `torch.where()` method to convert the predictions into
    0s and 1s: if the predicted probability is less than 0.5, we label the prediction
    as 0; otherwise, we label the prediction as 1\. We then compare these predictions
    with the actual labels to calculate the accuracy of the predictions. In the following
    listing, we use the trained model to make predictions on the test dataset.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的二分类模型的预测结果是一个介于0和1之间的数字。我们将使用`torch.where()`方法将预测结果转换为0和1：如果预测概率小于0.5，我们将预测结果标记为0；否则，我们将预测结果标记为1。然后我们将这些预测结果与实际标签进行比较，以计算预测的准确率。在下面的列表中，我们使用训练好的模型对测试数据集进行预测。
- en: Listing 2.7 Calculating the accuracy of the predictions
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 计算预测的准确率
- en: '[PRE42]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ① Iterates through all batches in the test set
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历测试集中的所有批次
- en: ② Makes predictions using the trained model
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用训练好的模型进行预测
- en: ③ Compares predictions with labels
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将预测与标签进行比较
- en: ④ Calculates accuracy in the batch
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在批次中计算准确率
- en: ⑤ Calculates accuracy in the test set
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 在测试集中计算准确率
- en: We iterate through all batches of data in the test set. The trained model produces
    a probability that the image is an ankle boot. We then convert the probability
    into 0 or 1 based on the cutoff value of 0.5, by using the `torch.where()` method.
    The predictions are either 0 (i.e., a t-shirt) or 1 (an ankle boot) after the
    conversion. We compare the predictions with the actual labels and see how many
    times the model gets it right. Results show that the accuracy of the predictions
    is 87.84% in the test set.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历测试集中所有批次的数据。训练好的模型会输出一个概率，表示图像是否为踝靴。然后我们根据0.5的截止值，使用`torch.where()`方法将概率转换为0或1。转换后，预测结果要么是0（即T恤），要么是1（踝靴）。我们将预测结果与实际标签进行比较，看模型正确预测的次数。结果显示，在测试集中预测的准确率为87.84%。
- en: 2.4 Multicategory classification
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 多分类分类
- en: In this section, we’ll build a deep neural network in PyTorch to classify the
    clothing items into one of the 10 categories. We’ll then train the model with
    the Fashion MNIST dataset. Finally, we’ll use the trained model to make predictions
    and see how accurate they are. We first create a validation set and define an
    early stopping class so that we can determine when to stop training.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用PyTorch构建一个深度神经网络来将服装项目分类到10个类别之一。然后我们将使用Fashion MNIST数据集训练模型。最后，我们将使用训练好的模型进行预测，并查看其准确率。我们首先创建一个验证集并定义一个提前停止类，以便我们可以确定何时停止训练。
- en: 2.4.1 Validation set and early stopping
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 验证集和提前停止
- en: When we build and train a deep neural network, there are many hyperparameters
    that we can choose (such as the learning rate and the number of epochs to train).
    These hyperparameters affect the performance of the model. To find the best hyperparameters,
    we can create a validation set to test the performance of the model with different
    hyperparameters.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建和训练一个深度神经网络时，有许多超参数可以选择（例如学习率和训练的epoch数）。这些超参数影响模型的性能。为了找到最佳超参数，我们可以创建一个验证集来测试不同超参数下模型的性能。
- en: To give you an example, we’ll create a validation set in the multicategory classification
    to determine the optimal number of epochs to train. The reason we do this in the
    validation set instead of the training set is to avoid overfitting, when a model
    performs well in the training set but poorly in out-of-the-sample tests (i.e.,
    on unseen data).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给您一个例子，我们将在多类别分类中创建一个验证集，以确定训练的最佳epoch数。我们之所以在验证集而不是在训练集中这样做，是为了避免过拟合，即模型在训练集中表现良好，但在样本外测试（即未见过的数据）中表现不佳。
- en: 'Here we divide 60,000 observations of the training dataset into a train set
    and a validation set:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将训练数据集的60,000个观测值分为一个训练集和一个验证集：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The original train set now becomes two sets: the new train set with 50,000
    observations and a validation set with the remaining 10,000 observations.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 原始训练集现在变成了两个集合：包含50,000个观测值的新训练集和包含剩余10,000个观测值的验证集。
- en: 'We use the `DataLoader` class in the PyTorch *utils* package to convert the
    train, validation, and test sets into three data iterators in batches:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用PyTorch *utils*包中的`DataLoader`类将训练集、验证集和测试集转换为三个数据迭代器，以批量形式：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Next, we define an `EarlyStop()` class and create an instance of the class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`EarlyStop()`类并创建该类的实例。
- en: Listing 2.8 The `EarlyStop()` class to determine when to stop training
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 `EarlyStop()`类用于确定何时停止训练
- en: '[PRE45]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ① Sets the default value of patience to 10
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将patience的默认值设置为10
- en: ② Defines the stop() method
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义了stop()方法
- en: ③ If a new minimum loss is reached, updates the value of min_loss
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果达到新的最小损失，则更新min_loss的值
- en: ④ Counts how many epochs since the last minimum loss
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 计算自上次最小损失以来的epoch数
- en: The `EarlyStop()` class determines if the loss in the validation set has stopped
    improving in the last `patience=10` epochs. We set the default value of `patience`
    argument to 10, but you can choose a different value when you instantiate the
    class. The value of `patience` measures how many epochs you want to train since
    the last time the model reached the minimum loss. The `stop()` method keeps a
    record of the minimum loss and the number of epochs since the minimum loss and
    compares the number to the value of `patience`. The method returns a value of
    `True` if the number of epochs since the minimum loss is greater than the value
    of `patience`.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStop()`类用于确定验证集中的损失在最后`patience=10`个epoch中是否停止了改进。我们将`patience`参数的默认值设置为10，但您在实例化类时可以选择不同的值。`patience`的值衡量了自上次模型达到最小损失以来您希望训练的epoch数。`stop()`方法记录最小损失和自最小损失以来的epoch数，并将该数字与`patience`的值进行比较。如果自最小损失以来的epoch数大于`patience`的值，则该方法返回`True`。'
- en: 2.4.2 Building and training a multicategory classification model
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 构建和训练一个多类别分类模型
- en: The Fashion MNIST dataset contains 10 different categories of clothing items.
    Therefore, we create a multicategory classification model to classify them. Next,
    you’ll learn how to create such a model and train it. You’ll also learn how to
    make predictions using the trained model and assess the accuracy of the predictions.
    We use PyTorch to create the neural network for multicategory classification in
    the following listing.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Fashion MNIST数据集包含10种不同的服装类别。因此，我们创建一个多类别分类模型来对它们进行分类。接下来，您将学习如何创建这样的模型并对其进行训练。您还将学习如何使用训练好的模型进行预测并评估预测的准确性。在下面的列表中，我们使用PyTorch创建多类别分类的神经网络。
- en: Listing 2.9 Creating a multicategory classification model
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.9 创建一个多类别分类模型
- en: '[PRE46]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ① There are 10 neurons in the output layer.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ① 输出层有10个神经元。
- en: ② Does not apply softmax activation on the output
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输出层不应用softmax激活
- en: 'Compared to the binary classification model we created in the last section,
    we have made a few changes here. First, the output now has 10 values in it, representing
    the 10 different types of clothing items in the dataset. Second, we have changed
    the number of neurons in the last hidden layer from 32 to 64\. A rule of thumb
    in creating deep neural networks is to gradually increase or decrease the number
    of neurons from one layer to the next. Since the number of output neurons has
    increased from 1 (in binary classification) to 10 (in multicategory classification),
    we change the number of neurons from 32 to 64 in the second to last layer to match
    the increase. However, there is nothing special about the number 64: if you use,
    say, 100 neurons in the second to last layer, you’ll get similar results.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 与上一节中我们创建的二分类模型相比，这里做了一些修改。首先，输出现在有10个值，代表数据集中10种不同的服装类型。其次，我们将最后一隐藏层的神经元数量从32改为64。创建深度神经网络的一个经验法则是从一层到下一层逐渐增加或减少神经元数量。由于输出神经元的数量从二分类中的1增加到多类别分类中的10，我们将第二到最后一层的神经元数量从32改为64以匹配增加。然而，64这个数字并没有什么特殊之处：如果你在第二到最后一层使用，比如说，100个神经元，你将得到相似的结果。
- en: 'We’ll use the PyTorch `nn.CrossEntropyLoss()` class as our loss function, which
    combines `nn.LogSoftmax()` and `nn.NLLLoss()` in one single class. See the documentation
    here for details: [https://mng.bz/pxd2](https://mng.bz/pxd2). In particular, the
    documentation states, “This criterion computes the cross entropy loss between
    input logits and target.” This explains why we didn’t apply the softmax activation
    in the proceeding listing. In the book’s GitHub repository, I have demonstrated
    that if we use `nn.LogSoftmax()` in the model and use `nn.NLLLoss()` as the loss
    function, we obtain identical results.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch的`nn.CrossEntropyLoss()`类作为我们的损失函数，该函数将`nn.LogSoftmax()`和`nn.NLLLoss()`合并到一个单独的类中。有关详细信息，请参阅此处文档：[https://mng.bz/pxd2](https://mng.bz/pxd2)。特别是，文档中提到，“此标准计算输入logits和目标之间的交叉熵损失。”这解释了为什么我们在前面的列表中没有应用softmax激活。在本书的GitHub仓库中，我演示了如果我们使用`nn.LogSoftmax()`在模型中，并使用`nn.NLLLoss()`作为损失函数，我们将获得相同的结果。
- en: As a result, the `nn.CrossEntropyLoss()` class will apply the softmax activation
    function on the output to squeeze the 10 numbers into the range [0, 1] before
    the logarithm operation. The preferred activation function on the output is sigmoid
    in binary classifications and softmax in multicategory classifications. Further,
    the 10 numbers after softmax activation add up to 1, which can be interpreted
    as the probabilities corresponding to the 10 types of clothing items. We’ll use
    the same learning rate and optimizer as those in the binary classification in
    the last section.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`nn.CrossEntropyLoss()`类将在对输出应用softmax激活函数之前，将10个数字压缩到[0, 1]的范围内进行对数运算。在二分类中，输出上首选的激活函数是sigmoid，而在多类别分类中是softmax。此外，softmax激活后的10个数字加起来等于1，这可以解释为对应于10种不同服装项目的概率。我们将使用与上一节二分类中相同的学习率和优化器。
- en: '[PRE47]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We define the `train_epoch()` as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义`train_epoch()`如下：
- en: '[PRE48]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The function trains the model for one epoch. The code is similar to what we
    have seen in the binary classification, except that the labels are from 0 to 9,
    instead of two numbers (0 and 1).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数用于训练模型一个epoch。代码与我们之前在二分类中看到的是相似的，只是标签从0到9，而不是两个数字（0和1）。
- en: 'We also define a `val_epoch()` function:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还定义了一个`val_epoch()`函数：
- en: '[PRE49]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The function uses the model to make predictions on images in the validation
    set and calculate the average loss per batch of data.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数使用模型对验证集中的图像进行预测，并计算数据批次平均损失。
- en: 'We now train the multicategory classifier:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在训练多类别分类器：
- en: '[PRE50]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We train a maximum of 100 epochs. In each epoch, we first train the model using
    the training set. We then calculate the average loss per batch in the validation
    set. We use the `EarlyStop()` class to determine if the training should stop by
    looking at the loss in the validation set. The training stops if the loss hasn’t
    improved in the last 10 epochs. After 19 epochs, the training stops.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最多训练100个epoch。在每个epoch中，我们首先使用训练集来训练模型。然后我们计算验证集中每个批次的平均损失。我们使用`EarlyStop()`类通过查看验证集中的损失来确定是否应该停止训练。如果损失在最后10个epoch中没有改善，则停止训练。经过19个epoch后，训练停止。
- en: The training takes about 5 minutes if you use GPU training, which is longer
    than the training process in binary classification since we have more observations
    in the training set now (10 clothing items instead of just 2).
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用GPU进行训练，训练大约需要5分钟，这比二分类中的训练过程要长，因为我们现在训练集中的观察结果更多（10种服装而不是只有2种）。
- en: The output from the model is a vector of 10 numbers. We use `torch.argmax()`
    to assign each observation a label based on the highest probability. We then compare
    the predicted label with the actual label. To illustrate how the prediction works,
    let’s look at the predictions on the first five images in the test set.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出是一个包含10个数字的向量。我们使用`torch.argmax()`根据最高概率为每个观察结果分配一个标签。然后，我们将预测标签与实际标签进行比较。为了说明预测是如何工作的，让我们看看测试集中前五张图像的预测结果。
- en: Listing 2.10 Testing the trained model on five images
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 在五张图像上测试训练好的模型
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ① Plots the first five images in the test set with their labels
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在测试集中绘制前五张图像及其标签
- en: ② Obtains the i-th image and label in the test set
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ② 获取测试集中的第i张图像和标签
- en: ③ Predicts using the trained model
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用训练好的模型进行预测
- en: ④ Uses the torch.argmax() method to obtain the predicted label
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用torch.argmax()方法获取预测标签
- en: ⑤ Prints out the actual label and the predicted label
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 打印出实际标签和预测标签
- en: We plot the first five clothing items in the test set in a 1 × 5 grid. We then
    use the trained model to make a prediction on each clothing item. The prediction
    is a tensor with 10 values. The `torch.argmax()` method returns the position of
    the highest probability in the tensor, and we use it as the predicted label. Finally,
    we print out both the actual label and the predicted label to compare and see
    if the predictions are correct. After running the previous code listing, you should
    see the image in figure 2.3.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以1 × 5的网格形式绘制测试集中的前五种服装。然后，我们使用训练好的模型对每种服装进行预测。预测结果是一个包含10个值的张量。`torch.argmax()`方法返回张量中最高概率的位置，我们将其用作预测标签。最后，我们打印出实际标签和预测标签以进行比较，看看预测是否正确。运行前面的代码列表后，你应该会看到图2.3中的图像。
- en: '![](../../OEBPS/Images/CH02_F03_Liu.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH02_F03_Liu.png)'
- en: Figure 2.3 The first five clothing items in the test dataset and their respective
    labels. Each clothing item has a text label and a numerical label between 0 and
    9.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 测试数据集中的前五种服装及其相应的标签。每种服装都有一个文本标签和一个介于0到9之间的数字标签。
- en: Figure 2.3 shows that the first five clothing items in the test set are ankle
    boot, pullover, trouser, trouser, and shirt, respectively, with numerical labels
    9, 2, 1, 1, and 6.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3显示了测试集中的前五种服装分别是踝靴、套头衫、裤子、裤子和外套，分别对应数字标签9、2、1、1和6。
- en: 'The output after running the code in listing 2.10 is as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 运行列表2.10中的代码后的输出如下：
- en: '[PRE52]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The preceding output shows that the model has made correct predictions on all
    five clothing items.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的输出显示，模型对所有五种服装都做出了正确的预测。
- en: Fixing the random state in PyTorch
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中固定随机状态
- en: The `torch.manual_seed()` method fixes the random state so the results are the
    same when you rerun your programs. However, you may get different results from
    those reported in this chapter even if you use the same random seed. This happens
    because different hardware and different versions of PyTorch handle floating point
    operations slightly differently. See, for example, the explanations at [https://mng.bz/RNva](https://mng.bz/RNva).
    The difference is generally minor, though, so no need to be alarmed.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`torch.manual_seed()`方法固定随机状态，以便当你重新运行程序时结果相同。然而，即使你使用相同的随机种子，你也可能得到与本章中报告的不同结果。这是因为不同的硬件和不同的PyTorch版本在处理浮点运算时略有不同。例如，请参阅[https://mng.bz/RNva](https://mng.bz/RNva)中的解释。尽管如此，这种差异通常很小，因此无需惊慌。'
- en: Next, we calculate the accuracy of the predictions on the whole test dataset.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算在整个测试数据集上的预测准确率。
- en: Listing 2.11 Testing the trained multicategory classification model
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 测试训练好的多类别分类模型
- en: '[PRE53]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ① Iterates through all batches in the test set
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历测试集中的所有批次
- en: ② Predicts using the trained model
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用训练好的模型进行预测
- en: ③ Converts probabilities to a predicted label
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将概率转换为预测标签
- en: ④ Compares the predicted label with the actual label
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将预测标签与实际标签进行比较
- en: ⑤ Calculates accuracy in the test set
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 计算测试集中的准确率
- en: The output is
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是
- en: '[PRE54]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: We iterate through all clothing items in the test set and use the trained model
    to make predictions. We then compare the predictions with the actual labels. The
    accuracy is about 88% in the out-of-sample test. Given that a random guess has
    an accuracy of about 10%, 88% accuracy is fairly high. This indicates that we
    have built and trained two successful deep learning models in PyTorch! You’ll
    use these skills quite often later in this book. For example, in chapter 3, the
    discriminator network you’ll construct is essentially a binary classification
    model, similar to what you have created in this chapter.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历测试集中的所有服装项目，并使用训练好的模型进行预测。然后我们将预测结果与实际标签进行比较。在样本外测试中的准确率约为88%。考虑到随机猜测的准确率约为10%，88%的准确率相当高。这表明我们在PyTorch中构建并训练了两个成功的深度学习模型！你将在本书后面的内容中经常使用这些技能。例如，在第3章中，你将构建的判别网络本质上是一个二元分类模型，类似于本章中你创建的模型。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In PyTorch, we use tensors to hold various forms of input data so we can feed
    them to deep learning models.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在PyTorch中，我们使用张量来存储各种形式的输入数据，以便我们可以将它们输入到深度学习模型中。
- en: You can index and slice PyTorch tensors, reshape them, and conduct mathematical
    operations on them.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以对PyTorch张量进行索引和切片，重塑它们，并在它们上执行数学运算。
- en: Deep learning is a type of ML method that uses deep artificial neural networks
    to learn the relation between input and output data.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习是一种使用深度人工神经网络来学习输入和输出数据之间关系的机器学习方法。
- en: The ReLU activation function decides whether a neuron should be turned on based
    on the weighted sum. It introduces nonlinearity to the output of a neuron.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU激活函数根据加权总和决定是否应该激活神经元。它为神经元的输出引入了非线性。
- en: Loss functions measure how well an ML model performs. The training of a model
    involves adjusting parameters to minimize the loss function.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数衡量机器学习模型的性能。模型的训练涉及调整参数以最小化损失函数。
- en: Binary classification is an ML model to classify observations into one of two
    categories.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类是一种将观察结果分类到两个类别之一的机器学习模型。
- en: Multicategory classification is an ML model to classify observations into one
    of multiple categories.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类别分类是一种将观察结果分类到多个类别之一的机器学习模型。
- en: '* * *'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^([1](#footnote-000-backlink))  Diederik Kingma and Jimmy Ba, 2014, “Adam:
    A Method for Stochastic Optimization.” [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink)) 迪德里克·金卡马和吉米·巴，2014年，“Adam：一种随机优化的方法。” [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980).
