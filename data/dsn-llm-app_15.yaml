- en: Chapter 12\. Retrieval-Augmented Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章\. 检索增强生成
- en: In [Chapter 10](ch10.html#ch10), we demonstrated how to vastly expand the capabilities
    of LLMs by interfacing them with external data and software. In [Chapter 11](ch11.html#chapter_llm_interfaces),
    we introduced the concept of embedding-based retrieval, a foundational technique
    for retrieving relevant data from data stores in response to queries. Armed with
    this knowledge, let’s explore the application paradigm of augmenting LLMs with
    external data, called retrieval-augmented generation (RAG), in a holistic fashion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#ch10)中，我们展示了如何通过将LLM与外部数据和软件接口来大幅扩展LLM的功能。在[第11章](ch11.html#chapter_llm_interfaces)中，我们介绍了基于嵌入的检索概念，这是一种从数据存储中检索相关数据的基础技术，用于响应查询。掌握了这些知识，让我们全面探索通过外部数据增强LLM的应用范式，即检索增强生成（RAG）。
- en: In this chapter, we will take a comprehensive view of the RAG pipeline, diving
    deep into each of the steps that make up a typical workflow of a RAG application.
    We will explore the various decisions involved in operationalizing RAG, including
    what kind of data we can retrieve, how to retrieve it, and when to retrieve it.
    We will highlight how RAG can help not only during model inference but also during
    model training and fine-tuning. We will also compare RAG with other paradigms
    and discuss scenarios where RAG shines in comparison to alternatives or vice versa.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将全面审视RAG管道，深入探讨构成典型RAG应用程序工作流程的每个步骤。我们将探索在实施RAG时涉及的各项决策，包括我们可以检索哪些数据，如何检索以及何时检索。我们将强调RAG如何帮助不仅在模型推理期间，而且在模型训练和微调期间。我们还将比较RAG与其他范式，并讨论RAG在与其他替代方案相比时表现优异的场景。
- en: The Need for RAG
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG的需求
- en: 'As introduced in [Chapter 10](ch10.html#ch10), RAG is an umbrella term used
    to describe a variety of techniques for using external data sources to augment
    the capabilities of an LLM. Here are some reasons we might want to use RAG:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](ch10.html#ch10)中所述，RAG是一个术语，用于描述使用外部数据源增强LLM功能的各种技术。以下是我们可能想要使用RAG的一些原因：
- en: We need the LLMs to access our private/proprietary data, or data that was not
    part of the LLM’s pre-training datasets. Using RAG is a much more lightweight
    option than pre-training an LLM on our private data.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要LLM访问我们的私有/专有数据，或者LLM预训练数据集之外的数据。使用RAG比在私有数据上预训练LLM要轻量得多。
- en: To reduce the risk of hallucinations, we would like the LLM to refer to data
    provided through a retrieval mechanism rather than rely on its own internal knowledge.
    RAG facilitates this. RAG also enables more accurate data citations, connecting
    LLM outputs to their ground-truth sources.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了降低幻觉风险，我们希望LLM能够通过检索机制引用提供的数据，而不是依赖其自身的内部知识。RAG促进了这一点。RAG还使数据引用更加准确，将LLM输出与其真实来源连接起来。
- en: We would like the LLM to answer questions about recent events and concepts that
    have emerged after the LLM was pre-trained. While there are memory editing techniques
    for updating LLM parameters with new knowledge like [MEMIT](https://oreil.ly/kxI3j),
    they are not yet reliable or scalable. As discussed in [Chapter 7](ch07.html#ch07),
    continually training an LLM to keep its knowledge up-to-date is expensive and
    risky.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望LLM能够回答关于LLM预训练后出现的近期事件和概念的问题。虽然存在像[MEMIT](https://oreil.ly/kxI3j)这样的记忆编辑技术，用于更新LLM参数以获得新知识，但它们目前还不可靠或可扩展。正如[第7章](ch07.html#ch07)中讨论的，持续训练LLM以保持其知识更新是昂贵且风险较高的。
- en: We would like the LLM to answer queries involving long-tail entities, which
    occur only rarely in the pre-training datasets.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望LLM能够回答涉及长尾实体的查询，这些实体在预训练数据集中出现的频率很低。
- en: Typical RAG Scenarios
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的RAG场景
- en: 'Now that we have seen *why* we need RAG, let’s explore *where* we can utilize
    it. The four most popular scenarios are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了*为什么*我们需要RAG，让我们探索*在哪里*我们可以利用它。最流行的四种场景是：
- en: Retrieving external knowledge
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 检索外部知识
- en: This is the predominant use case that has seen a lot of success with productionization.
    As discussed earlier in the chapter, we can use RAG to plug LLM knowledge gaps
    or to reduce hallucination risk.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是已经看到很多成功案例并实现商业化的主要用例。正如本章前面所讨论的，我们可以使用RAG来填补LLM的知识空白或降低幻觉风险。
- en: Retrieving context history
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 检索上下文历史
- en: LLMs have a limited context window, but often we need access to more context
    in order to answer a query than what fits in the context window. We would also
    like to have longer conversations with the LLM than what fits in the context window.
    In these cases, we could retrieve parts of the conversation history or session
    context when needed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLM有一个有限的上下文窗口，但通常我们需要访问比上下文窗口中适合的更多的上下文来回答查询。我们还想与LLM进行比上下文窗口中适合的更长时间的对话。在这些情况下，当需要时，我们可以检索对话历史或会话上下文的部分内容。
- en: Retrieving in-context training examples
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 检索上下文训练示例
- en: Few-shot learning is an effective approach to help LLMs get acquainted with
    the input-output mapping of a task. You can make few-shot learning more effective
    by dynamically selecting few-shot examples based on the current input. The few-shot
    examples can be retrieved from a training example data store at inference time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习是一种有效的方法，可以帮助大型语言模型（LLM）熟悉任务的输入输出映射。您可以通过根据当前输入动态选择少样本示例来提高少样本学习的有效性。在推理时，可以从训练示例数据存储中检索这些少样本示例。
- en: Retrieving tool-related information
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检索与工具相关的信息
- en: As described in [Chapter 10](ch10.html#ch10), LLMs can invoke software tools
    as part of their workflow. The list of tools available and their description is
    stored in a tool store. The LLM can then use retrieval for tool selection, selecting
    the tool best suited to the task. Tool-related information can also include API
    documentation, for instance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](ch10.html#ch10)所述，LLM可以作为其工作流程的一部分调用软件工具。可用的工具列表及其描述存储在工具存储中。然后，LLM可以使用检索来进行工具选择，选择最适合任务的工具。工具相关信息还可以包括API文档等。
- en: Deciding When to Retrieve
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定何时进行检索
- en: 'For each step in an agentic workflow, the LLM can advance its task using one
    of the following steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个代理工作流程的每个步骤中，LLM可以使用以下步骤之一来推进其任务：
- en: Use its internal capabilities
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其内部能力
- en: Choose from among several data stores
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从几个数据存储中选择
- en: Choose from among several software tools
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从几个软件工具中选择
- en: There can be tasks that the LLM can fully solve using its parametric memory,
    but one or more data stores may also contain the requisite data needed to solve
    them. In these cases, should we just default to using RAG, given all its benefits
    that we presented earlier?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 可能存在LLM可以使用其参数化内存完全解决的问题，但一个或多个数据存储也可能包含解决这些问题的必要数据。在这些情况下，鉴于我们之前提出的所有好处，我们是否应该默认使用RAG？
- en: We have seen earlier in the chapter that LLMs struggle with long-tail information,
    and that RAG can be an effective means to answer questions about long-tail entities.
    However, [Mallen et al.](https://oreil.ly/MF7Y1) show that for queries about more
    popular entities, the LLM might sometimes be better at answering queries than
    RAG. This is because of the inevitable limitations of the retrieval model, which
    might retrieve irrelevant or incorrect information that could mislead the LLM.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面看到，LLM在处理长尾信息方面存在困难，而RAG可以成为回答关于长尾实体的有效手段。然而，[Mallen等人](https://oreil.ly/MF7Y1)表明，对于关于更常见实体的查询，LLM有时可能比RAG更好地回答查询。这是因为检索模型不可避免的局限性，它可能会检索到无关或错误的信息，这可能会误导LLM。
- en: 'For a given query, you can dynamically determine whether to use retrieval or
    to rely on the LLM’s parametric memory. The rules determining the right approach
    to take include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的查询，您可以动态确定是使用检索还是依赖LLM的参数化内存。确定采取正确方法的规则包括：
- en: Whether the query is about a more frequently occurring entity. For example,
    the LLM is more likely to memorize the birthday of Taylor Swift than of a substitute
    drummer of a local band whose Wikipedia page is a stub.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询是否关于更频繁出现的实体。例如，LLM更有可能记住泰勒·斯威夫特的生日，而不是一个地方乐队的替补鼓手的生日，其维基百科页面是一个简短条目。
- en: Whether the query has timeliness constraints, i.e., if the data needed to address
    the query may not have existed before the LLM’s knowledge cutoff date.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询是否有时效性限制，即所需的数据可能在LLM的知识截止日期之前不存在。
- en: Whether the model has been continually pre-trained or memory tuned as described
    in [Chapter 7](ch07.html#ch07), and the given query relates to concepts over which
    the training was performed.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型是否已经根据[第7章](ch07.html#ch07)中描述的持续预训练或记忆调整，以及给定的查询与训练所涉及的概念相关。
- en: If you are using LLMs for general-purpose question answering, Mallen et al.
    show that you can use sources like Wikipedia as a pseudo-popularity metric for
    entities. If the entities present in your inputs have an entity count in Wikipedia
    greater than a threshold, then the LLM can choose to answer the question on its
    own without using RAG. Note that the threshold can change across LLMs. This strategy
    works only if you have a good understanding about the datasets the LLM has been
    pre-trained on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用LLM进行通用问答，Mallen等人表明，您可以使用诸如维基百科之类的来源作为实体的伪流行度指标。如果您的输入中存在的实体在维基百科中的计数超过一个阈值，那么LLM可以选择自行回答问题而不使用RAG。请注意，阈值可能在不同LLM之间有所不同。此策略仅在您对LLM预先训练的数据集有良好理解的情况下才有效。
- en: Dynamically deciding when to retrieve data can also help optimize the model’s
    latency and responsiveness, as the RAG pipeline will introduce additional overhead.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 动态决定何时检索数据也可以帮助优化模型的延迟和响应性，因为RAG管道将引入额外的开销。
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Dynamic retrieval is mostly useful when you are using very large LLMs. For smaller
    models (7B or below), it is almost always beneficial to prefer using RAG rather
    than relying on the LLM’s internal memory.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动态检索主要在您使用非常大的LLM时非常有用。对于较小的模型（7B或以下），几乎总是有益于优先使用RAG而不是依赖LLM的内部内存。
- en: The RAG Pipeline
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG管道
- en: A typical RAG application follows the *retrieve-read* framework, as discussed
    in [Chapter 11](ch11.html#chapter_llm_interfaces). In response to a query, a retrieval
    model identifies documents that are relevant to answering the query. These documents
    are then passed along to the LLM as context, which the LLM can rely on in addition
    to its internal capabilities to generate a response. In practice, we typically
    need to add a lot of bells and whistles to get RAG working in a production context.
    This involves adding several more optional stages to the retrieve-read framework.
    In practice, your pipeline stages might consist of a *rewrite-retrieve-read-refine-insert-generate*
    workflow, with some of these steps potentially comprising multiple stages. Later
    in the chapter, we will go through each of the steps in more detail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的RAG应用遵循如第11章中讨论的*检索-读取*框架。在响应查询时，检索模型识别出与回答查询相关的文档。然后，这些文档被传递给LLM作为上下文，LLM可以依赖这些上下文以及其内部能力来生成响应。在实践中，我们通常需要添加很多功能才能在生产环境中使RAG工作。这涉及到在检索-读取框架中添加几个更多可选阶段。在实践中，您的管道阶段可能包括一个*重写-检索-读取-精炼-插入-生成*工作流程，其中一些步骤可能包含多个阶段。在第章的后面，我们将更详细地介绍每个步骤。
- en: '[Figure 12-1](#RAG-pipeline) shows the various stages of the RAG pipeline and
    the components involved.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](#RAG-pipeline)展示了RAG管道的各个阶段和涉及的组件。'
- en: '![RAG-pipeline](assets/dllm_1201.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![RAG-pipeline](assets/dllm_1201.png)'
- en: Figure 12-1\. RAG pipeline
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. RAG管道
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As in the rest of the book, we refer to user or LLM requests to retrieve data
    as queries, and units of text retrieved from the data store as documents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书的其余部分一样，我们将用户或LLM请求检索数据称为查询，并将从数据存储中检索出的文本单元称为文档。
- en: Let’s illustrate with an example. Consider a RAG application that answers questions
    about Canadian politics and parliamentary activity. The application has access
    to a knowledge base containing transcripts of parliamentary proceedings. We will
    assume that the data is represented using the representation techniques described
    in [Chapter 11](ch11.html#chapter_llm_interfaces).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明。考虑一个回答有关加拿大政治和议会活动的RAG应用。该应用可以访问包含议会会议记录的知识库。我们将假设数据使用第11章中描述的表示技术表示。
- en: When a user issues a query, we might want to rephrase it before sending it to
    the retriever. Traditionally in the field of information retrieval (IR), this
    is referred to as query expansion. Query expansion is especially useful because
    of the vocabulary mismatch between the query space and the document space. The
    user might use different terminology in the query than that used in the documents.
    Rephrasing a query can help bridge the vocabulary gap. In general, we would like
    to rephrase the query in such a way that it improves the chances of the retriever
    fetching the most relevant documents. This stage is called the *rewrite* stage.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出查询时，我们可能希望在发送给检索器之前重新措辞。在信息检索（IR）领域，这通常被称为查询扩展。查询扩展特别有用，因为查询空间和文档空间之间存在词汇不匹配。用户在查询中可能使用与文档中不同的术语。重新措辞查询可以帮助弥合词汇差距。一般来说，我们希望以这种方式重新措辞查询，以提高检索器检索最相关文档的机会。这一阶段被称为*重写*阶段。
- en: Next, in the *retrieve* stage, a retrieval model is used to retrieve the documents
    relevant to the query. In [Chapter 11](ch11.html#chapter_llm_interfaces), we discussed
    embedding-based retrieval, a popular retrieval paradigm in the LLM era. The retrieval
    stage can be an extensive multi-stage pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在*检索*阶段，使用检索模型来检索与查询相关的文档。在[第11章](ch11.html#chapter_llm_interfaces)中，我们讨论了基于嵌入的检索，这是LLM时代流行的检索范式。检索阶段可以是一个广泛的多阶段管道。
- en: The retrieval can happen over a very large document space. In this case, it
    is computationally infeasible to use more advanced retrieval models. Therefore,
    retrieval is usually carried out in a two-step process, with the first step using
    faster methods (these days, typically embedding-based) to retrieve a list of potentially
    relevant documents (optimizing recall), and a second step that reranks the retrieved
    list based on relevance (optimizing precision) so that the top-k ranked documents
    are then taken as the context to be passed along to the LLM. This stage is called
    the *rerank* stage.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 检索可以在一个非常大的文档空间中进行。在这种情况下，使用更高级的检索模型在计算上是不可行的。因此，检索通常分为两步进行，第一步使用更快的方法（这些天通常是基于嵌入的方法）检索一份可能相关的文档列表（优化召回率），第二步根据相关性重新排序检索列表（优化精确率），以便将排名前k的文档作为上下文传递给LLM。这一阶段被称为*重新排序*阶段。
- en: After identifying the top-k documents relevant to the query, they need to be
    passed along to the LLM. However, the documents may not fit into the context window
    and thus need to be shortened. They also could potentially be rephrased in a way
    that makes it more likely for the LLM to use the context to generate the answer.
    This is done during the *refine* stage.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定与查询相关的top-k文档后，需要将它们传递给LLM。然而，文档可能不适合上下文窗口，因此需要缩短。它们也可能被重新措辞，使其更有可能让LLM使用上下文生成答案。这是在*细化*阶段完成的。
- en: Next, we provide the output of the refine step to the LLM. The default approach
    is to concatenate all the documents in the prompt. However, you could also pass
    them one at a time, and then ensemble the results. How the documents are ordered
    in the prompt can also make a difference. Several such techniques determine the
    way the context is fed to the LLM. This is called the *insert* stage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将细化步骤的输出提供给LLM。默认方法是将所有文档连接到提示中。然而，你也可以一次传递一个，然后合并结果。提示中文档的顺序也可能产生影响。几种这样的技术决定了上下文被输入到LLM的方式。这被称为*插入*阶段。
- en: Finally, in the *generate* stage, the LLM reads the prompt containing the query
    and the context and generates the response. The generation can happen all at once
    or the retrieval process can be interleaved with the generation, i.e., the model
    can generate a few tokens, then call the retrieval model again to retrieve additional
    content, generate a few more tokens, and then call the retrieval model again,
    and so on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*生成*阶段，LLM读取包含查询和上下文的提示并生成响应。生成可以一次性完成，或者检索过程可以与生成交织进行，即模型可以生成一些标记，然后再次调用检索模型以检索更多内容，生成更多标记，然后再次调用检索模型，依此类推。
- en: The output of each stage can be run through a *verify* stage to assess the quality
    of the outputs and even take corrective measures. The verify stage can employ
    either heuristics or AI-based methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个阶段的输出都可以通过一个*验证*阶段来评估输出质量，甚至采取纠正措施。验证阶段可以采用启发式方法或基于AI的方法。
- en: In this example, the query was generated by a human user. But if we consider
    RAG in the context of agentic workflows, the query might be generated by an LLM.
    In an agentic workflow, the agent can determine at any given point that it needs
    to retrieve data to progress with its task, which sets the aforementioned pipeline
    into motion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，查询是由人类用户生成的。但如果我们将RAG放在代理工作流程的背景下考虑，查询可能是由一个LLM生成的。在代理工作流程中，代理可以在任何给定时刻确定它需要检索数据以继续其任务，这就会启动上述管道。
- en: Apart from the retrieve and generate steps, the rest of the pipeline is optional,
    and including other steps depends on your performance and latency tradeoffs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检索和生成步骤之外，管道中的其余步骤是可选的，是否包含其他步骤取决于你的性能和延迟权衡。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Our example pertains to RAG when used at inference time. RAG can also be applied
    when pre-training or fine-tuning the model, which we will describe later in the
    chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的例子涉及在推理时间使用RAG。RAG也可以在预训练或微调模型时应用，我们将在本章后面描述。
- en: Let’s examine each step in the pipeline in detail.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细检查管道中的每个步骤。
- en: Rewrite
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重写
- en: After a query is issued, it might need to be rewritten to make it more amenable
    to retrieval. The rewriting process depends on the retrieval models used. As mentioned
    before, there is usually a mismatch between the query space and the document space,
    as the vocabulary, phrasing, and semantics used by the query might vary drastically
    from how the relevant concepts are conveyed in the document.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在发出查询后，可能需要对其进行重写以使其更易于检索。重写过程取决于所使用的检索模型。如前所述，查询空间和文档空间之间通常存在不匹配，因为查询所使用的词汇、措辞和语义可能与文档中传达的相关概念大相径庭。
- en: 'As an example, consider the query: “Which politicians have complained about
    the budget not being balanced?”'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑查询：“哪些政治家抱怨预算不平衡？”
- en: 'and the data store contains the text “Senator Paxton: ‘I just can’t stand the
    sight of our enormous deficit.’”'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储中包含文本“参议员帕克斯顿：‘我实在无法忍受我们巨大的赤字。’”
- en: If you are using traditional retrieval approaches that rely more on keywords,
    this text may not be selected as relevant during retrieval. Using embedding-based
    methods bridges the gap as embeddings of similar sentences are closer to each
    other in embedding space, but it does not entirely solve the problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是更依赖于关键词的传统检索方法，这段文本在检索过程中可能不会被选为相关。使用基于嵌入的方法可以缩小差距，因为相似句子的嵌入在嵌入空间中彼此更接近，但这并不完全解决问题。
- en: Tip
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the query is coming from the user, the user might add instructions along
    with the query, like, “Which politicians have complained about the budget not
    being balanced? Provide the results in the form of a table.” In this case you
    will have to separate the query from the instructions before feeding the query
    into the retrieval pipeline. This can be done by an LLM using prompting techniques
    like CoT, ReAct, etc., which we discussed in Chapters [5](ch05.html#chapter_utilizing_llms)
    and [10](ch10.html#ch10), respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查询来自用户，用户可能会在查询中添加指令，例如，“哪些政治家抱怨预算不平衡？以表格形式提供结果。”在这种情况下，你必须在将查询输入检索管道之前将查询与指令分开。这可以通过LLM使用提示技术如CoT、ReAct等来完成，这些技术我们在第[5](ch05.html#chapter_utilizing_llms)章和第[10](ch10.html#ch10)章中分别讨论过。
- en: For systems using traditional retrieval techniques, query rewriting is typically
    performed using query expansion techniques, in which the query is augmented with
    similar keywords. Basic query expansion techniques include adding synonyms of
    keywords and other topic information in your query.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用传统检索技术的系统，查询重写通常是通过查询扩展技术来执行的，其中查询被添加了相似的关键词。基本的查询扩展技术包括在查询中添加关键词的同义词和其他主题信息。
- en: A well-tested method for query expansion is pseudo-relevance feedback (PRF).
    In PRF, the original query is used to retrieve documents, and salient terms from
    these documents are extracted and added to the original query.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展的一个经过良好测试的方法是伪相关性反馈（PRF）。在PRF中，原始查询用于检索文档，并从这些文档中提取显著术语并添加到原始查询中。
- en: 'Let’s see how PRF would help with our query, ‘‘Which politicians have complained
    about the budget not being balanced?” We use a retrieval technique like BM25 (explained
    later in the chapter) to return a candidate set of k documents. We then use a
    technique like term frequency or, more effectively, [Tf-IDf](https://oreil.ly/5be9z)
    to extract the salient terms occurring in these returned documents. For this example
    the salient phrases turn out to be “fiscal policy,” “deficit,” “financial mismanagement,”
    and “budgetary reforms.” Adding these phrases to the original query will lead
    to the text:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看PRF如何帮助我们查询“哪些政治家抱怨预算不平衡？”我们使用像BM25（本章后面将解释）这样的检索技术来返回k个候选文档集。然后我们使用像词频或更有效的方法[Tf-IDf](https://oreil.ly/5be9z)来提取这些返回文档中出现的显著术语。对于这个例子，显著的短语是“财政政策”、“赤字”、“财务管理不善”和“预算改革”。将这些短语添加到原始查询中，将导致以下文本：
- en: '“Senator Paxton: ‘I just can’t stand the sight of our enormous deficit!’” being
    retrieved successfully.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “参议员帕克顿：‘我简直无法忍受我们巨大的赤字！’”被成功检索。
- en: In recent years, LLM-driven query expansion methods are gaining more prominence.
    Two such examples are [Query2doc](https://oreil.ly/BDJAN) and [Hypothetical Document
    Embeddings (HyDE)](https://oreil.ly/tZyUV).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由LLM驱动的查询扩展方法越来越受到重视。两个这样的例子是[Query2doc](https://oreil.ly/BDJAN)和[Hypothetical
    Document Embeddings (HyDE)](https://oreil.ly/tZyUV)。
- en: Let’s explore how LLM-based query expansion works. We pass a given query to
    an LLM and ask it to generate a hypothetical document that contains the answer
    to the query.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索基于LLM的查询扩展是如何工作的。我们将给定的查询传递给一个LLM，并要求它生成一个包含查询答案的假设文档。
- en: '*Prompt:* For the provided query, generate a hypothetical document in 2 sentences
    or less that answers the query. Assume the documents are transcripts of Canadian
    parliamentary proceedings, containing verbatim statements of representatives in
    the House and Senate.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 对于提供的查询，用2句话或更少的文字生成一个假设文档来回答查询。假设文档是加拿大议会程序的记录，包含下议院和参议院的代表的直接陈述。'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Query:* Which politicians have complained about the budget not being balanced?'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*查询:* 哪些政治家抱怨预算不平衡？'
- en: 'The prompt contains:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示包含：
- en: Instructions to generate a hypothetical document
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成假设文档的说明
- en: Desired length limit of the generated document
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的文档期望的长度限制
- en: Description of the document corpus containing relevant documents
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述包含相关文档的文档语料库
- en: The original query
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始查询
- en: 'The hypothetical document generated by the LLM is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成的假设文档如下：
- en: '*LLM:* “Honorable Senator Mark Wellington stated, *This government’s failure
    to balance the budget is unacceptable and jeopardizes our economic future.* Similarly,
    MP Emily Fraser remarked, *We cannot continue on this path of reckless spending
    without addressing the deficit—it’s time for real fiscal responsibility.*”'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*LLM:* “尊敬的马克·韦林顿参议员表示，*政府未能平衡预算是不可接受的，这危及我们的经济未来。*同样，艾米丽·弗拉泽议员评论说，*我们不能继续这种不负责任的支出路径而不解决赤字——是时候采取真正的财政责任了。*”'
- en: 'Using this hypothetical document as the query will return the sentence ‘‘Senator
    Paxton: ‘I just can’t stand the sight of our enormous deficit!”’ with a higher
    similarity score.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个假设文档作为查询将返回“参议员帕克顿：‘我简直无法忍受我们巨大的赤字！’”这个句子，其相似度得分更高。
- en: While this hypothetical document is not factually accurate, and laughably so
    (there are no senators or MPs in Canada named Mark Wellington and Emily Fraser),
    it does contain verbiage and semantics very similar to what an actual politician
    would say. If we use this document as the query (optionally combining with the
    original query), then the chances of it being semantically similar to actual instances
    of politicians talking about the topic is higher than if matched with the query
    alone.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个假设文档在事实上并不准确，甚至可以说是可笑的（加拿大没有名叫马克·韦林顿和艾米丽·弗拉泽的参议员或议员），但它确实包含与实际政治家所说非常相似的措辞和语义。如果我们使用这个文档作为查询（可选地与原始查询结合），那么它与实际政治家谈论该主题的实例在语义上相似的可能性比仅与查询匹配时更高。
- en: The length of the hypothetical document could be similar to the typical length
    of the retrieval unit. You can use a smaller LLM to generate the hypothetical
    document, as we do not care for factuality guarantees in this setting. However,
    smaller models are also not as adept as generating quality hypothetical documents,
    so you will have to manage the tradeoff. Both LangChain and LlamaIndex provide
    implementations of hypothetical document-based query rewriting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文档的长度可以与检索单元的典型长度相似。你可以使用一个较小的LLM来生成假设文档，因为在这样的设置中我们并不关心事实保证。然而，较小的模型在生成高质量的假设文档方面也不够熟练，所以你将不得不管理这种权衡。LangChain和LlamaIndex都提供了基于假设文档的查询重写的实现。
- en: If the model has been pre-trained or fine-tuned on the data corpus containing
    the relevant data, then adding descriptions of the corpus in the prompt as shown
    in the example will make it more likely that the generated document follows the
    structure, format, and linguistics of that data corpus.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型已经在包含相关数据的语料库上预训练或微调，那么在提示中添加语料库的描述，如示例所示，将更有可能使生成的文档遵循该数据语料库的结构、格式和语言学。
- en: Warning
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: One pitfall of query rewriting techniques is the risk of topic drift. In the
    case of hypothetical documents, the document may drift into irrelevant topics
    after the first few tokens. Upweighting the logits bias for tokens in the query
    can partially address this problem. PRF techniques are also susceptible to topic
    drift.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写技术的一个潜在问题是主题漂移的风险。在假设文档的情况下，文档可能在最初的几个标记之后漂移到不相关的主题。增加查询中标记的logits偏置可以部分解决这个问题。PRF技术也容易受到主题漂移的影响。
- en: You can also combine PRF style techniques with hypothetical documents. Instead
    of generating hypothetical documents to replace or augment the query, you can
    use them to extract keywords that you can add to the original query. [Li et al.](https://oreil.ly/cOnMs)
    propose a technique called *query2document2keyword*. In this technique, the LLM
    generates a hypothetical document using the query, similar to HyDE. The LLM is
    then prompted to extract salient keywords from this document.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以将PRF风格的技术与假设文档结合使用。不是生成假设文档来替换或增强查询，而是可以使用它们来提取可以添加到原始查询中的关键词。[Li等人](https://oreil.ly/cOnMs)提出了一种名为*query2document2keyword*的技术。在这种技术中，LLM使用查询生成一个假设文档，类似于HyDE。然后提示LLM从这个文档中提取显著关键词。
- en: We can then further improve the quality of the extracted keywords by taking
    them through a filtering step. The authors propose using the *self-consistency*
    method, which we discussed in [Chapter 5](ch05.html#chapter_utilizing_llms). To
    recap, in the self-consistency method, we repeat the keyword generation multiple
    times, and then select the top keywords based on the number of generations they
    are present in.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过过滤步骤进一步改进提取的关键词的质量。作者建议使用*自洽性*方法，我们已在[第5章](ch05.html#chapter_utilizing_llms)中讨论过。概括来说，在自洽性方法中，我们多次重复关键词生成，然后根据它们出现的生成次数选择顶级关键词。
- en: Another way to combine traditional retrieval with LLM-driven query rewriting
    is to first return the top-k documents from the initial retrieval step, then use
    LLMs to generate salient keywords from the returned documents and add them to
    the query.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种将传统检索与LLM驱动的查询重写结合的方法是首先从初始检索步骤返回top-k文档，然后使用LLM从返回的文档中生成显著关键词并将其添加到查询中。
- en: So far we have discussed techniques that bridge the query document mismatch
    problem by modifying the query and bringing it closer to the document space. An
    alternative approach to solve the mismatch problem is to represent the documents
    in a way that brings them closer to the query space. Examples of this approach
    include [doc2query](https://oreil.ly/CGUtP) and [contextual retrieval](https://oreil.ly/ZJuIu).
    While document rewriting techniques initially have a large cost if the data stores
    are very large, they can reduce latency during inference time as no or little
    query rewriting needs to be performed. On the other hand, query rewriting techniques
    are simple to implement and integrate into a RAG workflow.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了通过修改查询并将其更接近文档空间来弥合查询文档不匹配问题的技术。解决不匹配问题的另一种方法是按照将文档表示得接近查询空间的方式。这种方法的例子包括[doc2query](https://oreil.ly/CGUtP)和[上下文检索](https://oreil.ly/ZJuIu)。虽然如果数据存储非常大，文档重写技术最初可能会有很大的成本，但它们可以在推理时间减少延迟，因为不需要或只需要很少的查询重写。另一方面，查询重写技术易于实现并集成到RAG工作流程中。
- en: Yet another form of query rewriting is called query decomposition. For complex
    queries in an agentic workflow, we can have the LLM divide the task into multiple
    queries that can be executed sequentially or in parallel, depending on how the
    query was decomposed. We discussed query decomposition techniques in [Chapter 10](ch10.html#ch10).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种查询重写形式称为查询分解。对于代理工作流程中的复杂查询，我们可以让LLM将任务分解成多个可以顺序或并行执行的查询，具体取决于查询是如何分解的。我们在第10章中讨论了查询分解技术。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If your external data is in a structured form like databases, then the query
    needs to be rewritten into a SQL query or equivalent, as discussed in [Chapter 10](ch10.html#ch10).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的外部数据以结构化形式存在，如数据库，那么查询需要被重写为SQL查询或等效查询，如第10章所述。
- en: Now that we have discussed the query rewriting step of the pipeline, let’s move
    on to the retrieve step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了管道中的查询重写步骤，让我们继续讨论检索步骤。
- en: Retrieve
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: 'The retrieve step is the most crucial stage of the RAG pipeline. It is easy
    to see why: all RAG applications are bottlenecked by the quality of retrieval.
    Even if you are working with the world’s best language model, you won’t be able
    to get the correct results if the retrieval step didn’t retrieve the correct documents
    needed to answer the query. Therefore, this step of the pipeline should focus
    on increasing recall.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检索步骤是RAG管道中最关键的阶段。原因显而易见：所有RAG应用都受检索质量的制约。即使你使用的是世界上最好的语言模型，如果检索步骤没有检索到回答查询所需的正确文档，你也无法得到正确的结果。因此，这个步骤应该专注于提高召回率。
- en: Embedding-based retrieval, which we discussed in detail in [Chapter 11](ch11.html#chapter_llm_interfaces),
    is highly popular. However, traditional information-retrieval techniques should
    not be dismissed. The right technique to use depends on the expected nature of
    queries (can a significant proportion of them be answered by just keyword or regex
    match?), the expected degree of query-document vocabulary mismatch, latency and
    compute limitations, and performance requirements.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第11章中详细讨论过的基于嵌入的检索非常受欢迎。然而，传统的信息检索技术不应被忽视。使用哪种技术取决于查询预期的性质（是否可以通过仅关键词或正则表达式匹配来回答其中很大一部分？），查询与文档词汇不匹配的预期程度，延迟和计算限制，以及性能要求。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The information retrieval (IR) research field has been studying these problems
    for a long time. Now that retrieval is more relevant than ever in NLP, I am noticing
    a lot of efforts to reinvent the wheel rather than reusing IR insights. For insights
    in retrieval research, check out papers from leading IR research conferences like
    SIGIR, ECIR, TREC, etc.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索（IR）研究领域已经研究了这些问题很长时间。现在，随着检索在NLP中的相关性日益增强，我注意到很多人在重新发明轮子，而不是重用IR的见解。关于检索研究的见解，可以查看SIGIR、ECIR、TREC等领先IR研究会议的论文。
- en: Embedding-based retrieval methods are not always suitable when you would like
    all documents containing a specific word or phrase to be retrieved. Therefore
    it is customary to combine keyword-based methods with embedding methods, called
    hybrid search. The results from the two methods are combined and fed to the next
    step of the retrieval pipeline. Most vector databases support hybrid search in
    some shape or form.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望检索包含特定单词或短语的全部文档时，基于嵌入的检索方法并不总是合适的。因此，通常将基于关键词的方法与嵌入方法结合，称为混合搜索。两种方法的结果被合并并输入到检索管道的下一步。大多数向量数据库都支持某种形式的混合搜索。
- en: '[Figure 12-2](#hybrid-search) shows the retrieval stage in action, using hybrid
    search.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-2](#hybrid-search)展示了使用混合搜索的检索阶段。'
- en: '![Hybrid-Search](assets/dllm_1202.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![混合搜索](assets/dllm_1202.png)'
- en: Figure 12-2\. Hybrid search
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2. 混合搜索
- en: I also highly recommend metadata filters for improving retrieval. The more metadata
    you gather during the data representation and storage phase, the better the retrieval
    results. For example, if you have performed topic modeling of your data store
    in advance, you can restrict your search results to a subset of topics, with the
    filters being applied either using a hardcoded set of rules or determined by an
    LLM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我还强烈推荐使用元数据过滤器来提高检索效果。在数据表示和存储阶段收集的元数据越多，检索结果就越好。例如，如果你已经提前对你的数据存储进行了主题建模，你可以通过硬编码的规则或由LLM确定的方式，将搜索结果限制在特定主题的子集。
- en: Next, let’s discuss promising recent advances in retrieval.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论检索领域最近的一些有希望的进展。
- en: Generative retrieval
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成式检索
- en: What if the LLM could identify the right documents(s) that need to be retrieved
    in response to a query, thus removing the need for retrieval techniques? This
    is called generative retrieval.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM能够识别出响应查询所需检索的正确文档（或文档集），从而消除检索技术的需求，这被称为生成式检索。
- en: 'Generative retrieval is implemented by assigning identifiers to documents called
    docIDs, and teaching the LLM the association between documents and docIDs. A document
    can be associated with one or more docIDs. Typical docIDs can be:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式检索通过为文档分配称为docID的标识符，并教授LLM文档与docID之间的关联来实现。一个文档可以与一个或多个docID相关联。典型的docID可以是：
- en: Single tokens
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 单个标记
- en: Each document can be represented by a new token in the vocabulary. This means
    that, during inference, the model needs to output only a single token for each
    document it wants to retrieve. [Pradeep et al.](https://oreil.ly/7JYOM) use a
    T5 model where the encoder vocabulary is the standard T5 vocabulary but the decoder
    vocabulary contains the docIDs. This approach is feasible only with a small document
    corpus.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档都可以由词汇表中的一个新标记表示。这意味着，在推理过程中，模型只需要为它想要检索的每个文档输出一个标记。[Pradeep等人](https://oreil.ly/7JYOM)使用了一个T5模型，其中编码器词汇表是标准的T5词汇表，但解码器词汇表包含docID。这种方法仅适用于小型文档语料库。
- en: Prefix/subset tokens
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀/子集标记
- en: '[Tay et al.](https://oreil.ly/1p1C8) use the first 64 tokens of a document
    as the docID, while [Wang et al.](https://oreil.ly/lg3g3) use 64 randomly selected
    contiguous tokens from the document.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tay等人](https://oreil.ly/1p1C8)使用文档的前64个标记作为docID，而[Wang等人](https://oreil.ly/lg3g3)使用从文档中随机选择的64个连续标记。'
- en: Cluster tokens
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类标记
- en: You can also perform hierarchical clustering of your document corpus based on
    its semantics (using embeddings, for example), and the docID can be a concatenation
    of the cluster IDs at each level of the hierarchy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以根据其语义（例如使用嵌入）对文档语料库进行层次聚类，docID可以是层次结构中每个级别的聚类ID的连接。
- en: Salient keyword tokens
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显著关键词标记
- en: The docIDs can also contain salient keywords representing the topics and themes
    contained in the document. For example, a document about the Transformer architecture
    can be represented by the docID “transformer_self-attention_architecture.”
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的docID也可以包含代表文档中主题和主题的关键词。例如，一篇关于Transformer架构的文档可以用docID“transformer_self-attention_architecture”来表示。
- en: One way to teach the LLM the association between documents and docIDs is by
    fine-tuning the model. This is referred to as training-based indexing. However,
    fine-tuning needs a lot of resources and is not suitable in scenarios in which
    new documents are frequently added to the corpus.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 教授LLM文档与docID之间关联的一种方法是通过微调模型。这被称为基于训练的索引。然而，微调需要大量资源，并且不适用于频繁向语料库添加新文档的场景。
- en: '[Askari et al.](https://oreil.ly/K5TAB) show that we can use few-shot learning
    to build a generative retrieval system without needing to train the model. First,
    for each document in the corpus, pseudo queries are generated using a language
    model. The pseudo queries are the queries whose answers are present in the document.
    These pseudo queries are then fed to a language model in a few-shot setting and
    asked to generate docIDs. [Figure 12-3](#generative-retrieval) shows training-free
    generative retrieval in action.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[Askari等人](https://oreil.ly/K5TAB)表明，我们可以使用少样本学习来构建一个不需要训练模型的生成式检索系统。首先，对于语料库中的每个文档，使用语言模型生成伪查询。这些伪查询是其中包含文档中答案的查询。然后，将这些伪查询在少样本设置中输入到语言模型，并要求生成docID。[图12-3](#generative-retrieval)显示了无需训练的生成式检索的实际应用。'
- en: '![Generative-Retrieval](assets/dllm_1203.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![Generative-Retrieval](assets/dllm_1203.png)'
- en: Figure 12-3\. Generative retrieval
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 生成式检索
- en: During inference, the model is provided with a query similar to the setup in
    [Figure 12-3](#generative-retrieval) and asked to generate the correct docID(s)
    that are relevant to the query. Constrained beam search is used to ensure that
    the docID generated by the model corresponds to a valid docID in the corpus.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，模型被提供与[图12-3](#generative-retrieval)中设置类似的查询，并要求生成与查询相关的正确docID（或docID集）。使用约束束搜索以确保模型生成的docID对应于语料库中的有效docID。
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also use generative retrieval to retrieve documents based on their metadata.
    For example, the model could ask to retrieve Apple’s 2024 annual report. The model
    can be made to generate the right identifier by either fine-tuning the model or
    using few-shot learning, as shown in this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用基于元数据的生成检索来检索文档。例如，模型可以请求检索苹果公司的2024年年度报告。模型可以通过微调模型或使用本节所示的小样本学习来生成正确的标识符。
- en: Ultimately, generative retrieval is suitable only if your document corpus is
    relatively small, there is limited redundancy within the corpus, or the documents
    belong to a set of well-defined categories (annual reports of all public companies
    in the US, for instance).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，生成检索仅适用于您的文档语料库相对较小、语料库内冗余有限或文档属于一组定义良好的类别（例如，美国所有上市公司的年度报告）的情况。
- en: Next, let’s discuss tightly-coupled retrievers, another new topic in the retrieval
    space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论紧密耦合的检索器，这是检索空间中的另一个新话题。
- en: Tightly-coupled retrievers
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 紧密耦合的检索器
- en: As seen in [Chapter 11](ch11.html#chapter_llm_interfaces), in embedding-based
    retrieval, the embedding model is typically independent of the language model
    to which the retrieval results are fed. We will refer to them as *loosely-coupled*
    retrievers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第11章](ch11.html#chapter_llm_interfaces)中所示，在基于嵌入的检索中，嵌入模型通常独立于检索结果所提供的语言模型。我们将它们称为**松散耦合**的检索器。
- en: In contrast, a *tightly-coupled* retriever is trained such that it learns from
    LLM feedback; the model learns to retrieve text that best positions the LLM to
    generate the correct output for a given query. Tightly-coupled retrievers can
    be trained together with the generator LLM as part of a single architecture, or
    they can be trained separately using feedback from the trained LLM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，一个**紧密耦合**的检索器是经过训练的，以便它从LLM反馈中学习；模型学会检索文本，以最佳方式定位LLM为给定查询生成正确的输出。紧密耦合的检索器可以作为单一架构的一部分与生成LLM一起训练，或者它们可以单独使用训练好的LLM的反馈进行训练。
- en: 'An example of the latter is [Zhang et al.’s LLM-Embedder](https://oreil.ly/Q__8M),
    a unified embedding model that can support a variety of retrieval needs in a single
    model, ranging from knowledge retrieval to retrieving optimal few-shot examples.
    The model is trained from two types of signals: a contrastive learning setup typically
    used to train embedding models (presented in [Chapter 11](ch11.html#chapter_llm_interfaces))
    and LLM feedback. A retrieval candidate receives a larger reward from LLM feedback
    if it improves the performance of the LLM in answering the query.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的一个例子是[张等人](https://oreil.ly/Q__8M)的LLM-Embedder，这是一个统一的嵌入模型，可以在单个模型中支持各种检索需求，从知识检索到检索最优的少量示例。该模型从两种类型的信号中进行训练：用于训练嵌入模型的对比学习设置（在第11章[第11章](ch11.html#chapter_llm_interfaces)中介绍）和LLM反馈。如果检索候选者提高了LLM回答查询的性能，它将从LLM反馈中获得更大的奖励。
- en: Tightly-coupled retrievers are another tool in your toolkit for improving retrieval.
    They are by no means a necessary step in the RAG pipeline. As always, experimentation
    will show how much of a lift (if any) they provide for your application.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 紧密耦合的检索器是您工具箱中用于改进检索的另一个工具。它们绝不是RAG管道中的必要步骤。像往常一样，实验将显示它们为您的应用提供了多少提升（如果有的话）。
- en: Finally, let’s discuss GraphRAG, an up-and-coming retrieval paradigm that leverages
    knowledge graphs for better retrieval.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论GraphRAG，这是一种新兴的检索范式，它利用知识图来实现更好的检索。
- en: GraphRAG
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GraphRAG
- en: A key limitation of the retrieval approaches we have discussed so far is their
    inability to facilitate answering questions that require drawing connections between
    different parts of the document corpus, as well as questions that involve summarizing
    high-level themes across the dataset. For example, all the retrieval techniques
    we discussed so far would do poorly on a query like, “What are the key topics
    discussed in this dataset?”
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的检索方法的一个关键限制是它们无法促进回答需要在不同文档语料库的不同部分之间建立联系的问题，以及涉及总结数据集高级主题的问题。例如，我们之前讨论的所有检索技术在对“在这个数据集中讨论的关键主题是什么？”这样的查询上表现不佳。
- en: One way to address these limitations is by employing knowledge graphs. Microsoft
    released [GraphRAG](https://oreil.ly/V4n_S), a graph-based RAG system. GraphRAG
    works by creating a knowledge graph from the underlying data corpus by extraction
    entities and relationships. The graph is then used to perform hierarchical semantic
    clustering, with summaries generated for each cluster. These summaries enable
    answering of thematic questions like, “What are the key topics discussed in this
    dataset?”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些限制的一种方法是通过采用知识图谱。微软发布了[GraphRAG](https://oreil.ly/V4n_S)，一个基于图的知识图谱系统。GraphRAG通过从底层数据语料库中提取实体和关系来创建知识图谱。然后，该图被用于执行分层语义聚类，并为每个簇生成摘要。这些摘要使得回答主题性问题成为可能，例如，“在这个数据集中讨论的关键主题是什么？”
- en: GraphRAG requires a lot of initial compute to prepare the knowledge graph. This
    can be prohibitive for larger datasets. While it is easy to extract entities,
    extracting relevant relationships is harder.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: GraphRAG需要大量的初始计算来准备知识图谱。这对于更大的数据集来说可能是限制性的。虽然提取实体很容易，但提取相关关系则更困难。
- en: Now that we have explored the retrieval stage of the RAG pipeline, let’s move
    on to the rerank stage.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了RAG管道的检索阶段，让我们继续到重排序阶段。
- en: Rerank
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重排序
- en: The retrieval process can be broken into a two-stage or multi-stage process,
    where the initial stage retrieves a list of documents relevant to the query, followed
    by one or more *reranking* stages that take the documents and sort them by relevance.
    The reranker is generally a more complex model than the retriever and thus is
    run only on the retrieved results (or else we would have just used the reranker
    as the retriever).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 检索过程可以被分解为两阶段或多阶段过程，其中第一阶段检索与查询相关的文档列表，随后是一或多个*重排序*阶段，这些阶段将文档按相关性排序。重排序器通常比检索器更复杂，因此仅在检索结果上运行（否则我们就可以直接使用重排序器作为检索器）。
- en: The reranker is usually a language model fine-tuned on the specific use case.
    You can use BERT-like models for building a relevance classifier, where given
    a query and a document, the model outputs the probability of the document being
    relevant to answering the query. These models are called *cross-encoders*, as
    in these models the query and document are encoded together, as opposed to embedding-based
    retrieval models we have discussed, called bi-encoders, where the query and document
    are encoded as separate vectors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 重排序器通常是在特定用例上微调的语言模型。你可以使用类似BERT的模型来构建一个相关性分类器，其中给定一个查询和一个文档，模型输出文档与回答查询的相关概率。这些模型被称为*跨编码器*，因为这些模型中查询和文档是共同编码的，与之前讨论的基于嵌入的检索模型不同，后者称为双编码器，其中查询和文档被编码为单独的向量。
- en: 'The input for a BERT model acting as a cross-encoder is of the format:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作为跨编码器作用的BERT模型的输入格式为：
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Sentence Transformers library provides access to cross-encoders, which
    can be used as rerankers in the RAG pipeline:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 句子转换库提供了访问跨编码器的能力，这些编码器可以用作RAG管道中的重排序器：
- en: '[PRE1]`` `''On September 22, 2023, I lined up at the Central Park store for
    the launch of` [PRE2] `ranks` `=` `model``.``rank``(``query``,` `documents``)`
    `for` `rank` `in` `ranks``:`    `print``(``rank``[``''score''``],` `documents``[``rank``[``''corpus_id''``]])`
    [PRE3]` [PRE4]'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE1]`` `''On September 22, 2023, I lined up at the Central Park store for
    the launch of` [PRE2] `ranks` `=` `model``.``rank``(``query``,` `documents``)`
    `for` `rank` `in` `ranks``:`    `print``(``rank``[``''score''``],` `documents``[``rank``[``''corpus_id''``]])`
    [PRE3]` [PRE4]'
- en: '[PRE5] ``Because we have set `num_labels = 1`, the model will treat it as a
    regression task, using the sigmoid activation function to output a score between
    0 and 1.    These days, more advanced models like [Contextualized Late Interaction
    over BERT (ColBERT)](https://oreil.ly/N3fOv) are used for reranking. As opposed
    to the cross-encoder setup we just discussed, ColBERT-style models allow for pre-computation
    of document representations, leading to faster inference.    In ColBERT, the query
    and documents are encoded separately using BERT, generating token-level embedding
    vectors for each token in the query and documents. For each token in the query,
    the corresponding embedding is compared to the embeddings of each of the token
    embeddings of the document, generating similarity scores. The maximum similarity
    scores for each query token are summed, resulting in the final relevance score.
    This type of architecture is called *late interaction*, since the query and document
    are not encoded together but interact together only later in the process. Late
    interaction saves time compared to traditional cross-encoders, as document embeddings
    can be created and stored in advance.    [Figure 12-4](#cross-encoders) depicts
    a ColBERT model in action, illustrating the late interaction between query and
    documents.  ![cross-encodersl](assets/dllm_1204.png)  ###### Figure 12-4\. ColBERT    Next,
    let’s explore a few advanced reranking techniques.    ### Query likelihood model
    (QLM)    A QLM estimates the probability of generating the query given a candidate
    document as input. You can treat an LLM as a QLM, utilizing its zero-shot capabilities
    to rank candidate documents based on the query token probabilities. Alternatively,
    you can fine-tune an LLM on query generation tasks to improve its suitability
    as a QLM.    A typical prompt for a QLM would look like: “Generate a question
    that is most relevant to the given document <document content>”.    After getting
    the top-k documents relevant to a query from the retrieval stage, each document
    is fed to the LLM with this prompt. The likelihood of the query tokens is then
    calculated using the model logits. The documents are then sorted by likelihood,
    providing a relevance ranking.    ###### Warning    [Zhuang et al.](https://oreil.ly/QnWWh)
    show that an instruction-tuned model that doesn’t contain query generation tasks
    in its instruction-tuning training set loses its capability to be an effective
    zero-shot QLM. This is yet another case of instruction-tuned models exhibiting
    degraded performance compared to base models, on tasks they have not been trained
    on.    Note that to calculate the probability of the query tokens, we need access
    to the model logits. Most proprietary model providers including OpenAI do not
    yet provide full access to the model logits as of this book’s writing. Thus, the
    LLM-as-a-QLM approach can be implemented only using open source models.    In
    the interest of reducing latency, you would ideally like the QLM to be as small
    a model as possible. However, smaller models are less effective QLMs. Effectively
    fine-tuning a smaller LLM for query generation might be the sweet spot.    ###
    LLM distillation for ranking    Earlier in the chapter, we saw how encoder-only
    models like BERT could serve as rerankers. More recently, decoder LLMs are also
    being trained to directly rank candidate documents in three ways:    Pointwise
    ranking      Each candidate document is fed separately to the LLM. The LLM provides
    a Boolean judgment on its relevance. Alternatively, it can also provide a numerical
    score, although this is much less reliable.      Pairwise ranking      For each
    candidate document pair, the LLM indicates which document is more relevant. To
    get a complete ranking, N² such comparisons need to be made.      Listwise ranking      All
    the candidate documents are tagged with identifiers and fed to the LLM, and the
    LLM is asked to generate a ranked list of identifiers according to decreasing
    order of relevance of corresponding documents.      In general, pointwise ranking
    is the easiest to use but may not be the [most effective](https://oreil.ly/DvmtC).
    Listwise ranking might need a large context window, while pairwise ranking needs
    lots of comparisons. Pairwise ranking is the most effective of these techniques,
    since it involves direct comparison. [Figure 12-5](#llm-rerankers) shows how pointwise,
    pairwise, and listwise rankings work.    Examples of ranking LLMs include [RankGPT](https://oreil.ly/6XoOG),
    [RankVicuna](https://oreil.ly/00Dan), and [RankZephyr](https://oreil.ly/AAbUE).    These
    models are trained by distilling from larger LLMs, a technique we first learned
    in [Chapter 9](ch09.html#ch09). For example, the process for training RankVicuna
    is:    *   Queries in the training set are fed through a first-level retriever
    like BM25 to generate a list of candidate documents.           *   This list is
    passed to a larger LLM, which generates a rank-ordered list of candidates.           *   The
    query and the rank-ordered list are used to fine-tune the smaller LLM.              The
    creators of [RankVicuna](https://oreil.ly/cFLSc) show that as the effectiveness
    of the first-level retrieval increases, the possible performance gains from RankVicuna
    decreases due to diminished returns. They also reported that augmenting the dataset
    by shuffling the input order of the candidate documents improved model performance.  ![llm-rerankers](assets/dllm_1205.png)  ######
    Figure 12-5\. Decoder LLM rerankers    ###### Tip    You can combine the results
    of the retrieve and the rerank stages to get the final relevance ranking of candidate
    documents. This is needed to enforce keyword weighting, for example. You can also
    weight your relevance ranking by metadata like published date (more recent documents
    are weighted more).    Now that we have discussed the rerank stage, let’s move
    on to the refine step of the RAG pipeline.`` [PRE6]`  [PRE7]  [PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE5] ``由于我们已将 `num_labels = 1` 设置，该模型将将其视为回归任务，使用 sigmoid 激活函数输出介于 0 和 1
    之间的分数。这些天，更多像 [ColBERT (基于 BERT 的上下文化后期交互)](https://oreil.ly/N3fOv) 这样的先进模型被用于重新排序。与刚刚讨论的交叉编码器设置相反，ColBERT
    风格的模型允许预先计算文档表示，从而实现更快的推理。在 ColBERT 中，查询和文档分别使用 BERT 进行编码，为查询和文档中的每个标记生成标记级嵌入向量。对于查询中的每个标记，相应的嵌入与文档中每个标记嵌入的嵌入进行比较，生成相似度分数。每个查询标记的最大相似度分数相加，得到最终的关联分数。这种架构称为
    *后期交互*，因为查询和文档不是一起编码，而是在处理过程中的后期才交互。后期交互与传统交叉编码器相比节省了时间，因为文档嵌入可以预先创建和存储。![图 12-4](#cross-encoders)
    展示了 ColBERT 模型在行动中的样子，说明了查询和文档之间的后期交互。![cross-encodersl](assets/dllm_1204.png)  ######
    图 12-4\. ColBERT    接下来，让我们探索一些高级的重新排序技术。    ### 查询似然模型 (QLM)    QLM 估计给定候选文档作为输入生成查询的概率。您可以将一个
    LLM 视为一个 QLM，利用其零样本能力根据查询标记概率对候选文档进行排序。或者，您可以对 LLM 进行微调，以改进其在 QLM 中的适用性。    一个典型的
    QLM 提示可能看起来像：“生成一个与给定文档 <文档内容> 最相关的问题”。    在检索阶段从查询中获取与查询相关的 top-k 文档后，将每个文档用此提示输入到
    LLM 中。然后使用模型 logits 计算查询标记的概率。然后根据似然度对文档进行排序，提供相关性排名。    ###### 警告    [庄等](https://oreil.ly/QnWWh)
    表明，在指令微调训练集中不包含查询生成任务的指令微调模型会失去作为有效零样本 QLM 的能力。这是指令微调模型在未训练任务上与基模型相比性能下降的又一案例。    注意，为了计算查询标记的概率，我们需要访问模型
    logits。截至本书编写时，大多数专有模型提供商，包括 OpenAI，尚未提供对模型 logits 的完全访问。因此，将 LLM 作为 QLM 的方法只能使用开源模型。    为了减少延迟，您可能希望
    QLM 尽可能小。然而，较小的模型作为 QLM 的效果较差。有效地微调较小的 LLM 以进行查询生成可能是最佳选择。    ### 排序的 LLM 精炼    在本章前面，我们看到了像
    BERT 这样的仅编码器模型可以作为重新排序器。最近，解码器 LLM 也被训练以直接以三种方式对候选文档进行排序：    点对点排序      将每个候选文档单独输入到
    LLM 中。LLM 提供对其相关性的布尔判断。或者，它也可以提供一个数值分数，尽管这不太可靠。      对偶排序      对于每个候选文档对，LLM 指示哪个文档更相关。为了获得完整的排名，需要进行
    N² 次这样的比较。      列表排序      将所有候选文档标记为标识符并输入到 LLM 中，并要求 LLM 根据相应文档的相关性递减顺序生成标识符的排序列表。      通常，点对点排序最容易使用，但可能不是
    [最有效的](https://oreil.ly/DvmtC)。列表排序可能需要一个大的上下文窗口，而对偶排序需要大量的比较。对偶排序是这些技术中最有效的，因为它涉及直接比较。[图
    12-5](#llm-rerankers) 展示了点对点、对偶和列表排序的工作方式。    排序 LLM 的例子包括 [RankGPT](https://oreil.ly/6XoOG)、[RankVicuna](https://oreil.ly/00Dan)
    和 [RankZephyr](https://oreil.ly/AAbUE)。    这些模型是通过从更大的 LLM 中蒸馏训练的，这是我们首次在 [第 9
    章](ch09.html#ch09) 中学到的技术。例如，训练 RankVicuna 的过程如下：    *   训练集中的查询通过第一级检索器（如 BM25）生成候选文档列表。           *   将此列表传递给更大的
    LLM，生成候选文档的排序列表。           *   使用查询和排序列表微调较小的 LLM。              [RankVicuna](https://oreil.ly/cFLSc)
    的创建者表明，随着第一级检索的有效性提高，RankVicuna 的性能提升空间会减小，因为回报递减。他们还报告说，通过打乱候选文档的输入顺序来扩充数据集可以提高模型性能。  ![llm-rerankers](assets/dllm_1205.png)  ######
    图 12-5\. 解码器 LLM 重新排序器    ###### 小贴士    您可以将检索和重新排序阶段的结果结合起来，以获得候选文档的最终相关性排名。例如，这需要强制关键字加权。您还可以通过元数据（如发布日期）对相关性排名进行加权（较新的文档加权更高）。    现在，我们已经讨论了重新排序阶段，让我们继续讨论
    RAG 管道的精炼步骤。`` [PRE6]`  [PRE7]  [PRE8]'
