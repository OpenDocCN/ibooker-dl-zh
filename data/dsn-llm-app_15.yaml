- en: Chapter 12\. Retrieval-Augmented Generation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章. 检索增强生成
- en: In [Chapter 10](ch10.html#ch10), we demonstrated how to vastly expand the capabilities
    of LLMs by interfacing them with external data and software. In [Chapter 11](ch11.html#chapter_llm_interfaces),
    we introduced the concept of embedding-based retrieval, a foundational technique
    for retrieving relevant data from data stores in response to queries. Armed with
    this knowledge, let’s explore the application paradigm of augmenting LLMs with
    external data, called retrieval-augmented generation (RAG), in a holistic fashion.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#ch10)中，我们展示了如何通过将LLM与外部数据和软件接口来大幅扩展LLM的功能。在[第11章](ch11.html#chapter_llm_interfaces)中，我们介绍了基于嵌入的检索概念，这是一种从数据存储中检索相关数据的基础技术。掌握了这些知识，让我们全面地探讨通过外部数据增强LLM的应用范式，即检索增强生成（RAG）。
- en: In this chapter, we will take a comprehensive view of the RAG pipeline, diving
    deep into each of the steps that make up a typical workflow of a RAG application.
    We will explore the various decisions involved in operationalizing RAG, including
    what kind of data we can retrieve, how to retrieve it, and when to retrieve it.
    We will highlight how RAG can help not only during model inference but also during
    model training and fine-tuning. We will also compare RAG with other paradigms
    and discuss scenarios where RAG shines in comparison to alternatives or vice versa.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将全面审视RAG管道，深入探讨构成典型RAG应用程序工作流程的每个步骤。我们将探索实施RAG所涉及的各项决策，包括我们可以检索什么类型的数据、如何检索以及何时检索。我们将强调RAG如何帮助不仅在模型推理期间，而且在模型训练和微调期间。我们还将比较RAG与其他范式，并讨论RAG在与其他替代方案相比时表现优异的场景。
- en: The Need for RAG
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG的需求
- en: 'As introduced in [Chapter 10](ch10.html#ch10), RAG is an umbrella term used
    to describe a variety of techniques for using external data sources to augment
    the capabilities of an LLM. Here are some reasons we might want to use RAG:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第10章](ch10.html#ch10)中所述，RAG是一个术语，用于描述使用外部数据源增强LLM能力的一系列技术。以下是我们可能想要使用RAG的一些原因：
- en: We need the LLMs to access our private/proprietary data, or data that was not
    part of the LLM’s pre-training datasets. Using RAG is a much more lightweight
    option than pre-training an LLM on our private data.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要LLM访问我们的私有/专有数据，或者LLM预训练数据集之外的数据。使用RAG是一个比在私有数据上预训练LLM更轻量级的选择。
- en: To reduce the risk of hallucinations, we would like the LLM to refer to data
    provided through a retrieval mechanism rather than rely on its own internal knowledge.
    RAG facilitates this. RAG also enables more accurate data citations, connecting
    LLM outputs to their ground-truth sources.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了减少幻觉的风险，我们希望LLM通过检索机制提供的数据而不是依赖其自身的内部知识。RAG促进了这一点。RAG还使数据引用更加准确，将LLM的输出与其真实来源相连接。
- en: We would like the LLM to answer questions about recent events and concepts that
    have emerged after the LLM was pre-trained. While there are memory editing techniques
    for updating LLM parameters with new knowledge like [MEMIT](https://oreil.ly/kxI3j),
    they are not yet reliable or scalable. As discussed in [Chapter 7](ch07.html#ch07),
    continually training an LLM to keep its knowledge up-to-date is expensive and
    risky.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望LLM能够回答关于LLM预训练后出现的近期事件和概念的问题。虽然存在像[MEMIT](https://oreil.ly/kxI3j)这样的记忆编辑技术，用于用新知识更新LLM参数，但它们目前还不可靠或可扩展。正如[第7章](ch07.html#ch07)中讨论的，持续训练LLM以保持其知识更新是昂贵且风险较高的。
- en: We would like the LLM to answer queries involving long-tail entities, which
    occur only rarely in the pre-training datasets.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望LLM能够回答涉及长尾实体的查询，这些实体在预训练数据集中出现的频率很低。
- en: Typical RAG Scenarios
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 典型的RAG场景
- en: 'Now that we have seen *why* we need RAG, let’s explore *where* we can utilize
    it. The four most popular scenarios are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了*为什么*我们需要RAG，那么让我们来探讨*哪里*我们可以利用它。最流行的四种场景是：
- en: Retrieving external knowledge
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 检索外部知识
- en: This is the predominant use case that has seen a lot of success with productionization.
    As discussed earlier in the chapter, we can use RAG to plug LLM knowledge gaps
    or to reduce hallucination risk.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是已经看到很多成功案例并实现商业化的主要用例。正如本章前面所讨论的，我们可以使用RAG来填补LLM的知识空白或降低幻觉风险。
- en: Retrieving context history
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 检索上下文历史
- en: LLMs have a limited context window, but often we need access to more context
    in order to answer a query than what fits in the context window. We would also
    like to have longer conversations with the LLM than what fits in the context window.
    In these cases, we could retrieve parts of the conversation history or session
    context when needed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的上下文窗口有限，但通常我们需要比上下文窗口能容纳的更多上下文来回答查询。我们也希望与LLM进行比上下文窗口能容纳的更长时间的对话。在这些情况下，当需要时，我们可以检索对话历史或会话上下文的一部分。
- en: Retrieving in-context training examples
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 检索上下文训练示例
- en: Few-shot learning is an effective approach to help LLMs get acquainted with
    the input-output mapping of a task. You can make few-shot learning more effective
    by dynamically selecting few-shot examples based on the current input. The few-shot
    examples can be retrieved from a training example data store at inference time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习是一种有效的途径，可以帮助LLMs熟悉任务的输入-输出映射。您可以通过根据当前输入动态选择少样本示例来提高少样本学习的有效性。在推理时，这些少样本示例可以从训练示例数据存储库中检索。
- en: Retrieving tool-related information
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 检索工具相关信息
- en: As described in [Chapter 10](ch10.html#ch10), LLMs can invoke software tools
    as part of their workflow. The list of tools available and their description is
    stored in a tool store. The LLM can then use retrieval for tool selection, selecting
    the tool best suited to the task. Tool-related information can also include API
    documentation, for instance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如第10章所述，LLMs可以将软件工具作为其工作流程的一部分来调用。可用的工具列表及其描述存储在工具存储库中。然后LLM可以使用检索来进行工具选择，选择最适合任务的工具。工具相关信息也可以包括API文档，例如。
- en: Deciding When to Retrieve
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决定何时检索
- en: 'For each step in an agentic workflow, the LLM can advance its task using one
    of the following steps:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个代理工作流程的每个步骤中，LLM可以使用以下步骤之一来推进其任务：
- en: Use its internal capabilities
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其内部能力
- en: Choose from among several data stores
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从几个数据存储库中选择
- en: Choose from among several software tools
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从几种软件工具中选择
- en: There can be tasks that the LLM can fully solve using its parametric memory,
    but one or more data stores may also contain the requisite data needed to solve
    them. In these cases, should we just default to using RAG, given all its benefits
    that we presented earlier?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能LLM可以使用其参数化记忆完全解决的任务，但一个或多个数据存储库也可能包含解决这些任务所需的数据。在这些情况下，鉴于我们之前提出的所有好处，我们应该默认使用RAG吗？
- en: We have seen earlier in the chapter that LLMs struggle with long-tail information,
    and that RAG can be an effective means to answer questions about long-tail entities.
    However, [Mallen et al.](https://oreil.ly/MF7Y1) show that for queries about more
    popular entities, the LLM might sometimes be better at answering queries than
    RAG. This is because of the inevitable limitations of the retrieval model, which
    might retrieve irrelevant or incorrect information that could mislead the LLM.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章前面看到，LLMs在处理长尾信息方面存在困难，而RAG可以成为回答关于长尾实体的有效手段。然而，[Mallen等人](https://oreil.ly/MF7Y1)表明，对于关于更常见实体的查询，LLM有时可能比RAG更好地回答查询。这是由于检索模型的不可避免限制，它可能会检索到无关或不正确的信息，这可能会误导LLM。
- en: 'For a given query, you can dynamically determine whether to use retrieval or
    to rely on the LLM’s parametric memory. The rules determining the right approach
    to take include:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的查询，您可以动态确定是使用检索还是依赖LLM的参数化记忆。决定采取正确方法的规则包括：
- en: Whether the query is about a more frequently occurring entity. For example,
    the LLM is more likely to memorize the birthday of Taylor Swift than of a substitute
    drummer of a local band whose Wikipedia page is a stub.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询是否关于更频繁出现的实体。例如，LLM更有可能记住泰勒·斯威夫特的生日，而不是一个地方乐队的替补鼓手的生日，其维基百科页面是一个简短条目。
- en: Whether the query has timeliness constraints, i.e., if the data needed to address
    the query may not have existed before the LLM’s knowledge cutoff date.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询是否有时效性限制，即所需的数据可能在LLM的知识截止日期之前不存在。
- en: Whether the model has been continually pre-trained or memory tuned as described
    in [Chapter 7](ch07.html#ch07), and the given query relates to concepts over which
    the training was performed.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无论模型是否如第7章所述进行了持续预训练或记忆调整，以及给定的查询与训练所涉及的概念相关。
- en: If you are using LLMs for general-purpose question answering, Mallen et al.
    show that you can use sources like Wikipedia as a pseudo-popularity metric for
    entities. If the entities present in your inputs have an entity count in Wikipedia
    greater than a threshold, then the LLM can choose to answer the question on its
    own without using RAG. Note that the threshold can change across LLMs. This strategy
    works only if you have a good understanding about the datasets the LLM has been
    pre-trained on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用LLM进行通用问答，Mallen等人表明，您可以使用像维基百科这样的来源作为实体的伪流行度指标。如果您的输入中存在的实体在维基百科中的计数超过一个阈值，那么LLM可以选择在没有使用RAG的情况下自行回答问题。请注意，阈值可能在不同LLM之间有所不同。这种策略仅在您对LLM预先训练的数据集有良好理解的情况下才有效。
- en: Dynamically deciding when to retrieve data can also help optimize the model’s
    latency and responsiveness, as the RAG pipeline will introduce additional overhead.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 动态决定何时检索数据也可以帮助优化模型的延迟和响应性，因为RAG管道将引入额外的开销。
- en: Tip
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Dynamic retrieval is mostly useful when you are using very large LLMs. For smaller
    models (7B or below), it is almost always beneficial to prefer using RAG rather
    than relying on the LLM’s internal memory.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 动态检索在您使用非常大的LLM时非常有用。对于较小的模型（7B或以下），几乎总是有益于优先使用RAG而不是依赖LLM的内部内存。
- en: The RAG Pipeline
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG管道
- en: A typical RAG application follows the *retrieve-read* framework, as discussed
    in [Chapter 11](ch11.html#chapter_llm_interfaces). In response to a query, a retrieval
    model identifies documents that are relevant to answering the query. These documents
    are then passed along to the LLM as context, which the LLM can rely on in addition
    to its internal capabilities to generate a response. In practice, we typically
    need to add a lot of bells and whistles to get RAG working in a production context.
    This involves adding several more optional stages to the retrieve-read framework.
    In practice, your pipeline stages might consist of a *rewrite-retrieve-read-refine-insert-generate*
    workflow, with some of these steps potentially comprising multiple stages. Later
    in the chapter, we will go through each of the steps in more detail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的RAG应用遵循*检索-读取*框架，如[第11章](ch11.html#chapter_llm_interfaces)中讨论的那样。在响应查询时，检索模型会识别与回答查询相关的文档。然后，这些文档被传递给LLM作为上下文，LLM可以依赖这些上下文以及其内部能力来生成响应。在实践中，我们通常需要添加很多功能才能在生产环境中使RAG工作。这涉及到在检索-读取框架中添加几个额外的可选阶段。在实践中，您的管道阶段可能包括一个*重写-检索-读取-精炼-插入-生成*工作流程，其中一些步骤可能包含多个阶段。在本章的后面部分，我们将更详细地介绍每个步骤。
- en: '[Figure 12-1](#RAG-pipeline) shows the various stages of the RAG pipeline and
    the components involved.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-1](#RAG-pipeline)展示了RAG管道的各个阶段和涉及的组件。'
- en: '![RAG-pipeline](assets/dllm_1201.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![RAG-pipeline](assets/dllm_1201.png)'
- en: Figure 12-1\. RAG pipeline
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1\. RAG管道
- en: Tip
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As in the rest of the book, we refer to user or LLM requests to retrieve data
    as queries, and units of text retrieved from the data store as documents.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本书的其他部分一样，我们将用户或LLM请求检索数据称为查询，并将从数据存储中检索到的文本单元称为文档。
- en: Let’s illustrate with an example. Consider a RAG application that answers questions
    about Canadian politics and parliamentary activity. The application has access
    to a knowledge base containing transcripts of parliamentary proceedings. We will
    assume that the data is represented using the representation techniques described
    in [Chapter 11](ch11.html#chapter_llm_interfaces).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来说明。考虑一个关于加拿大政治和议会活动的RAG应用。该应用可以访问包含议会会议记录的知识库。我们将假设数据使用[第11章](ch11.html#chapter_llm_interfaces)中描述的表示技术进行表示。
- en: When a user issues a query, we might want to rephrase it before sending it to
    the retriever. Traditionally in the field of information retrieval (IR), this
    is referred to as query expansion. Query expansion is especially useful because
    of the vocabulary mismatch between the query space and the document space. The
    user might use different terminology in the query than that used in the documents.
    Rephrasing a query can help bridge the vocabulary gap. In general, we would like
    to rephrase the query in such a way that it improves the chances of the retriever
    fetching the most relevant documents. This stage is called the *rewrite* stage.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户提出查询时，我们可能希望在将其发送给检索器之前对其进行重新表述。在信息检索（IR）领域，这种做法传统上被称为查询扩展。查询扩展特别有用，因为查询空间和文档空间之间存在词汇不匹配。用户可能在查询中使用与文档中使用的不同的术语。重新表述查询可以帮助弥合词汇差距。一般来说，我们希望以这种方式重新表述查询，以提高检索器检索最相关文档的机会。这一阶段被称为*重写*阶段。
- en: Next, in the *retrieve* stage, a retrieval model is used to retrieve the documents
    relevant to the query. In [Chapter 11](ch11.html#chapter_llm_interfaces), we discussed
    embedding-based retrieval, a popular retrieval paradigm in the LLM era. The retrieval
    stage can be an extensive multi-stage pipeline.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在*检索*阶段，使用检索模型来检索与查询相关的文档。在[第11章](ch11.html#chapter_llm_interfaces)中，我们讨论了基于嵌入的检索，这是LLM时代流行的检索范式。检索阶段可能是一个多阶段的复杂管道。
- en: The retrieval can happen over a very large document space. In this case, it
    is computationally infeasible to use more advanced retrieval models. Therefore,
    retrieval is usually carried out in a two-step process, with the first step using
    faster methods (these days, typically embedding-based) to retrieve a list of potentially
    relevant documents (optimizing recall), and a second step that reranks the retrieved
    list based on relevance (optimizing precision) so that the top-k ranked documents
    are then taken as the context to be passed along to the LLM. This stage is called
    the *rerank* stage.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 检索可能发生在非常大的文档空间中。在这种情况下，使用更高级的检索模型在计算上是不可行的。因此，检索通常分为两个步骤，第一步使用更快的检索方法（这些天通常是基于嵌入的方法）来检索一份可能相关的文档列表（优化召回率），第二步根据相关性重新排序检索到的列表，以便将排名前k的文档作为传递给LLM的上下文。这一阶段被称为*重新排序*阶段。
- en: After identifying the top-k documents relevant to the query, they need to be
    passed along to the LLM. However, the documents may not fit into the context window
    and thus need to be shortened. They also could potentially be rephrased in a way
    that makes it more likely for the LLM to use the context to generate the answer.
    This is done during the *refine* stage.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定与查询相关的top-k文档后，需要将它们传递给LLM。然而，文档可能不适合上下文窗口，因此需要缩短。它们还可能被重新表述，以便更有可能让LLM使用上下文来生成答案。这是在*精炼*阶段完成的。
- en: Next, we provide the output of the refine step to the LLM. The default approach
    is to concatenate all the documents in the prompt. However, you could also pass
    them one at a time, and then ensemble the results. How the documents are ordered
    in the prompt can also make a difference. Several such techniques determine the
    way the context is fed to the LLM. This is called the *insert* stage.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将精炼步骤的输出提供给LLM。默认方法是将提示中的所有文档连接起来。然而，您也可以一次传递一个，然后汇总结果。提示中文档的顺序也可能产生影响。几种这样的技术决定了上下文如何被输入到LLM中。这被称为*插入*阶段。
- en: Finally, in the *generate* stage, the LLM reads the prompt containing the query
    and the context and generates the response. The generation can happen all at once
    or the retrieval process can be interleaved with the generation, i.e., the model
    can generate a few tokens, then call the retrieval model again to retrieve additional
    content, generate a few more tokens, and then call the retrieval model again,
    and so on.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*生成*阶段，LLM读取包含查询和上下文的提示并生成响应。生成可以一次性完成，或者检索过程可以与生成过程交织进行，即模型可以生成一些标记，然后再次调用检索模型来检索更多内容，生成更多标记，然后再次调用检索模型，依此类推。
- en: The output of each stage can be run through a *verify* stage to assess the quality
    of the outputs and even take corrective measures. The verify stage can employ
    either heuristics or AI-based methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每一阶段的输出都可以通过一个*验证*阶段来评估输出质量，甚至采取纠正措施。验证阶段可以采用启发式方法或基于AI的方法。
- en: In this example, the query was generated by a human user. But if we consider
    RAG in the context of agentic workflows, the query might be generated by an LLM.
    In an agentic workflow, the agent can determine at any given point that it needs
    to retrieve data to progress with its task, which sets the aforementioned pipeline
    into motion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，查询是由人类用户生成的。但如果我们将RAG放在代理工作流程的背景下考虑，查询可能是由一个LLM生成的。在代理工作流程中，代理可以在任何给定时刻确定它需要检索数据以推进其任务，这就会启动上述流程。
- en: Apart from the retrieve and generate steps, the rest of the pipeline is optional,
    and including other steps depends on your performance and latency tradeoffs.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了检索和生成步骤之外，其余的流程是可选的，是否包含其他步骤取决于你的性能和延迟权衡。
- en: Note
  id: totrans-51
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Our example pertains to RAG when used at inference time. RAG can also be applied
    when pre-training or fine-tuning the model, which we will describe later in the
    chapter.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的例子涉及在推理时间使用RAG。RAG也可以在预训练或微调模型时应用，我们将在本章后面进行描述。
- en: Let’s examine each step in the pipeline in detail.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细检查流程中的每一步。
- en: Rewrite
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重写
- en: After a query is issued, it might need to be rewritten to make it more amenable
    to retrieval. The rewriting process depends on the retrieval models used. As mentioned
    before, there is usually a mismatch between the query space and the document space,
    as the vocabulary, phrasing, and semantics used by the query might vary drastically
    from how the relevant concepts are conveyed in the document.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在发出查询后，可能需要对其进行重写以使其更易于检索。重写过程取决于所使用的检索模型。如前所述，查询空间和文档空间之间通常存在不匹配，因为查询所使用的词汇、措辞和语义可能与文档中传达的相关概念大相径庭。
- en: 'As an example, consider the query: “Which politicians have complained about
    the budget not being balanced?”'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下查询：“哪些政治家抱怨预算不平衡？”
- en: 'and the data store contains the text “Senator Paxton: ‘I just can’t stand the
    sight of our enormous deficit.’”'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储中包含以下文本：“参议员帕克斯顿：‘我实在无法忍受我们巨大的赤字。’”
- en: If you are using traditional retrieval approaches that rely more on keywords,
    this text may not be selected as relevant during retrieval. Using embedding-based
    methods bridges the gap as embeddings of similar sentences are closer to each
    other in embedding space, but it does not entirely solve the problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用依赖关键词的传统检索方法，那么在检索过程中，这段文本可能不会被选为相关内容。使用基于嵌入的方法可以缩小这一差距，因为相似句子的嵌入在嵌入空间中彼此更接近，但这并不能完全解决问题。
- en: Tip
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the query is coming from the user, the user might add instructions along
    with the query, like, “Which politicians have complained about the budget not
    being balanced? Provide the results in the form of a table.” In this case you
    will have to separate the query from the instructions before feeding the query
    into the retrieval pipeline. This can be done by an LLM using prompting techniques
    like CoT, ReAct, etc., which we discussed in Chapters [5](ch05.html#chapter_utilizing_llms)
    and [10](ch10.html#ch10), respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果查询来自用户，用户可能会在查询中添加指令，例如，“哪些政治家抱怨预算不平衡？请以表格的形式提供结果。”在这种情况下，你将不得不在将查询输入到检索流程之前将查询与指令分开。这可以通过LLM使用提示技术如CoT、ReAct等来完成，我们分别在第五章[5](ch05.html#chapter_utilizing_llms)和第十章[10](ch10.html#ch10)中进行了讨论。
- en: For systems using traditional retrieval techniques, query rewriting is typically
    performed using query expansion techniques, in which the query is augmented with
    similar keywords. Basic query expansion techniques include adding synonyms of
    keywords and other topic information in your query.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用传统检索技术的系统，查询重写通常是通过查询扩展技术来执行的，其中查询被添加了相似的关键词。基本的查询扩展技术包括在查询中添加关键词的同义词和其他主题信息。
- en: A well-tested method for query expansion is pseudo-relevance feedback (PRF).
    In PRF, the original query is used to retrieve documents, and salient terms from
    these documents are extracted and added to the original query.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 查询扩展的一个经过良好测试的方法是伪相关性反馈（PRF）。在PRF中，原始查询用于检索文档，并从这些文档中提取显著术语并添加到原始查询中。
- en: 'Let’s see how PRF would help with our query, ‘‘Which politicians have complained
    about the budget not being balanced?” We use a retrieval technique like BM25 (explained
    later in the chapter) to return a candidate set of k documents. We then use a
    technique like term frequency or, more effectively, [Tf-IDf](https://oreil.ly/5be9z)
    to extract the salient terms occurring in these returned documents. For this example
    the salient phrases turn out to be “fiscal policy,” “deficit,” “financial mismanagement,”
    and “budgetary reforms.” Adding these phrases to the original query will lead
    to the text:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看PRF如何帮助我们查询，“哪些政治家抱怨预算不平衡？”我们使用类似BM25（本章后面将解释）的检索技术来返回一组k个候选文档。然后我们使用诸如词频或更有效的方法[Tf-IDf](https://oreil.ly/5be9z)来提取这些返回文档中出现的显著术语。对于这个例子，显著的短语是“财政政策”、“赤字”、“财务管理不善”和“预算改革”。将这些短语添加到原始查询中，将导致以下文本：
- en: '“Senator Paxton: ‘I just can’t stand the sight of our enormous deficit!’” being
    retrieved successfully.'
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “参议员帕克斯顿：‘我简直无法忍受我们巨大的赤字！’”检索成功。
- en: In recent years, LLM-driven query expansion methods are gaining more prominence.
    Two such examples are [Query2doc](https://oreil.ly/BDJAN) and [Hypothetical Document
    Embeddings (HyDE)](https://oreil.ly/tZyUV).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，由LLM驱动的查询扩展方法越来越受到重视。两个这样的例子是[Query2doc](https://oreil.ly/BDJAN)和[假设文档嵌入（HyDE）](https://oreil.ly/tZyUV)。
- en: Let’s explore how LLM-based query expansion works. We pass a given query to
    an LLM and ask it to generate a hypothetical document that contains the answer
    to the query.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索基于LLM的查询扩展是如何工作的。我们将给定的查询传递给LLM，并要求它生成一个包含查询答案的假设文档。
- en: '*Prompt:* For the provided query, generate a hypothetical document in 2 sentences
    or less that answers the query. Assume the documents are transcripts of Canadian
    parliamentary proceedings, containing verbatim statements of representatives in
    the House and Senate.'
  id: totrans-67
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 对于提供的查询，用2句话或更少的文字生成一个假设文档来回答查询。假设文档是加拿大议会程序的记录，包含下议院和参议院的代表的直接陈述。'
- en: ''
  id: totrans-68
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Query:* Which politicians have complained about the budget not being balanced?'
  id: totrans-69
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*查询:* 哪些政治家抱怨预算不平衡？'
- en: 'The prompt contains:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 提示包含：
- en: Instructions to generate a hypothetical document
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成假设文档的说明
- en: Desired length limit of the generated document
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文档的期望长度限制
- en: Description of the document corpus containing relevant documents
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述包含相关文档的文档语料库
- en: The original query
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始查询
- en: 'The hypothetical document generated by the LLM is as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LLM生成的假设文档如下：
- en: '*LLM:* “Honorable Senator Mark Wellington stated, *This government’s failure
    to balance the budget is unacceptable and jeopardizes our economic future.* Similarly,
    MP Emily Fraser remarked, *We cannot continue on this path of reckless spending
    without addressing the deficit—it’s time for real fiscal responsibility.*”'
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*LLM:* “尊敬的马克·韦林顿参议员表示，*政府未能平衡预算是不可接受的，这危及我们的经济未来。*同样，埃米莉·弗莱塞议员评论说，*我们不能继续这种不负责任的支出之路而不解决赤字——是时候采取真正的财政责任了。*”'
- en: 'Using this hypothetical document as the query will return the sentence ‘‘Senator
    Paxton: ‘I just can’t stand the sight of our enormous deficit!”’ with a higher
    similarity score.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个假设文档作为查询将返回句子“参议员帕克斯顿：‘我简直无法忍受我们巨大的赤字！’”并具有更高的相似度分数。
- en: While this hypothetical document is not factually accurate, and laughably so
    (there are no senators or MPs in Canada named Mark Wellington and Emily Fraser),
    it does contain verbiage and semantics very similar to what an actual politician
    would say. If we use this document as the query (optionally combining with the
    original query), then the chances of it being semantically similar to actual instances
    of politicians talking about the topic is higher than if matched with the query
    alone.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个假设文档在事实上并不准确，甚至有点可笑（加拿大没有名叫马克·韦林顿和埃米莉·弗莱塞的参议员或议员），但它确实包含与实际政治家所说非常相似的措辞和语义。如果我们使用这个文档作为查询（可选地与原始查询结合），那么它与实际政治家谈论该主题的实例在语义上相似的可能性比仅与查询匹配时更高。
- en: The length of the hypothetical document could be similar to the typical length
    of the retrieval unit. You can use a smaller LLM to generate the hypothetical
    document, as we do not care for factuality guarantees in this setting. However,
    smaller models are also not as adept as generating quality hypothetical documents,
    so you will have to manage the tradeoff. Both LangChain and LlamaIndex provide
    implementations of hypothetical document-based query rewriting.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设文档的长度可能与检索单元的典型长度相似。您可以使用较小的LLM来生成假设文档，因为我们在这个设置中并不关心事实保证。然而，较小的模型在生成高质量的假设文档方面也不够熟练，因此您将不得不管理这种权衡。LangChain和LlamaIndex都提供了基于假设文档的查询重写的实现。
- en: If the model has been pre-trained or fine-tuned on the data corpus containing
    the relevant data, then adding descriptions of the corpus in the prompt as shown
    in the example will make it more likely that the generated document follows the
    structure, format, and linguistics of that data corpus.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型已经在包含相关数据的语料库上预训练或微调，那么在提示中添加语料库的描述，如示例所示，将更有可能使生成的文档遵循该数据语料库的结构、格式和语言学。
- en: Warning
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: One pitfall of query rewriting techniques is the risk of topic drift. In the
    case of hypothetical documents, the document may drift into irrelevant topics
    after the first few tokens. Upweighting the logits bias for tokens in the query
    can partially address this problem. PRF techniques are also susceptible to topic
    drift.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重写技术的另一个陷阱是主题漂移的风险。在假设文档的情况下，文档可能在最初的几个标记之后漂移到不相关的主题。增加查询中标记的logits偏差权重可以部分解决这个问题。PRF技术也容易受到主题漂移的影响。
- en: You can also combine PRF style techniques with hypothetical documents. Instead
    of generating hypothetical documents to replace or augment the query, you can
    use them to extract keywords that you can add to the original query. [Li et al.](https://oreil.ly/cOnMs)
    propose a technique called *query2document2keyword*. In this technique, the LLM
    generates a hypothetical document using the query, similar to HyDE. The LLM is
    then prompted to extract salient keywords from this document.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以将PRF风格技术与假设文档相结合。不是生成假设文档来替换或增强查询，而是可以使用它们来提取可以添加到原始查询中的关键词。[Li等人](https://oreil.ly/cOnMs)提出了一种称为*query2document2keyword*的技术。在这种技术中，LLM使用查询生成一个假设文档，类似于HyDE。然后，LLM被提示从这个文档中提取显著关键词。
- en: We can then further improve the quality of the extracted keywords by taking
    them through a filtering step. The authors propose using the *self-consistency*
    method, which we discussed in [Chapter 5](ch05.html#chapter_utilizing_llms). To
    recap, in the self-consistency method, we repeat the keyword generation multiple
    times, and then select the top keywords based on the number of generations they
    are present in.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过过滤步骤进一步改进提取出的关键词的质量。作者提出了使用*自洽性*方法，我们已在[第5章](ch05.html#chapter_utilizing_llms)中讨论过。概括来说，在自洽性方法中，我们多次重复关键词生成，然后根据它们出现的生成次数选择顶级关键词。
- en: Another way to combine traditional retrieval with LLM-driven query rewriting
    is to first return the top-k documents from the initial retrieval step, then use
    LLMs to generate salient keywords from the returned documents and add them to
    the query.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 将传统检索与LLM驱动的查询重写相结合的另一种方法是首先从初始检索步骤返回顶级-k文档，然后使用LLM从返回的文档中生成显著关键词并将其添加到查询中。
- en: So far we have discussed techniques that bridge the query document mismatch
    problem by modifying the query and bringing it closer to the document space. An
    alternative approach to solve the mismatch problem is to represent the documents
    in a way that brings them closer to the query space. Examples of this approach
    include [doc2query](https://oreil.ly/CGUtP) and [contextual retrieval](https://oreil.ly/ZJuIu).
    While document rewriting techniques initially have a large cost if the data stores
    are very large, they can reduce latency during inference time as no or little
    query rewriting needs to be performed. On the other hand, query rewriting techniques
    are simple to implement and integrate into a RAG workflow.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了通过修改查询并将其带到文档空间附近来桥接查询文档不匹配问题的技术。解决不匹配问题的另一种方法是按照使文档更接近查询空间的方式表示文档。这种方法包括[doc2query](https://oreil.ly/CGUtP)和[上下文检索](https://oreil.ly/ZJuIu)。虽然如果数据存储非常大，文档重写技术最初可能会有很大的成本，但它们可以在推理时间减少延迟，因为不需要或只需要很少的查询重写。另一方面，查询重写技术易于实现并集成到RAG工作流程中。
- en: Yet another form of query rewriting is called query decomposition. For complex
    queries in an agentic workflow, we can have the LLM divide the task into multiple
    queries that can be executed sequentially or in parallel, depending on how the
    query was decomposed. We discussed query decomposition techniques in [Chapter 10](ch10.html#ch10).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种查询重写的形式称为查询分解。对于代理工作流程中的复杂查询，我们可以让LLM将任务分解成多个可以顺序或并行执行的查询，具体取决于查询是如何分解的。我们在第10章中讨论了查询分解技术。
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If your external data is in a structured form like databases, then the query
    needs to be rewritten into a SQL query or equivalent, as discussed in [Chapter 10](ch10.html#ch10).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的外部数据以结构化形式存在，如数据库，那么查询需要被重写为一个SQL查询或等效的查询，正如在第10章中讨论的那样。
- en: Now that we have discussed the query rewriting step of the pipeline, let’s move
    on to the retrieve step.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了管道中的查询重写步骤，让我们继续讨论检索步骤。
- en: Retrieve
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检索
- en: 'The retrieve step is the most crucial stage of the RAG pipeline. It is easy
    to see why: all RAG applications are bottlenecked by the quality of retrieval.
    Even if you are working with the world’s best language model, you won’t be able
    to get the correct results if the retrieval step didn’t retrieve the correct documents
    needed to answer the query. Therefore, this step of the pipeline should focus
    on increasing recall.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 检索步骤是RAG管道中最关键的阶段。为什么这样说很容易理解：所有RAG应用都受检索质量的限制。即使你使用的是世界上最好的语言模型，如果检索步骤没有检索到回答查询所需的正确文档，你也不会得到正确的结果。因此，这个管道步骤应专注于提高召回率。
- en: Embedding-based retrieval, which we discussed in detail in [Chapter 11](ch11.html#chapter_llm_interfaces),
    is highly popular. However, traditional information-retrieval techniques should
    not be dismissed. The right technique to use depends on the expected nature of
    queries (can a significant proportion of them be answered by just keyword or regex
    match?), the expected degree of query-document vocabulary mismatch, latency and
    compute limitations, and performance requirements.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第11章中详细讨论的基于嵌入的检索非常受欢迎。然而，传统的信息检索技术不应被忽视。使用哪种技术取决于查询预期的性质（是否可以通过仅关键词或正则表达式匹配来回答其中很大一部分？），查询与文档词汇不匹配的预期程度，延迟和计算限制，以及性能要求。
- en: Note
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The information retrieval (IR) research field has been studying these problems
    for a long time. Now that retrieval is more relevant than ever in NLP, I am noticing
    a lot of efforts to reinvent the wheel rather than reusing IR insights. For insights
    in retrieval research, check out papers from leading IR research conferences like
    SIGIR, ECIR, TREC, etc.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索（IR）研究领域已经研究了这些问题很长时间。现在，随着检索在NLP中的相关性日益增强，我注意到很多人在重新发明轮子而不是重用IR见解。关于检索研究的见解，可以查看SIGIR、ECIR、TREC等领先IR研究会议的论文。
- en: Embedding-based retrieval methods are not always suitable when you would like
    all documents containing a specific word or phrase to be retrieved. Therefore
    it is customary to combine keyword-based methods with embedding methods, called
    hybrid search. The results from the two methods are combined and fed to the next
    step of the retrieval pipeline. Most vector databases support hybrid search in
    some shape or form.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当你希望检索包含特定单词或短语的全部文档时，基于嵌入的检索方法并不总是合适的。因此，通常将基于关键词的方法与嵌入方法相结合，称为混合搜索。两种方法的结果被合并并输入到检索管道的下一步。大多数向量数据库以某种形式支持混合搜索。
- en: '[Figure 12-2](#hybrid-search) shows the retrieval stage in action, using hybrid
    search.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-2](#hybrid-search)展示了使用混合搜索的检索阶段，展示了其作用。'
- en: '![Hybrid-Search](assets/dllm_1202.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![混合-搜索](assets/dllm_1202.png)'
- en: Figure 12-2\. Hybrid search
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. 混合搜索
- en: I also highly recommend metadata filters for improving retrieval. The more metadata
    you gather during the data representation and storage phase, the better the retrieval
    results. For example, if you have performed topic modeling of your data store
    in advance, you can restrict your search results to a subset of topics, with the
    filters being applied either using a hardcoded set of rules or determined by an
    LLM.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我还强烈推荐使用元数据过滤器来提高检索效果。在数据表示和存储阶段收集的元数据越多，检索结果就越好。例如，如果你已经提前对你的数据存储进行了主题建模，你可以通过使用硬编码的规则集或由一个LLM确定来限制你的搜索结果只包含特定主题的子集。
- en: Next, let’s discuss promising recent advances in retrieval.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论检索方面的有希望的近期进展。
- en: Generative retrieval
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成式检索
- en: What if the LLM could identify the right documents(s) that need to be retrieved
    in response to a query, thus removing the need for retrieval techniques? This
    is called generative retrieval.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果LLM能够识别出需要检索的正确的文档（s），从而消除检索技术的需求，这被称为生成检索。
- en: 'Generative retrieval is implemented by assigning identifiers to documents called
    docIDs, and teaching the LLM the association between documents and docIDs. A document
    can be associated with one or more docIDs. Typical docIDs can be:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 生成检索通过为文档分配称为docID的标识符，并教授LLM文档与docID之间的关联来实现。一个文档可以与一个或多个docID相关联。典型的docID可以是：
- en: Single tokens
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 单个标记
- en: Each document can be represented by a new token in the vocabulary. This means
    that, during inference, the model needs to output only a single token for each
    document it wants to retrieve. [Pradeep et al.](https://oreil.ly/7JYOM) use a
    T5 model where the encoder vocabulary is the standard T5 vocabulary but the decoder
    vocabulary contains the docIDs. This approach is feasible only with a small document
    corpus.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档都可以由词汇表中的一个新标记表示。这意味着，在推理过程中，模型只需要为它想要检索的每个文档输出一个标记。[Pradeep等人](https://oreil.ly/7JYOM)使用了一个T5模型，其中编码器词汇表是标准的T5词汇表，但解码器词汇表包含docID。这种方法仅适用于小型文档语料库。
- en: Prefix/subset tokens
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀/子集标记
- en: '[Tay et al.](https://oreil.ly/1p1C8) use the first 64 tokens of a document
    as the docID, while [Wang et al.](https://oreil.ly/lg3g3) use 64 randomly selected
    contiguous tokens from the document.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[Tay等人](https://oreil.ly/1p1C8)使用文档的前64个标记作为docID，而[Wang等人](https://oreil.ly/lg3g3)使用从文档中随机选择的64个连续标记。'
- en: Cluster tokens
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类标记
- en: You can also perform hierarchical clustering of your document corpus based on
    its semantics (using embeddings, for example), and the docID can be a concatenation
    of the cluster IDs at each level of the hierarchy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以根据其语义（例如使用嵌入）对文档语料库进行分层聚类，docID 可以是层次结构中每个级别的聚类ID的连接。
- en: Salient keyword tokens
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 显著关键词标记
- en: The docIDs can also contain salient keywords representing the topics and themes
    contained in the document. For example, a document about the Transformer architecture
    can be represented by the docID “transformer_self-attention_architecture.”
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: docID也可以包含代表文档中主题和主题的显著关键词。例如，关于Transformer架构的文档可以用docID“transformer_self-attention_architecture”表示。
- en: One way to teach the LLM the association between documents and docIDs is by
    fine-tuning the model. This is referred to as training-based indexing. However,
    fine-tuning needs a lot of resources and is not suitable in scenarios in which
    new documents are frequently added to the corpus.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 教授LLM文档与docID之间关联的一种方法是通过微调模型。这被称为基于训练的索引。然而，微调需要大量资源，并且不适用于频繁向语料库添加新文档的场景。
- en: '[Askari et al.](https://oreil.ly/K5TAB) show that we can use few-shot learning
    to build a generative retrieval system without needing to train the model. First,
    for each document in the corpus, pseudo queries are generated using a language
    model. The pseudo queries are the queries whose answers are present in the document.
    These pseudo queries are then fed to a language model in a few-shot setting and
    asked to generate docIDs. [Figure 12-3](#generative-retrieval) shows training-free
    generative retrieval in action.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[Askari等人](https://oreil.ly/K5TAB)表明我们可以使用少量样本学习来构建一个不需要训练模型的生成检索系统。首先，对于语料库中的每个文档，使用语言模型生成伪查询。这些伪查询是答案存在于文档中的查询。然后，将这些伪查询在少量样本设置中输入到语言模型，并要求生成docID。[图12-3](#generative-retrieval)显示了无训练的生成检索的实际应用。'
- en: '![Generative-Retrieval](assets/dllm_1203.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![生成-检索](assets/dllm_1203.png)'
- en: Figure 12-3\. Generative retrieval
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3. 生成检索
- en: During inference, the model is provided with a query similar to the setup in
    [Figure 12-3](#generative-retrieval) and asked to generate the correct docID(s)
    that are relevant to the query. Constrained beam search is used to ensure that
    the docID generated by the model corresponds to a valid docID in the corpus.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，模型被提供与[图12-3](#generative-retrieval)中设置类似的查询，并被要求生成与查询相关的正确docID（s）。使用约束束搜索以确保模型生成的docID对应于语料库中的有效docID。
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also use generative retrieval to retrieve documents based on their metadata.
    For example, the model could ask to retrieve Apple’s 2024 annual report. The model
    can be made to generate the right identifier by either fine-tuning the model or
    using few-shot learning, as shown in this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用生成式检索根据文档的元数据检索文档。例如，模型可以要求检索苹果公司的2024年年度报告。模型可以通过微调模型或使用本节中展示的少样本学习来生成正确的标识符。
- en: Ultimately, generative retrieval is suitable only if your document corpus is
    relatively small, there is limited redundancy within the corpus, or the documents
    belong to a set of well-defined categories (annual reports of all public companies
    in the US, for instance).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，生成式检索仅适用于您的文档语料库相对较小、语料库内冗余有限或文档属于一组定义良好的类别（例如，美国所有上市公司的年度报告）的情况。
- en: Next, let’s discuss tightly-coupled retrievers, another new topic in the retrieval
    space.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论紧耦合检索器，这是检索空间中的另一个新话题。
- en: Tightly-coupled retrievers
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 紧耦合检索器
- en: As seen in [Chapter 11](ch11.html#chapter_llm_interfaces), in embedding-based
    retrieval, the embedding model is typically independent of the language model
    to which the retrieval results are fed. We will refer to them as *loosely-coupled*
    retrievers.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如第11章[第11章](ch11.html#chapter_llm_interfaces)中所示，在基于嵌入的检索中，嵌入模型通常独立于检索结果输入的语言模型。我们将它们称为*松耦合*检索器。
- en: In contrast, a *tightly-coupled* retriever is trained such that it learns from
    LLM feedback; the model learns to retrieve text that best positions the LLM to
    generate the correct output for a given query. Tightly-coupled retrievers can
    be trained together with the generator LLM as part of a single architecture, or
    they can be trained separately using feedback from the trained LLM.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一个*紧耦合*检索器被训练成从LLM反馈中学习；模型学会检索文本，以最佳方式定位LLM生成给定查询的正确输出。紧耦合检索器可以作为单一架构的一部分与生成LLM一起训练，或者它们可以单独使用训练好的LLM的反馈进行训练。
- en: 'An example of the latter is [Zhang et al.’s LLM-Embedder](https://oreil.ly/Q__8M),
    a unified embedding model that can support a variety of retrieval needs in a single
    model, ranging from knowledge retrieval to retrieving optimal few-shot examples.
    The model is trained from two types of signals: a contrastive learning setup typically
    used to train embedding models (presented in [Chapter 11](ch11.html#chapter_llm_interfaces))
    and LLM feedback. A retrieval candidate receives a larger reward from LLM feedback
    if it improves the performance of the LLM in answering the query.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 后者的一个例子是[张等人提出的LLM-Embedder](https://oreil.ly/Q__8M)，这是一个统一的嵌入模型，可以在单个模型中支持各种检索需求，从知识检索到检索最优的少样本示例。该模型从两种类型的信号中进行训练：一种对比学习设置，通常用于训练嵌入模型（在第11章[第11章](ch11.html#chapter_llm_interfaces)中介绍）和LLM反馈。如果一个检索候选者通过改进LLM回答查询的性能而获得更大的奖励，那么它将从LLM反馈中获得更大的奖励。
- en: Tightly-coupled retrievers are another tool in your toolkit for improving retrieval.
    They are by no means a necessary step in the RAG pipeline. As always, experimentation
    will show how much of a lift (if any) they provide for your application.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 紧耦合检索器是您工具箱中用于改进检索的另一个工具。它们绝不是RAG管道中的必要步骤。一如既往，实验将显示它们为您的应用提供了多少提升（如果有的话）。
- en: Finally, let’s discuss GraphRAG, an up-and-coming retrieval paradigm that leverages
    knowledge graphs for better retrieval.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论GraphRAG，这是一种新兴的检索范式，它利用知识图谱来提高检索效果。
- en: GraphRAG
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GraphRAG
- en: A key limitation of the retrieval approaches we have discussed so far is their
    inability to facilitate answering questions that require drawing connections between
    different parts of the document corpus, as well as questions that involve summarizing
    high-level themes across the dataset. For example, all the retrieval techniques
    we discussed so far would do poorly on a query like, “What are the key topics
    discussed in this dataset?”
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论的检索方法的一个关键限制是它们无法促进回答需要在不同部分之间建立联系的问题，以及涉及在数据集上总结高级主题的问题。例如，我们之前讨论的所有检索技术在对“在这个数据集中讨论的关键主题是什么？”这样的查询上表现不佳。
- en: One way to address these limitations is by employing knowledge graphs. Microsoft
    released [GraphRAG](https://oreil.ly/V4n_S), a graph-based RAG system. GraphRAG
    works by creating a knowledge graph from the underlying data corpus by extraction
    entities and relationships. The graph is then used to perform hierarchical semantic
    clustering, with summaries generated for each cluster. These summaries enable
    answering of thematic questions like, “What are the key topics discussed in this
    dataset?”
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这些限制的一种方法是通过使用知识图谱。微软发布了 [GraphRAG](https://oreil.ly/V4n_S)，这是一个基于图的 RAG 系统。GraphRAG
    通过从底层数据语料库中提取实体和关系来创建知识图谱。然后，该图被用于执行层次语义聚类，并为每个聚类生成摘要。这些摘要使得回答诸如“在这个数据集中讨论的关键主题是什么？”之类的主题性问题成为可能。
- en: GraphRAG requires a lot of initial compute to prepare the knowledge graph. This
    can be prohibitive for larger datasets. While it is easy to extract entities,
    extracting relevant relationships is harder.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: GraphRAG 需要大量的初始计算来准备知识图谱。这对于较大的数据集来说可能是限制性的。虽然提取实体相对容易，但提取相关关系则更为困难。
- en: Now that we have explored the retrieval stage of the RAG pipeline, let’s move
    on to the rerank stage.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了 RAG 流程中的检索阶段，接下来让我们转向重排序阶段。
- en: Rerank
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重排序
- en: The retrieval process can be broken into a two-stage or multi-stage process,
    where the initial stage retrieves a list of documents relevant to the query, followed
    by one or more *reranking* stages that take the documents and sort them by relevance.
    The reranker is generally a more complex model than the retriever and thus is
    run only on the retrieved results (or else we would have just used the reranker
    as the retriever).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 检索过程可以被分解为两阶段或多阶段过程，其中初始阶段检索与查询相关的文档列表，随后是一或多个 *重排序* 阶段，这些阶段将文档按相关性排序。重排序器通常比检索器更复杂，因此仅在检索结果上运行（否则我们就会直接使用重排序器作为检索器）。
- en: The reranker is usually a language model fine-tuned on the specific use case.
    You can use BERT-like models for building a relevance classifier, where given
    a query and a document, the model outputs the probability of the document being
    relevant to answering the query. These models are called *cross-encoders*, as
    in these models the query and document are encoded together, as opposed to embedding-based
    retrieval models we have discussed, called bi-encoders, where the query and document
    are encoded as separate vectors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 重排序器通常是在特定用例上微调的语言模型。您可以使用类似 BERT 的模型来构建相关性分类器，其中给定一个查询和一个文档，模型输出文档与回答查询的相关概率。这些模型被称为
    *跨编码器*，在这些模型中，查询和文档一起编码，与我们所讨论的基于嵌入的检索模型（称为双编码器）不同，在双编码器中，查询和文档被编码为单独的向量。
- en: 'The input for a BERT model acting as a cross-encoder is of the format:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 作为跨编码器作用的 BERT 模型的输入格式为：
- en: '[PRE0]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The Sentence Transformers library provides access to cross-encoders, which
    can be used as rerankers in the RAG pipeline:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: Sentence Transformers 库提供了访问跨编码器的方法，这些跨编码器可以用作 RAG 流程中的重排序器：
- en: '[PRE1]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Because we have set `num_labels = 1`, the model will treat it as a regression
    task, using the sigmoid activation function to output a score between 0 and 1.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已将 `num_labels` 设置为 `1`，该模型将将其视为回归任务，使用 sigmoid 激活函数输出介于 0 和 1 之间的分数。
- en: These days, more advanced models like [Contextualized Late Interaction over
    BERT (ColBERT)](https://oreil.ly/N3fOv) are used for reranking. As opposed to
    the cross-encoder setup we just discussed, ColBERT-style models allow for pre-computation
    of document representations, leading to faster inference.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，更先进的模型如 [基于 BERT 的上下文晚期交互 (ColBERT)](https://oreil.ly/N3fOv) 被用于重排序。与刚才讨论的跨编码器设置不同，ColBERT
    风格的模型允许预先计算文档表示，从而实现更快的推理。
- en: In ColBERT, the query and documents are encoded separately using BERT, generating
    token-level embedding vectors for each token in the query and documents. For each
    token in the query, the corresponding embedding is compared to the embeddings
    of each of the token embeddings of the document, generating similarity scores.
    The maximum similarity scores for each query token are summed, resulting in the
    final relevance score. This type of architecture is called *late interaction*,
    since the query and document are not encoded together but interact together only
    later in the process. Late interaction saves time compared to traditional cross-encoders,
    as document embeddings can be created and stored in advance.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在ColBERT中，查询和文档分别使用BERT进行编码，为查询和文档中的每个标记生成标记级嵌入向量。对于查询中的每个标记，相应的嵌入与文档中每个标记嵌入的嵌入进行比较，生成相似度分数。每个查询标记的最大相似度分数相加，得到最终的关联度分数。这种架构称为*后期交互*，因为查询和文档不是一起编码，而是在处理过程中的后期才进行交互。与传统的交叉编码器相比，后期交互可以节省时间，因为文档嵌入可以预先创建和存储。
- en: '[Figure 12-4](#cross-encoders) depicts a ColBERT model in action, illustrating
    the late interaction between query and documents.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-4](#cross-encoders)展示了ColBERT模型在运行中的情况，说明了查询和文档之间的后期交互。'
- en: '![cross-encodersl](assets/dllm_1204.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![cross-encodersl](assets/dllm_1204.png)'
- en: Figure 12-4\. ColBERT
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. ColBERT
- en: Next, let’s explore a few advanced reranking techniques.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们探索一些高级的重排序技术。
- en: Query likelihood model (QLM)
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询似然模型（QLM）
- en: A QLM estimates the probability of generating the query given a candidate document
    as input. You can treat an LLM as a QLM, utilizing its zero-shot capabilities
    to rank candidate documents based on the query token probabilities. Alternatively,
    you can fine-tune an LLM on query generation tasks to improve its suitability
    as a QLM.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: QLM（查询语言模型）估计给定候选文档作为输入生成查询的概率。你可以将一个LLM（大型语言模型）视为一个QLM，利用其零样本能力根据查询标记的概率对候选文档进行排序。或者，你可以对LLM进行微调，以查询生成任务来提高其作为QLM的适用性。
- en: 'A typical prompt for a QLM would look like: “Generate a question that is most
    relevant to the given document <document content>”.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的QLM提示可能如下所示：“生成一个与给定文档<文档内容>最相关的疑问”。
- en: After getting the top-k documents relevant to a query from the retrieval stage,
    each document is fed to the LLM with this prompt. The likelihood of the query
    tokens is then calculated using the model logits. The documents are then sorted
    by likelihood, providing a relevance ranking.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索阶段从查询中获取与查询相关的top-k文档后，每个文档都使用此提示输入到LLM中。然后使用模型的对数几率计算查询标记的似然性。然后根据似然性对文档进行排序，提供相关性排名。
- en: Warning
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: '[Zhuang et al.](https://oreil.ly/QnWWh) show that an instruction-tuned model
    that doesn’t contain query generation tasks in its instruction-tuning training
    set loses its capability to be an effective zero-shot QLM. This is yet another
    case of instruction-tuned models exhibiting degraded performance compared to base
    models, on tasks they have not been trained on.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[庄等人](https://oreil.ly/QnWWh)表明，在其指令微调训练集中不包含查询生成任务的指令微调模型会失去成为有效零样本QLM的能力。这是指令微调模型在未训练的任务上与基模型相比性能下降的又一案例。'
- en: Note that to calculate the probability of the query tokens, we need access to
    the model logits. Most proprietary model providers including OpenAI do not yet
    provide full access to the model logits as of this book’s writing. Thus, the LLM-as-a-QLM
    approach can be implemented only using open source models.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了计算查询标记的概率，我们需要访问模型的对数几率。截至本书编写时，大多数专有模型提供商，包括OpenAI，尚未提供对模型对数几率的完全访问。因此，LLM作为QLM的方法只能使用开源模型来实现。
- en: In the interest of reducing latency, you would ideally like the QLM to be as
    small a model as possible. However, smaller models are less effective QLMs. Effectively
    fine-tuning a smaller LLM for query generation might be the sweet spot.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少延迟，你理想情况下希望QLM尽可能小。然而，较小的模型作为QLM的效果较差。有效地微调较小的LLM以进行查询生成可能是最佳选择。
- en: LLM distillation for ranking
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLM蒸馏用于排序
- en: 'Earlier in the chapter, we saw how encoder-only models like BERT could serve
    as rerankers. More recently, decoder LLMs are also being trained to directly rank
    candidate documents in three ways:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期，我们看到了仅编码器模型如BERT可以作为重排序器。最近，解码器LLM也被训练以三种方式直接对候选文档进行排序：
- en: Pointwise ranking
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 点对点排序
- en: Each candidate document is fed separately to the LLM. The LLM provides a Boolean
    judgment on its relevance. Alternatively, it can also provide a numerical score,
    although this is much less reliable.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 每个候选文档单独输入到大型语言模型（LLM）中。LLM 提供对其相关性的布尔判断。或者，它也可以提供一个数值分数，尽管这不太可靠。
- en: Pairwise ranking
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 成对排名
- en: For each candidate document pair, the LLM indicates which document is more relevant.
    To get a complete ranking, N² such comparisons need to be made.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一对候选文档，LLM 指示哪篇文档更相关。为了得到完整的排名，需要进行 N² 次这样的比较。
- en: Listwise ranking
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表排名
- en: All the candidate documents are tagged with identifiers and fed to the LLM,
    and the LLM is asked to generate a ranked list of identifiers according to decreasing
    order of relevance of corresponding documents.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 所有候选文档都贴上标识符并输入到 LLM 中，并要求 LLM 根据对应文档的相关性递减顺序生成一个排序的标识符列表。
- en: In general, pointwise ranking is the easiest to use but may not be the [most
    effective](https://oreil.ly/DvmtC). Listwise ranking might need a large context
    window, while pairwise ranking needs lots of comparisons. Pairwise ranking is
    the most effective of these techniques, since it involves direct comparison. [Figure 12-5](#llm-rerankers)
    shows how pointwise, pairwise, and listwise rankings work.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，点对排名是最容易使用的，但可能不是[最有效的](https://oreil.ly/DvmtC)。列表排名可能需要一个大的上下文窗口，而成对排名需要大量的比较。成对排名是这些技术中最有效的，因为它涉及直接比较。[图
    12-5](#llm-rerankers) 展示了点对、成对和列表排名是如何工作的。
- en: Examples of ranking LLMs include [RankGPT](https://oreil.ly/6XoOG), [RankVicuna](https://oreil.ly/00Dan),
    and [RankZephyr](https://oreil.ly/AAbUE).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 排名 LLM 的例子包括 [RankGPT](https://oreil.ly/6XoOG)、[RankVicuna](https://oreil.ly/00Dan)
    和 [RankZephyr](https://oreil.ly/AAbUE)。
- en: 'These models are trained by distilling from larger LLMs, a technique we first
    learned in [Chapter 9](ch09.html#ch09). For example, the process for training
    RankVicuna is:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型是通过从更大的 LLM 中提取知识来训练的，这是我们首次在[第 9 章](ch09.html#ch09)中学习的。例如，训练 RankVicuna
    的过程如下：
- en: Queries in the training set are fed through a first-level retriever like BM25
    to generate a list of candidate documents.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集中的查询通过一级检索器（如 BM25）生成候选文档列表。
- en: This list is passed to a larger LLM, which generates a rank-ordered list of
    candidates.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个列表被传递给一个更大的 LLM，该 LLM 生成一个按候选者排序的列表。
- en: The query and the rank-ordered list are used to fine-tune the smaller LLM.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询和排序列表用于微调较小的 LLM。
- en: The creators of [RankVicuna](https://oreil.ly/cFLSc) show that as the effectiveness
    of the first-level retrieval increases, the possible performance gains from RankVicuna
    decreases due to diminished returns. They also reported that augmenting the dataset
    by shuffling the input order of the candidate documents improved model performance.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[RankVicuna](https://oreil.ly/cFLSc) 的创造者表明，随着一级检索的有效性提高，RankVicuna 可能的性能提升因回报递减而降低。他们还报告说，通过打乱候选文档的输入顺序来扩充数据集可以提升模型性能。'
- en: '![llm-rerankers](assets/dllm_1205.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![llm-rerankers](assets/dllm_1205.png)'
- en: Figure 12-5\. Decoder LLM rerankers
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-5\. 解码器 LLM 重新排序器
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can combine the results of the retrieve and the rerank stages to get the
    final relevance ranking of candidate documents. This is needed to enforce keyword
    weighting, for example. You can also weight your relevance ranking by metadata
    like published date (more recent documents are weighted more).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将检索和重新排序阶段的结果结合起来，以获得候选文档的最终相关性排名。这需要执行关键字加权，例如。你也可以通过元数据（如发布日期）来加权你的相关性排名（更近期的文档权重更高）。
- en: Now that we have discussed the rerank stage, let’s move on to the refine step
    of the RAG pipeline.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了重新排序阶段，让我们继续到 RAG 管道的精细调整步骤。
- en: Refine
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精细调整
- en: Once the candidate texts relevant to the given query are retrieved and selected,
    they can be fed to the LLM. However, the LLM context window is limited, so we
    might want to reduce the length of the retrieved texts. We might also want to
    rephrase it so that it is more amenable to being processed by the LLM. Another
    possible operation could be to filter out some of the retrieved texts based on
    certain rules. All of these are conducted during the *refine* stage. In this section,
    we will discuss two such techniques, summarization and chain-of-note. Let’s start
    with discussing how we can summarize the retrieved texts.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦检索并选择了与给定查询相关的候选文本，它们可以被输入到LLM中。然而，LLM的上下文窗口是有限的，所以我们可能想要缩短检索到的文本的长度。我们可能还想重新措辞，使其更适合LLM处理。另一个可能的操作是根据某些规则过滤掉一些检索到的文本。所有这些都是在*精炼*阶段进行的。在本节中，我们将讨论两种这样的技术，即摘要化和笔记链。让我们先讨论如何对检索到的文本进行摘要化。
- en: Tip
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The refine stage can be a standalone stage, or it can be paired with the final
    generate stage, where the final response is provided immediately after refining
    the retrieved documents, as part of the same prompt or prompt chain.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 精炼阶段可以是一个独立的阶段，也可以与最终的生成阶段配对，在精炼检索到的文档后立即提供最终响应，作为同一提示或提示链的一部分。
- en: Summarization
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要化
- en: Summarization is useful if the retrieval chunks are relatively large. It can
    be either extractive or abstractive. Extractive summaries extract key sentences
    from the original text without modifying it. Abstractive summaries are generated
    from scratch, drawing on content from the original text. The summarizer can also
    act as a quality filter; it can output an empty summary if the document is irrelevant
    to the query. Summaries should be relevant, concise, and faithful to the original
    text.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检索到的片段相对较大，摘要化是有用的。它可以是提取式或抽象式。提取式摘要从原始文本中提取关键句子，而不对其进行修改。抽象式摘要从头开始生成，借鉴原始文本的内容。摘要器还可以作为质量过滤器；如果文档与查询不相关，它可以输出一个空摘要。摘要应该是相关的、简洁的，并且忠实于原始文本。
- en: Note
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These summaries are not meant for human consumption but instead meant to be
    consumed by the LLM. Therefore, they do not always share the same objectives as
    traditional summarizers. The primary objective here is to generate a summary that
    helps the LLM output the correct answer.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些摘要不是为了人类消费，而是为了被LLM消费。因此，它们不一定与传统的摘要器具有相同的目标。这里的主要目标是生成一个帮助LLM输出正确答案的摘要。
- en: Should you choose extractive or abstractive summarization? Extractive summaries
    are almost always faithful as they preserve the meaning of the original text.
    Abstractive summaries come with the risk of hallucinations. On the other hand,
    abstractive summaries can potentially be more relevant because of their ability
    to combine information from different locations within a document and across documents.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该选择提取式还是抽象式摘要？提取式摘要几乎总是忠实于原文，因为它保留了原文的意义。抽象式摘要存在幻觉的风险。另一方面，由于它们能够结合文档内部和跨文档不同位置的信息，抽象式摘要可能更具相关性。
- en: While you can leverage the LLM’s zero-shot capabilities for both extractive
    and abstractive summarization, it is more effective (albeit expensive) to fine-tune
    them so that the summaries generated are specifically optimized to enable the
    LLM to generate the correct answer. We will call these tightly-coupled summarizers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以利用LLM的零样本能力进行提取式和抽象式摘要，但微调它们以使生成的摘要专门优化以使LLM能够生成正确答案会更有效（尽管成本较高）。我们将这些紧密耦合的摘要器称为。
- en: '[Xu et al.](https://oreil.ly/XCpyr) introduce techniques for training both
    extractive and abstractive summarizers. Let’s go through them in detail.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[Xu 等人](https://oreil.ly/XCpyr)介绍了训练提取式和抽象式摘要器的技术。让我们详细地了解一下。'
- en: For extractive summarization, we would like to extract a subset of sentences
    from the retrieved document as its summary. This is done by generating embeddings
    for the input query and for each sentence in the retrieved document. The top-k
    sentences that are most similar to the input query in the embedding space are
    selected as the summary. The embedding distance is a measure of how effective
    the document sentence is in enabling the LLM to generate the correct output.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提取式摘要，我们希望从检索到的文档中提取一个子句集作为其摘要。这是通过为输入查询和检索文档中的每个句子生成嵌入来完成的。在嵌入空间中最相似于输入查询的前k个句子被选为摘要。嵌入距离是衡量文档句子在使LLM生成正确输出方面有效性的度量。
- en: 'The extractive summarizer is trained with contrastive learning, which we discussed
    in [Chapter 11](ch11.html#chapter_llm_interfaces). Each training example in contrastive
    learning is a triplet: the anchor sentence, positive example similar to the anchor
    sentence, and negative examples dissimilar to the anchor sentence. To generate
    the training examples, for each sentence in the retrieved document, we prefix
    it to the input query and calculate the likelihood of gold truth output tokens
    being generated. The sentence with the highest likelihood is taken as the positive
    example. For negative examples, we choose up to five sentences whose likelihood
    is below a threshold. This dataset is then used to train the model.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 提取式摘要器是通过对比学习进行训练的，这一点我们在[第11章](ch11.html#chapter_llm_interfaces)中讨论过。在对比学习中，每个训练示例都是一个三元组：锚文本、与锚文本相似的正面示例，以及与锚文本不相似的负面示例。为了生成训练示例，对于检索到的文档中的每一句话，我们在输入查询前加上这句话，并计算生成黄金真实输出标记的概率。概率最高的句子被选为正面示例。对于负面示例，我们选择最多五个概率低于阈值的句子。这个数据集随后被用来训练模型。
- en: For abstractive summarization, we can distill a larger LLM, i.e., use the outputs
    from it to fine-tune a smaller LLM.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于抽象式摘要，我们可以提炼一个更大的LLM，即使用它的输出微调一个较小的LLM。
- en: To generate the training dataset, we can construct some prompt templates and
    use them with a larger LLM to generate zero-shot summaries of our retrieved documents.
    Note that we are generating a single summary of all the retrieved documents. Similar
    to the extractive summarization technique, for each generated summary, we prefix
    it to the input text and calculate the likelihood of the correct output tokens.
    We choose the summary with the highest likelihood to be part of our training set.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成训练数据集，我们可以构建一些提示模板，并使用更大的LLM来生成检索文档的无样本摘要。请注意，我们正在生成所有检索文档的单个摘要。类似于提取式摘要技术，对于每个生成的摘要，我们将它添加到输入文本前，并计算正确输出标记的概率。我们选择概率最高的摘要作为我们的训练集的一部分。
- en: During inference, if prefixing any given summary has a lower likelihood of generating
    the correct output than not prefixing any summary at all, then we deem the text
    represented by the summary to be irrelevant, and an empty summary is generated.
    This allows us to filter out irrelevant documents.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，如果给定的摘要前缀生成正确输出的概率低于不添加任何摘要的情况，那么我们认为摘要所代表的文本是不相关的，并生成一个空摘要。这允许我们过滤掉不相关的文档。
- en: '[Figure 12-6](#abstractive-summarization) depicts the workflow of a tightly-coupled
    abstractive summarizer during training.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-6](#abstractive-summarization)展示了紧耦合抽象式摘要器在训练过程中的工作流程。'
- en: '![abstractive-summarization](assets/dllm_1206.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![抽象式摘要](assets/dllm_1206.png)'
- en: Figure 12-6\. Abstractive summarization
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6\. 抽象式摘要
- en: Tip
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you are planning to change your target LLM, you might want to retrain the
    summary models. While the summarizers can transfer across models, there is still
    a slight performance degradation.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划更改目标LLM，你可能需要重新训练摘要模型。虽然摘要器可以在模型之间迁移，但仍然会有轻微的性能下降。
- en: Tightly-coupled summarizers, while expensive to train initially, can be an effective
    means of removing irrelevant information from the retrieved text while rephrasing
    it in a form that reduces ambiguity for the LLM.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 紧耦合的摘要器，虽然初始训练成本较高，但可以是一种有效的方法，在重新表述文本的同时，从检索文本中移除不相关信息，并降低LLM的歧义性。
- en: Chain-of-note
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 笔记链
- en: Another way to rephrase the retrieved text is to generate *notes*.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种重新表述检索文本的方法是生成*笔记*。
- en: 'It would be detrimental if the retrieved text contains irrelevant content that
    might mislead the model. In essence, the LLM has to contend with three types of
    scenarios:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检索到的文本包含可能误导模型的不相关信息，这将是有害的。本质上，LLM必须应对三种类型的场景：
- en: The retrieved documents contain the answer to the user query, and the LLM can
    use it to generate the correct output.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索到的文档包含用户查询的答案，LLM可以使用它来生成正确的输出。
- en: The retrieved documents do not contain the answer to the user query, but they
    provide valuable context that the LLM can then combine with its internal knowledge
    to come up with the answer.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索到的文档不包含用户查询的答案，但它们提供了LLM可以结合其内部知识来得出答案的有价值背景信息。
- en: The retrieved documents are irrelevant to the user query and should be ignored.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索到的文档与用户查询不相关，应该被忽略。
- en: LLMs are not very good at distinguishing between relevant and irrelevant context.
    One way to address this is by generating notes for each retrieved document that
    contains a summary of the retrieved document along with indicating whether it
    contains the answer to the user query or only contains relevant context but not
    the answer outright, or is irrelevant. This technique is called chain-of-note
    (CoN), introduced by [Yu et al.](https://oreil.ly/hPkKm)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 在区分相关和不相关上下文方面并不擅长。解决这一问题的方法是为每个检索到的文档生成笔记，其中包含检索文档的摘要，并指明它是否包含用户查询的答案，或者只包含相关上下文但不是直接的答案，或者是不相关的。这种技术被称为笔记链（CoN），由
    [Yu 等人](https://oreil.ly/hPkKm) 提出。
- en: 'Here is an example of how these notes look:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这些笔记的示例：
- en: '*Prompt:* Who succeeded Brian Mulroney as the prime minister of Canada in 1993?'
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示：* 1993 年谁接替了加拿大总理布莱恩·马尔罗尼？'
- en: ''
  id: totrans-206
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Doc 1: Kim Campbell took over the reins of leadership in Canada in 1993, marking
    her entry into history as the country’s first female Prime Minister in 1993\.
    Her tenure, however, was short-lived, as she led the Progressive Conservative
    Party into a challenging federal election later that year.'
  id: totrans-207
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文档 1：金·坎贝尔在 1993 年接管了加拿大的领导权，标志着她作为该国第一位女性总理进入历史。然而，她的任期很短，因为她带领进步保守党在那年晚些时候的联邦选举中面临挑战。
- en: ''
  id: totrans-208
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Doc 2: Following Brian Mulroney’s tenure, Canada witnessed the historic appointment
    of its first female Prime Minister, marking a groundbreaking shift in the nation’s
    political landscape.'
  id: totrans-209
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文档 2：在布莱恩·马尔罗尼的任期之后，加拿大见证了其第一位女性总理的历史性任命，标志着该国政治格局的一次突破性转变。
- en: While the first text says Kim Campbell became the prime minister in 1993, it
    is not apparent whether she immediately succeeded Brian Mulroney or if there was
    an intervening prime minister. However, the second text makes it clear that it
    is indeed Kim Campbell, as the text mentions that Brian Mulroney was succeeded
    by a female prime minister.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一段文本说金·坎贝尔在 1993 年成为总理，但并不明显她是否立即接替了布莱恩·马尔罗尼，或者中间是否有其他总理。然而，第二段文本清楚地表明确实是金·坎贝尔，因为文本提到布莱恩·马尔罗尼被一位女性总理接替。
- en: 'The corresponding note generated by the model is the following:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 模型生成的相应笔记如下：
- en: '*LLM:* The first statement mentions that Kim Campbell took over the leadership
    in Canada in 1993, and the second statement refers to the period following Brian
    Mulroney’s tenure, leading to the appointment of Canada’s first female prime minister.
    This sequence implies that Kim Campbell’s assumption of the prime minister role
    directly followed Brian Mulroney’s tenure, making her his successor.'
  id: totrans-212
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*LLM:* 第一句话提到金·坎贝尔在 1993 年接管了加拿大的领导权，第二句话指的是布莱恩·马尔罗尼任期之后的时期，导致加拿大第一位女总理的任命。这一序列暗示了金·坎贝尔担任总理的角色直接接替了布莱恩·马尔罗尼的任期，使她成为他的继任者。'
- en: Using the note as context, the LLM can then generate the correct answer.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用笔记作为上下文，LLM 可以然后生成正确的答案。
- en: The true value of chain-of-note becomes apparent when the context contains irrelevant
    or insufficient information. The generated note can help the LLM output “I don’t
    know.”
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 当上下文中包含无关或不充分的信息时，笔记链的真正价值变得明显。生成的笔记可以帮助 LLM 输出“我不知道”。
- en: 'Consider this query:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下查询：
- en: '*Prompt:* Did the Green Party of Canada support the proposed 2023 Public Transit
    bill?'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示：* 加拿大绿党是否支持提出的 2023 年公共交通法案？'
- en: 'and the retrieved documents are:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以及检索到的文档是：
- en: 'Doc 1: The Green Party of Canada has historically emphasized the importance
    of sustainable transportation as a key component of their environmental platform,
    advocating for policies that promote green infrastructure.'
  id: totrans-218
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文档 1：加拿大绿党历史上一直强调可持续交通的重要性，将其作为其环境平台的关键组成部分，倡导促进绿色基础设施的政策。
- en: ''
  id: totrans-219
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Doc 2: The proposed 2023 Public Transit bill garnered widespread attention
    and popularity, reflecting a growing public consensus on the importance of sustainable
    transportation solutions.'
  id: totrans-220
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 文档 2：提出的 2023 年公共交通法案受到了广泛关注和欢迎，反映了公众对可持续交通解决方案重要性的日益增长的共识。
- en: 'The retrieved documents do not contain the answer to the query. The corresponding
    note generated is:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 检索到的文档中没有包含查询的答案。生成的相应笔记是：
- en: '*LLM:* While the first passage indicates the party’s historical emphasis on
    sustainable transportation, and the second passage mentions the bill’s popularity,
    neither confirms the party’s support or opposition to the 2023 bill.'
  id: totrans-222
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*LLM:* 虽然第一段提到该党历史上对可持续交通的重视，第二段提到该法案的受欢迎程度，但都没有确认该党对 2023 年法案的支持或反对。'
- en: Using this note, the LLM can generate “unknown” when the retrieved documents
    do not contain the answer to the query.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个笔记，当检索到的文档不包含查询的答案时，LLM可以生成“未知”。
- en: 'An example of a CoN prompt can be:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CoN提示的例子可以是：
- en: '*Prompt:* You are provided a query along with {K} passages that potentially
    contain information that can be used to answer the query. Write notes summarizing
    the key points from these passages. Discuss the relevance of each of these passages
    to the given question and state whether the answer to the query can be deduced
    from the content in these passages.'
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 你将获得一个查询以及{K}个段落，这些段落可能包含可用于回答查询的信息。写笔记总结这些段落的关键点。讨论每个段落与给定问题的相关性，并说明查询的答案是否可以从这些段落的内容中推断出来。'
- en: Again, we can train tightly-coupled CoN models to make it more effective. This
    can be done by fine-tuning an LLM to elicit CoN behavior.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以训练紧密耦合的CoN模型以提高其有效性。这可以通过微调LLM以引发CoN行为来实现。
- en: To generate the fine-tuning dataset, you can prompt an LLM to generate candidate
    notes for example queries. Human evaluation can then filter out incorrect or poor-quality
    notes. The final dataset consists of the CoN prompt, the input query, and the
    retrieved documents as the input, and the corresponding note and the query answer
    as the output. An LLM can then be fine-tuned on this dataset.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成微调数据集，你可以提示LLM为示例查询生成候选笔记。然后，人工评估可以筛选出错误或质量差的笔记。最终数据集包括CoN提示、输入查询和检索到的文档作为输入，以及相应的笔记和查询答案作为输出。LLM可以在此基础上进行微调。
- en: The authors (Yu et al.) introduce a weighted loss scheme during training. The
    note can be much longer than the answer, and thus equally weighting the loss across
    all tokens will lead to the note getting significantly more importance during
    training. This harms model convergence. The weighted loss scheme involves calculating
    loss across answer tokens 50% of the time.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 作者（Yu等人）在训练期间引入了一种加权损失方案。笔记可以比答案长得多，因此在整个标记上均匀加权损失会导致笔记在训练期间获得显著的重要性。这会损害模型收敛。加权损失方案涉及50%的时间内计算答案标记的损失。
- en: Using a CoN step is very useful, especially if the retrieval results are known
    to contain a lot of noise or there is a higher possibility of no relevant documents
    available to service the query. CoN behavior is harder for smaller models, thus
    a sufficiently larger model should be used.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoN步骤非常有用，特别是如果检索结果已知包含大量噪声，或者有更高的可能性没有相关文档可供查询服务。CoN行为对较小的模型来说更难，因此应该使用足够大的模型。
- en: Now that we have discussed the refine step of the RAG pipeline, let’s move to
    the insert step.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了RAG管道的精炼步骤，让我们转到插入步骤。
- en: Insert
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 插入
- en: Once we have determined the content to be fed to the LLM that is going to generate
    the final response to a query, whether the original retrieved documents or their
    summaries or notes, we need to decide how we are going to arrange it inside the
    prompt.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们确定了要提供给LLM以生成查询最终响应的内容，无论是原始检索到的文档、它们的摘要还是笔记，我们需要决定如何在提示中安排它。
- en: The standard approach is to stuff all the content, or at least as much as can
    fit, into the context window. An alternative is to feed each retrieved document/summary/note
    prefixed to the input separately to the LLM, and then combine the outputs.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 标准的方法是将所有内容，或者至少尽可能多的内容，放入上下文窗口中。另一种方法是分别将检索到的每个文档/摘要/笔记作为输入前缀添加到LLM中，然后合并输出。
- en: '[Liu et al.](https://oreil.ly/LFR8r) show that language models are more adept
    at recalling information present at the beginning and the end of the context window
    as compared to the middle. We can exploit this knowledge to reorder the retrieved
    documents in the prompt.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '[刘等人](https://oreil.ly/LFR8r)表明，与中间部分相比，语言模型在回忆上下文窗口开始和结束处的信息方面更为擅长。我们可以利用这一知识来重新排序提示中的检索文档。'
- en: 'Let’s say we retrieved 10 documents for the given query. The documents are
    ordered according to their relevance: Doc1, Doc2,…Doc10\. These documents can
    now be arranged in the prompt in the following order:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们为给定查询检索了10个文档。这些文档根据相关性排序：Doc1, Doc2,…Doc10。现在，这些文档可以在提示中按以下顺序排列：
- en: Doc1, Doc3, Doc5, Doc7, Doc9, Doc10, Doc8, Doc6, Doc4, Doc2
  id: totrans-236
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Doc1, Doc3, Doc5, Doc7, Doc9, Doc10, Doc8, Doc6, Doc4, Doc2
- en: Thus the least relevant documents exist in the middle of the context window,
    where they are more likely to be ignored by the model due to current long context
    recall limitations.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最不相关的文档存在于上下文窗口的中间，它们更有可能因为当前长上下文召回限制而被模型忽略。
- en: 'Alternative approaches include arranging the documents in order of relevance,
    for example:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的方法包括按相关性排序文档，例如：
- en: Doc1, Doc2, Doc3, Doc4, Doc5, Doc6, Doc7, Doc8, Doc9, Doc10
  id: totrans-239
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Doc1, Doc2, Doc3, Doc4, Doc5, Doc6, Doc7, Doc8, Doc9, Doc10
- en: 'Or in reverse order of relevance, like:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 或者按相关性逆序，例如：
- en: Doc10, Doc9, Doc8, Doc7, Doc6, Doc5, Doc4, Doc3, Doc2, Doc1
  id: totrans-241
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Doc10, Doc9, Doc8, Doc7, Doc6, Doc5, Doc4, Doc3, Doc2, Doc1
- en: These ordering schemes are useful only if the input context is very long (upwards
    of 5,000 tokens).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这些排序方案仅在输入上下文非常长（超过5,000个标记）时才有用。
- en: Finally, let’s discuss the generate step in the RAG pipeline.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们讨论RAG管道中的生成步骤。
- en: Generate
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成
- en: The LLM generates the final response to the given query during this step. The
    standard approach is to generate the output all at once. However, you could also
    interleave the generation and the retrieval process, by generating some output
    and retrieving more context, and generating some more output, and retrieving more
    context, and so on.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，LLM生成对给定查询的最终响应。标准方法是一次性生成输出。然而，您也可以通过生成一些输出并检索更多上下文，然后再生成一些输出，再检索更多上下文，以此类推，来交错生成和检索过程。
- en: This approach can be useful in maintaining coherence in long-form text generation.
    The generated text determines what needs to be retrieved next. This process is
    called active retrieval.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在维护长文本生成中的连贯性方面可能很有用。生成的文本决定了接下来需要检索的内容。这个过程被称为主动检索。
- en: 'How do we decide when to stop generating and start a new retrieval step? We
    could:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何决定何时停止生成并开始新的检索步骤？我们可以：
- en: Retrieve after every N tokens are generated.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每生成N个标记后进行检索。
- en: Retrieve after each textual unit is generated. (A textual unit can be a sentence,
    paragraph, section, etc.)
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每生成每个文本单元后进行检索。（文本单元可以是句子、段落、部分等。）
- en: Retrieve when currently available context is deemed insufficient for generation.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当当前可用上下文被认为不足以生成时进行检索。
- en: 'There are several ways to implement the latter. One of them is Forward-Looking
    Active REtrieval-augmented generation (FLARE). The authors of [FLARE](https://oreil.ly/eZRdy)
    introduce two methods for active retrieval: FLARE-Instruct and FLARE-Direct.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以实现这一点。其中之一是向前看主动检索增强生成（FLARE）。[FLARE](https://oreil.ly/eZRdy)的作者介绍了两种主动检索方法：FLARE-Instruct和FLARE-Direct。
- en: In FLARE-Instruct, the LLM is prompted to generate queries in a specific syntax
    whenever it needs additional information to continue coherent generation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在FLARE-Instruct中，当LLM需要更多信息以继续连贯生成时，会提示它以特定语法生成查询。
- en: In FLARE-Direct, the LLM generates a candidate-next sentence. If any of the
    tokens in the generated sentence have probability lower than a threshold, then
    the retrieval process is activated. If not, then the candidate sentence is accepted
    as a valid continuation and the generation process continues. If retrieval is
    to take place, the generated sentence can be used as the query, by masking the
    low-probability tokens (since they might confuse the retriever if they are irrelevant/incorrect).
    You can also rephrase the sentence as a question about the low-probability token(s).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在FLARE-Direct中，LLM生成一个候选下一句。如果生成的句子中的任何标记的概率低于一个阈值，则激活检索过程。如果没有，则候选句子被视为有效的后续句子，生成过程继续。如果需要进行检索，则可以使用生成的句子作为查询，通过屏蔽低概率标记（因为如果它们不相关/不正确，可能会混淆检索器）。您还可以将句子重新表述为关于低概率标记的问题。
- en: 'Let’s look at an example using FLARE-Instruct:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过FLARE-Instruct看看一个例子：
- en: '*Prompt:* Write an article about Peruth Chemutai, the Ugandan Olympics medal
    winner.'
  id: totrans-255
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*提示:* 写一篇关于乌干达奥运金牌得主Peruth Chemutai的文章。'
- en: '*FLARE-Instruct:* Peruth Chemutai [Search(birthdate of Peruth Chemutai)] is
    a Ugandan long-distance runner who specializes in the 3000 meters steeplechase.
    She gained international recognition after [Search(what medals did Peruth Chemutai
    win)] winning the gold medal in the women’s 3000 meters steeplechase at the 2020
    Summer Olympics, becoming the first Ugandan woman to win an Olympic gold medal.'
  id: totrans-256
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*FLARE-Instruct:* Peruth Chemutai [搜索(Peruth Chemutai的出生日期)] 是一位专注于3000米障碍赛的乌干达长跑运动员。她在2020年夏季奥运会上赢得女子3000米障碍赛金牌后获得了国际认可，成为第一位赢得奥运金牌的乌干达女性。'
- en: ''
  id: totrans-257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Early Life
  id: totrans-258
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 早年生活
- en: ''
  id: totrans-259
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chemutai was born in the [Search(birthplace of Peruth Chemutai)], a region known
    for [Search(what is the birthplace of Peruth Chemutai known for?)].’
  id: totrans-260
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chemutai出生于[Search(Peruth Chemutai的出生地)]，一个以[Search(Peruth Chemutai的出生地以什么而闻名？)]而闻名的地区。’
- en: This is a contrived example, as the true benefits of FLARE can be better appreciated
    on lengthier outputs. As seen in the output, the model generates search queries
    that can be used to retrieve factually correct information from data sources.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个虚构的例子，因为FLARE的真实好处可以在更长的输出中更好地欣赏。如输出所示，模型生成可用于从数据源检索事实性信息的搜索查询。
- en: 'For the same query, using FLARE-Direct, the model generates the candidate article:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 对于相同的查询，使用FLARE-Direct，模型生成了候选文章：
- en: '*FLARE-Direct:* Peruth Chemutai ( born July 10, 1999) is a Ugandan long-distance
    runner who specializes in the 3000 meters steeplechase. She gained international
    recognition after winning the gold medal in the women’s 3000 meters steeplechase
    at the 2020 Summer Olympics, becoming the first Ugandan woman to win an Olympic
    gold medal.'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*FLARE-Direct:* Peruth Chemutai（出生于1999年7月10日）是一位乌干达长跑运动员，擅长3000米障碍赛。她在2020年夏季奥运会上获得女子3000米障碍赛金牌后获得了国际认可，成为第一位获得奥运金牌的乌干达女性。'
- en: ''
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Early Life
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 早年生活
- en: ''
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Chemutai was born in the Bukwo District, Uganda, a region known for its challenging
    terrain and passionate long-distance runners.
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Chemutai出生于乌干达的布库沃区，这是一个以地形崎岖和热情的长跑运动员而闻名的地区。
- en: The underlined tokens are low-probability tokens, which can be refilled by retrieving
    relevant text. We can either mask the low-probability tokens and use them as the
    retrieval query or generate standalone queries like, “When was Peruth Chemutai
    born?” based on the masked tokens.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 下划线的标记是低概率标记，可以通过检索相关文本来补充。我们可以要么屏蔽低概率标记并将它们用作检索查询，要么根据屏蔽的标记生成独立的查询，例如，“Peruth
    Chemutai什么时候出生？”
- en: A crucial aspect of generation includes adding appropriate citations to ground-truth
    sources. The LLM can be fine-tuned to make it provide citations along with the
    answer in response to user queries. One such model is [Cohere’s Command-R](https://oreil.ly/v0KUs)
    model.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的一个关键方面是添加适当的引用到事实来源。LLM可以被微调以使其在响应用户查询时提供与答案一起的引用。其中一个这样的模型是[Cohere的Command-R](https://oreil.ly/v0KUs)模型。
- en: As we can see, the RAG pipeline for knowledge retrieval can be rather lengthy.
    However, for a lot of RAG applications, latency is a key consideration. This increases
    the importance of smaller language models or faster, non-LLM-based approaches.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，知识检索的RAG管道可能相当长。然而，对于许多RAG应用来说，延迟是一个关键考虑因素。这增加了小型语言模型或更快、非LLM基础的方案的重要性。
- en: Let’s put it all together by revisiting the RAG pipeline diagram first introduced
    at the beginning of the chapter. [Figure 12-7](#RAG-pipeline2) depicts the workflow
    of a comprehensive RAG pipeline.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾本章开头首次介绍的RAG管道图来汇总所有内容。[图12-7](#RAG-pipeline2)展示了全面RAG管道的工作流程。
- en: '![RAG-pipeline](assets/dllm_1207.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![RAG-pipeline](assets/dllm_1207.png)'
- en: Figure 12-7\. Comprehensive RAG pipeline
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7\. 全面RAG管道
- en: So far, we have focused on using RAG for knowledge retrieval. Let’s now discuss
    a few other use cases.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于使用RAG进行知识检索。现在让我们讨论一些其他用例。
- en: RAG for Memory Management
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG用于内存管理
- en: 'An underrated application of RAG is expanding the context window of an LLM.
    To recap, an LLM prompt typically contains the following types of (optional) content:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: RAG的一个被低估的应用是扩展LLM的上下文窗口。为了回顾，LLM提示通常包含以下类型的（可选）内容：
- en: The pre-prompt or *system prompt*
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 预提示或*系统提示*
- en: These are the overarching instructions provided to the LLM included at the beginning
    of every query. Depending on your customization needs, the system prompt could
    occupy a significant part of the context window.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在每个查询开头提供的总体指令，包括给LLM的。根据您的定制需求，系统提示可能占据上下文窗口的很大一部分。
- en: The input prompt
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 输入提示
- en: This includes the current input and the instruction, optional few-shot training
    examples, and additional context, possibly fetched using retrieval.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括当前输入和指令，可选的几步训练示例，以及可能通过检索获取的附加上下文。
- en: Conversational history
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 对话历史
- en: This includes the history of conversations/interaction between the user and
    the LLM. Including this in the context window enables the user to have a long,
    coherent conversation with the LLM.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括用户和LLM之间对话/交互的历史。将此包含在上下文窗口中，使用户能够与LLM进行长时间、连贯的对话。
- en: Scratchpad
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 记事本
- en: This includes intermediate output generated by the LLM (discussed in [Chapter 8](ch08.html#ch8)),
    which can be referred to by the LLM when generating future output. Scratchpad
    content is an artifact of certain prompting techniques like CoT.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括由LLM（在第8章中讨论）生成的中间输出，当生成未来输出时，LLM可以引用这些输出。Scratchpad内容是某些提示技术（如CoT）的产物。
- en: In many cases, the LLM’s limited context window is simply insufficient to incorporate
    all this data. Moreover, we might like to make the conversational history available
    to the model through perpetuity, which means it keeps growing across time. Making
    all the conversational history available to the LLM is a key aspect in enabling
    personalization.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，LLM有限的上下文窗口不足以包含所有这些数据。此外，我们可能希望通过永久性使对话历史对模型可用，这意味着它随着时间的推移而持续增长。使所有对话历史对LLM可用是实现个性化的关键方面。
- en: It’s RAG to the rescue! RAG can be employed in facilitating LLM memory management
    by swapping in and out relevant content in the prompt as suitable. This is reminiscent
    of how memory management occurs in operating systems. Let’s explore this abstraction
    further.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: RAG（Retrieval-Augmented Generation）拯救了！RAG可以通过在提示中交换相关内容来促进LLM的内存管理。这让人联想到操作系统中的内存管理方式。让我们进一步探讨这个抽象。
- en: In an OS, memory is organized in a hierarchy, with fast (and expensive) memory
    being directly accessible to a processor, and higher levels of the hierarchy containing
    larger and slower (but relatively inexpensive) memory. When the processor needs
    to access some data, it tries to access it from the lowest level in the memory
    hierarchy. If the data is not present there, it searches the next level in the
    hierarchy. If present, it swaps the required data into the lower level and swaps
    out data that is not currently needed. This way, the OS can support a fast main
    memory that is directly accessible by the processor and a much larger virtual
    memory that can be swapped in whenever needed.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作系统中，内存以层次结构组织，快速（且昂贵）的内存可以直接被处理器访问，而层次结构的高层包含更大、更慢（但相对便宜）的内存。当处理器需要访问某些数据时，它会尝试从内存层次结构的最低层访问。如果数据不在那里，它会搜索层次结构的下一层。如果数据存在，它会将所需数据交换到低层，并交换出当前不需要的数据。这样，操作系统可以支持一个处理器可以直接访问的快速主内存，以及一个可以随时交换的更大的虚拟内存。
- en: This is a very simplified explanation of OS memory management. For a more detailed
    explanation, check out Tony’s [“Operating System — Hierarchy of Memory”](https://oreil.ly/vcciM).
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对操作系统内存管理的一个非常简化的解释。要了解更多详细信息，请查看Tony的[“操作系统 — 内存层次结构”](https://oreil.ly/vcciM)。
- en: '[Figure 12-8](#os-hierarchy) shows the memory hierarchy of a typical OS.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-8](#os-hierarchy)显示了典型操作系统的内存层次结构。'
- en: '![os-hierarchy](assets/dllm_1208.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![os-hierarchy](assets/dllm_1208.png)'
- en: Figure 12-8\. Typical OS memory hierarchy
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-8\. 典型操作系统的内存层次结构
- en: Similarly in LLMs, the context window is analogous to the main memory as it
    is directly accessible to the LLM. However, we can expand the context window indefinitely
    by implementing a memory system analogous to the OS virtual memory. This helps
    in personalizing LLMs, providing them with the full access to a user’s conversational
    history and their implicit and explicit preferences.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在LLMs中，上下文窗口类似于主内存，因为它可以直接被LLM访问。然而，我们可以通过实现类似于操作系统虚拟内存的内存系统来无限扩展上下文窗口。这有助于个性化LLMs，为它们提供对用户对话历史和他们的隐含和显式偏好的完全访问。
- en: Examples of libraries supporting memory management for LLMs include [Letta (formerly
    MemGPT)](https://oreil.ly/1p8Vu) and [Mem0](https://oreil.ly/dgJaZ).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 支持LLM内存管理的库示例包括[Letta（原名MemGPT）](https://oreil.ly/1p8Vu)和[Mem0](https://oreil.ly/dgJaZ)。
- en: Note
  id: totrans-294
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An alternative or complement to swapping memory in and out is to recursively
    summarize the conversational history. However, summarization is a lossy process
    and may not be able to preserve the semantics of the text. Valuable nuances like
    the tone of the writer can be lost during summarization.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 交换内存的另一种或补充方法是递归地总结对话历史。然而，总结是一个有损过程，可能无法保留文本的语义。在总结过程中可能会丢失有价值的信息，如作者的语气。
- en: RAG for Selecting In-Context Training Examples
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG用于选择上下文中的训练示例
- en: As mentioned at the beginning of the chapter, another application of RAG is
    to dynamically select training examples for few-shot learning by retrieving the
    optimal examples from a data store containing a list of training examples. For
    a given input, the retrieved few-shot examples are supposed to maximize the LLM’s
    chance of generating the correct answer to a user query.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，RAG的另一个应用是通过从包含一系列训练示例的数据存储中检索最佳示例来动态选择少量学习的训练示例。对于给定的输入，检索出的少量示例应该最大化LLM生成用户查询正确答案的概率。
- en: A simple method is to generate embeddings of the input and retrieve examples
    whose embeddings are most similar to the input embedding. While this technique
    is a promising start, we can do much better.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是生成输入的嵌入，并检索与输入嵌入最相似的示例。虽然这项技术是一个有希望的起点，但我们能做得更好。
- en: '[Wang et al.](https://oreil.ly/r8735) introduce a method called LLM Retriever
    (LLM-R) that trains a model using LLM feedback to retrieve few-shot training examples
    whose inclusion will increase the probability of the LLM generating the correct
    answer. [Figure 12-9](#llm-r) illustrates the LLM-R technique.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '[王等人](https://oreil.ly/r8735)介绍了一种名为LLM Retriever (LLM-R)的方法，该方法通过使用LLM反馈来训练模型，检索出少量示例训练样本，其包含将增加LLM生成正确答案的概率。[图12-9](#llm-r)展示了LLM-R技术。'
- en: '![llm-r](assets/dllm_1209.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![llm-r](assets/dllm_1209.png)'
- en: Figure 12-9\. LLM-R workflow
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9\. LLM-R工作流程
- en: For each input query in the training set, we retrieve the top-k few-shot examples
    by using a retrieval model like BM25\. We then rerank the examples by using LLM
    feedback. Each example is prefixed to the input and the probability of the ground-truth
    output tokens is calculated. The examples are then ranked by decreasing order
    of their log-probabilities. The ranked examples are then used to train a reward
    model, which is distilled to train the final retrieval model.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集中的每个输入查询，我们使用类似BM25的检索模型检索出top-k的少量示例。然后，我们使用LLM反馈对这些示例进行重新排序。每个示例都添加到输入之前，并计算真实输出标记的概率。然后，根据它们的对数概率递减顺序对这些示例进行排序。排序后的示例随后用于训练一个奖励模型，该模型被蒸馏以训练最终的检索模型。
- en: RAG for Model Training
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG用于模型训练
- en: So far, all the RAG applications we have explored are applied during LLM inference.
    Can we use RAG during model pre-training and fine-tuning as well? Yes, we can!
    This is an underrated area of study, and I expect to see more LLMs leveraging
    this in the coming years. Let’s look at an example in detail.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们探索的所有RAG应用都是在LLM推理期间应用的。我们能否在模型预训练和微调期间也使用RAG？是的，我们可以！这是一个被低估的研究领域，我预计在未来几年中会有更多的LLM利用这一点。让我们详细看看一个例子。
- en: Retrieval-Augmented Language Model (REALM) is one of the pioneering works in
    the RAG space. REALM integrates the retrieval and generation tasks into a single
    model. [Figure 12-10](#realm) shows the REALM framework for pre-training and fine-tuning.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 检索增强语言模型（REALM）是RAG领域中的开创性工作之一。REALM将检索和生成任务集成到一个单一模型中。[图12-10](#realm)显示了REALM的预训练和微调框架。
- en: '![realm](assets/dllm_1210.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![realm](assets/dllm_1210.png)'
- en: Figure 12-10\. REALM architecture
  id: totrans-307
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-10\. REALM架构
- en: 'The REALM architecture is composed of two components: a knowledge retriever
    and a knowledge-augmented encoder, which is a BERT-like encoder-only model. Both
    components are differentiable and thus trained together.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: REALM架构由两个组件组成：一个知识检索器和一个知识增强编码器，这是一个类似于BERT的仅编码器模型。这两个组件都是可微分的，因此一起训练。
- en: The knowledge retriever is used to generate embeddings for all documents in
    the external knowledge base. Retrieval is performed by finding documents with
    maximum embedding similarity to the input. During the masked-language modeling
    pre-training phase, the retriever loss function encourages it to fetch text that
    helps predict the masked tokens. The masked tokens are then predicted by attending
    to both the input text and the retrieved text. The retrieved text is supposed
    to contain relevant context that makes predicting the masked tokens much easier.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 知识检索器用于为外部知识库中的所有文档生成嵌入。检索是通过找到与输入具有最大嵌入相似度的文档来进行的。在掩码语言模型预训练阶段，检索器损失函数鼓励它检索有助于预测掩码标记的文本。然后，通过关注输入文本和检索到的文本来预测掩码标记。检索到的文本应该包含使预测掩码标记变得容易的相关上下文。
- en: 'REALM also employs these strategies to optimize training:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: REALM也采用这些策略来优化训练：
- en: Named entities or dates are masked so that the model can learn to predict them
    using retrieved context.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体或日期被掩码，以便模型可以学习使用检索到的上下文来预测它们。
- en: Not all masked tokens need external knowledge for their prediction. To accommodate
    this, an empty document is always added to the retrieved documents.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有掩码标记的预测都需要外部知识。为了适应这一点，总是向检索到的文档中添加一个空文档。
- en: The retrieved documents ideally contain the context required to predict the
    masked token, and not the token itself. Therefore, trivial retrievals that contain
    the masked token in the retrieved text are not included.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理想的检索文档应包含预测掩码标记所需的上下文，而不是掩码标记本身。因此，包含检索文本中掩码标记的简单检索不包括在内。
- en: Limitations of RAG
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG的局限性
- en: 'While RAG is a powerful paradigm that expands the usefulness of LLMs and reduces
    hallucinations, it doesn’t resolve all the limitations of LLMs. Some pitfalls
    of using RAG include:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然RAG是一个强大的范式，它扩展了LLM的有用性并减少了幻觉，但它并没有解决LLM的所有局限性。使用RAG的一些陷阱包括：
- en: Relying on retrieval of text snippets can cause the LLM to depend on surface-level
    information to answer queries, rather than a deeper understanding of the problem
    space.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 依赖于文本片段的检索可能导致LLM依赖于表面信息来回答查询，而不是对问题空间的深入理解。
- en: Retrieval becomes the limiting factor of the pipeline. If the retrieval process
    fails to extract suitable candidate text, the LLM’s powerful capabilities will
    all be for nothing.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索成为管道的瓶颈。如果检索过程未能提取合适的候选文本，LLM强大的能力都将毫无用处。
- en: Sometimes the retrieval process can extract documents that are contradictory
    to the knowledge contained in the LLM’s parametric memory. Without access to the
    ground truth, it is difficult for the LLM to resolve these contradictions.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时检索过程可能会提取出与LLM参数化记忆中包含的知识相矛盾的文档。没有访问到事实真相，LLM很难解决这些矛盾。
- en: RAG Versus Long Context
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG与长上下文
- en: As discussed in [Chapter 5](ch05.html#chapter_utilizing_llms), one of the limitations
    of LLMs is the limited effective context window available to them. However, this
    is one of the areas where rapid advances have been made recently. Context windows
    of at most a few thousand tokens were standard until early 2023, after which companies
    like [Anthropic](https://oreil.ly/ucbD-) announced support for context windows
    spanning over 100,000 tokens. In early 2024, Google announced [Gemini 1.5 Pro](https://oreil.ly/rp7pi),
    with support for one million tokens of context.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第5章](ch05.html#chapter_utilizing_llms)所述，LLM的一个局限性是它们可用的有效上下文窗口有限。然而，这是最近取得快速进展的一个领域。最多几千个标记的上下文窗口是标准，直到2023年初，此后像[Anthropic](https://oreil.ly/ucbD-)这样的公司宣布支持跨越10万多个标记的上下文窗口。到2024年初，谷歌宣布了[Gemini
    1.5 Pro](https://oreil.ly/rp7pi)，支持一百万个标记的上下文。
- en: To assess the impact on LLM performance as the context size increases, several
    needle-in-a-haystack tests have been devised. One such implementation by [Greg
    Kamradt](https://oreil.ly/M8Jc9) facilitates adding a random fact or statement
    (the needle) to the middle of the context (the haystack) and then asking the LLM
    questions for which the needle is the answer.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估上下文大小增加对LLM性能的影响，已经设计了几种“大海捞针”测试。其中一种由[Greg Kamradt](https://oreil.ly/M8Jc9)实施，它便于将一个随机事实或陈述（“针”）添加到上下文（“草堆”）的中间，然后向LLM提出问题，其中“针”是答案。
- en: However, it is wise to take these tests with a grain of salt as they often evaluate
    only the information recall capabilities of an LLM. Moreover, very few problems
    in the real world are needle-in-the-haystack problems; LLMs are probably not the
    right tool to solve them anyway. Cheaper and faster retrieval models could adequately
    perform most needle retrieval tasks.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，明智的做法是以批判的眼光看待这些测试，因为它们通常只评估LLM的信息回忆能力。此外，现实世界中的问题很少是“大海捞针”的问题；LLM可能也不是解决这些问题的正确工具。更便宜、更快的检索模型可以充分执行大多数“大海捞针”检索任务。
- en: In many needle-in-a-haystack tests, random sentences or paragraphs are added
    to the context window as needles, with the rest of the content in the context
    window being orthogonal to the needle. But this does not mirror the situation
    in the real world, where most co-occurring text is related in some way. Related
    text can often act as distractors, preventing the LLM from drawing the right conclusions.
    In fact, it is one of the reasons for developing rigorous rerank and refine steps
    in the RAG pipeline!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多“大海捞针”的测试中，随机句子或段落被添加到上下文窗口中作为“针”，而上下文窗口中的其余内容与“针”正交。但这并不反映现实世界的情况，在现实世界中，大多数共现文本以某种方式相关。相关的文本往往充当干扰项，阻止LLM得出正确的结论。事实上，这也是在RAG管道中开发严格的重排序和细化步骤的原因之一！
- en: Long-context models can be useful for analyzing very long documents and also
    can reduce the complexity of the rerank and refine steps. I recommend empirically
    calculating the trade-offs where feasible.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 长上下文模型可以用于分析非常长的文档，还可以减少重新排序和细化步骤的复杂性。我建议在可行的情况下进行经验计算以权衡利弊。
- en: Finally, cost is also an important consideration for the long context versus
    retrieval debate. No doubt, the cost for long-context models will drop significantly
    in the future, but retrieval will still be relatively cheaper. Forgoing retrieval
    completely in favor of using long-context models is akin to buying a laptop and
    storing all your files in RAM instead of disk.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，成本也是长上下文与检索辩论中的一个重要考虑因素。毫无疑问，未来长上下文模型的成本将显著降低，但检索仍然相对便宜。完全放弃检索而选择使用长上下文模型，就像购买一台笔记本电脑并将所有文件存储在RAM中而不是磁盘上一样。
- en: RAG Versus Fine-Tuning
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RAG与微调的比较
- en: 'The debate around using RAG versus fine-tuning boils down to the more fundamental
    question: what aspects of the task can I perform using the LLM versus relying
    on external sources?'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RAG与微调的辩论归结为一个更基本的问题：我可以用LLM执行哪些任务方面，而不是依赖外部来源？
- en: In cases where external knowledge is required to solve a task, both retrieval
    and fine-tuning can be used. Retrieval can be used to integrate the knowledge
    on demand, with the drawback being that the LLM is only exposed to surface-level
    information and is not provided the chance to learn from connections between the
    data. On the other end, continued pre-training or fine-tuning can also be used
    to integrate external knowledge, albeit with an expensive training step.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在需要外部知识来解决任务的情况下，可以使用检索和微调。检索可用于按需集成知识，缺点是LLM仅接触到表面信息，没有机会从数据之间的联系中学习。另一方面，持续的预训练或微调也可以用于集成外部知识，尽管需要昂贵的训练步骤。
- en: '[Ovadia et al.](https://oreil.ly/Agodo) compared RAG and fine-tuning on tasks
    requiring external knowledge. They showed that RAG consistently outperformed fine-tuning
    for knowledge-intensive tasks. As shown earlier in this chapter, LLMs need a lot
    of samples to memorize a concept or fact. Thus, fine-tuning effectiveness can
    be improved by repetition or augmentation of the fine-tuning dataset.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[Ovadia等人](https://oreil.ly/Agodo)比较了RAG和微调在需要外部知识任务上的表现。他们表明，对于知识密集型任务，RAG始终优于微调。正如本章前面所展示的，LLM需要大量样本来记忆一个概念或事实。因此，通过重复或增强微调数据集可以提高微调的有效性。'
- en: Even for knowledge-intensive tasks, RAG versus fine-tuning need not be an either-or
    decision. If you are working on a specialized domain or need your outputs in a
    certain style or format, you can fine-tune your LLM on domain- and task-specific
    data, and use RAG with this fine-tuned model for your downstream applications.
    In a large proportion of use cases, RAG should be sufficient, and fine-tuning
    shouldn’t be the first choice of solution.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于知识密集型任务，RAG与微调也不必是二选一的决定。如果你正在从事一个专业领域或需要以特定风格或格式输出结果，你可以在领域和任务特定数据上微调你的LLM，并使用微调后的模型进行下游应用。在大量用例中，RAG应该是足够的，微调不应成为首选解决方案。
- en: RAG and fine-tuning can be complementary. Earlier in this chapter, we saw how
    each step of the RAG pipeline can be optimized using fine-tuning. Similarly, we
    also saw how RAG can be used to optimize the fine-tuning process. Thus, both retrieval
    and fine-tuning are powerful parts of your LLM toolkit, and I hope that these
    chapters have sufficiently prepared you to implement and deploy them in the wild.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: RAG和微调可以互补。在本章前面，我们看到了如何使用微调来优化RAG管道的每一步。同样，我们也看到了如何使用RAG来优化微调过程。因此，检索和微调都是你LLM工具箱中的强大部分，我希望这些章节已经充分准备了你将它们实施和部署到实际应用中。
- en: Summary
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we conducted a deep dive into the RAG pipeline, exploring in
    detail the *rewrite-retrieve-rerank-refine-insert-generate* pipeline. We highlighted
    the effectiveness of RAG in various scenarios, including integration of external
    knowledge, retrieval of past conversational history, dynamic selection of few-shot
    learning examples, and tool selection. We also explored the limitations of RAG
    and scenarios where RAG may not be effective.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了RAG管道，详细介绍了*重写-检索-重新排序-细化-插入-生成*管道。我们强调了RAG在各种场景下的有效性，包括外部知识的集成、过去对话历史的检索、动态选择少样本学习示例以及工具选择。我们还探讨了RAG的局限性以及RAG可能无效的场景。
- en: In the final chapter, we will explore how we can utilize all the concepts we
    learned so far to architect and package LLM-driven products that bring value to
    end users. Effective product design has become all the more important in the age
    of LLMs, given that a successful LLM product leverages the LLM the best it can
    for the capabilities it excels at, while at the same time limiting end-user exposure
    to LLM limitations by means of clever product design. We will also look at several
    LLM design patterns that put together all the concepts we learned in reusable,
    debuggable abstractions.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们将探讨如何利用我们迄今为止学到的所有概念来设计和打包由LLM（大型语言模型）驱动的产品，这些产品能为最终用户提供价值。在LLM时代，有效的产品设计变得尤为重要，因为一个成功的LLM产品能够最大限度地利用LLM在擅长领域的功能，同时通过巧妙的产品设计来限制最终用户对LLM局限性的接触。我们还将探讨几种LLM设计模式，这些模式将我们学到的所有概念整合成可重用、可调试的抽象。
