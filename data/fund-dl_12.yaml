- en: Chapter 12\. Memory Augmented Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章。记忆增强神经网络
- en: '[Mostafa Samir](https://mostafa-samir.github.io)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[Mostafa Samir](https://mostafa-samir.github.io)'
- en: So far we’ve seen how effective an RNN can be at solving a complex problem like
    machine translation. However, we’re still far from reaching its full potential!
    In [Chapter 9](ch09.xhtml#ch07) we mentioned that it’s theoretically proven that
    the RNN architecture is a universal functional representer; a more precise statement
    of the same result is that RNNs are *Turing complete*. This simply means that
    given proper wiring and adequate parameters, an RNN can learn to solve any computable
    problem, which is basically any problem that can be solved by a computer algorithm
    or, equivalently, a Turing machine.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到RNN在解决像机器翻译这样的复杂问题时有多么有效。然而，我们离其充分潜力还有很远！在[第9章](ch09.xhtml#ch07)中，我们提到RNN架构在理论上被证明是一个通用的功能表达器；同一结果的更精确陈述是RNN是*Turing完全*的。这只是意味着，给定适当的有线和充分的参数，RNN可以学会解决任何可计算问题，基本上是任何可以通过计算机算法或等效地图灵机解决的问题。
- en: Neural Turing Machines
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经图灵机
- en: Though theoretically possible, it’s extremely difficult to achieve that kind
    of universality in practice. This difficulty stems from the fact that we’re looking
    at an immensely huge search space of possible wirings and parameter values of
    RNNs, a space so vastly large for gradient descent to find an appropriate solution
    for any arbitrary problem. However, in this chapter we’ll start exploring some
    approaches at the edge of research that will allow us to start tapping into that
    potential.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在理论上可能，但在实践中实现这种普遍性非常困难。这种困难源于我们正在看一个巨大的可能性有线和RNN参数值的搜索空间，这个空间对于梯度下降来找到任意问题的适当解决方案来说是如此之大。然而，在本章中，我们将开始探索一些接近研究边缘的方法，这将使我们开始利用这种潜力。
- en: 'Let’s think for a while about a very simple reading comprehension question
    like the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下一个非常简单的阅读理解问题，就像下面这样：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The answer is so trivial: it’s two. But what actually happened in our brains
    that allowed us to come up with the answer so trivially? If we thought about how
    we could solve that comprehension question using a simple computer program, our
    approach would probably go like this:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是如此微不足道：是两。但是我们的大脑实际上发生了什么，使我们能够如此轻松地得出答案？如果我们考虑如何使用一个简单的计算机程序来解决那个理解问题，我们的方法可能会是这样的：
- en: '[PRE1]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: It turns out that our brains tackle the same task in a similar way to that simple
    computer program. Once we start reading, we start allocating memory (just as our
    computer program) and store the pieces of information we receive. We start by
    storing that location of Mary, which after the first sentence is the hallway.
    In the second sentence we store the objects Mary is carrying, and by now it’s
    only a glass of milk. Once we see the third sentence, our brain modifies the first
    memory location to point to the office. By the end of the fourth sentence, the
    second memory location is modified to include both the milk and the apple. When
    we finally encounter the question, our brains quickly query the second memory
    location and count the information there, which turns out to be two. In neuroscience
    and cognitive psychology, such a system of transient storing and manipulation
    of information is called a *working memory*, and it’s the main inspiration behind
    the line of research we’ll be discussing in the rest of this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 事实证明，我们的大脑以与那个简单的计算机程序类似的方式处理相同的任务。一旦我们开始阅读，我们就开始分配内存（就像我们的计算机程序一样），并存储我们收到的信息片段。我们首先存储玛丽的位置，第一句话后是走廊。在第二句中，我们存储玛丽携带的物品，到目前为止只有一杯牛奶。一旦我们看到第三句，我们的大脑修改第一个内存位置，指向办公室。到第四句结束时，第二个内存位置被修改为包括牛奶和苹果。当我们最终遇到问题时，我们的大脑迅速查询第二个内存位置，并计算那里的信息，结果是两个。在神经科学和认知心理学中，这种信息的瞬时存储和操作系统被称为*工作记忆*，这是我们将在本章剩余部分讨论的研究线的主要灵感来源。
- en: In 2014, Graves et al. from Google DeepMind started this line of work in a paper
    called [“Neural Turing Machines”](https://arxiv.org/abs/1410.5401) in which they
    introduced a new neural architecture with the same name, a *Neural Turing Machine*
    (NTM), that consists of a controller neural network (usually an RNN) with an external
    memory that resembles the brain’s working memory. For the close resemblance between
    the working memory model and the computer model we just saw, [Figure 12-1](#architecture_of_a_modern_day_computer)
    shows that the same resemblance holds for the NTM architecture, with the external
    memory in place of the RAM, the read/write heads in place of the read/write buses,
    and the controller network in place of the CPU, except for the fact that the controller
    learns its program, unlike the CPU, which is fed its program. [Figure 12-1](#architecture_of_a_modern_day_computer)
    has a single read head and a single write head, but an NTM can have several in
    practice.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，来自Google DeepMind的Graves等人在一篇名为[“神经图灵机”](https://arxiv.org/abs/1410.5401)的论文中开始了这一工作线，他们介绍了一种同名的新神经架构，*神经图灵机*（NTM），它由一个控制器神经网络（通常是RNN）和一个类似大脑工作记忆的外部存储器组成。由于工作记忆模型和我们刚刚看到的计算机模型之间的密切相似性，[图12-1](#architecture_of_a_modern_day_computer)显示了NTM架构的相同相似性，外部存储器取代了RAM，读/写头取代了读/写总线，控制器网络取代了CPU，唯一的区别是控制器学习其程序，而CPU则被输入其程序。[图12-1](#architecture_of_a_modern_day_computer)有一个读头和一个写头，但在实践中，NTM可以有几个。
- en: '![](Images/fdl2_1201.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1201.png)'
- en: Figure 12-1\. Comparing the architecture of a modern-day computer, which is
    fed its program (left) to an NTM that learns its program (right)
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1。比较现代计算机的架构，它被输入其程序（左）到一个学习其程序的NTM（右）
- en: 'If we thought about NTMs in light of our earlier discussion of RNN’s Turing
    completeness, we’ll find that augmenting the RNN with an external memory for transient
    storage prunes a large portion out of that search space, as we now don’t care
    about exploring RNNs that can both process and store the information; we’re just
    looking for the RNNs that can process the information stored outside of them.
    This pruning of the search space allows us to start tapping into some of the RNN
    potentials that were locked away before augmenting it with a memory, evident by
    the variety of tasks that the NTM could learn: from copying input sequences after
    seeing them, to emulating N-gram models, to performing a priority sort on data.
    We’ll even see by the end of the chapter how an extension to the NTM can learn
    to do reading comprehension tasks like the one we saw earlier, with nothing more
    than a gradient-based search.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据之前讨论的 RNN 的图灵完备性来思考 NTM，我们会发现，通过为瞬时存储增加外部内存来增强 RNN，可以剪枝掉搜索空间中的大部分内容，因为我们现在不再关心能够同时处理和存储信息的
    RNN，我们只是寻找能够处理存储在外部的信息的 RNN。这种搜索空间的剪枝使我们能够开始利用一些之前在增加内存之前被锁定的 RNN 潜力，这一点可以从 NTM
    能够学习的各种任务中看出：从在看到输入序列后复制它们，到模拟 N-gram 模型，再到对数据执行优先排序。我们甚至会在本章结束时看到，NTM 的扩展可以学会像我们之前看到的那样进行阅读理解任务，而这只需要基于梯度的搜索，没有其他更多的东西。
- en: Attention-Based Memory Access
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于注意力的内存访问
- en: To be able to train an NTM with a gradient-based search method, we need to make
    sure that the whole architecture is differentiable so that we can compute the
    gradient of some output loss with respect to the model’s parameters that process
    the input. This property is called *end-to-end-differentiable*, with one end being
    the inputs and the other the outputs. If we attempted to access the NTM’s memory
    in the same way a digital computer accesses its RAM, via discrete values of addresses,
    the discreteness of the addresses would introduce discontinuities in gradients
    of the output, and hence we would lose the ability to train the model with a gradient-based
    method. We need a continuous way to access the memory while being able to “focus”
    on a specific location in it. This kind of continuous focusing can be achieved
    via attention methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用基于梯度的搜索方法训练 NTM，我们需要确保整个架构是可微分的，这样我们就可以计算某个输出损失相对于处理输入的模型参数的梯度。这种属性被称为
    *端到端可微分*，一个端是输入，另一个端是输出。如果我们尝试以数字计算机访问其 RAM 的方式访问 NTM 的内存，通过地址的离散值，地址的离散性会引入输出梯度的不连续性，因此我们将失去使用基于梯度的方法训练模型的能力。我们需要一种连续的方式来访问内存，同时能够“聚焦”于其中的特定位置。这种连续的聚焦可以通过注意力方法实现。
- en: 'Instead of generating a discrete memory address, we let each head generate
    a normalized softmax attention vector with the same size as the number of memory
    locations. With this attention vector, we’ll be accessing all the memory locations
    at the same time in a blurry manner, with each value in the vector telling us
    how much we’re going to focus on the corresponding location, or how likely we’re
    going to access it. For example, to read a vector at a time step *t* out of our
    <math alttext="upper N times upper W"><mrow><mi>N</mi> <mo>×</mo> <mi>W</mi></mrow></math>
     NTM’s memory matrix denoted by <math alttext="upper M Subscript t"><msub><mi>M</mi>
    <mi>t</mi></msub></math>  (where *N* is the number of locations and <math alttext="upper
    W"><mi>W</mi></math>  is the size of the location), we generate an attention vector,
    or a weighting vector <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
     of size *N*, and our read vector can be calculated via the product:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不是生成离散的内存地址，而是让每个头部生成一个归一化的 softmax 注意力向量，大小与内存位置数量相同。通过这个注意力向量，我们将以模糊的方式同时访问所有内存位置，向量中的每个值告诉我们我们将如何专注于相应位置，或者我们将如何访问它的可能性。例如，要在时间步
    *t* 从我们的 <math alttext="upper N times upper W"><mrow><mi>N</mi> <mo>×</mo> <mi>W</mi></mrow></math>
     NTM 的内存矩阵中读取一个向量，记为 <math alttext="upper M Subscript t"><msub><mi>M</mi> <mi>t</mi></msub></math>
     （其中 *N* 是位置数量，<math alttext="upper W"><mi>W</mi></math>  是位置的大小），我们生成一个大小为 *N*
    的注意力向量，或者权重向量 <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    ，我们的读取向量可以通过以下乘积计算：
- en: <math alttext="bold r Subscript t Baseline equals upper M Subscript t Superscript
    down-tack Baseline w Subscript t"><mrow><msub><mi>𝐫</mi> <mi>t</mi></msub> <mo>=</mo>
    <msubsup><mi>M</mi> <mi>t</mi> <mi>⊤</mi></msubsup> <msub><mi>w</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold r Subscript t Baseline equals upper M Subscript t Superscript
    down-tack Baseline w Subscript t"><mrow><msub><mi>𝐫</mi> <mi>t</mi></msub> <mo>=</mo>
    <msubsup><mi>M</mi> <mi>t</mi> <mi>⊤</mi></msubsup> <msub><mi>w</mi> <mi>t</mi></msub></mrow></math>
- en: where <math alttext="Superscript down-tack"><msup><mi>⊤</mi></msup></math>  denotes
    the matrix transpose operation. [Figure 12-2](#demo_of_blurry_attention_based_reading)
    shows how with the weights attending to a specific location, we can retrieve a
    read vector that approximately contains the same information as the content of
    that memory location.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="Superscript down-tack"><msup><mi>⊤</mi></msup></math>  表示矩阵转置操作。[图
    12-2](#demo_of_blurry_attention_based_reading) 显示了通过权重关注特定位置，我们可以检索一个读取向量，其中大致包含与该内存位置内容相同的信息。
- en: '![](Images/fdl2_1202.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1202.png)'
- en: Figure 12-2\. A demonstration of how a blurry attention-based reading can retrieve
    a vector containing approximately the same information as in the focused-on location
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-2\. 演示了模糊注意力读取如何检索包含与聚焦位置中大致相同信息的向量
- en: 'A similar attention weighting method is used for the write head: a weighting
    vector <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    is generated and used for erasing specific information from the memory, as specified
    by the controller in an erase vector <math alttext="e Subscript t"><msub><mi>e</mi>
    <mi>t</mi></msub></math>  that has <math alttext="upper W"><mi>W</mi></math> values
    between 0 and 1 specifying what to erase and what to keep. Then we use the same
    weighting for writing to the erased memory matrix some new information, also specified
    by the controller in a write vector <math alttext="v Subscript t"><msub><mi>v</mi>
    <mi>t</mi></msub></math> containing <math alttext="upper W"><mi>W</mi></math>
     values:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 写头也使用类似的注意力加权方法：生成一个权重向量<math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>，用于从存储器中擦除特定信息，由控制器在擦除向量<math
    alttext="e Subscript t"><msub><mi>e</mi> <mi>t</mi></msub></math>中指定，该向量有<math
    alttext="upper W"><mi>W</mi></math>个值在0和1之间指定要擦除和要保留的内容。然后我们使用相同的权重将一些新信息写入已擦除的存储器矩阵，也由控制器在包含<math
    alttext="upper W"><mi>W</mi></math>个值的写入向量<math alttext="v Subscript t"><msub><mi>v</mi>
    <mi>t</mi></msub></math>中指定：
- en: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus w Subscript t Baseline e Subscript
    t Superscript down-tack Baseline right-parenthesis plus w Subscript t Baseline
    normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msub><mi>w</mi> <mi>t</mi></msub>
    <msubsup><mi>e</mi> <mi>t</mi> <mi>⊤</mi></msubsup> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>w</mi> <mi>t</mi></msub> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi>
    <mi>⊤</mi></msubsup></mrow></math>
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus w Subscript t Baseline e Subscript
    t Superscript down-tack Baseline right-parenthesis plus w Subscript t Baseline
    normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi> <mi>t</mi></msub>
    <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msub><mi>w</mi> <mi>t</mi></msub>
    <msubsup><mi>e</mi> <mi>t</mi> <mi>⊤</mi></msubsup> <mo>)</mo></mrow> <mo>+</mo>
    <msub><mi>w</mi> <mi>t</mi></msub> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi>
    <mi>⊤</mi></msubsup></mrow></math>
- en: where <math alttext="upper E"><mi>E</mi></math>  is a matrix of ones and <math
    alttext="ring"><mo>∘</mo></math> is element-wise multiplication. Similar to the
    reading case, the weighting <math alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>
    tells us where to focus our erasing (the first term of the equation) and writing
    operations (the second term).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="upper E"><mi>E</mi></math>是一个全为1的矩阵，<math alttext="ring"><mo>∘</mo></math>是逐元素乘法。与读取情况类似，权重<math
    alttext="w Subscript t"><msub><mi>w</mi> <mi>t</mi></msub></math>告诉我们在哪里集中我们的擦除（方程的第一项）和写入操作（方程的第二项）。
- en: NTM Memory Addressing Mechanisms
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: NTM存储器寻址机制
- en: Now that we understand how NTMs access their memories in a continuous manner
    via attention weighting, we’re left with how these weightings are generated and
    what forms of memory addressing mechanisms they represent. We can understand that
    by exploring what NTMs are expected to do with their memories, and based on the
    model they are mimicking (the Turning machine), we expect them to be able to access
    a location by the value it contains, and to be able to go forward or backward
    from a given location.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了NTM如何通过注意力加权以连续方式访问其存储器，我们还需要了解这些权重是如何生成的以及它们代表什么形式的存储器寻址机制。我们可以通过探索NTM被期望如何处理其存储器以及基于其模型（图灵机）的预期来理解这一点，我们期望它们能够通过其包含的值访问位置，并能够从给定位置向前或向后移动。
- en: 'The first mode of behavior can be achieved with an access mechanism that we’ll
    call *content-based addressing*. In this form of addressing, the controller emits
    the value that it’s looking for, which we’ll call a key <math alttext="k Subscript
    t"><msub><mi>k</mi> <mi>t</mi></msub></math> , then it measures its similarity
    to the information stored in each location and focuses the attention on the most
    similar one. This kind of weighting can be calculated via:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种行为模式可以通过我们将称之为*基于内容的寻址*的访问机制实现。在这种寻址形式中，控制器发出它正在寻找的值，我们将其称为关键<math alttext="k
    Subscript t"><msub><mi>k</mi> <mi>t</mi></msub></math>，然后测量它与存储在每个位置的信息的相似性，并将注意力集中在最相似的位置上。这种加权可以通过以下方式计算：
- en: '*C*(*M*,*k*, *β*) = <math alttext="StartFraction exp left-parenthesis beta
    script upper D left-parenthesis upper M comma k right-parenthesis right-parenthesis
    Over sigma-summation Underscript i equals 0 Overscript upper N Endscripts exp
    left-parenthesis beta script upper D left-parenthesis upper M left-bracket i right-bracket
    comma k right-parenthesis right-parenthesis EndFraction"><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><mi>β</mi><mi>𝒟</mi><mo>(</mo><mi>M</mi><mo>,</mo><mi>k</mi><mo>)</mo><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><mi>β</mi><mi>𝒟</mi><mrow><mo>(</mo><mi>M</mi><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>,</mo><mi>k</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></math>'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '*C*(*M*,*k*, *β*) = <math alttext="StartFraction exp left-parenthesis beta
    script upper D left-parenthesis upper M comma k right-parenthesis right-parenthesis
    Over sigma-summation Underscript i equals 0 Overscript upper N Endscripts exp
    left-parenthesis beta script upper D left-parenthesis upper M left-bracket i right-bracket
    comma k right-parenthesis right-parenthesis EndFraction"><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><mi>β</mi><mi>𝒟</mi><mo>(</mo><mi>M</mi><mo>,</mo><mi>k</mi><mo>)</mo><mo>)</mo></mrow>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <mo form="prefix">exp</mo><mrow><mo>(</mo><mi>β</mi><mi>𝒟</mi><mrow><mo>(</mo><mi>M</mi><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow><mo>,</mo><mi>k</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></math>'
- en: where *D* is some similarity measure, like the cosine similarity. The equation
    is nothing more than a normalized softmax distribution over the similarity scores.
    There is, however, an extra parameter <math alttext="beta"><mi>β</mi></math> that
    is used to attenuate the attention weights if needed. We call that the *key strength*.
    The main idea behind that parameter is that for some tasks, the key emitted by
    the controller may not be close to any of the information in the memory, which
    would result in seemingly uniform attention weights. [Figure 12-3](#indecisive_key_with_unit_strength_results)
    shows how the key strength allows the controller to learn how to attenuate such
    uniform attention to be more focused on a single location that is the most probable;
    the controller then learns what value of the strength to emit with each possible
    key it emits.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*D*是一些相似度度量，比如余弦相似度。该方程实际上只是相似度分数上的归一化softmax分布。然而，还有一个额外的参数<math alttext="beta"><mi>β</mi></math>，用于需要时减弱注意力权重。我们称之为*关键强度*。该参数背后的主要思想是，对于某些任务，控制器发出的关键可能与存储器中的任何信息都不接近，这将导致看似均匀的注意力权重。[图12-3](#indecisive_key_with_unit_strength_results)展示了关键强度如何使控制器学会如何减弱这种均匀的注意力，更加专注于最有可能的单个位置；然后控制器学习发出每个可能的关键时要发出的强度值。
- en: '![](Images/fdl2_1203.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1203.png)'
- en: Figure 12-3\. An indecisive key with unit strength results in a nearly uniform
    attention vector; increasing the strength for keys like that focuses the attention
    on the most probable location
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3。一个犹豫不决的关键具有单位强度，导致几乎均匀的注意力向量；增加像这样的关键的强度会将注意力集中在最可能的位置上
- en: 'To move forward and backward in the memory, we first need to know where we
    are we standing now, and such information is located in the access weighting from
    the last time step <math alttext="w Subscript t minus 1"><msub><mi>w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    . So to preserve the information about our current location with the new content-based
    weighting <math alttext="w Subscript t Superscript c"><msubsup><mi>w</mi> <mi>t</mi>
    <mi>c</mi></msubsup></math>  we just got, we interpolate between the two weighting
    using a scalar  <math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>
     that lies between 0 and 1:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要在内存中前进和后退，我们首先需要知道我们现在站在哪里，这样的信息位于上一个时间步的访问加权<math alttext="w Subscript t minus
    1"><msub><mi>w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>中。因此，为了保留关于我们当前位置的信息，我们使用一个介于0和1之间的标量<math
    alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>来插值新的基于内容的加权<math
    alttext="w Subscript t Superscript c"><msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup></math>和我们刚刚得到的加权之间：
- en: <math alttext="w Subscript t Superscript g Baseline equals g Subscript t Baseline
    w Subscript t Superscript c Baseline plus left-parenthesis 1 minus g Subscript
    t Baseline right-parenthesis w Subscript t minus 1"><mrow><msubsup><mi>w</mi>
    <mi>t</mi> <mi>g</mi></msubsup> <mo>=</mo> <msub><mi>g</mi> <mi>t</mi></msub>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>g</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript t Superscript g Baseline equals g Subscript t Baseline
    w Subscript t Superscript c Baseline plus left-parenthesis 1 minus g Subscript
    t Baseline right-parenthesis w Subscript t minus 1"><mrow><msubsup><mi>w</mi>
    <mi>t</mi> <mi>g</mi></msubsup> <mo>=</mo> <msub><mi>g</mi> <mi>t</mi></msub>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>c</mi></msubsup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <msub><mi>g</mi> <mi>t</mi></msub> <mo>)</mo></mrow> <msub><mi>w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
- en: We call  <math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>
     the *interpolation gate*, and it’s also emitted by the controller to control
    the kind of information we want to use in the current time step. When the gate’s
    value is close to 1, we favor the addressing given by content lookup. However,
    when it’s close to 0, we tend to pass the information about our current location
    through and ignore the content-based addressing. The controller learns to use
    this gate so that, for example, it could set it to 0 when iteration through consecutive
    locations is desired and information about the current location is crucial. The
    type of information the controller chooses to gate through is denoted by the *gated
    weighting*  <math alttext="w Subscript t Superscript g"><msubsup><mi>w</mi> <mi>t</mi>
    <mi>g</mi></msubsup></math> .
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称<math alttext="g Subscript t"><msub><mi>g</mi> <mi>t</mi></msub></math>为*插值门*，它也由控制器发出，用于控制我们在当前时间步中想要使用的信息类型。当门的值接近1时，我们更倾向于基于内容查找给出的寻址。然而，当它接近0时，我们倾向于传递关于我们当前位置的信息，并忽略基于内容的寻址。控制器学会使用这个门，例如，当需要迭代通过连续位置时，以及当前位置的信息至关重要时，可以将其设置为0。控制器选择通过的信息类型由*门控加权*
    <math alttext="w Subscript t Superscript g"><msubsup><mi>w</mi> <mi>t</mi> <mi>g</mi></msubsup></math>表示。
- en: 'To start moving around the memory we need a way to take our current gated weighting
    and shift the focus from one location to another. This can be done by convoluting
    the gated weighting with a *shift weighting* <math alttext="s Subscript t"><msub><mi>s</mi>
    <mi>t</mi></msub></math> , also emitted by the controller. This shift weighting
    is a normalized softmax attention vector of size  <math alttext="n plus 1"><mrow><mi>n</mi>
    <mo>+</mo> <mn>1</mn></mrow></math> , where  <math alttext="n"><mi>n</mi></math>
     is an even integer specifying the number of possible shifts around the focused-on
    location in the gated weighting; for example, if it has a size of 3, then there
    are two possible shifts around a location: one forward and one backward. [Figure 12-4](#shift_weighting_focused_on_the_rights)
    shows how a shift weighting can move around the focused-on location in gated weighting.
    The shifting occurs by convoluting the gated weighting by the shift weighting
    in pretty much the same way we convoluted images with feature maps back in [Chapter 7](ch07.xhtml#convolutional_neural_networks).
    The only exception is how we handle the case when the shift weightings go outside
    the gated weighting. Instead of using padding like we did before, we use a rotational
    convolution operator where overflown weights get applied to the values at the
    other end of the gated weighting, as shown in the middle panel of [Figure 12-4](#shift_weighting_focused_on_the_rights).
    This operation can be expressed element-wise as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始在内存中移动，我们需要一种方法来获取当前的门控加权，并将焦点从一个位置转移到另一个位置。这可以通过将门控加权与由控制器发出的*移位加权* <math
    alttext="s Subscript t"><msub><mi>s</mi> <mi>t</mi></msub></math> 进行卷积来实现，这个移位加权是一个大小为<math
    alttext="n plus 1"><mrow><mi>n</mi> <mo>+</mo> <mn>1</mn></mrow></math>的归一化softmax注意力向量，其中<math
    alttext="n"><mi>n</mi></math>是一个指定门控加权中焦点周围可能移动的次数的偶数整数；例如，如果它的大小为3，则在一个位置周围有两个可能的移位：一个向前，一个向后。[图12-4](#shift_weighting_focused_on_the_rights)展示了移位加权如何在门控加权中移动焦点。移位是通过将门控加权与移位加权进行卷积来实现的，这与我们在[第7章](ch07.xhtml#convolutional_neural_networks)中用特征图卷积图像的方式几乎相同。唯一的例外是当移位加权超出门控加权时我们如何处理的情况。与之前使用填充不同，我们使用旋转卷积运算符，其中溢出的权重被应用于门控加权另一端的值，如[图12-4](#shift_weighting_focused_on_the_rights)的中间面板所示。这个操作可以逐元素地表示为：
- en: <math alttext="ModifyingAbove w With tilde Subscript t Baseline left-bracket
    i right-bracket equals sigma-summation Underscript j equals 0 Overscript StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue Endscripts w Subscript t Superscript g
    Baseline left-bracket left-parenthesis i plus StartFraction StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue minus 1 Over 2 EndFraction minus j right-parenthesis
    mod upper N right-bracket s Subscript t Baseline left-bracket j right-bracket"><mrow><msub><mover
    accent="true"><mi>w</mi> <mo>˜</mo></mover> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi> <mi>t</mi></msub> <mrow><mo>|</mo></mrow></mrow></msubsup>
    <msubsup><mi>w</mi> <mi>t</mi> <mi>g</mi></msubsup> <mfenced separators="" open="["
    close="]"><mfenced separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mfrac><mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi>
    <mi>t</mi></msub> <mrow><mo>|</mo><mo>-</mo><mn>1</mn></mrow></mrow> <mn>2</mn></mfrac>
    <mo>-</mo> <mi>j</mi></mfenced> <mo form="prefix">mod</mo> <mi>N</mi></mfenced>
    <msub><mi>s</mi> <mi>t</mi></msub> <mfenced open="[" close="]"><mi>j</mi></mfenced></mrow></math>![](Images/fdl2_1204.png)
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove w With tilde Subscript t Baseline left-bracket
    i right-bracket equals sigma-summation Underscript j equals 0 Overscript StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue Endscripts w Subscript t Superscript g
    Baseline left-bracket left-parenthesis i plus StartFraction StartAbsoluteValue
    s Subscript t Baseline EndAbsoluteValue EndFraction minus 1 Over 2 EndFraction
    minus j right-parenthesis mod upper N right-bracket s Subscript t Baseline left-bracket
    j right-bracket"><mrow><msub><mover accent="true"><mi>w</mi> <mo>˜</mo></mover>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow> <mo>=</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>0</mn></mrow> <mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi>
    <mi>t</mi></msub> <mrow><mo>|</mo></mrow></mrow></msubsup> <msubsup><mi>w</mi>
    <mi>t</mi> <mi>g</mi></msubsup> <mfenced separators="" open="[" close="]"><mfenced
    separators="" open="(" close=")"><mi>i</mi> <mo>+</mo> <mfrac><mrow><mrow><mo>|</mo></mrow><msub><mi>s</mi>
    <mi>t</mi></msub> <mrow><mo>|</mo><mo>-</mo><mn>1</mn></mrow></mrow> <mn>2</mn></mfrac>
    <mo>-</mo> <mi>j</mi></mfenced> <mo form="prefix">mod</mo> <mi>N</mi></mfenced>
    <msub><mi>s</mi> <mi>t</mi></msub> <mfenced open="[" close="]"><mi>j</mi></mfenced></mrow></math>![](Images/fdl2_1204.png)
- en: Figure 12-4\. A shift weighting focused on the right shifts the gated weighting
    one location to the right (left). Rotational convolution on a left-focused shift
    weighting, shifting the gated weighting to the left (middle). A nonsharp centered
    shift weighting keeps the gated weighting intact but disperses it (right).
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 右侧集中的移位权重将门控权重向右移动一个位置（左侧）。左侧集中的移位权重上的旋转卷积，将门控权重向左移动（中间）。一个非尖锐的中心移位权重保持门控权重不变，但会使其散开（右侧）。
- en: 'With the introduction of the shifting operation, our heads’ weightings can
    now move around the memory freely forward and backward. However, a problem occurs
    if at any time the shift weighting is not sharp enough. Because of the nature
    of the convolution operation, a nonsharp shift weighting (as in the right panel
    of [Figure 12-4](#shift_weighting_focused_on_the_rights)) disperses the original
    gated weightings around its surroundings and results in a less-focused shifted
    weighting. To overcome that blurring effect, we run the shifted weightings through
    one last operation: a sharpening operation. The controller emits one last scalar 
    <math alttext="gamma Subscript t Baseline greater-than-or-equal-to 1"><mrow><msub><mi>γ</mi>
    <mi>t</mi></msub> <mo>≥</mo> <mn>1</mn></mrow></math>  that sharpens the shifted
    weightings via:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 引入了移位操作后，我们的头部权重现在可以自由地在内存中前后移动。然而，如果在任何时候移位权重不够尖锐，就会出现问题。由于卷积操作的性质，一个非尖锐的移位权重（如[图12-4](#shift_weighting_focused_on_the_rights)右侧面板所示）会使原始的门控权重在周围散开，导致移位权重不够集中。为了克服这种模糊效果，我们通过最后一个操作来运行移位权重：一个锐化操作。控制器发出一个最后的标量
    <math alttext="gamma Subscript t Baseline greater-than-or-equal-to 1"><mrow><msub><mi>γ</mi>
    <mi>t</mi></msub> <mo>≥</mo> <mn>1</mn></mrow></math> 来通过以下方式锐化移位权重：
- en: <math alttext="w Subscript t Baseline equals StartFraction w overTilde Subscript
    t Superscript gamma Super Subscript t Superscript Baseline Over sigma-summation
    Underscript i equals 0 Overscript upper N Endscripts w overTilde Subscript t Baseline
    left-bracket i right-bracket Superscript gamma Super Subscript t Superscript Baseline
    EndFraction"><mrow><msub><mi>w</mi> <mi>t</mi></msub> <mo>=</mo> <mfrac><msubsup><mover
    accent="true"><mi>w</mi> <mo>˜</mo></mover> <mi>t</mi> <msub><mi>γ</mi> <mi>t</mi></msub></msubsup>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <msub><mover accent="true"><mi>w</mi> <mo>˜</mo></mover> <mi>t</mi></msub> <msup><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow>
    <msub><mi>γ</mi> <mi>t</mi></msub></msup></mrow></mfrac></mrow></math>
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="w Subscript t Baseline equals StartFraction w overTilde Subscript
    t Superscript gamma Super Subscript t Superscript Baseline Over sigma-summation
    Underscript i equals 0 Overscript upper N Endscripts w overTilde Subscript t Baseline
    left-bracket i right-bracket Superscript gamma Super Subscript t Superscript Baseline
    EndFraction"><mrow><msub><mi>w</mi> <mi>t</mi></msub> <mo>=</mo> <mfrac><msubsup><mover
    accent="true"><mi>w</mi> <mo>˜</mo></mover> <mi>t</mi> <msub><mi>γ</mi> <mi>t</mi></msub></msubsup>
    <mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow> <mi>N</mi></msubsup>
    <msub><mover accent="true"><mi>w</mi> <mo>˜</mo></mover> <mi>t</mi></msub> <msup><mrow><mo>[</mo><mi>i</mi><mo>]</mo></mrow>
    <msub><mi>γ</mi> <mi>t</mi></msub></msup></mrow></mfrac></mrow></math>
- en: 'Starting from interpolation down to the final weighting vector out of sharpening,
    this process constitutes the second addressing mechanism of NTMs: the *location-based
    mechanism*. Using a combination of both addressing mechanisms, an NTM is able
    to utilize its memory to learn to solve various tasks. One of these tasks that
    would allow us to get a deeper look into the NTM in action is the copy task shown
    in [Figure 12-5](#viz_of_an_ntm_trained_on_the_copy_task). In this task, we present
    the model with a sequence of random binary vectors that terminate with a special
    end symbol. We then request the same input sequence to be copied to the output.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 从插值到最终的锐化权重向量，这个过程构成了NTM的第二个寻址机制：*基于位置的机制*。通过结合这两种寻址机制，NTM能够利用其内存来学习解决各种任务。其中一个任务是让我们更深入地了解NTM的工作原理的复制任务，如[图12-5](#viz_of_an_ntm_trained_on_the_copy_task)所示。在这个任务中，我们向模型呈现一系列以特殊结束符号结束的随机二进制向量。然后我们要求将相同的输入序列复制到输出中。
- en: '![](Images/fdl2_1205.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1205.png)'
- en: Figure 12-5\. An NTM trained on the copy task^([1](ch12.xhtml#idm45934167096448))
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5\. 在复制任务上训练的NTM^([1](ch12.xhtml#idm45934167096448))
- en: The visualization shows how at the input time, the NTM starts writing the inputs
    step-by-step into consecutive locations in the memory. In the output time, the
    NTM goes back at the first written vector and iterates through the next locations
    to read and return the previously written input sequence. The original NTM paper
    contains several other visualizations of NTMs trained on different problems that
    are worth checking. These visualizations demonstrate the architecture’s ability
    to utilize the addressing mechanisms to adapt to and learn to solve various tasks.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化显示了在输入时间，NTM开始逐步将输入写入内存中连续的位置。在输出时间，NTM回到第一个写入的向量，并遍历下一个位置以读取并返回先前写入的输入序列。原始的NTM论文包含了几个在不同问题上训练的NTM的可视化，值得一看。这些可视化展示了该架构利用寻址机制来适应和学习解决各种任务的能力。
- en: We’ll suffice with our current understanding of NTMs and skip its implementation.
    Instead, we will spend the rest of the chapter exploring the drawbacks of NTMs
    and how the novel architecture of the differentiable neural computer (DNC) was
    able to overcome these drawbacks. We’ll conclude our discussion by implementing
    that novel architecture on simple reading comprehension tasks like the one we
    saw earlier.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将满足于我们对NTMs的当前理解，并跳过其实现。相反，我们将在本章的其余部分探讨NTMs的缺点，以及可微分神经计算机（DNC）的新颖架构是如何克服这些缺点的。我们将通过在简单的阅读理解任务上实现这种新颖架构来结束我们的讨论，就像我们之前看到的那样。
- en: Differentiable Neural Computers
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可微分神经计算机
- en: Despite the power of NTMs, they have a few limitations regarding their memory
    mechanisms. The first of these limitations is that NTMs have no way to ensure
    that no interference or overlap between written data would occur. This is due
    to the nature of the “differentiable” writing operation in which we write new
    data everywhere in the memory to some extent specified by the attention. Usually,
    the attention mechanisms learn to focus the write weightings strongly on a single
    memory location, and the NTM converges to a mostly interference-free behavior,
    but that’s not guaranteed.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管NTMs的功能强大，但它们在内存机制方面存在一些限制。其中之一是NTMs无法确保写入数据之间不会发生干扰或重叠。这是由于“可微分”写入操作的性质，其中我们在内存中的各个位置写入新数据，程度由注意力机制指定。通常，注意机制会学习将写入权重强烈集中在单个内存位置上，NTM会收敛到大部分无干扰行为，但这并不是保证。
- en: 'However, even when the NTM converges to an interference-free behavior, once
    a memory location has been written to, there’s no way to reuse that location again,
    even when the data stored in it becomes irrelevant. The inability to free and
    reuse memory locations is the second limitation of the NTM architecture. This
    results in new data being written to new locations that are likely to be contiguous,
    as we saw with the copy task. This contiguous writing fashion is the only way
    for an NTM to record any temporal information about the data being written: consecutive
    data is stored in consecutive locations. If the write head jumped to another place
    in the memory while writing some consecutive data, a read head won’t be able to
    recover the temporal link between the data written before and after the jump:
    this constitutes the third limitation of NTMs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使NTM收敛到无干扰行为，一旦一个内存位置被写入，即使其中存储的数据变得无关紧要，也无法再次重用该位置。无法释放和重用内存位置是NTM架构的第二个限制。这导致新数据被写入可能是连续的新位置，就像我们在复制任务中看到的那样。这种连续的写入方式是NTM记录有关正在写入的数据的任何时间信息的唯一方式：连续的数据存储在连续的位置。如果写头在写入一些连续数据时跳到内存中的另一个位置，读头将无法恢复跳跃前后写入的数据之间的时间链接：这构成了NTMs的第三个限制。
- en: 'In October 2016, Graves et al. from DeepMind published in *Nature* a paper
    titled, [“Hybrid Computing Using a Neural Network with Dynamic External Memory,”](http://go.nature.com/2peM8m2)
    in which they introduced a new memory-augmented neural architecture called *differentiable
    neural computer* (DNC) that improves on NTMs and addresses the limitations we
    just discussed. Similar to NTMs, DNCs consists of a controller that interacts
    with an external memory. The memory consists of *N* words of size <math alttext="upper
    W"><mi>W</mi></math> , making up an <math alttext="upper N times upper W"><mrow><mi>N</mi>
    <mo>×</mo> <mi>W</mi></mrow></math>  matrix we’ll be calling  *M*. The controller
    takes in an input vector of size <math alttext="upper X"><mi>X</mi></math> and
    the *R* vectors of size <math alttext="upper W"><mi>W</mi></math>  read from memory
    in the previous step, where *R* is the number of read heads. The controller then
    processes them through a neural network and returns two pieces of information:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年10月，DeepMind的Graves等人在《自然》杂志上发表了一篇题为[“使用具有动态外部存储器的神经网络进行混合计算”](http://go.nature.com/2peM8m2)的论文，在其中介绍了一种名为*可微分神经计算机*（DNC）的新记忆增强神经架构，改进了NTMs并解决了我们刚刚讨论的限制。与NTMs类似，DNC由一个与外部内存交互的控制器组成。内存由大小为<math
    alttext="upper W"><mi>W</mi></math>的*N*个单词组成，构成我们将称之为*M*的<math alttext="upper
    N times upper W"><mrow><mi>N</mi> <mo>×</mo> <mi>W</mi></mrow></math>矩阵。控制器接收大小为<math
    alttext="upper X"><mi>X</mi></math>的输入向量和上一步从内存中读取的大小为<math alttext="upper W"><mi>W</mi></math>的*R*个向量，其中*R*是读头的数量。然后，控制器通过神经网络处理它们，并返回两个信息：
- en: An *interface vector* that contains all the necessary information to query the
    memory (i.e., write and read from it)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含查询内存所需的所有信息的*接口*向量（即，写入和读取）
- en: A *pre-output* vector of size *Y*
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个大小为*Y*的*预输出*向量
- en: The external memory then takes in the interface vector, performs the necessary
    writing through a single write head, then reads *R* new vectors from the memory.
    It returns the newly read vectors to the controller to be added with the pre-output
    vector, producing the final output vector of size *Y*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，外部内存接收接口向量，通过单个写头执行必要的写入，然后从内存中读取*R*个新向量。它将新读取的向量返回给控制器，与预输出向量相加，生成大小为*Y*的最终输出向量。
- en: '[Figure 12-6](#overview_of_dncs_architecture_and_operation) summarizes the
    operation of the DNC that we just described. We can see that unlike NTMs, DNCs
    keep other data structures alongside the memory itself to keep track of the state
    of the memory. As we’ll shortly see, with these data structures and some clever
    new attention mechanisms, DNCs are able to successfully overcome NTM’s limitations.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-6](#overview_of_dncs_architecture_and_operation)总结了我们刚刚描述的DNC的操作。我们可以看到，与NTMs不同，DNC保留了除内存本身之外的其他数据结构，以跟踪内存的状态。正如我们将很快看到的，借助这些数据结构和一些巧妙的新注意机制，DNC能够成功地克服NTM的局限性。'
- en: '![](Images/fdl2_1206.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1206.png)'
- en: Figure 12-6\. An overview of DNC’s architecture and operation
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6。DNC的架构和操作概述
- en: 'To make the whole architecture differentiable, DNCs access the memory through
    weight vectors of size *N* whose elements determine how much the heads focus on
    each memory location. There are *R* weightings for the read heads  <math alttext="normal
    w Subscript t Superscript r comma 1 Baseline comma ellipsis comma normal w Subscript
    t Superscript r comma upper R"><mrow><msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    where  <math alttext="t"><mi>t</mi></math> denotes the time step. On the other
    hand, there’s one write weighting  <math alttext="normal w Subscript t Superscript
    w"><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
     for the single write head. Once we obtain these weightings, we can modify the
    memory matrix and get updated via:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使整个架构可微分，DNCs通过大小为*N*的权重向量访问内存，其元素确定头部在每个内存位置上的关注程度。读头有*R*个权重 <math alttext="normal
    w Subscript t Superscript r comma 1 Baseline comma ellipsis comma normal w Subscript
    t Superscript r comma upper R"><mrow><msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>，其中
    <math alttext="t"><mi>t</mi></math> 表示时间步。另一方面，单个写头有一个写权重 <math alttext="normal
    w Subscript t Superscript w"><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math>。一旦我们获得这些权重，我们可以通过以下方式修改内存矩阵并进行更新：
- en: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus normal w Subscript t Superscript
    w Baseline e Subscript t Superscript down-tack Baseline right-parenthesis plus
    normal w Subscript t Superscript w Baseline normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <msubsup><mi>e</mi> <mi>t</mi> <mi>⊤</mi></msubsup>
    <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi> <mi>⊤</mi></msubsup></mrow></math>
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper M Subscript t Baseline equals upper M Subscript t minus
    1 Baseline ring left-parenthesis upper E minus normal w Subscript t Superscript
    w Baseline e Subscript t Superscript down-tack Baseline right-parenthesis plus
    normal w Subscript t Superscript w Baseline normal v Subscript t Superscript down-tack"><mrow><msub><mi>M</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <mrow><mo>(</mo> <mi>E</mi> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <msubsup><mi>e</mi> <mi>t</mi> <mi>⊤</mi></msubsup>
    <mo>)</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msubsup><mi mathvariant="normal">v</mi> <mi>t</mi> <mi>⊤</mi></msubsup></mrow></math>
- en: and <math alttext="e Subscript t Baseline comma normal v Subscript t Baseline"><mrow><msub><mi>e</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi mathvariant="normal">v</mi> <mi>t</mi></msub></mrow></math>
     are the *erase* and *write* vectors we saw earlier with NTMs, coming from the
    controller through the interface vector as instructions about what to erase from
    and write to the memory.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 而 <math alttext="e Subscript t Baseline comma normal v Subscript t Baseline"><mrow><msub><mi>e</mi>
    <mi>t</mi></msub> <mo>,</mo> <msub><mi mathvariant="normal">v</mi> <mi>t</mi></msub></mrow></math>
    是我们之前在NTMs中看到的*擦除*和*写入*向量，通过控制器通过接口向量传递指令，指示要从内存中擦除和写入的内容。
- en: 'As soon as we get the updated memory matrix  <math alttext="upper M Subscript
    t"><msub><mi>M</mi> <mi>t</mi></msub></math> , we can read out the new read vectors 
    <math alttext="normal r Subscript t Superscript 1 Baseline comma normal r Subscript
    t Superscript 2 Baseline comma ellipsis comma normal r Subscript t Superscript
    upper R"><mrow><msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup>
    <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math>  using the following equation for each read
    weighting:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得更新后的内存矩阵 <math alttext="upper M Subscript t"><msub><mi>M</mi> <mi>t</mi></msub></math>，我们可以通过以下方程式读取新的读向量
    <math alttext="normal r Subscript t Superscript 1 Baseline comma normal r Subscript
    t Superscript 2 Baseline comma ellipsis comma normal r Subscript t Superscript
    upper R"><mrow><msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup>
    <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>2</mn></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math>，对每个读权重使用以下方程式：
- en: <math alttext="normal r Subscript t Superscript i Baseline equals upper M Subscript
    t Superscript down-tack Baseline normal w Subscript t Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">r</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi>M</mi>
    <mi>t</mi> <mi>⊤</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal r Subscript t Superscript i Baseline equals upper M Subscript
    t Superscript down-tack Baseline normal w Subscript t Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">r</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi>M</mi>
    <mi>t</mi> <mi>⊤</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
- en: Up until now, it seems that there’s nothing different from how NTMs write to
    and read from memory. However, the differences will start to show up when we discuss
    the attention mechanisms DNCs use to obtain their access weightings. While they
    both share the content-based addressing mechanism *C*(*M*, *k*, *β*) defined earlier,
    DNCs use more sophisticated mechanisms to attend more efficiently to the memory.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，似乎与NTMs写入和读取内存的方式没有什么不同。然而，当我们讨论DNCs用于获取访问权重的注意机制时，差异将开始显现。虽然它们都共享之前定义的基于内容的寻址机制
    *C*(*M*, *k*, *β*)，但DNCs使用更复杂的机制更有效地关注内存。
- en: Interference-Free Writing in DNCs
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNC中的无干扰写入
- en: The first limitation we discussed for NTMs was their inability to ensure an
    interference-free writing behavior. An intuitive way to address this issue is
    to design the architecture to focus strongly on a single, free memory location
    and not wait for NTM to learn to do so. To keep track of which locations are free
    and which are busy, we need to introduce a new data structure that can hold this
    kind of information. We’ll call it the *usage vector.*
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论NTMs的第一个限制是它们无法确保无干扰的写入行为。解决这个问题的直观方法是设计架构，强烈关注单个空闲内存位置，而不是等待NTM学习如何做到这一点。为了跟踪哪些位置是空闲的，哪些是忙碌的，我们需要引入一种新的数据结构来保存这种信息。我们将其称为*使用向量*。
- en: The usage vector <math alttext="normal u Subscript t"><msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub></math>  is a vector of size *N*, where each element holds a
    value between 0 and 1 that represents how much of the corresponding memory location
    is used; with 0 indicating a completely free location, and 1 indicating a completely
    used one.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量 <math alttext="normal u Subscript t"><msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub></math> 是一个大小为*N*的向量，其中每个元素的值介于0和1之间，表示相应内存位置的使用程度；0表示完全空闲的位置，1表示完全使用的位置。
- en: 'The usage vector initially contains zeros  <math alttext="normal u 0 equals
    bold 0"><mrow><msub><mi mathvariant="normal">u</mi> <mn>0</mn></msub> <mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></math> and gets updated with the usage information
    across the steps. Using this information, it’s clear that the location to which
    the weights should attend most strongly is the one with the least usage value.
    To obtain such weighting, we need to first sort the usage vector and obtain the
    list of location indices in ascending order of the usage; we call such a list
    a *free list* and denote it by  <math alttext="phi Subscript t"><msub><mi>φ</mi>
    <mi>t</mi></msub></math> . Using that free list, we can construct an intermediate
    weighting called the *allocation weighting  <math alttext="normal a Subscript
    t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>* that would
    determine which memory location should be allocated for new data. We calculate 
    <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>
     using:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用向量最初包含零 <math alttext="normal u 0 equals bold 0"><mrow><msub><mi mathvariant="normal">u</mi>
    <mn>0</mn></msub> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> 并随着步骤的进行而更新使用信息。利用这些信息，很明显权重应该最强烈地关注的位置是具有最小使用值的位置。为了获得这样的权重，我们需要首先对使用向量进行排序，并按照使用量的升序获得位置索引列表；我们称这样的列表为*自由列表*，并用
    <math alttext="phi Subscript t"><msub><mi>φ</mi> <mi>t</mi></msub></math> 表示。利用这个自由列表，我们可以构建一个称为*分配权重
    <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub></math>*
    的中间权重，用于确定应该为新数据分配哪个内存位置。我们通过以下方式计算 <math alttext="normal a Subscript t"><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub></math>：
- en: <math alttext="normal a Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket j right-bracket right-bracket equals left-parenthesis 1 minus normal
    u Subscript t Baseline left-bracket phi Subscript t Baseline left-bracket j right-bracket
    right-bracket right-parenthesis product Underscript i equals 1 Overscript j minus
    1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket i right-bracket right-bracket where j element-of 1 comma ellipsis
    comma upper N"><mrow><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>φ</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>]</mo></mrow> <mtext>where</mtext> <mi>j</mi>
    <mo>∈</mo> <mn>1</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>N</mi></mrow></math>
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal a Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket j right-bracket right-bracket equals left-parenthesis 1 minus normal
    u Subscript t Baseline left-bracket phi Subscript t Baseline left-bracket j right-bracket
    right-bracket right-parenthesis product Underscript i equals 1 Overscript j minus
    1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript t Baseline
    left-bracket i right-bracket right-bracket where j element-of 1 comma ellipsis
    comma upper N"><mrow><msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>φ</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow>
    <mo>)</mo></mrow> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup> <msub><mi mathvariant="normal">u</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>]</mo></mrow> <mo>]</mo></mrow> <mtext>where</mtext> <mi>j</mi>
    <mo>∈</mo> <mn>1</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <mi>N</mi></mrow></math>
- en: 'This equation may look incomprehensible at first glance. A good way to understand
    it is to work through it with a numerical example, for example, when  <math alttext="normal
    u Subscript t Baseline equals left-bracket 1 comma 0.7 comma 0.2 comma 0.4 right-bracket"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>7</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>]</mo></mrow></mrow></math> .
    We’ll leave the details for you to go through. In the end, you should arrive at
    the allocation weighting being  <math alttext="normal a Subscript t Baseline equals
    left-bracket 0 comma 0.024 comma 0.8 comma 0.12 right-bracket"><mrow><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>0</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>024</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo>
    <mn>8</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>12</mn> <mo>]</mo></mrow></mrow></math>
    . As we go through the calculations, we’ll begin to understand how this formula
    works: the <math alttext="1 minus normal u Subscript t Baseline left-bracket phi
    Subscript t Baseline left-bracket j right-bracket right-bracket"><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <msub><mi>φ</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow>
    <mo>]</mo></mrow></mrow></math> makes the location weight proportional to how
    free it is. By noticing that the product  <math alttext="product Underscript i
    equals 1 Overscript j minus 1 Endscripts normal u Subscript t Baseline left-bracket
    phi Subscript t Baseline left-bracket j right-bracket right-bracket"><mrow><msubsup><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    gets smaller and smaller as we iterate through the free list (because we keep
    multiplying small values between 0 and 1), we can see that this product decreases
    the location weight even more as we go from the least used location to the most
    used one, which finally results in the least used location having the largest
    weight, while the most used one gets the smallest weight. So we’re able to guarantee
    the ability to focus on a single location by design without the the need to hope
    for the model to learn it on its own from scratch; this means more reliability
    as well as faster training time.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程乍一看可能难以理解。理解它的一个好方法是通过一个数值示例来逐步解释，例如，当 <math alttext="normal u Subscript
    t Baseline equals left-bracket 1 comma 0.7 comma 0.2 comma 0.4 right-bracket"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>1</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>7</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>2</mn>
    <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>4</mn> <mo>]</mo></mrow></mrow></math> 时。我们将留下细节让您自行查看。最终，您应该得出分配权重为
    <math alttext="normal a Subscript t Baseline equals left-bracket 0 comma 0.024
    comma 0.8 comma 0.12 right-bracket"><mrow><msub><mi mathvariant="normal">a</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <mn>0</mn> <mo>,</mo> <mn>0</mn>
    <mo>.</mo> <mn>024</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>8</mn> <mo>,</mo>
    <mn>0</mn> <mo>.</mo> <mn>12</mn> <mo>]</mo></mrow></mrow></math> 。当我们进行计算时，我们将开始理解这个公式的工作原理：
    <math alttext="1 minus normal u Subscript t Baseline left-bracket phi Subscript
    t Baseline left-bracket j right-bracket right-bracket"><mrow><mn>1</mn> <mo>-</mo>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    使得位置权重与其自由程度成比例。通过注意到乘积 <math alttext="product Underscript i equals 1 Overscript
    j minus 1 Endscripts normal u Subscript t Baseline left-bracket phi Subscript
    t Baseline left-bracket j right-bracket right-bracket"><mrow><msubsup><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mrow><mi>j</mi><mo>-</mo><mn>1</mn></mrow></msubsup>
    <msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <msub><mi>φ</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>]</mo></mrow></mrow></math>
    随着我们在自由列表中迭代变得越来越小（因为我们不断地将介于0和1之间的小值相乘），我们可以看到这个乘积会使位置权重在从最不常用的位置到最常用的位置时减小得更多，最终导致最不常用的位置具有最大的权重，而最常用的位置具有最小的权重。因此，我们能够通过设计来保证能够专注于单个位置，而无需希望模型能够自行从头开始学习；这意味着更可靠以及更快的训练时间。
- en: 'With the allocation weighting  <math alttext="normal a Subscript t"><msub><mi
    mathvariant="normal">a</mi> <mi>t</mi></msub></math> and lookup weighting  <math
    alttext="normal c Subscript t Superscript w"><msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></math>  we get from the content-based addressing
    mechanism <math alttext="normal c Subscript t Superscript w Baseline equals script
    upper C left-parenthesis upper M Subscript t minus 1 Baseline comma k Subscript
    t Superscript w Baseline comma beta Subscript t Superscript w Baseline right-parenthesis"><mrow><msubsup><mi
    mathvariant="normal">c</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>𝒞</mi>
    <mrow><mo>(</mo> <msub><mi>M</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>,</mo> <msubsup><mi>k</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>,</mo> <msubsup><mi>β</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow></mrow></math> , where  <math
    alttext="k Subscript t Superscript w Baseline comma beta Subscript t Superscript
    w"><mrow><msubsup><mi>k</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>,</mo> <msubsup><mi>β</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math>  are the lookup key and the lookup
    strength we receive through the interface vector, we can now construct our final
    write weighting:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从基于内容的寻址机制中获得的分配权重 <math alttext="normal a Subscript t"><msub><mi mathvariant="normal">a</mi>
    <mi>t</mi></msub></math> 和查找权重 <math alttext="normal c Subscript t Superscript
    w"><msubsup><mi mathvariant="normal">c</mi> <mi>t</mi> <mi>w</mi></msubsup></math>，我们现在可以构建我们的最终写入权重：
- en: <math alttext="normal w Subscript t Superscript w Baseline equals g Subscript
    t Superscript w Baseline left-bracket g Subscript t Superscript a Baseline normal
    a Subscript t Baseline plus left-parenthesis 1 minus g Subscript t Superscript
    a Baseline right-parenthesis normal c Subscript t Superscript w Baseline right-bracket"><mrow><msubsup><mi
    mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <msubsup><mi>g</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mfenced separators="" open="[" close="]"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup> <msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>g</mi> <mi>t</mi>
    <mi>a</mi></msubsup> <mo>)</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mfenced></mrow></math>
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal w Subscript t Superscript w Baseline equals g Subscript
    t Superscript w Baseline left-bracket g Subscript t Superscript a Baseline normal
    a Subscript t Baseline plus left-parenthesis 1 minus g Subscript t Superscript
    a Baseline right-parenthesis normal c Subscript t Superscript w Baseline right-bracket"><mrow><msubsup><mi
    mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <msubsup><mi>g</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mfenced separators="" open="[" close="]"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup> <msub><mi mathvariant="normal">a</mi> <mi>t</mi></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi>g</mi> <mi>t</mi>
    <mi>a</mi></msubsup> <mo>)</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mfenced></mrow></math>
- en: where  <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math> and <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math>  are values between 0 and 1 and are called
    the write and allocation gates, which we also get from the controller through
    the interface vector. These gates control the writing operation, with  <math alttext="g
    Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
     determining if any writing is going to happen in the first place, and <math alttext="g
    Subscript t Superscript a"><msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup></math>
     specifying whether we’ll write to a new location using the allocation weighting
    or modify an existing value specified by the lookup weighting.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi>
    <mi>w</mi></msubsup></math> 和 <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math> 是介于0和1之间的值，称为写入门和分配门，我们也从控制器通过接口向量中获取。这些门控制写入操作，其中
    <math alttext="g Subscript t Superscript w"><msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup></math>
    确定首先是否会发生任何写入，而 <math alttext="g Subscript t Superscript a"><msubsup><mi>g</mi>
    <mi>t</mi> <mi>a</mi></msubsup></math> 指定我们是否将使用分配权重写入新位置，或者修改由查找权重指定的现有值。
- en: DNC Memory Reuse
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNC内存重用
- en: What if while we calculate the allocation weighting we find that all locations
    are used, or in other words,  <math alttext="normal u Subscript t Baseline equals
    bold 1"><mrow><msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo>
    <mn mathvariant="bold">1</mn></mrow></math> ? This means that the allocation weightings
    will turn out all zeros and no new data can be allocated to memory. This raises
    the need for the ability to free and reuse the memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算分配权重时，如果我们发现所有位置都被使用，或者换句话说，<math alttext="normal u Subscript t Baseline
    equals bold 1"><mrow><msub><mi mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo>
    <mn mathvariant="bold">1</mn></mrow></math>？这意味着分配权重将全部变为零，没有新数据可以分配到内存中。这就引发了释放和重用内存的需求。
- en: 'In order to know which locations can be freed and which cannot, we construct
    a *retention vector*  <math alttext="psi Subscript t"><msub><mi>ψ</mi> <mi>t</mi></msub></math>
     of size *N* that specifies how much of each location should be retained and not
    get freed. Each element of this vector takes a value between 0 and 1, with 0 indicating
    that the corresponding location can be freed, and 1 indicating that it should
    be retained. This vector is calculated using:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了知道哪些位置可以被释放，哪些不能，我们构建了一个大小为*N*的*保留向量* <math alttext="psi Subscript t"><msub><mi>ψ</mi>
    <mi>t</mi></msub></math>，指定每个位置应该保留多少，不应该被释放。该向量的每个元素取值介于0和1之间，0表示相应位置可以被释放，1表示应该保留。这个向量是通过以下公式计算的：
- en: <math alttext="psi Subscript t Baseline equals product Underscript i equals
    1 Overscript upper R Endscripts left-parenthesis bold 1 minus f Subscript t Superscript
    i Baseline normal w Subscript t minus 1 Superscript r comma i Baseline right-parenthesis"><mrow><msub><mi>ψ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>R</mi></msubsup> <mrow><mo>(</mo> <mn mathvariant="bold">1</mn> <mo>-</mo>
    <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="psi Subscript t Baseline equals product Underscript i equals
    1 Overscript upper R Endscripts left-parenthesis bold 1 minus f Subscript t Superscript
    i Baseline normal w Subscript t minus 1 Superscript r comma i Baseline right-parenthesis"><mrow><msub><mi>ψ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msubsup><mo>∏</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>R</mi></msubsup> <mrow><mo>(</mo> <mn mathvariant="bold">1</mn> <mo>-</mo>
    <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math>
- en: This equation is basically saying that the degree to which a memory location
    should be freed is proportional to how much is read from it in the last time steps
    by the various read heads (represented by the values of the read weightings <math
    alttext="normal w Subscript t minus 1 Superscript r comma i"><msubsup><mi mathvariant="normal">w</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></math>
    ). However, continuously freeing a memory location once its data is read is not
    generally preferable as we might still need the data afterward. We let the controller
    decide when to free and when to retain a location after reading by emitting a
    set of *R* free gates <math alttext="f Subscript t Superscript 1 Baseline comma
    ellipsis comma f Subscript t Superscript upper R"><mrow><msubsup><mi>f</mi> <mi>t</mi>
    <mn>1</mn></msubsup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math> that have a value between 0 and 1\. This determines
    how much freeing should be done based on the fact that the location was just read
    from. The controller will then learn how to use these gates to achieve the behavior
    it desires.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程基本上是说，应该释放内存位置的程度与最后几个时间步中各读头读取的量成正比（由读取权重的值表示）。然而，一旦读取数据，连续释放内存位置通常不是首选，因为我们可能之后仍需要这些数据。我们让控制器决定何时在读取后释放或保留位置，通过发出一组*R*自由门，其值介于0和1之间。这决定了基于刚刚从中读取的位置应该进行多少释放。控制器将学习如何使用这些门来实现其所需的行为。
- en: 'Once the retention vector is obtained, we can use it to update the usage vector
    to reflect any freeing or retention made via:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦获得保留向量，我们可以使用它来更新使用向量，以反映通过以下方式进行的任何释放或保留：
- en: <math alttext="normal u Subscript t Baseline equals left-parenthesis normal
    u Subscript t minus 1 Baseline plus normal w Subscript t minus 1 Superscript w
    Baseline minus normal u Subscript t minus 1 Baseline ring normal w Subscript t
    minus 1 Superscript w Baseline right-parenthesis ring psi Subscript t"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup> <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mfenced> <mo>∘</mo> <msub><mi>ψ</mi> <mi>t</mi></msub></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal u Subscript t Baseline equals left-parenthesis normal
    u Subscript t minus 1 Baseline plus normal w Subscript t minus 1 Superscript w
    Baseline minus normal u Subscript t minus 1 Baseline ring normal w Subscript t
    minus 1 Superscript w Baseline right-parenthesis ring psi Subscript t"><mrow><msub><mi
    mathvariant="normal">u</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup> <mo>-</mo> <msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mfenced> <mo>∘</mo> <msub><mi>ψ</mi> <mi>t</mi></msub></mrow></math>
- en: 'This equation can be read as follows: a location will be used if it has been
    retained (its value in  <math alttext="psi Subscript t Baseline almost-equals
    1"><mrow><msub><mi>ψ</mi> <mi>t</mi></msub> <mo>≈</mo> <mn>1</mn></mrow></math>
    ) and either it’s already in use or has just been written to (indicated by its
    value in  <math alttext="normal u Subscript t minus 1 Baseline plus normal w Subscript
    t minus 1 Superscript w"><mrow><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mrow></math> ). Subtracting the element-wise product  <math
    alttext="normal u Subscript t minus 1 Baseline ring normal w Subscript t minus
    1 Superscript w"><mrow><msub><mi mathvariant="normal">u</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>∘</mo> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mi>w</mi></msubsup></mrow></math> brings the whole expression back between 0
    and 1 to be a valid usage value in case the addition between the previous usage
    got the write weighting past 1.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程可以解读如下：如果一个位置已被保留（在ψt中的值几乎等于1），并且它已经在使用中或刚刚被写入（在ut-1加wt-1w中的值表示），则该位置将被使用。逐元素相乘ut-1wt-1w的差值将整个表达式带回0和1之间，以便在先前使用之间的加法使写入权重超过1时成为有效使用值。
- en: By doing this usage update step before calculating the allocation, we can introduce
    some free memory for possible new data. We’re also able to use and reuse a limited
    amount of memory efficiently and overcome the second limitation of NTMs.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在计算分配之前执行此使用更新步骤，我们可以为可能的新数据引入一些空闲内存。我们还能够有效地使用和重复使用有限数量的内存，并克服NTMs的第二个限制。
- en: Temporal Linking of DNC Writes
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNC写入的时间链接
- en: With the dynamic memory management mechanisms that DNCs use, each time a memory
    location is requested for allocation, we’re going to get the most unused location,
    and there’ll be no positional relation between that location and the location
    of the previous write. With this type of memory access, NTM’s way of preserving
    temporal relation with contiguity is not suitable. We’ll need to keep an explicit
    record of the order of the written data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DNC使用的动态内存管理机制，每次请求分配内存位置时，我们将获得最不常用的位置，并且该位置与先前写入的位置之间没有位置关系。使用这种类型的内存访问，NTM保持时间关系与连续性的方式不适用。我们需要明确记录写入数据的顺序。
- en: 'This explicit recording is achieved in DNCs via two additional data structures
    alongside the memory matrix and the usage vector. The first is called a *precedence
    vector*  <math alttext="normal p Subscript t"><msub><mi mathvariant="normal">p</mi>
    <mi>t</mi></msub></math> , an *N*-sized vector considered to be a probability
    distribution over the memory locations, with each value indicating how likely
    the corresponding location was the last one written to. The precedence is initially
    set to <math alttext="normal p 0 equals bold 0"><mrow><msub><mi mathvariant="normal">p</mi>
    <mn>0</mn></msub> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> and gets
    updated in the following steps via:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 通过内存矩阵和使用向量旁边的两个额外数据结构，在DNC中实现了这种明确记录。第一个被称为*优先向量*pt，是一个N大小的向量，被认为是对内存位置的概率分布，每个值表示相应位置最后一次写入的可能性有多大。优先级最初设置为p0=0，并通过以下步骤进行更新。
- en: <math alttext="normal p Subscript t Baseline equals left-parenthesis 1 minus
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts normal w
    Subscript t Superscript w Baseline left-bracket i right-bracket right-parenthesis
    normal p Subscript t minus 1 Baseline plus normal w Subscript t Superscript w"><mrow><msub><mi
    mathvariant="normal">p</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><mn>1</mn> <mo>-</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow></mfenced> <msub><mi mathvariant="normal">p</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal p Subscript t Baseline equals left-parenthesis 1 minus
    sigma-summation Underscript i equals 1 Overscript upper N Endscripts normal w
    Subscript t Superscript w Baseline left-bracket i right-bracket right-parenthesis
    normal p Subscript t minus 1 Baseline plus normal w Subscript t Superscript w"><mrow><msub><mi
    mathvariant="normal">p</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="(" close=")"><mn>1</mn> <mo>-</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>N</mi></msubsup> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow></mfenced> <msub><mi mathvariant="normal">p</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup></mrow></math>
- en: Updating is done by first resetting the previous values of the precedence with
    a reset factor that is proportionate to how much writing was just made to the
    memory (indicated by the summation of the write weighting’s components). Then
    the value of write weighting is added to the reset value so that a location with
    a large write weighting (that is the most recent location written to) would also
    get a large value in the precedence vector.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 更新是通过首先使用与刚刚写入内存的写入量成比例的重置因子重置优先值的先前值来完成的（由写入权重的分量求和表示）。然后将写入权重的值添加到重置值中，以便具有较大写入权重（即最近写入的位置）的位置也会在优先向量中获得较大的值。
- en: 'The second data structure we need to record temporal information is the *link
    matrix*  <math alttext="normal upper L Subscript t"><msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub></math> . The link matrix is an  <math alttext="upper N times
    upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>  matrix in which
    the element  <math alttext="normal upper L Subscript t Baseline left-bracket i
    comma j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
     has a value between 0,1, indicating how likely it is that location *i* was written
    after location *j*. This matrix is also initialized to zeros, and the diagonal
    elements are kept at zero throughout the time  <math alttext="normal upper L Subscript
    t Baseline left-bracket i comma i right-bracket equals 0"><mrow><msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub> <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>=</mo> <mn>0</mn></mrow></math> , as it’s meaningless to track if a location
    was written after itself when the previous data has already been overwritten and
    lost. However, each other element in the matrix is updated using:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要记录时间信息的第二个数据结构是*链接矩阵* <math alttext="normal upper L Subscript t"><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub></math>。链接矩阵是一个<math alttext="upper
    N times upper N"><mrow><mi>N</mi> <mo>×</mo> <mi>N</mi></mrow></math>矩阵，其中元素<math
    alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo>
    <mi>j</mi> <mo>]</mo></mrow></mrow></math>的值在0和1之间，表示位置*i*在位置*j*之后被写入的可能性有多大。该矩阵也初始化为零，并且对角元素在整个时间内保持为零<math
    alttext="normal upper L Subscript t Baseline left-bracket i comma i right-bracket
    equals 0"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub> <mrow><mo>[</mo>
    <mi>i</mi> <mo>,</mo> <mi>i</mi> <mo>]</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>，因为跟踪一个位置在自身之后被写入是没有意义的，当先前的数据已经被覆盖和丢失时。然而，矩阵中的每个其他元素都会使用以下方式进行更新：
- en: <math alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket
    equals left-parenthesis 1 minus normal w Subscript t Superscript w Baseline left-bracket
    i right-bracket minus normal w Subscript t Superscript w Baseline left-bracket
    j right-bracket right-parenthesis normal upper L Subscript t minus 1 Baseline
    left-bracket i comma j right-bracket plus normal w Subscript t Superscript w Baseline
    left-bracket i right-bracket normal p Subscript t minus 1 Baseline left-bracket
    j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>)</mo></mrow> <msub><mi mathvariant="normal">L</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mrow><mo>[</mo> <mi>i</mi>
    <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper L Subscript t Baseline left-bracket i comma j right-bracket
    equals left-parenthesis 1 minus normal w Subscript t Superscript w Baseline left-bracket
    i right-bracket minus normal w Subscript t Superscript w Baseline left-bracket
    j right-bracket right-parenthesis normal upper L Subscript t minus 1 Baseline
    left-bracket i comma j right-bracket plus normal w Subscript t Superscript w Baseline
    left-bracket i right-bracket normal p Subscript t minus 1 Baseline left-bracket
    j right-bracket"><mrow><msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub>
    <mrow><mo>[</mo> <mi>i</mi> <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>=</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <mo>-</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>)</mo></mrow> <msub><mi mathvariant="normal">L</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mrow><mo>[</mo> <mi>i</mi>
    <mo>,</mo> <mi>j</mi> <mo>]</mo></mrow> <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi>
    <mi>t</mi> <mi>w</mi></msubsup> <mrow><mo>[</mo> <mi>i</mi> <mo>]</mo></mrow>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mrow><mo>[</mo> <mi>j</mi> <mo>]</mo></mrow></mrow></math>
- en: 'The equation follows the same pattern we saw with other update rules: first
    the link element is reset by a factor proportional to how much writing had been
    done on locations <math alttext="i comma j"><mrow><mi>i</mi> <mo>,</mo> <mi>j</mi></mrow></math>
    . Then the link is updated by the correlation (represented here by multiplication)
    between the write weighting at location *i* and the previous precedence value
    of location *j*. This eliminates NTM’s third limitation; now we can keep track
    of temporal information no matter how the write head hops around the memory.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 方程遵循我们看到的其他更新规则相同的模式：首先，链接元素会根据在位置*i*和*j*上写入了多少进行重置。然后，链接会根据位置*i*的写入权重和位置*j*的先前优先值之间的相关性（在这里表示为乘法）进行更新。这消除了NTM的第三个限制；现在我们可以跟踪时间信息，无论写头如何在内存中跳跃。
- en: Understanding the DNC Read Head
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解DNC读头
- en: 'Once the write head has finished updating the memory matrix and the associated
    data structures, the read head is now ready to work. Its operation is simple:
    it needs to be able to look up values in the memory and be able to iterate forward
    and backward in temporal ordering between data. The lookup ability can simply
    be achieved with content-based addressing: for each read head *i*, we calculate
    an intermediate weighting  <math alttext="normal c Subscript t Superscript r comma
    i Baseline equals script upper C left-parenthesis upper M Subscript t Baseline
    comma k Subscript t Superscript r comma i Baseline comma beta Subscript t Superscript
    r comma i Baseline right-parenthesis"><mrow><msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>=</mo> <mi>𝒞</mi>
    <mrow><mo>(</mo> <msub><mi>M</mi> <mi>t</mi></msub> <mo>,</mo> <msubsup><mi>k</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>,</mo> <msubsup><mi>β</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>)</mo></mrow></mrow></math>
    , where  <math alttext="k Subscript t Superscript r comma 1 Baseline comma ellipsis
    comma k Subscript t Superscript r comma upper R"><mrow><msubsup><mi>k</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <msubsup><mi>k</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    and <math alttext="beta Subscript t Superscript r comma 1 Baseline comma ellipsis
    comma beta Subscript t Superscript r comma upper R"><mrow><msubsup><mi>β</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
     are two sets of *R* read keys and strengths received from the controller in the
    interface vector.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦写头完成更新内存矩阵和相关数据结构，读头现在准备好工作。它的操作很简单：它需要能够在内存中查找值，并能够在数据之间的时间顺序中向前和向后迭代。查找能力可以通过基于内容的寻址简单实现：对于每个读头
    *i*，我们计算一个中间权重 <math alttext="normal c Subscript t Superscript r comma i Baseline
    equals script upper C left-parenthesis upper M Subscript t Baseline comma k Subscript
    t Superscript r comma i Baseline comma beta Subscript t Superscript r comma i
    Baseline right-parenthesis"><mrow><msubsup><mi mathvariant="normal">c</mi> <mi>t</mi>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup> <mo>=</mo> <mi>𝒞</mi> <mrow><mo>(</mo>
    <msub><mi>M</mi> <mi>t</mi></msub> <mo>,</mo> <msubsup><mi>k</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>,</mo> <msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow></mrow></math> ，其中 <math alttext="k Subscript t Superscript r
    comma 1 Baseline comma ellipsis comma k Subscript t Superscript r comma upper
    R"><mrow><msubsup><mi>k</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup>
    <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi>k</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    和 <math alttext="beta Subscript t Superscript r comma 1 Baseline comma ellipsis
    comma beta Subscript t Superscript r comma upper R"><mrow><msubsup><mi>β</mi>
    <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mn>1</mn></mrow></msubsup> <mo>,</mo> <mo>⋯</mo>
    <mo>,</mo> <msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>R</mi></mrow></msubsup></mrow></math>
    是控制器在接口向量中接收的两组 *R* 个读取键和强度。
- en: 'To achieve forward and backward iterations, we need to make the weightings
    go a step ahead or back from the location they recently read from. We can achieve
    that for the forward iteration by multiplying the link matrix by the last read
    weightings. This shifts the weights from the last read location to the location
    of the last write specified by the link matrix and constructs an intermediate
    forward weighting for each read head *i*: <math alttext="normal f Subscript t
    Superscript i Baseline equals normal upper L Subscript t Baseline normal w Subscript
    t minus 1 Superscript r comma i"><mrow><msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msub><mi mathvariant="normal">L</mi>
    <mi>t</mi></msub> <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math> . Similarly,
    we construct an intermediate backward weighting by multiplying the transpose of
    the link matrix by the last read weightings  <math alttext="normal b Subscript
    t Superscript i Baseline equals normal upper L Subscript t minus 1 Superscript
    down-tack Baseline normal w Subscript t minus 1 Superscript r comma i"><mrow><msubsup><mi
    mathvariant="normal">b</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi
    mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>⊤</mi></msubsup>
    <msubsup><mi mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math> .'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现向前和向后迭代，我们需要使权重从最近读取的位置向前或向后移动一步。我们可以通过将链接矩阵乘以上次读取的权重来实现向前迭代。这将把权重从上次读取的位置移动到链接矩阵指定的上次写入位置，并为每个读头
    *i* 构建一个中间向前权重：<math alttext="normal f Subscript t Superscript i Baseline equals
    normal upper L Subscript t Baseline normal w Subscript t minus 1 Superscript r
    comma i"><mrow><msubsup><mi mathvariant="normal">f</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mo>=</mo> <msub><mi mathvariant="normal">L</mi> <mi>t</mi></msub> <msubsup><mi
    mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
    。类似地，我们通过将链接矩阵的转置乘以上次读取的权重来构建一个中间向后权重 <math alttext="normal b Subscript t Superscript
    i Baseline equals normal upper L Subscript t minus 1 Superscript down-tack Baseline
    normal w Subscript t minus 1 Superscript r comma i"><mrow><msubsup><mi mathvariant="normal">b</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <msubsup><mi mathvariant="normal">L</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>⊤</mi></msubsup> <msubsup><mi
    mathvariant="normal">w</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup></mrow></math>
    。
- en: 'We can now construct the new read weightings for each read using the following
    rule:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下规则为每个读头构建新的读取权重：
- en: <math alttext="normal w Subscript t Superscript r comma i Baseline equals pi
    Subscript t Superscript i Baseline left-bracket 1 right-bracket normal b Subscript
    t Superscript i Baseline plus pi Subscript t Superscript i Baseline left-bracket
    2 right-bracket normal c Subscript t Superscript i Baseline plus pi Subscript
    t Superscript i Baseline left-bracket 3 right-bracket normal f Subscript t Superscript
    i"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup> <mrow><mo>[</mo>
    <mn>1</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">b</mi> <mi>t</mi>
    <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>2</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>3</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup></mrow></math>
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal w Subscript t Superscript r comma i Baseline equals pi
    Subscript t Superscript i Baseline left-bracket 1 right-bracket normal b Subscript
    t Superscript i Baseline plus pi Subscript t Superscript i Baseline left-bracket
    2 right-bracket normal c Subscript t Superscript i Baseline plus pi Subscript
    t Superscript i Baseline left-bracket 3 right-bracket normal f Subscript t Superscript
    i"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup> <mrow><mo>[</mo>
    <mn>1</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">b</mi> <mi>t</mi>
    <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>2</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">c</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>+</mo> <msubsup><mi>π</mi> <mi>t</mi> <mi>i</mi></msubsup>
    <mrow><mo>[</mo> <mn>3</mn> <mo>]</mo></mrow> <msubsup><mi mathvariant="normal">f</mi>
    <mi>t</mi> <mi>i</mi></msubsup></mrow></math>
- en: 'where  <math alttext="pi Subscript t Superscript 1 Baseline comma ellipsis
    comma pi Subscript t Superscript upper R"><mrow><msubsup><mi>π</mi> <mi>t</mi>
    <mn>1</mn></msubsup> <mo>,</mo> <mo>⋯</mo> <mo>,</mo> <msubsup><mi>π</mi> <mi>t</mi>
    <mi>R</mi></msubsup></mrow></math>  are called the *read modes*. Each of these
    are a softmax distribution over three elements that come from the controller on
    the interface vector. Its three values determine the emphasis the read head should
    put on each read mechanism: backward, lookup, and forward, respectively. The controller
    learns to use these modes to instruct the memory on how data should be read.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 πt1，...，πtR 被称为*读取模式*。这些都是从控制器对接口向量的三个元素中得出的softmax分布。它的三个值确定了读头应该在每个读取机制上放置的重点：向后、查找和向前。控制器学会使用这些模式来指导内存如何读取数据。
- en: The DNC Controller Network
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNC控制器网络
- en: 'Now that we’ve figured out the internal workings of the external memory in
    the DNC architecture, we’re left with understanding how the controller that coordinates
    all the memory operations work. The controller’s operation is simple: in its heart
    there’s a neural network (recurrent or feed-forward) that takes in the input step
    along with the read-vectors from the last step and outputs a vector whose size
    depends on the architecture we chose for the network. Let’s denote that vector
    by *N*(χ[t]), where *N* denotes whatever function is computed by the neural network,
    and  <math alttext="chi Subscript t"><msub><mi>χ</mi> <mi>t</mi></msub></math>
     denotes the concatenation of the input step and the last read vectors  <math
    alttext="chi Subscript t Baseline equals left-bracket x Subscript t Baseline semicolon
    normal r Subscript t minus 1 Superscript 1 Baseline semicolon ellipsis semicolon
    normal r Subscript t minus 1 Superscript upper R Baseline right-bracket"><mrow><msub><mi>χ</mi>
    <mi>t</mi></msub> <mo>=</mo> <mrow><mo>[</mo> <msub><mi>x</mi> <mi>t</mi></msub>
    <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow>
    <mn>1</mn></msubsup> <mo>;</mo> <mo>⋯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow> <mi>R</mi></msubsup> <mo>]</mo></mrow></mrow></math>
    . This concatenation of the last read vectors serves a similar purpose as the
    hidden state in a regular LSTM: to condition the output on the past.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经弄清楚了DNC架构中外部内存的内部工作原理，我们需要理解协调所有内存操作的控制器是如何工作的。控制器的操作很简单：在其核心有一个神经网络（循环或前馈），它接收输入步骤以及上一步的读取向量，并输出一个大小取决于我们为网络选择的架构的向量。让我们用
    *N*(χ[t]) 表示该向量，其中 *N* 表示神经网络计算的任何函数，χt 表示输入步骤和上次读取向量的串联。这些上次读取向量的串联类似于常规LSTM中的隐藏状态：用于将输出与过去的情况联系起来。
- en: From that vector emitted from the neural network, we need two pieces of information.
    The first one is the interface vector ζ[t]. As we saw, the interface vector holds
    all the information for the memory to carry out its operation. We can look at
    the ζ[t] vector as a concatenation of the individual elements we encountered before,
    as depicted in [Figure 12-7](#interface_vector_decomposed).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经网络发出的向量中，我们需要两个信息。第一个是接口向量 ζ[t]。正如我们所看到的，接口向量包含了内存执行操作所需的所有信息。我们可以将 ζ[t]
    向量看作是我们之前遇到的各个元素的串联，如 [图12-7](#interface_vector_decomposed) 所示。
- en: '![](Images/fdl2_1207.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1207.png)'
- en: Figure 12-7\. The interface vector decomposed to its individual components
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7。接口向量分解为其各个组件
- en: 'By summing up the sizes along the components, we can consider the  <math alttext="zeta
    Subscript t"><msub><mi>ζ</mi> <mi>t</mi></msub></math>  vector as one big vector
    of size <math alttext="upper R times upper W plus 3 upper W plus 5 upper R plus
    3"><mrow><mi>R</mi> <mo>×</mo> <mi>W</mi> <mo>+</mo> <mn>3</mn> <mi>W</mi> <mo>+</mo>
    <mn>5</mn> <mi>R</mi> <mo>+</mo> <mn>3</mn></mrow></math> . So in order to obtain
    that vector from the network output, we construct a learnable <math alttext="StartAbsoluteValue
    script upper N EndAbsoluteValue times left-parenthesis upper R times upper W plus
    3 upper W plus 5 upper R plus 3 right-parenthesis"><mrow><mo>|</mo> <mi>𝒩</mi>
    <mo>|</mo> <mo>×</mo> <mo>(</mo> <mi>R</mi> <mo>×</mo> <mi>W</mi> <mo>+</mo> <mn>3</mn>
    <mi>W</mi> <mo>+</mo> <mn>5</mn> <mi>R</mi> <mo>+</mo> <mn>3</mn> <mo>)</mo></mrow></math>
     weights matrix  <math alttext="upper W Subscript zeta"><msub><mi>W</mi> <mi>ζ</mi></msub></math>
    , where  <math alttext="StartAbsoluteValue script upper N EndAbsoluteValue"><mrow><mo>|</mo>
    <mi>𝒩</mi> <mo>|</mo></mrow></math>  is the size of the network’s output, such
    that:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 通过沿着组件对大小进行求和，我们可以将 ζt 向量视为一个大小为 R×W+3W+5R+3 的大向量。因此，为了从网络输出中获得该向量，我们构建一个可学习的
    𝒩×(R×W+3W+5R+3) 权重矩阵 Wζ，其中 |𝒩| 是网络输出的大小，如下所示：
- en: <math alttext="zeta Subscript t Baseline equals upper W Subscript zeta Baseline
    script upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>ζ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>ζ</mi></msub> <mi>𝒩</mi> <mrow><mo>(</mo>
    <msub><mi>χ</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="zeta Subscript t Baseline equals upper W Subscript zeta Baseline
    script upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>ζ</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>ζ</mi></msub> <mi>𝒩</mi> <mrow><mo>(</mo>
    <msub><mi>χ</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'Before passing that  <math alttext="zeta Subscript t"><msub><mi>ζ</mi> <mi>t</mi></msub></math>
     vector to the memory, we need to make sure that each component has a valid value.
    For example, all the gates as well as the erase vector must have values between
    0 and 1, so we pass them through a sigmoid function to ensure that requirement:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在将<math alttext="zeta Subscript t"><msub><mi>ζ</mi> <mi>t</msub></math>向量传递给内存之前，我们需要确保每个组件具有有效值。例如，所有门以及擦除向量必须具有0到1之间的值，因此我们通过sigmoid函数将它们传递以确保这一要求：
- en: <math alttext="e Subscript t Baseline equals sigma left-parenthesis e Subscript
    t Baseline right-parenthesis comma f Subscript t Superscript i Baseline equals
    sigma left-parenthesis f Subscript t Superscript i Baseline right-parenthesis
    comma g Subscript t Superscript a Baseline equals sigma left-parenthesis g Subscript
    t Superscript a Baseline right-parenthesis comma g Subscript t Superscript w Baseline
    equals sigma left-parenthesis g Subscript t Superscript w Baseline right-parenthesis
    where sigma left-parenthesis z right-parenthesis equals StartFraction 1 Over 1
    plus e Superscript negative z Baseline EndFraction"><mrow><msub><mi>e</mi> <mi>t</mi></msub>
    <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>e</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow>
    <mtext>where</mtext> <mi>σ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="e Subscript t Baseline equals sigma left-parenthesis e Subscript
    t Baseline right-parenthesis comma f Subscript t Superscript i Baseline equals
    sigma left-parenthesis f Subscript t Superscript i Baseline right-parenthesis
    comma g Subscript t Superscript a Baseline equals sigma left-parenthesis g Subscript
    t Superscript a Baseline right-parenthesis comma g Subscript t Superscript w Baseline
    equals sigma left-parenthesis g Subscript t Superscript w Baseline right-parenthesis
    where sigma left-parenthesis z right-parenthesis equals StartFraction 1 Over 1
    plus e Superscript negative z Baseline EndFraction"><mrow><msub><mi>e</mi> <mi>t</mi></msub>
    <mo>=</mo> <mi>σ</mi> <mrow><mo>(</mo> <msub><mi>e</mi> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>f</mi> <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>a</mi></msubsup> <mo>)</mo></mrow>
    <mo>,</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>=</mo> <mi>σ</mi>
    <mrow><mo>(</mo> <msubsup><mi>g</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>)</mo></mrow>
    <mtext>where</mtext> <mi>σ</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Also, all the lookup strengths need to have a value larger than or equal to
    1, so we pass them through a *oneplus *function first:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，所有查找强度都需要具有大于或等于1的值，因此我们首先通过*oneplus*函数传递它们：
- en: <math alttext="beta Subscript t Superscript r comma i Baseline equals normal
    o normal n normal e normal p normal l normal u normal s left-parenthesis beta
    Subscript t Superscript r comma i Baseline right-parenthesis comma beta Subscript
    t Superscript w Baseline equals normal o normal n normal e normal p normal l normal
    u normal s left-parenthesis beta Subscript t Superscript w Baseline right-parenthesis
    where normal o normal n normal e normal p normal l normal u normal s left-parenthesis
    z right-parenthesis equals 1 plus log left-parenthesis 1 plus e Superscript z
    Baseline right-parenthesis"><mrow><msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow> <mo>,</mo> <msubsup><mi>β</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>β</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>)</mo></mrow> <mtext>where</mtext> <mi>oneplus</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="beta Subscript t Superscript r comma i Baseline equals normal
    o normal n normal e normal p normal l normal u normal s left-parenthesis beta
    Subscript t Superscript r comma i Baseline right-parenthesis comma beta Subscript
    t Superscript w Baseline equals normal o normal n normal e normal p normal l normal
    u normal s left-parenthesis beta Subscript t Superscript w Baseline right-parenthesis
    where normal o normal n normal e normal p normal l normal u normal s left-parenthesis
    z right-parenthesis equals 1 plus log left-parenthesis 1 plus e Superscript z
    Baseline right-parenthesis"><mrow><msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>β</mi> <mi>t</mi> <mrow><mi>r</mi><mo>,</mo><mi>i</mi></mrow></msubsup>
    <mo>)</mo></mrow> <mo>,</mo> <msubsup><mi>β</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>=</mo> <mi>oneplus</mi> <mrow><mo>(</mo> <msubsup><mi>β</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <mo>)</mo></mrow> <mtext>where</mtext> <mi>oneplus</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mn>1</mn> <mo>+</mo> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>+</mo> <msup><mi>e</mi> <mi>z</mi></msup> <mo>)</mo></mrow></mrow></math>
- en: 'And finally, the read modes must have a valid softmax distribution:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，读取模式必须具有有效的softmax分布：
- en: <math alttext="pi Subscript t Superscript i Baseline equals normal s normal
    o normal f normal t normal m normal a normal x left-parenthesis pi Subscript t
    Superscript i Baseline right-parenthesis where normal s normal o normal f normal
    t normal m normal a normal x left-parenthesis z right-parenthesis equals StartFraction
    e Superscript z Baseline Over sigma-summation Underscript j Endscripts e Superscript
    z Super Subscript j Superscript Baseline EndFraction"><mrow><msubsup><mi>π</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo> <msubsup><mi>π</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mtext>where</mtext> <mi>softmax</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mi>z</mi></msup> <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi>
    <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="pi Subscript t Superscript i Baseline equals normal s normal
    o normal f normal t normal m normal a normal x left-parenthesis pi Subscript t
    Superscript i Baseline right-parenthesis where normal s normal o normal f normal
    t normal m normal a normal x left-parenthesis z right-parenthesis equals StartFraction
    e Superscript z Baseline Over sigma-summation Underscript j Endscripts e Superscript
    z Super Subscript j Superscript Baseline EndFraction"><mrow><msubsup><mi>π</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>=</mo> <mi>softmax</mi> <mrow><mo>(</mo> <msubsup><mi>π</mi>
    <mi>t</mi> <mi>i</mi></msubsup> <mo>)</mo></mrow> <mtext>where</mtext> <mi>softmax</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><msup><mi>e</mi>
    <mi>z</mi></msup> <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <msup><mi>e</mi> <msub><mi>z</mi>
    <mi>j</mi></msub></msup></mrow></mfrac></mrow></math>
- en: 'By these transformations, the interface vector is now ready to be passed to
    the memory; and while it guides the memory in its operations, we’ll be needing
    a second piece of information from the neural network, the *pre-output* vector 
    <math alttext="v Subscript t"><msub><mi>v</mi> <mi>t</mi></msub></math> . This
    is a vector of the same size of the final output vector, but it’s not the final
    output vector. By using another learnable  <math alttext="StartAbsoluteValue script
    upper N EndAbsoluteValue times upper Y"><mrow><mo>|</mo> <mi>𝒩</mi> <mo>|</mo>
    <mo>×</mo> <mi>Y</mi></mrow></math>  weights matrix <math alttext="upper W Subscript
    y"><msub><mi>W</mi> <mi>y</mi></msub></math> , we can obtain the pre-output via:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些转换，接口向量现在已准备好传递给内存；在引导内存进行操作的同时，我们还需要来自神经网络的第二个信息片段，*预输出*向量<math alttext="v
    Subscript t"><msub><mi>v</mi> <mi>t</mi></msub></math>。这是与最终输出向量相同大小的向量，但不是最终输出向量。通过使用另一个可学习的<math
    alttext="StartAbsoluteValue script upper N EndAbsoluteValue times upper Y"><mrow><mo>|</mo>
    <mi>𝒩</mi> <mo>|</mo> <mo>×</mo> <mi>Y</mi></mrow></math>权重矩阵<math alttext="upper
    W Subscript y"><msub><mi>W</mi> <mi>y</mi></msub></math>，我们可以通过以下方式获得预输出：
- en: <math alttext="v Subscript t Baseline equals upper W Subscript y Baseline script
    upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>v</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>y</mi></msub> <mi>𝒩</mi> <mrow><mo>(</mo>
    <msub><mi>χ</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="v Subscript t Baseline equals upper W Subscript y Baseline script
    upper N left-parenthesis chi Subscript t Baseline right-parenthesis"><mrow><msub><mi>v</mi>
    <mi>t</mi></msub> <mo>=</mo> <msub><mi>W</mi> <mi>y</mi></msub> <mi>𝒩</mi> <mrow><mo>(</mo>
    <msub><mi>χ</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: 'This pre-output vector gives us the ability to condition our final output not
    just on the network output, but also on the recently read vectors  <math alttext="normal
    r Subscript t"><msub><mi mathvariant="normal">r</mi> <mi>t</mi></msub></math>
     from memory. Via a third learnable <math alttext="left-parenthesis upper R times
    upper W right-parenthesis times upper Y"><mrow><mo>(</mo> <mi>R</mi> <mo>×</mo>
    <mi>W</mi> <mo>)</mo> <mo>×</mo> <mi>Y</mi></mrow></math>  weights matrix <math
    alttext="upper W Subscript r"><msub><mi>W</mi> <mi>r</mi></msub></math> , we can
    get the final output as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个预输出向量使我们能够不仅根据网络输出，还根据最近从内存中读取的向量<math alttext="normal r Subscript t"><msub><mi
    mathvariant="normal">r</mi> <mi>t</mi></msub></math>来调整我们的最终输出。通过第三个可学习的<math
    alttext="left-parenthesis upper R times upper W right-parenthesis times upper
    Y"><mrow><mo>(</mo> <mi>R</mi> <mo>×</mo> <mi>W</mi> <mo>)</mo> <mo>×</mo> <mi>Y</mi></mrow></math>权重矩阵<math
    alttext="upper W Subscript r"><msub><mi>W</mi> <mi>r</mi></msub></math>，我们可以得到最终输出：
- en: <math alttext="y Subscript t Baseline equals v Subscript t Baseline plus upper
    W Subscript r Baseline left-bracket normal r Subscript t Superscript 1 Baseline
    semicolon ellipsis semicolon normal r Subscript t Superscript upper R Baseline
    right-bracket"><mrow><msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>v</mi>
    <mi>t</mi></msub> <mo>+</mo> <msub><mi>W</mi> <mi>r</mi></msub> <mrow><mo>[</mo>
    <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup> <mo>;</mo>
    <mo>⋯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mi>R</mi></msubsup>
    <mo>]</mo></mrow></mrow></math>
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y Subscript t Baseline equals v Subscript t Baseline plus upper
    W Subscript r Baseline left-bracket normal r Subscript t Superscript 1 Baseline
    semicolon ellipsis semicolon normal r Subscript t Superscript upper R Baseline
    right-bracket"><mrow><msub><mi>y</mi> <mi>t</mi></msub> <mo>=</mo> <msub><mi>v</mi>
    <mi>t</mi></msub> <mo>+</mo> <msub><mi>W</mi> <mi>r</mi></msub> <mrow><mo>[</mo>
    <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mn>1</mn></msubsup> <mo>;</mo>
    <mo>⋯</mo> <mo>;</mo> <msubsup><mi mathvariant="normal">r</mi> <mi>t</mi> <mi>R</mi></msubsup>
    <mo>]</mo></mrow></mrow></math>
- en: Given that the controller knows nothing about the memory except for the word
    size  <math alttext="upper W"><mi>W</mi></math> , an already learned controller
    can be scaled to a larger memory with more locations without any need for retraining.
    Also, the fact that we didn’t specify any particular structure for the neural
    network or any particular loss function makes DNC a universal architecture that
    can be applied to a variety of tasks and learning problems.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到控制器除了单词大小<math alttext="upper W"><mi>W</mi></math>之外对内存一无所知，已经学习的控制器可以扩展到具有更多位置的更大内存，而无需重新训练。此外，我们没有为神经网络指定任何特定结构或任何特定损失函数的事实使得DNC成为一个通用架构，可以应用于各种任务和学习问题。
- en: Visualizing the DNC in Action
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 展示DNC的运行情况
- en: One way to see DNC’s operation in action is to train it on a simple task that
    would allow us to look at the weightings and the parameters’ values and visualize
    them in an interpretable way. For this simple task, we’ll use the copy problem
    we already saw with NTMs, but in a slightly modified form.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 观察DNC的运行方式之一是在一个简单的任务上对其进行训练，这样我们可以查看权重和参数值，并以一种可解释的方式对其进行可视化。对于这个简单的任务，我们将使用我们已经在NTMs中看到的复制问题，但稍作修改。
- en: Instead of trying to copy a single sequence of binary vectors, our task here
    will be to copy a series of such sequences. In [Figure 12-8](#single_v_series_of_input_sequences), (a)
    shows the single sequence input. After processing such single sequence input and
    copying the same sequence to the output, the DNC would have finished its program,
    and its memory would be reset in a way that will not allow us to see how it can
    dynamically manage it. Instead we’ll treat a series of such sequences, shown in
    [Figure 12-8](#single_v_series_of_input_sequences) (b), as a single input.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的任务不是尝试复制单个二进制向量序列，而是复制一系列这样的序列。在[图12-8](#single_v_series_of_input_sequences)中，(a)显示了单个序列输入。在处理这样的单个序列输入并将相同序列复制到输出后，DNC将完成其程序，并且其内存将以一种不允许我们看到它如何动态管理的方式被重置。相反，我们将一系列这样的序列，如[图12-8](#single_v_series_of_input_sequences)中(b)所示，视为单个输入。
- en: '![](Images/fdl2_1208.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1208.png)'
- en: Figure 12-8\. Single sequence input versus a series of input sequences
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-8。单序列输入与一系列输入序列
- en: '[Figure 12-9](#viz_of_the_dnc_operation_on_copy_problem) shows a visualization
    of the DNC operation after being trained on a series of length 4 where each sequence
    contains five binary vectors and an end mark. The DNC used here has only 10 memory
    locations, so there’s no way it can store all 20 vectors in the input. A feed-forward
    controller is used to ensure that nothing would be stored in a recurrent state,
    and only one read head is used to make the visualization more clear. These constraints
    should force the DNC to learn how to deallocate and reuse memory to successfully
    copy the whole input, and indeed it does.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-9](#viz_of_the_dnc_operation_on_copy_problem)显示了DNC在训练了一系列长度为4的序列后的操作可视化，每个序列包含五个二进制向量和一个结束标记。这里使用的DNC只有10个内存位置，因此无法存储输入中的所有20个向量。使用前馈控制器确保不会将任何内容存储在循环状态中，并且只使用一个读头使得可视化更清晰。这些约束应该迫使DNC学会如何释放和重复使用内存以成功复制整个输入，事实上它确实做到了。'
- en: We can see in that visualization how the DNC is writing each vector of the five
    in a sequence into a single memory location. As soon as the end mark is seen,
    the read head starts reading from these locations in the exact same order of writing.
    We can see how both the allocation and free gates alternate in activation between
    writing and reading phases of each sequence in the series. From the usage vector
    chart at the bottom, we can also see how after a memory location is written to,
    its usage becomes exactly 1, and how it drops to 0 just after reading from that
    location, indicating that it was freed and can be reused again.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到在该可视化中，DNC将五个向量中的每一个按顺序写入单个内存位置。一旦看到结束标记，读头就开始按照完全相同的顺序从这些位置读取。我们可以看到分配和释放门在系列中每个序列的写入和读取阶段之间交替激活。从底部的使用向量图表中，我们还可以看到，一旦写入到内存位置，其使用率就变为1，而在从该位置读取后立即降为0，表示已被释放并可以再次重用。
- en: '![](Images/fdl2_1209.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1209.png)'
- en: Figure 12-9\. Visualization of the DNC operation on the copy problem
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9。在复制问题上执行DNC操作的可视化
- en: This visualization is part of the open source implementation of the DNC architecture
    by [Mostafa Samir](https://oreil.ly/TtKJ8). In the next section we’ll learn the
    important tips and tricks that will allow us to implement a simpler version of
    DNC on the reading comprehension tasks.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化是由[Mostafa Samir](https://oreil.ly/TtKJ8)开源实现的DNC架构的一部分。在下一节中，我们将学习一些重要的技巧和窍门，这些技巧和窍门将使我们能够在阅读理解任务中实现一个更简单的DNC版本。
- en: Implementing the DNC in PyTorch
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在PyTorch中实现DNC
- en: Implementing the DNC architecture is essentially a direct application of the
    math we just discussed. So with the full implementation in the code repository
    associated with the book, we’ll just be focusing on the tricky parts and introduce
    some new PyTorch practice while we’re at it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中实现DNC架构基本上是我们刚刚讨论的数学的直接应用。因此，通过与与本书相关的代码库中的完整实现，我们将专注于棘手的部分，并在此过程中介绍一些新的PyTorch实践。
- en: 'The main part of the implementation resides in the *mem_ops.py* file where
    all of the attention and access mechanisms are implemented. This file is then
    imported to be used with the controller. Two operations that might be a little
    tricky to implement are the link matrix update and the allocation weighting calculation.
    Both of these operations can be naively implemented with `for` loops, but using
    `for` loops in creating a computational graph is generally not a good idea. Let’s
    take the link matrix update operation first and see how it looks with a loop-based
    implementation:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实现的主要部分位于*mem_ops.py*文件中，其中实现了所有的注意力和访问机制。然后将该文件导入以与控制器一起使用。可能有点棘手的两个操作是链接矩阵更新和分配权重计算。这两个操作都可以用`for`循环天真地实现，但是在创建计算图时使用`for`循环通常不是一个好主意。让我们首先看一下链接矩阵更新操作，看看基于循环的实现是什么样子的：
- en: '[PRE2]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After that computational graph is fully defined, it’s then fed with concrete
    values and executed. With that in mind, we can see, as depicted in [Figure 12-10](#computational_graph_of_the_link_matrix),
    how in most of the iterations of the `for` loop, a new set of nodes representing
    the loop body gets added in the computational graph. So for  *N* memory locations,
    we end up with <math alttext="upper N squared minus upper N"><mrow><msup><mi>N</mi>
    <mn>2</mn></msup> <mo>-</mo> <mi>N</mi></mrow></math>  identical copies of the
    same nodes, each for each iteration, each taking up a chunk of our RAM and needing
    its own time to be processed before the next can be. When *N* is a small number,
    say 5, we get 20 identical copies, which is not so bad. However, if we want to
    use a larger memory, like with <math alttext="upper N equals 256"><mrow><mi>N</mi>
    <mo>=</mo> <mn>256</mn></mrow></math> , we get 65,280 identical copies of the
    nodes, which is catastrophic for both the memory usage and the execution time.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全定义了该计算图之后，它将被提供具体的值并执行。考虑到这一点，我们可以看到，正如[图12-10](#computational_graph_of_the_link_matrix)所示，在`for`循环的大多数迭代中，表示循环体的一组新节点被添加到计算图中。因此，对于*N*个内存位置，我们最终得到了<math
    alttext="upper N squared minus upper N"><mrow><msup><mi>N</mi> <mn>2</mn></msup>
    <mo>-</mo> <mi>N</mi></mrow></math>个相同的节点副本，每个迭代一个，每个占用我们的RAM的一部分，并需要在下一个节点之前处理自己的时间。当*N*是一个小数字时，比如5，我们得到20个相同的副本，这并不太糟糕。然而，如果我们想使用更大的内存，比如<math
    alttext="upper N equals 256"><mrow><mi>N</mi> <mo>=</mo> <mn>256</mn></mrow></math>，我们得到65,280个节点的相同副本，这对内存使用和执行时间都是灾难性的。
- en: '![](Images/fdl2_1210.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1210.png)'
- en: Figure 12-10\. The computational graph of the link matrix update operation built
    with the `for` loop implementation
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-10。使用`for`循环实现的链接矩阵更新操作的计算图
- en: 'One possible way to overcome such an issue is *vectorization*. In vectorization,
    we take an array operation that is originally defined in terms of individual elements
    and rewrite it as an operation on the whole array at once. For the link matrix
    update, we can rewrite the operation as:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 克服这种问题的一种可能方式是*向量化*。在向量化中，我们将最初以单个元素形式定义的数组操作重写为一次对整个数组的操作。对于链接矩阵更新，我们可以将操作重写为：
- en: <math alttext="normal upper L Subscript t Baseline equals left-bracket left-parenthesis
    1 minus normal w Subscript t Superscript w Baseline circled-plus normal w Subscript
    t Superscript w Baseline right-parenthesis ring normal upper L Subscript t minus
    1 Baseline plus normal w Subscript t Superscript w Baseline normal p Subscript
    t minus 1 Baseline right-bracket ring left-parenthesis 1 minus upper I right-parenthesis"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>⊕</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></mfenced>
    <mo>∘</mo> <msub><mi mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfenced>
    <mo>∘</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>I</mi></mfenced></mrow></math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="normal upper L Subscript t Baseline equals left-bracket left-parenthesis
    1 minus normal w Subscript t Superscript w Baseline circled-plus normal w Subscript
    t Superscript w Baseline right-parenthesis ring normal upper L Subscript t minus
    1 Baseline plus normal w Subscript t Superscript w Baseline normal p Subscript
    t minus 1 Baseline right-bracket ring left-parenthesis 1 minus upper I right-parenthesis"><mrow><msub><mi
    mathvariant="normal">L</mi> <mi>t</mi></msub> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <mo>⊕</mo>
    <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup></mfenced>
    <mo>∘</mo> <msub><mi mathvariant="normal">L</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msubsup><mi mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup>
    <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mfenced>
    <mo>∘</mo> <mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mi>I</mi></mfenced></mrow></math>
- en: 'Where  <math alttext="upper I"><mi>I</mi></math>  is the identity matrix, and
    the product <math alttext="normal w Subscript t Superscript w Baseline normal
    p Subscript t minus 1"><mrow><msubsup><mi mathvariant="normal">w</mi> <mi>t</mi>
    <mi>w</mi></msubsup> <msub><mi mathvariant="normal">p</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
     is an outer product. To achieve this vectorization, we define a new operator,
    the pairwise-addition of vectors, denoted by  <math alttext="circled-plus"><mo>⊕</mo></math>
    . This new operator is simply defined as:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="upper I"><mi>I</mi></math>是单位矩阵，而乘积<math alttext="normal w
    Subscript t Superscript w Baseline normal p Subscript t minus 1"><mrow><msubsup><mi
    mathvariant="normal">w</mi> <mi>t</mi> <mi>w</mi></msubsup> <msub><mi mathvariant="normal">p</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>是外积。为了实现这种向量化，我们定义了一个新的运算符，即向量的成对加法，表示为<math
    alttext="circled-plus"><mo>⊕</mo></math>。这个新运算符简单地定义为：
- en: <math alttext="u circled-plus v equals Start 3 By 3 Matrix 1st Row 1st Column
    u 1 plus v 1 2nd Column  ellipsis 3rd Column u 1 plus v Subscript n Baseline 2nd
    Row 1st Column  ellipsis 2nd Column  ellipsis 3rd Column  ellipsis 3rd Row 1st
    Column u Subscript n Baseline plus v 1 2nd Column  ellipsis 3rd Column u Subscript
    n Baseline plus v Subscript n EndMatrix"><mrow><mi>u</mi> <mo>⊕</mo> <mi>v</mi>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><msub><mi>u</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd> <mtd><mo>⋱</mo></mtd> <mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><msub><mi>u</mi>
    <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="u circled-plus v equals Start 3 By 3 Matrix 1st Row 1st Column
    u 1 plus v 1 2nd Column  ellipsis 3rd Column u 1 plus v Subscript n Baseline 2nd
    Row 1st Column  ellipsis 2nd Column  ellipsis 3rd Column  ellipsis 3rd Row 1st
    Column u Subscript n Baseline plus v 1 2nd Column  ellipsis 3rd Column u Subscript
    n Baseline plus v Subscript n EndMatrix"><mrow><mi>u</mi> <mo>⊕</mo> <mi>v</mi>
    <mo>=</mo> <mfenced open="(" close=")"><mtable><mtr><mtd><mrow><msub><mi>u</mi>
    <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mn>1</mn></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr>
    <mtr><mtd><mo>⋮</mo></mtd> <mtd><mo>⋱</mo></mtd> <mtd><mo>⋮</mo></mtd></mtr> <mtr><mtd><mrow><msub><mi>u</mi>
    <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mn>1</mn></msub></mrow></mtd> <mtd><mo>⋯</mo></mtd>
    <mtd><mrow><msub><mi>u</mi> <mi>n</mi></msub> <mo>+</mo> <msub><mi>v</mi> <mi>n</mi></msub></mrow></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'This operator adds a little bit to the memory requirements of the implementation,
    but not as much as the case in the loop-based implementation. With this vectorized
    reformulation of the update rule, we rewrite a more memory- and time-efficient
    implementation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作符增加了实现的内存需求，但不像基于循环的实现那样多。通过这种矢量化重构更新规则，我们重写了一个更节省内存和时间的实现：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A similar process could be made for the allocation weightings rule. Instead
    of having a single rule for each element in the weighting vector, we can decompose
    it into a few operations that work on the whole vector at once:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分配权重规则也可以采用类似的过程。我们可以将权重向量的每个元素的单一规则分解为一些同时作用于整个向量的操作：
- en: While sorting the usage vector to get the free list, we also grab the sorted
    usage vector itself.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对使用情况向量进行排序以获取自由列表时，我们还获取了排序后的使用情况向量本身。
- en: We calculate the cumulative product vector of the sorted usage. Each element
    of that vector is the same as the product term in our original element-wise rule.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算排序后使用情况的累积乘积向量。该向量的每个元素与我们原始逐元素规则中的乘积项相同。
- en: We multiply the cumulative product vector by (1 – the sorted usage vector).
    The resulting vector is the allocation weighting but in the sorted order, not
    the original order of the memory location.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将累积乘积向量乘以（1减去排序后的使用情况向量）。得到的向量是分配权重，但是按照排序顺序，而不是内存位置的原始顺序。
- en: For each element of that out-of-order allocation weighting, we take its value
    and put it in the corresponding index in the free list. The resulting vector is
    now the correct allocation weighting that we want.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于那些无序分配权重的每个元素，我们取其值并将其放入自由列表中相应的索引中。现在得到的向量是我们想要的正确分配权重。
- en: '[Figure 12-11](#fig0744) summarizes this process with a numerical example.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[图12-11](#fig0744)用一个数值示例总结了这个过程。'
- en: '![](Images/fdl2_1211.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_1211.png)'
- en: Figure 12-11\. The vectorized process of calculating the allocation weightings
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-11。计算分配权重的矢量化过程
- en: It may seem that we still need loops for the sorting operation in step 1 and
    for reordering the weights in step 4, but fortunately PyTorch provides symbolic
    operations that would allow us to carry out these operations without the need
    for a Python loop.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤1中对使用情况向量进行排序和在步骤4中重新排序权重似乎仍然需要循环，但幸运的是PyTorch提供了符号操作，可以让我们执行这些操作而无需Python循环。
- en: 'For sorting we’ll be using `torch.topk`. This operation takes a tensor and
    a number <math alttext="k"><mi>k</mi></math> , and returns both the sorted top
    <math alttext="k"><mi>k</mi></math> values in descending order and the indices
    of these values. To get the sorted usage vector in ascending order, we need to
    get the top *N* values of the negative of the usage vector. We can bring back
    the sorted values to their original signs by multiplying the resulting vector
    by <math alttext="negative 1"><mrow><mo>-</mo> <mn>1</mn></mrow></math> :'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于排序，我们将使用`torch.topk`。该操作接受一个张量和一个数字<math alttext="k"><mi>k</mi></math>，并返回按降序排列的前<math
    alttext="k"><mi>k</mi></math>个值以及这些值的索引。为了按升序获取排序后的使用情况向量，我们需要获取使用情况向量的负值的前*N*个值。通过将结果向量乘以<math
    alttext="negative 1"><mrow><mo>-</mo> <mn>1</mn></mrow></math>，我们可以将排序后的值恢复为其原始符号：
- en: '[PRE4]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'For reordering the allocation weights, we first create an empty tensor array
    of size N to be the container of the weights in their correct order, and then
    put the values in their correct places using the instance method `scatter(indices,
    values)`. This method takes in its second argument a tensor, and scatters the
    values along its first dimension across the array, with the first argument being
    a list of indices of the locations to which we want to scatter the corresponding
    values. In our case here, the first argument is the free list, and the second
    is the out-of-order allocation weightings. Once we get the array with the weights
    in the correct places, we use another instance method, `pack()`, to wrap up the
    whole array into a `Tensor` object:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新排序分配权重，我们首先创建一个大小为N的空张量数组，用于容纳权重的正确顺序，然后使用实例方法`scatter(indices, values)`将值放入其正确位置。该方法的第二个参数是一个张量，并且沿着其第一维度将值散布到数组中，第一个参数是我们想要将相应值散布到的位置的索引列表。在这里，第一个参数是自由列表，第二个参数是无序分配权重。一旦我们得到了权重正确放置的数组，我们使用另一个实例方法`pack()`将整个数组包装成一个`Tensor`对象：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last part of the implementation that requires looping is the controller
    loop itself—the loop that goes over each step of the input sequence to process
    it. Because vectorization works only when operations are defined element-wise,
    the controller’s loop can’t be vectorized. Fortunately, PyTorch still provides
    us with a method to escape Python’s `for` loops and their massive performance
    hit; this method is the *symbolic loop. *A symbolic loop works like most of our
    symbolic operations:  instead of unrolling the actual loop into the graph, it
    defines a node that would be executed as a loop when the graph is executed.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 实现中最后需要循环的部分是控制器循环本身——遍历输入序列的每个步骤以进行处理的循环。因为矢量化仅在操作被定义为逐元素时才有效，控制器的循环无法进行矢量化。幸运的是，PyTorch仍然为我们提供了一种方法来避免Python的`for`循环及其巨大的性能损失；这种方法是*符号循环*。符号循环的工作方式与我们大多数符号操作相同：它不会将实际循环展开到图中，而是定义一个节点，当执行图时将作为循环执行。
- en: We’ll leave the symbolic loop implementation in PyTorch up to the reader. More
    information on how you can use symbolic loops in PyTorch can be found in the torch.fx
    [documentation](https://oreil.ly/qtgBt).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将符号循环的实现留给读者自行完成。关于如何在PyTorch中使用符号循环的更多信息可以在torch.fx [文档](https://oreil.ly/qtgBt)中找到。
- en: The TensorFlow implementation of our symbolic loop can be found in the *train_babi.py*
    file in the code repository.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们符号循环的TensorFlow实现可以在代码库中的*train_babi.py*文件中找到。
- en: Teaching a DNC to Read and Comprehend
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 教授DNC阅读和理解
- en: Earlier in the chapter, when we were talking about neural n-grams, we said that
    it’s not of the complexity of an AI that can answer questions after reading a
    story. Now we have reached the point where we can build such a system because
    this is exactly what DNCs do when applied on the bAbI dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的前面，当我们谈论神经n-gram时，我们说这不是一个能够阅读故事后回答问题的人工智能的复杂性。现在我们已经达到了可以构建这样一个系统的程度，因为这正是应用在bAbI数据集上的DNC所做的事情。
- en: 'The bAbI dataset is a synthetic dataset consisting of 20 sets of stories, questions
    on those stories, and their answers. Each set represents a specific and unique
    task of reasoning and inference from text. In the version we’ll use, each task
    contains 10,000 questions for training and 1,000 questions for testing. For example,
    the following story (from which the passage we saw earlier was adapted) is from
    the *lists-and-sets* task where the answers to the questions are lists/sets of
    objects mentioned in the story:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: bAbI数据集是一个合成数据集，包含20组故事、关于这些故事的问题以及它们的答案。每组代表一个特定且独特的推理和推断任务。在我们将使用的版本中，每个任务包含10,000个用于训练的问题和1,000个用于测试的问题。例如，以下故事（从中我们之前看到的段落改编而来）来自*lists-and-sets*任务，其中问题的答案是故事中提到的对象的列表/集合：
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This is taken directly from the dataset, and as you can see, a story is organized
    into numbered sentences that start from 1\. Each question ends with a question
    mark, and the words that directly follow the question mark are the answers. If
    an answer consists of more than one word, the words are separated by commas. The
    numbers that follow the answers are supervisory signals that point to the sentences
    that contain the answers’ words.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这是直接从数据集中获取的，正如你所看到的，一个故事被组织成从1开始编号的句子。每个问题以问号结尾，紧接问号的单词是答案。如果答案由多个单词组成，这些单词之间用逗号分隔。跟随答案的数字是指向包含答案单词的句子的监督信号。
- en: To make the tasks more challenging, we’ll discard these supervisory signals
    and let the system learn to read the text and figure out the answer on its own.
    Following the DNC paper, we’ll preprocess our dataset by removing all the numbers
    and punctuation except for “?” and “.”, bringing all the words to lowercase, and
    replacing the answer words with dashes “-” in the input sequence. After this we
    get 159 unique words and marks (lexicons) across all the tasks, so we’ll encode
    each lexicon as a one-hot vector of size 159, no embeddings, just the plain words
    directly. Finally, we combine all of the 200,000 training questions to train the
    model jointly on them, and we keep each task’s test questions separate to test
    the trained model afterward on each task individually. This whole process is implemented
    in the *preprocess.py* file in the code repository.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使任务更具挑战性，我们将放弃这些监督信号，让系统学会阅读文本并自行找出答案。遵循DNC论文，我们将通过删除所有数字和标点（除了“？”和“.”）来预处理我们的数据集，将所有单词转换为小写，并在输入序列中用破折号“-”替换答案单词。之后，我们得到了跨所有任务的159个唯一单词和标记（词汇表），因此我们将每个词汇表编码为大小为159的独热向量，没有嵌入，只是直接的纯单词。最后，我们将所有20万个训练问题组合在一起，共同训练模型，并将每个任务的测试问题保持分开，以便在每个任务上单独测试训练后的模型。整个过程在代码库中的*preprocess.py*文件中实现。
- en: To train the model, we randomly sample a story from the encoded training data,
    pass it through the DNC with an LSTM controller, and get the corresponding output
    sequence. We then measure the loss between the output sequence and the desired
    sequence using the softmax cross-entropy loss, but only on the steps that contain
    answers. All the other steps are ignored by weighting the loss with a weights
    vector that has 1 at the answer’s steps and 0 elsewhere. This process is implemented
    in the *train_babi.py* file.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型时，我们从编码的训练数据中随机抽取一个故事，通过带有LSTM控制器的DNC，得到相应的输出序列。然后，我们使用softmax交叉熵损失来衡量输出序列与期望序列之间的损失，但仅在包含答案的步骤上进行。所有其他步骤都通过使用权重向量来忽略损失，该向量在答案步骤处为1，在其他地方为0。这个过程在*train_babi.py*文件中实现。
- en: After the model is trained, we test its performance on the remaining test questions.
    Our metric will be the percentage of questions the model failed to answer in each
    task. An answer to a question is the word with the largest softmax value in the
    output, or the most probable word. A question is considered to be answered correctly
    if all of its answer’s words are the correct words. If the model failed to answer
    more than 5% of a task’s questions, we consider that the model failed on that
    task. The testing procedure is found in the *test_babi.py* file.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们将在剩余的测试问题上测试其性能。我们的度量标准将是模型未能回答每个任务中的问题的百分比。问题的答案是输出中具有最大softmax值的单词，或者是最有可能的单词。如果所有答案单词都是正确的单词，则认为问题被正确回答。如果模型未能回答某个任务的问题超过5％，我们认为模型在该任务上失败。测试过程在*test_babi.py*文件中找到。
- en: 'After training the model for about 500,000 iterations (caution—it takes a long
    time!), we can see that it’s performing pretty well on most of the tasks. At the
    same time, it’s performing badly on more difficult tasks like *pathfinding*, where
    the task is to answer questions about how to get from one place to another. The
    following report compares our model’s results to the mean values reported in the
    original DNC paper:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型约500,000次迭代后（注意-需要很长时间！），我们可以看到它在大多数任务上表现得相当不错。与此同时，在更困难的任务（如*寻路*）上表现不佳，该任务是回答如何从一个地方到另一个地方的问题。以下报告将我们模型的结果与原始DNC论文中报告的平均值进行比较：
- en: '[PRE7]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we’ve explored the cutting edge of deep learning research with
    NTMs and DNCs, culminating with the implementation of a model that can solve an
    involved reading comprehension task.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探索了深度学习研究的前沿，使用NTMs和DNCs，最终实现了一个可以解决复杂阅读理解任务的模型。
- en: In the final chapter of this book, we’ll begin to explore a very different space
    of problems known as reinforcement learning. We’ll build an intuition for this
    new class of tasks and develop an algorithmic foundation to tackle these problems
    using the deep learning tools we’ve developed thus far.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一章中，我们将开始探索一个非常不同的问题空间，即强化学习。我们将建立对这一新类任务的直觉，并利用迄今为止我们开发的深度学习工具来解决这些问题的算法基础。
- en: '^([1](ch12.xhtml#idm45934167096448-marker)) Source: [Graves et al. “Neural
    Turing Machines.” (2014)](https://arxiv.org/abs/1410.5401)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch12.xhtml#idm45934167096448-marker)) 来源：[格雷夫斯等人“神经图灵机。”（2014）](https://arxiv.org/abs/1410.5401)
