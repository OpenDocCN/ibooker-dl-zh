- en: 'Chapter 3\. Cats Versus Dogs: Transfer Learning in 30 Lines with Keras'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章。猫与狗：使用Keras中的30行进行迁移学习
- en: Imagine that we want to learn how to play the melodica, a wind instrument in
    the form of a handheld keyboard. Without a musical background, and the melodica
    being our very first instrument, it might take us a few months to become proficient
    at playing it. In contrast, if we were already skilled at playing another instrument,
    such as the piano, it might just take a few days, given how similar the two instruments
    are. Taking the learnings from one task and fine tuning them on a similar task
    is something we often do in real life (as illustrated in [Figure 3-1](part0005.html#transfer_learning_in_real_life)).
    The more similar the two tasks are, the easier it is to adapt the learning from
    one task to the other.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想学习如何演奏口琴，这是一种手持键盘形式的吹奏乐器。如果没有音乐背景，口琴是我们的第一件乐器，可能需要我们几个月的时间才能熟练演奏。相比之下，如果我们已经擅长演奏另一种乐器，比如钢琴，可能只需要几天的时间，因为这两种乐器非常相似。将一个任务的经验应用到另一个类似任务上进行微调是我们在现实生活中经常做的事情（如[图3-1](part0005.html#transfer_learning_in_real_life)所示）。两个任务越相似，将一个任务的经验应用到另一个任务上就越容易。
- en: We can apply this phenomenon from real life to the world of deep learning. Starting
    a deep learning project can be relatively quick when using a pretrained model,
    which reuses the knowledge that it learned during its training, and adapt it to
    the task at hand. This process is known as *transfer learning*.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将现实生活中的这种现象应用到深度学习的世界中。使用预训练模型开始一个深度学习项目可能会相对快速，因为它重新利用了在训练过程中学到的知识，并将其适应到手头的任务中。这个过程被称为*迁移学习*。
- en: In this chapter, we use transfer learning to modify existing models by training
    our own classifier in minutes using Keras. By the end of this chapter, we will
    have several tools in our arsenal to create high-accuracy image classifiers for
    any task.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们使用迁移学习来通过在几分钟内使用Keras训练我们自己的分类器来修改现有模型。到本章结束时，我们将拥有几种工具来创建任何任务的高准确度图像分类器。
- en: '![Transfer learning in real life](../images/00169.jpeg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![现实生活中的迁移学习](../images/00169.jpeg)'
- en: Figure 3-1\. Transfer learning in real life
  id: totrans-5
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-1。现实生活中的迁移学习
- en: Adapting Pretrained Models to New Tasks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将预训练模型适应新任务
- en: 'Before we discuss the process of transfer learning, let’s quickly take a step
    back and review the primary reasons for the boom in deep learning:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论迁移学习的过程之前，让我们快速回顾一下深度学习蓬勃发展的主要原因：
- en: Availability of bigger and better-quality datasets like ImageNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像ImageNet这样更大更高质量的数据集的可用性
- en: Better compute available; i.e., faster and cheaper GPUs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的计算资源可用；即更快速和更便宜的GPU
- en: Better algorithms (model architecture, optimizer, and training procedure)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的算法（模型架构、优化器和训练过程）
- en: Availability of pretrained models that have taken months to train but can be
    quickly reused
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复使用的预训练模型，它们经过数月的训练，但可以快速重复使用
- en: The last point is probably one of the biggest reasons for the widespread adoption
    of deep learning by the masses. If every training task took a month, not more
    than a handful of researchers with deep pockets would be working in this area.
    Thanks to transfer learning, the underappreciated hero of training models, we
    can now modify an existing model to suit our task in as little as a few minutes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点可能是普及深度学习的最重要原因之一。如果每个训练任务都需要一个月的时间，只有少数资金雄厚的研究人员才会在这个领域工作。由于迁移学习，训练模型的被低估的英雄，我们现在可以在几分钟内修改现有模型以适应我们的任务。
- en: For example, we saw in [Chapter 2](part0004.html#3Q283-13fa565533764549a6f0ab7f11eed62b)
    that the pretrained ResNet-50 model, which is trained on ImageNet, can predict
    feline and canine breeds, among thousands of other categories. So, if we just
    want to classify between the high-level “cat” and “dog” categories (and not the
    lower-level breeds), we can begin with the ResNet-50 model and quickly retrain
    this model to classify cats and dogs. All we need to do is show it a dataset with
    these two categories during training, which should take anywhere between a few
    minutes to a few hours. In comparison, if we had to train a cat versus dog model
    without a pretrained model, it could take several hours to days.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们在[第2章](part0004.html#3Q283-13fa565533764549a6f0ab7f11eed62b)中看到，预训练的ResNet-50模型，它在ImageNet上训练，可以预测猫和狗的品种，以及其他成千上万个类别。因此，如果我们只想在高级别的“猫”和“狗”类别之间进行分类（而不是低级别的品种），我们可以从ResNet-50模型开始，快速重新训练此模型以分类猫和狗。我们只需要在训练期间向其展示包含这两个类别的数据集，这应该需要几分钟到几小时不等。相比之下，如果我们不使用预训练模型来训练猫与狗的模型，可能需要几个小时到几天的时间。
- en: A Shallow Dive into Convolutional Neural Networks
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对卷积神经网络的浅层探讨
- en: We have been using the term “model” to refer to the part of AI that makes our
    predictions. In deep learning for computer vision, that model is usually a special
    type of neural network called a CNN. Although we explore CNNs in greater detail
    later in the book, we look at them very briefly in relation to training them via
    transfer learning here.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直使用术语“模型”来指代AI中用于做出预测的部分。在计算机视觉的深度学习中，该模型通常是一种称为CNN的特殊类型的神经网络。尽管我们稍后会更详细地探讨CNN，但在这里我们简要地看一下如何通过迁移学习训练它们。
- en: 'In machine learning, we need to convert data into a set of discernible features
    and then add a classification algorithm to classify them. It’s the same with CNNs.
    They consist of two parts: convolutional layers and fully connected layers. The
    job of the convolutional layers is to take the large number of pixels of an image
    and convert them into a much smaller representation; that is, features. The fully
    connected layers convert these features into probabilities. A fully connected
    layer is really a neural network with hidden layers, as we saw in [Chapter 1](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b).
    In summary, the convolutional layers act as feature extractors, whereas the fully
    connected layers act as classifiers. [Figure 3-2](part0005.html#a_high-level_overview_of_a_convolutional)
    shows a high-level overview of a CNN.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们需要将数据转换为一组可识别的特征，然后添加一个分类算法对它们进行分类。CNN也是如此。它们由两部分组成：卷积层和全连接层。卷积层的工作是将图像的大量像素转换为一个更小的表示；即特征。全连接层将这些特征转换为概率。全连接层实际上是一个具有隐藏层的神经网络，正如我们在[第1章](part0003.html#2RHM3-13fa565533764549a6f0ab7f11eed62b)中看到的那样。总之，卷积层充当特征提取器，而全连接层充当分类器。[图3-2](part0005.html#a_high-level_overview_of_a_convolutional)显示了CNN的高级概述。
- en: '![A high-level overview of a Convolutional Neural Network](../images/00082.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![卷积神经网络的高级概述](../images/00082.jpeg)'
- en: Figure 3-2\. A high-level overview of a CNN
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2. CNN的高级概述
- en: Imagine that we want to detect a human face. We might want to use a CNN to classify
    an image and determine whether it contains a face. Such a CNN would be composed
    of several layers connected one after another. These layers represent mathematical
    operations. The output of one layer is the input to the next. The first (or the
    lowermost) layer is the input layer, where the input image is fed. The last (or
    the topmost) layer is the output layer, which gives the predictions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要检测一个人脸。我们可能想要使用CNN对图像进行分类，并确定其中是否包含人脸。这样的CNN由几个层连接在一起组成。这些层代表数学运算。一个层的输出是下一个层的输入。第一个（或最底层）是输入层，输入图像被馈送到这里。最后一个（或最顶层）是输出层，给出预测。
- en: The way it works is the image is fed into the CNN and passes through a series
    of layers, with each performing a mathematical operation and passing the result
    to the subsequent layer. The resulting output is a list of object classes and
    their probabilities. For example, categories like ball—65%, grass—20%, and so
    on. If the output for an image contains a “face” class with a 70% probability,
    we conclude that there is a 70% likelihood that the image contains a human face.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作方式是将图像馈送到CNN中，并通过一系列层，每个层执行数学运算并将结果传递给下一个层。最终的输出是一个对象类别列表及其概率。例如，类别如球—65%，草—20%，等等。如果图像的输出包含一个“人脸”类别，概率为70%，我们可以得出结论，图像中包含人脸的可能性为70%。
- en: Note
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An intuitive (and overly simplified) way to look at CNNs is to see them as a
    series of filters. As the word filter implies, each layer acts as a sieve of information,
    letting something “pass through” only if it recognizes it. (If you have heard
    of high-pass and low-pass filters in electronics, this might seem familiar.) We
    say that the layer was “activated” for that information. Each layer is activated
    for visual patterns resembling parts of cats, dogs, cars, and so forth. If a layer
    does not recognize information (due to what it learned while training), its output
    is close to zero. CNNs are the “bouncers” of the deep learning world!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看待CNN的一种直观（和过于简化的）方式是将它们视为一系列滤波器。正如“滤波器”一词所暗示的，每个层都充当信息的筛子，只有在识别到信息时才“通过”。（如果你听说过电子学中的高通和低通滤波器，这可能会很熟悉。）我们说该层对该信息“激活”。每个层对类似猫、狗、汽车等部分的视觉模式被激活。如果一个层没有识别信息（由于训练时学到的内容），其输出接近于零。CNN是深度学习世界的“保安”！
- en: In the facial detection example, lower-level layers ([Figure 3-3](part0005.html#left_parenthesisaright_parenthesis_lower),
    a; layers that are closer to the input image) are “activated” for simpler shapes;
    for example, edges and curves. Because these layers activate only for basic shapes,
    they can be easily reused for a different purpose than face recognition such as
    detecting a car (every image is composed of edges and curves, after all). Middle-level
    layers ([Figure 3-3](part0005.html#left_parenthesisaright_parenthesis_lower) b)
    are activated for more complex shapes such as eyes, noses, and lips. These layers
    are not nearly as reusable as the lower-level layers. They might not be as useful
    for detecting a car, but might still be useful for detecting animals. And higher-level
    layers ([Figure 3-3](part0005.html#left_parenthesisaright_parenthesis_lower) c)
    are activated for even more complex shapes; for example, most of the human face.
    These layers tend to be more task-specific and thus the least reusable across
    other image classification problems.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在人脸检测示例中，较低级别的层（[图3-3](part0005.html#left_parenthesisaright_parenthesis_lower)
    a; 靠近输入图像的层）被“激活”以获取更简单的形状；例如，边缘和曲线。因为这些层仅对基本形状激活，所以它们可以很容易地被重新用于不同于人脸识别的目的，比如检测汽车（毕竟每个图像都由边缘和曲线组成）。中级别的层（[图3-3](part0005.html#left_parenthesisaright_parenthesis_lower)
    b）被激活以获取更复杂的形状，比如眼睛、鼻子和嘴唇。这些层不像较低级别的层那样容易被重复使用。它们可能不太适用于检测汽车，但可能仍然适用于检测动物。更高级别的层（[图3-3](part0005.html#left_parenthesisaright_parenthesis_lower)
    c）被激活以获取更复杂的形状，例如大部分人脸。这些层往往更具任务特定性，因此在其他图像分类问题中最不可重复使用。
- en: '![(a) Lower level activations, followed by (b) mid-level activations and (c)
    upper layer activations (image source: Convolutional Deep Belief Networks for
    Scalable Unsupervised Learning of Hierarchical Representations, Lee et al., ICML
    2009)](../images/00122.jpeg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: （a）较低级别的激活，接着是（b）中级别的激活和（c）上层的激活（图片来源：Lee等人的《用于可扩展无监督学习的分层表示的卷积深度信念网络》，ICML
    2009）
- en: 'Figure 3-3\. (a) Lower-level activations, followed by (b) midlevel activations
    and (c) upper-layer activations (image source: Convolutional Deep Belief Networks
    for Scalable Unsupervised Learning of Hierarchical Representations, Lee et al.,
    ICML 2009)'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-3. (a) 低层激活，接着是(b) 中层激活和(c) 上层激活（图片来源：Convolutional Deep Belief Networks
    for Scalable Unsupervised Learning of Hierarchical Representations, Lee et al.,
    ICML 2009）
- en: The complexity and power of what a layer can recognize increases as we approach
    the final layers. Conversely, the reusability of a layer decreases as we get closer
    to the output. This will become apparent very soon when we look at what these
    layers learn.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们接近最后的层，一层可以识别的复杂性和能力增加。相反，随着我们接近输出，一层的可重用性减少。当我们看到这些层学习的内容时，这很快就会变得明显。
- en: Transfer Learning
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 迁移学习
- en: If we want to transfer knowledge from one model to another, we want to reuse
    more of the *generic* layers (closer to the input) and fewer of the *task-specific*
    layers (closer to the output). In other words, we want to remove the last few
    layers (typically the fully connected layers) so that we can utilize the more
    generic ones, and add layers that are geared toward our specific classification
    task. Once training begins, the generic layers (which form the majority of our
    new model) are kept frozen (i.e., they are unmodifiable), whereas the newly added
    task-specific layers are allowed to be modified. This is how transfer learning
    helps quickly train new models. [Figure 3-4](part0005.html#an_overview_of_transfer_learning)
    illustrates this process for a pretrained model trained for task X adapted to
    task Y.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要从一个模型转移知识到另一个模型，我们希望重复使用更多*通用*层（靠近输入）和更少*任务特定*层（靠近输出）。换句话说，我们想要移除最后几层（通常是全连接层），以便我们可以利用更通用的层，并添加针对我们特定分类任务的层。一旦训练开始，通用层（构成我们新模型的大部分）将保持冻结（即，它们是不可修改的），而新添加的任务特定层将被允许修改。这就是迁移学习如何帮助快速训练新模型的方式。[图3-4](part0005.html#an_overview_of_transfer_learning)说明了这个过程，即针对任务X训练的预训练模型如何适应任务Y。
- en: '![An overview of transfer learning](../images/00284.jpeg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![迁移学习概述](../images/00284.jpeg)'
- en: Figure 3-4\. An overview of transfer learning
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4. 迁移学习概述
- en: Fine Tuning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: Basic transfer learning gets us only so far. We usually add only two to three
    fully connected layers after the generic layers to make the new classifier model.
    If we want higher accuracy, we must allow more layers to be trained. This means
    unfreezing some of the layers that would have otherwise been frozen in transfer
    learning. This is known as *fine tuning*. [Figure 3-5](part0005.html#fine-tuning_a_convolutional_neural_netwo)
    shows an example where some convolutional layers near the head/top are unfrozen
    and trained for the task at hand.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的迁移学习只能带我们走这么远。我们通常在通用层之后只添加两到三个全连接层来构建新的分类器模型。如果我们想要更高的准确性，我们必须允许更多的层被训练。这意味着解冻一些在迁移学习中本来会被冻结的层。这被称为*微调*。[图3-5](part0005.html#fine-tuning_a_convolutional_neural_netwo)展示了一个例子，其中一些接近头部/顶部的卷积层被解冻并针对手头的任务进行训练。
- en: '![Fine-tuning a convolutional neural network](../images/00243.jpeg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![微调卷积神经网络](../images/00243.jpeg)'
- en: Figure 3-5\. Fine tuning a convolutional neural network
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5. 微调卷积神经网络
- en: It’s obvious that compared to basic transfer learning, more layers are tweaked
    to our dataset during fine tuning. Because a higher number of layers have adapted
    to our task compared to transfer learning, we can achieve greater accuracy for
    our task. The decision on how many layers to fine tune is dependent on the amount
    of data at hand as well as the similarity of the target task to the original dataset
    on which the pretrained model was trained.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，与基本的迁移学习相比，在微调过程中会调整更多的层到我们的数据集中。因为与迁移学习相比，更多的层已经适应了我们的任务，我们可以为我们的任务实现更高的准确性。微调多少层的决定取决于手头的数据量以及目标任务与预训练模型训练的原始数据集的相似性。
- en: We often hear data scientists saying, “I fine tuned the model,” which means
    that they took a pretrained model, removed task-specific layers and added new
    ones, froze the lower layers, and trained the upper part of the network on the
    new dataset they had.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常听到数据科学家说，“我微调了模型”，这意味着他们拿了一个预训练模型，移除了任务特定层并添加了新的层，冻结了较低层，然后在新数据集上训练网络的上部分。
- en: Note
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In daily lingo, transfer learning and fine tuning are used interchangeably.
    When spoken, transfer learning is used more as a general concept, whereas fine
    tuning is referred to as its implementation.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在日常用语中，迁移学习和微调是可以互换使用的。在口语中，迁移学习更多地被用作一个概念，而微调则被称为其实施。
- en: How Much to Fine Tune
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调多少
- en: 'How many layers of a CNN should we fine tune? This can be guided by the following
    two factors:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该微调卷积神经网络的多少层？这可以由以下两个因素来指导：
- en: How much data do we have?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多少数据？
- en: If we have a couple hundred labeled images, it would be difficult to train and
    test a freshly defined model from scratch (i.e., define a model architecture with
    random seed weights) because we need a lot more data. The danger of training with
    such a small amount of data is that these powerful networks might potentially
    memorize it, leading to undesirable overfitting (which we explore later in the
    chapter). Instead, we will borrow a pretrained network and fine tune the last
    few layers. But if we had a million labeled images, it would be feasible to fine
    tune all layers of the network and, if necessary, train from scratch. So, the
    amount of task-specific data dictates whether we can fine tune, and how much.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有几百张标记的图像，从头开始训练和测试一个全新定义的模型（即，定义一个具有随机种子权重的模型架构）将会很困难，因为我们需要更多的数据。用这么少的数据进行训练的危险是这些强大的网络可能会潜在地记住它，导致不良的过拟合（我们将在本章后面探讨）。相反，我们将借用一个预训练的网络并微调最后几层。但如果我们有一百万张标记的图像，微调网络的所有层是可行的，如果必要，可以从头开始训练。因此，任务特定数据的数量决定了我们是否可以微调，以及微调多少。
- en: How similar is the data?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 数据有多相似？
- en: If the task-specific data is similar to the data used for the pretrained network,
    we can fine tune the last few layers. But if our task is identifying different
    bones in an X-ray image and we want to start out from an ImageNet trained network,
    the high dissimilarity between regular ImageNet images and X-ray images would
    require nearly all layers to be trained.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务特定数据与预训练网络使用的数据相似，我们可以调整最后几层。但是，如果我们的任务是在X射线图像中识别不同的骨骼，并且我们想要从ImageNet训练的网络开始，那么常规ImageNet图像和X射线图像之间的高差异将要求几乎所有层都进行训练。
- en: To summarize, [Table 3-1](part0005.html#cheatsheet_for_when_and_how_to_fine_tune)
    offers an easy-to-follow cheat sheet.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，[表3-1](part0005.html#cheatsheet_for_when_and_how_to_fine_tune)提供了一个易于遵循的备忘单。
- en: Table 3-1\. Cheatsheet for when and how to fine tune
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-1。调整的时间和方式的备忘单
- en: '|   | **High similarity among datasets** | **Low similarity among datasets**
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|   | **数据集之间相似度高** | **数据集之间相似度低** |'
- en: '| --- | --- | --- |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Large amount of training data** | Fine tune all layers | Train from scratch,
    or fine tune all layers |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| **大量训练数据** | 调整所有层 | 从头开始训练，或者调整所有层 |'
- en: '| **Small amount of training data** | Fine tune last few layers | Tough luck!
    Train on a smaller network with heavy data augmentation or somehow get more data
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **少量训练数据** | 调整最后几层 | 运气不佳！使用较小的网络进行训练，进行大量数据增强，或以某种方式获取更多数据 |'
- en: Enough theory, let’s see it in action.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 足够的理论，让我们看看实际操作。
- en: Building a Custom Classifier in Keras with Transfer Learning
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Keras中使用迁移学习构建自定义分类器
- en: 'As promised, it’s time to build our state-of-the-art classifier in 30 lines
    or less. At a high level, we will use the following steps:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如承诺的那样，现在是用30行或更少行构建我们的最先进分类器的时候了。在高层次上，我们将使用以下步骤：
- en: Organize the data. Download labeled images of cats and dogs and then divide
    the images into training and validation folders.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织数据。下载标记的猫和狗的图像，然后将图像分成训练和验证文件夹。
- en: Build the data pipeline. Define a pipeline for reading data, including preprocessing
    the images (e.g., resizing) and grouping multiple images together into batches.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建数据管道。定义一个用于读取数据的管道，包括对图像进行预处理（例如，调整大小）和将多个图像组合成批次。
- en: Augment the data. In the absence of a ton of training images, make small changes
    (augmentation) like rotation, zooming, and so on to increase variation in training
    data.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加数据。在缺少大量训练图像的情况下，进行小的更改（增强），如旋转、缩放等，以增加训练数据的变化。
- en: Define the model. Take a pretrained model, remove the last few task-specific
    layers, and append a new classifier layer. Freeze the weights of original layers
    (i.e., make them unmodifiable). Select an optimizer algorithm and a metric to
    track (like accuracy).
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型。采用预训练模型，删除最后几个任务特定层，并附加一个新的分类器层。冻结原始层的权重（即，使它们不可修改）。选择一个优化算法和一个要跟踪的指标（如准确性）。
- en: Train and test. Train for a few iterations until our validation accuracy is
    high. Save the model to eventually load as part of any application for predictions.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练和测试。训练几次迭代，直到我们的验证准确率很高。保存模型，以便最终加载到任何应用程序中进行预测。
- en: This will all make sense pretty soon. Let’s explore this process in detail.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切很快就会变得清晰起来。让我们详细探讨这个过程。
- en: Organize the Data
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组织数据
- en: It’s essential to understand the distinction between train, validation, and
    test data. Let’s look at a real-world analogy of a student preparing for standardized
    exams (e.g., SAT in the US, the Gaokao in China, JEE in India, CSAT in Korea,
    etc.). The in-class instruction and homework assignments are analogous to the
    training process. The quizzes, midterms, and other tests in school are the equivalent
    to the validation—the student is able to take them frequently, assess performance,
    and make improvements in their study plan. They’re ultimately optimizing for their
    performance in the final standardized exam for which they get only one chance.
    The final exam is equivalent to the test set—the student does not get an opportunity
    to improve here (ignoring the ability to retake the test). This is their one shot
    at showing what they have learned.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理解训练、验证和测试数据之间的区别至关重要。让我们看一个学生为标准化考试（例如美国的SAT、中国的高考、印度的JEE、韩国的CSAT等）做准备的现实类比。课堂教学和家庭作业类比于训练过程。学校中的小测验、期中考试和其他测试相当于验证，学生可以经常参加这些测试，评估表现，并在学习计划中做出改进。他们最终是为了在只有一次机会的最终标准化考试中表现最佳。期末考试相当于测试集，学生在这里没有机会改进（忽略重考的能力）。这是他们展示所学内容的唯一机会。
- en: 'Similarly, our aim is to give the best predictions in the real world. To enable
    this, we divide our data into three parts: train, validation, and test. A typical
    distribution would be 80% for train, 10% for validation, and 10% for test. Note
    that we randomly divide our data into these three sets in order to ensure the
    least amount of *bias* that might creep in unknowingly. The final accuracy of
    the model is determined by the accuracy on the *test set*, much like the student’s
    score is determined only on their performance on the standardized exam.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们的目标是在现实世界中提供最佳预测。为此，我们将数据分为三部分：训练、验证和测试。典型的分布为80%用于训练，10%用于验证，10%用于测试。请注意，我们随机将数据分成这三组，以确保可能潜入的最少*偏见*。模型的最终准确性由*测试集*上的准确性确定，就像学生的分数仅由他们在标准化考试中的表现确定一样。
- en: The model learns from the training data and uses the validation set to evaluate
    its performance. Machine learning practitioners take this performance as feedback
    to find opportunities to improve their models on a continuous basis, similar to
    how students improve their preparation with the help of quizzes. There are several
    knobs that we can tune to improve performance; for example, the number of layers
    to train.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 模型从训练数据中学习，并使用验证集来评估其性能。机器学习从业者将这种性能作为反馈，以找到持续改进模型的机会，类似于学生如何通过小测验改进他们的准备工作。我们可以调整几个旋钮来提高性能；例如，要训练的层数。
- en: In many research competitions (including *[Kaggle.com](http://Kaggle.com)*),
    contestants receive a test set that is separate from the data they can use for
    building the model. This ensures uniformity across the competition when it comes
    to reporting accuracy. It is up to the contestants to divide the available data
    into training and validation sets. Similarly, during our experiments in this book,
    we will continue to divide data in these two sets, keeping in mind that a test
    dataset is still essential to report real-world numbers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多研究竞赛中（包括*[Kaggle.com](http://Kaggle.com)*），参赛者会收到一个与用于构建模型的数据分开的测试集。这确保了在报告准确性时竞赛中的一致性。参赛者需要将可用数据划分为训练和验证集。同样，在本书的实验中，我们将继续将数据划分为这两个集合，记住测试数据集仍然是报告现实世界数字的必要条件。
- en: So why even use a validation set? Data is sometimes difficult to obtain, so
    why not use all the available samples for training, and then report accuracy on
    them? Sure, when the model begins to learn, it will gradually give higher accuracy
    predictions on the training dataset (called training accuracy). But because they
    are so powerful, deep neural networks can potentially memorize the training data,
    even resulting in 100% accuracy on the training data sometimes. However, its real-world
    performance will be quite poor. It’s like if the student knew the questions that
    would be on the exam before taking it. This is why a validation set, not used
    to train the model, gives a realistic assessment of the model performance. Even
    though we might assign 10-15% of the data as a validation set, it will go a long
    way in guiding us on how good our model really is.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么要使用验证集呢？有时数据很难获取，为什么不使用所有可用样本进行训练，然后在其上报告准确性呢？当模型开始学习时，它将逐渐在训练数据集上给出更高准确性的预测（称为训练准确性）。但由于它们非常强大，深度神经网络有可能记住训练数据，有时甚至在训练数据上达到100%的准确性。然而，其在现实世界中的表现将非常糟糕。这就好像学生在考试之前就知道会出现的问题一样。这就是为什么验证集，不用于训练模型，可以给出模型性能的真实评估。即使我们可能将10-15%的数据分配为验证集，它将在很大程度上指导我们了解我们的模型到底有多好。
- en: 'For the training process, we need to store our dataset in the proper folder
    structure. We’ll divide the images into two sets: training and validation. For
    an image file, Keras will automatically assign the name of the *class* (category)
    based on its parent folder name. [Figure 3-6](part0005.html#example_directory_structure_of_the_train)
    depicts the ideal structure to recreate.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练过程，我们需要将数据集存储在正确的文件夹结构中。我们将图像分为两组：训练和验证。对于图像文件，Keras将根据其父文件夹名称自动分配*类*（类别）的名称。[图3-6](part0005.html#example_directory_structure_of_the_train)展示了重新创建的理想结构。
- en: '![Example directory structure of the training and validation data for different
    classes](../images/00204.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![不同类别的训练和验证数据的示例目录结构](../images/00204.jpeg)'
- en: Figure 3-6\. Example directory structure of the training and validation data
    for different classes
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6\. 不同类别的训练和验证数据的示例目录结构
- en: 'The following sequence of commands can help download the data and achieve this
    directory structure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下一系列命令可以帮助下载数据并实现这个目录结构：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The 25,000 files within the data folder are prefixed with “cat” and “dog.”
    Now, move the files into their respective directories. To keep our initial experiment
    short, we pick 250 random files per class and place them in training and validation
    folders. We can increase/decrease this number anytime, to experiment with a trade-off
    between accuracy and speed:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 数据文件夹中的25,000个文件以“cat”和“dog”为前缀。现在，将文件移动到它们各自的目录中。为了保持我们最初的实验简短，我们每类选择250个随机文件，并将它们放入训练和验证文件夹中。我们可以随时增加/减少这个数字，以尝试在准确性和速度之间取得平衡：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Build the Data Pipeline
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建数据管道
- en: 'To start off with our Python program, we begin by importing the necessary packages:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始我们的Python程序，我们首先导入必要的包：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Place the following lines of configuration right after the import statements,
    which we can modify based on our dataset:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下配置行放在导入语句之后，我们可以根据我们的数据集进行修改：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Number of Classes
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 类别数
- en: 'With two classes to distinguish between, we can treat this problem as one of
    the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个类别需要区分，我们可以将这个问题视为以下之一：
- en: A binary classification task
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个二元分类任务
- en: A multiclass classification task
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类任务
- en: Binary classification
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二元分类
- en: As a binary classification task, it’s important to note that “cat versus dog
    is really “cat versus not cat.” A dog would be classified as a “not cat” much
    like a desk or a ball would. For a given image, the model will give a single probability
    value corresponding to the “cat” class—hence the probability of “not cat” is 1
    - *P(cat)*. If the probability is higher than 0.5, we predict it as “cat”; otherwise,
    “not cat.” To keep things simple, we assume that it’s guaranteed that the test
    set would contain only images of either cats or dogs. Because “cat versus not
    cat” is a binary classification task, we set the number of classes to 1; that
    is, “cat.” Anything that cannot be classified as “cat” will be classified as “not
    cat.”
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作为二元分类任务，重要的是要注意，“猫与狗”实际上是“猫与非猫”。狗会被分类为“非猫”，就像桌子或球一样。对于给定的图像，模型将给出一个与“cat”类对应的单个概率值，因此“非猫”的概率为1
    - *P(cat)*。如果概率高于0.5，我们将预测为“cat”；否则为“非猫”。为了简化问题，我们假设测试集中只包含猫或狗的图像。因为“猫与非猫”是一个二元分类任务，我们将类别数设置为1；即“cat”。任何无法分类为“cat”的内容将被分类为“非猫”。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Keras processes the input data in the alphabetical order of the folder names.
    Because “cat” comes before “dog” alphabetically, our first class for prediction
    is “cat.” For a multiclass task, we can apply the same concept and infer each
    class identifier (index) based on the folder sort order. Note that the class index
    starts at 0 for the first class.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Keras按照文件夹名称的字母顺序处理输入数据。因为按字母顺序，“cat”在“dog”之前，我们的第一个预测类别是“cat”。对于多类任务，我们可以应用相同的概念，并根据文件夹排序顺序推断每个类别标识符（索引）。请注意，类别索引从第一个类别开始为0。
- en: Multiclass classification
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多类分类
- en: In a hypothetical world that had only cats and dogs and nothing else, a “not
    cat” would always be a dog. So the label “not cat” could simply be replaced with
    the label “dog.” However, in the real world, we have more than two types of objects.
    As explained before, a ball or a sofa would also be classified as “dog,” which
    would be incorrect. Hence, for a real-world scenario, treating this as a multiclass
    classification task instead of a binary classification task is far more useful.
    As a multiclass classification task, we predict separate probability values for
    each class, and the highest one is our winner. In the case of “cat versus dog,”
    we set the number of classes to two. To keep our code reusable for future tasks,
    we will treat this as a multiclassification task.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个假设的世界中，只有猫和狗，没有其他东西，一个“非猫”总是狗。因此，“非猫”标签可以简单地替换为“狗”标签。然而，在现实世界中，我们有超过两种类型的物体。如前所述，球或沙发也会被分类为“狗”，这是不正确的。因此，在真实场景中，将其视为多类别分类任务而不是二元分类任务要更有用。作为多类别分类任务，我们为每个类别预测单独的概率值，最高的概率值是我们的赢家。在“猫与狗”案例中，我们将类别数设置为两。为了使我们的代码可重用于未来的任务，我们将把这视为多类别任务。
- en: Batch Size
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量大小
- en: 'At a high level, the training process includes the following steps:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，训练过程包括以下步骤：
- en: Make predictions on images (*forward pass*).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对图像进行预测（*前向传递*）。
- en: Determine which predictions were incorrect and propagate back the difference
    between the prediction and the true value (*backpropagation*).
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定哪些预测是错误的，并将预测与真实值之间的差异传播回去（*反向传播*）。
- en: Rinse and repeat until the predictions become sufficiently accurate.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反复进行，直到预测变得足够准确。
- en: It’s quite likely that the initial iteration would have close to 0% accuracy.
    Repeating the process several times, however, can yield a highly accurate model
    (>90%).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 初始迭代很可能准确率接近0%。然而，重复这个过程几次可能会产生一个高度准确的模型（>90%）。
- en: The batch size defines how many images are seen by the model at a time. It’s
    important that each batch has a good variety of images from different classes
    in order to prevent large fluctuations in the accuracy metric between iterations.
    A sufficiently large batch size would be necessary for that. However, it’s important
    not to set the batch size too large; a batch that is too large might not fit in
    GPU memory, resulting in an “out of memory” crash. Usually, batch sizes are set
    as powers of 2\. A good number to start with is 64 for most problems, and we can
    play with the number by increasing or decreasing it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小定义了模型一次看到多少张图片。重要的是，每个批次中有来自不同类别的各种图片，以防止在迭代之间的准确度指标出现大幅波动。为此，需要一个足够大的批量大小。然而，重要的是不要设置批量大小过大；太大的批量可能无法适应GPU内存，导致“内存不足”崩溃。通常，批量大小设置为2的幂。对于大多数问题，一个好的起点是64，我们可以通过增加或减少数量来调整。
- en: Data Augmentation
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强
- en: Usually, when we hear deep learning, we associate it with millions of images.
    So, 500 images like what we have might be a low number for real-world training.
    While these deep neural networks are powerful, a little too powerful for small
    quantities of data, the danger of a limited set of training images is that the
    neural network might memorize our training data, and show great prediction performance
    on the training set, but bad accuracy on the validation set. In other words, the
    model has overtrained and does not generalize on previously unseen images. And
    we definitely don’t want that.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们听到深度学习时，我们会将其与数百万张图片联系在一起。因此，我们拥有的500张图片可能对于真实世界的训练来说是一个较低的数字。虽然这些深度神经网络非常强大，但对于少量数据来说可能过于强大，训练图像集数量有限的危险在于神经网络可能会记住我们的训练数据，并在训练集上表现出色，但在验证集上准确度较低。换句话说，模型已经过度训练，无法泛化到以前未见过的图像。我们绝对不希望出现这种情况。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Often, when we attempt to train a neural network on a small amount of data,
    the result is a model that performs extremely well on the training data itself
    but makes rather poor predictions on data that it has not seen before. Such a
    model would be described as an *overfitted* model and the problem itself is known
    as *overfitting*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们尝试在少量数据上训练神经网络时，结果是模型在训练数据上表现非常好，但在之前未见过的数据上做出相当糟糕的预测。这样的模型将被描述为*过度拟合*模型，问题本身被称为*过度拟合*。
- en: '[Figure 3-7](part0005.html#underfittingcomma_overfittingcomma_and_i) illustrates
    this phenomenon for a distribution of points close to a sine curve (with little
    noise). The dots represent the training data visible to our network, and the crosses
    represent the testing data that was not seen during training. On one extreme (underfitting),
    an unsophisticated model, such as a linear predictor, will not be able to represent
    the underlying distribution well and a high error rate on both the training data
    and the test data will result. On the other extreme (overfitting), a powerful
    model (such as a deep neural network) might have the capacity to memorize the
    training data, which would result in a really low error on the training data,
    but still a high error on the testing data. What we want is the happy middle where
    the training error and the testing error are both modestly low, which ideally
    ensures that our model will perform just as well in the real world as it does
    during training.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-7](part0005.html#underfittingcomma_overfittingcomma_and_i)说明了这种现象，即接近正弦曲线的点的分布（几乎没有噪音）。点代表我们的网络可见的训练数据，叉代表在训练期间未见过的测试数据。在一个极端情况下（欠拟合），一个简单的模型，如线性预测器，将无法很好地表示基础分布，导致训练数据和测试数据上的高错误率。在另一个极端情况下（过拟合），一个强大的模型（如深度神经网络）可能有能力记住训练数据，这将导致训练数据上的错误率非常低，但在测试数据上仍然有很高的错误率。我们希望的是在训练错误和测试错误都相对较低的愉快中间位置，这理想情况下确保我们的模型在真实世界中的表现与训练期间一样好。'
- en: '![Underfitting, overfitting, and ideal fitting for points close to a sine curve](../images/00170.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![对于接近正弦曲线的点的欠拟合、过拟合和理想拟合](../images/00170.jpeg)'
- en: Figure 3-7\. Underfitting, overfitting, and ideal fitting for points close to
    a sine curve
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7。欠拟合、过拟合和理想拟合对于接近正弦曲线的点
- en: 'With great power comes great responsibility. It’s our responsibility to ensure
    that our powerful deep neural network does not overfit on our data. Overfitting
    is common when we have little training data. We can reduce this likelihood in
    a few different ways:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 伟大的力量伴随着伟大的责任。我们有责任确保我们强大的深度神经网络不会在我们的数据上过拟合。当我们有少量训练数据时，过拟合是常见的。我们可以通过几种不同的方式减少这种可能性：
- en: Somehow get more data
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以某种方式获取更多数据
- en: Heavily augment existing data
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大幅度增强现有数据
- en: Fine tune fewer layers
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调更少的层
- en: 'There are often situations for which there’s not enough data available. Perhaps
    we’re working on a niche problem and data is difficult to come by. But there are
    a few ways that we can artificially augment our dataset for classification:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通常存在数据不足的情况。也许我们正在处理一个小众问题，数据很难获得。但我们可以通过一些方式人为地增加我们的分类数据集：
- en: Rotation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转
- en: In our example, we might want to rotate the 500 images randomly by 20 degrees
    in either direction, yielding up to 20,000 possible unique images.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们可能希望随机将这500张图像旋转20度，向任一方向，从而产生多达20,000个可能的独特图像。
- en: Random Shift
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 随机移动
- en: Shift the images slightly to the left, or to the right.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像稍微向左或向右移动。
- en: Zoom
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放
- en: Zoom in and out slightly of the image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 稍微放大或缩小图像。
- en: By combining rotation, shifting, and zooming, the program can generate an almost
    infinite number of unique images. This important step is called *data augmentation*.
    Data augmentation is useful not only for adding more data, but also for training
    more robust models for real-world scenarios. For example, not all images have
    the cat properly centered in the middle or at a perfect 0-degree angle. Keras
    provides the `ImageDataGenerator` function that augments the data while it is
    being loaded from the directory. To illustrate what data augmentations of images
    look like, [Figure 3-8](part0005.html#possible_image_augmentations_generated_f)
    showcases example augmentations generated by the [imgaug](https://oreil.ly/KYA9O)
    library for a sample image. (Note that we will not be using imgaug for our actual
    training.)
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合旋转、移动和缩放，程序可以生成几乎无限数量的独特图像。这一重要步骤称为*数据增强*。数据增强不仅有助于添加更多数据，还有助于为真实场景训练更健壮的模型。例如，并非所有图像都将猫正确地放在中间或以完美的0度角。Keras提供了`ImageDataGenerator`函数，该函数在从目录加载数据时增强数据。为了说明图像增强的效果，[图3-8](part0005.html#possible_image_augmentations_generated_f)展示了由[imgaug](https://oreil.ly/KYA9O)库为一个示例图像生成的增强示例。（请注意，我们实际训练时不会使用imgaug。）
- en: '![Possible image augmentations generated from a single image](../images/00123.jpeg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![从单个图像生成的可能的图像增强](../images/00123.jpeg)'
- en: Figure 3-8\. Possible image augmentations generated from a single image
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8。从单个图像生成的可能的图像增强
- en: 'Colored images usually have three channels: red, green, and blue. Each channel
    has an intensity value ranging from 0 to 255\. To normalize it (i.e., scale down
    the value to between 0 and 1), we use the `preprocess_input` function (which,
    among other things, divides each pixel by 255):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 彩色图像通常有三个通道：红色、绿色和蓝色。每个通道的强度值范围从0到255。为了将其归一化（即将值缩小到0到1之间），我们使用`preprocess_input`函数（其中，除了其他操作外，还将每个像素除以255）：
- en: '[PRE4]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Sometimes knowing the label of a training image can be useful in determining
    appropriate ways of augmenting it. For example, when training a digit recognizer,
    you might be okay with augmentation by flipping vertically for an image of the
    digit “8,” but not for “6” and “9.”
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，知道训练图像的标签可以帮助确定适当的增强方式。例如，在训练数字识别器时，您可能会同意对数字“8”的图像进行垂直翻转增强，但不适用于“6”和“9”。
- en: Unlike our training set, we don’t want to augment our validation set. The reason
    is that with dynamic augmentation, the validation set would keep changing in each
    iteration, and the resulting accuracy metric would be inconsistent and difficult
    to compare across other iterations.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的训练集不同，我们不希望增强我们的验证集。原因是，使用动态增强，验证集将在每次迭代中保持变化，导致的准确度指标将不一致且难以在其他迭代中进行比较。
- en: 'It’s time to load the data from its directories. Training one image at a time
    can be pretty inefficient, so we can batch them into groups. To introduce more
    randomness during the training process, we’ll keep shuffling the images in each
    batch. To bring reproducibility during multiple runs of the same program, we’ll
    give the random number generator a seed value:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是从目录加载数据的时候了。一次训练一张图像可能效率不高，所以我们可以将它们分成组。为了在训练过程中引入更多的随机性，我们将在每个批次中保持图像的随机顺序。为了在同一程序的多次运行中保持可重现性，我们将为随机数生成器提供一个种子值：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Model Definition
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型定义
- en: 'Now that the data is taken care of, we come to the most crucial component of
    our training process: the model. In the code that follows, we reuse a CNN previously
    trained on the ImageNet dataset (MobileNet in our case), throw away the last few
    layers, called fully connected layers (i.e., ImageNet-specific classifier layers),
    and replace them with our own classifier suited to the task at hand.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经处理好了，我们来到我们训练过程中最关键的组件：模型。在接下来的代码中，我们重复使用了之前在ImageNet数据集上训练过的CNN（在我们的案例中是MobileNet），丢弃了最后几层，称为全连接层（即ImageNet特定的分类器层），并用适合当前任务的自己的分类器替换它们。
- en: For transfer learning, we “freeze” the weights of the original model; that is,
    set those layers as unmodifiable, so only the layers of the new classifier (that
    we’ll add) can be modified. We use MobileNet here to keep things fast, but this
    method will work just as well for any neural network. The following lines include
    a few terms such as `Dense`, `Dropout`, and so on. Although we won’t explore them
    in this chapter, you can find explanations in [Appendix A](part0021.html#K0RQ3-13fa565533764549a6f0ab7f11eed62b).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于迁移学习，我们“冻结”原始模型的权重；也就是说，将这些层设置为不可修改，因此只有新分类器的层（我们将添加的）可以被修改。我们在这里使用MobileNet来保持速度，但这种方法对于任何神经网络都同样有效。以下几行包括一些术语，如`Dense`、`Dropout`等。虽然我们在本章不会探讨它们，但你可以在[附录A](part0021.html#K0RQ3-13fa565533764549a6f0ab7f11eed62b)中找到解释。
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Train the Model
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: Set Training Parameters
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置训练参数
- en: With both the data and model ready, all we have left to do is train the model.
    This is also known as *fitting the model to the data*. For training a model, we
    need to select and modify a few different training parameters.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 数据和模型都准备好了，我们唯一需要做的就是训练模型。这也被称为*将模型拟合到数据*。为了训练一个模型，我们需要选择和修改一些不同的训练参数。
- en: Loss function
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数
- en: The `loss` function is the penalty we impose on the model for incorrect predictions
    during the training process. It is the value of this function that we seek to
    *minimize*. For example, in a task to predict house prices, the `loss` function
    could be the root-mean-square error.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`loss`函数是我们在训练过程中对模型进行惩罚的方式。我们希望*最小化*这个函数的值。例如，在预测房价的任务中，`loss`函数可以是均方根误差。'
- en: Optimizer
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: This is an algorithm that helps minimize the `loss` function. We use Adam, one
    of the fastest optimizers out there.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个帮助最小化`loss`函数的算法。我们使用Adam，这是目前最快的优化器之一。
- en: Learning rate
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率
- en: Learning is incremental. The learning rate tells the optimizer how big of a
    step to take toward the solution; in other words, where the loss is minimum. Take
    too big of a step, and we end up wildly swinging and overshooting our target.
    Take too small a step, and it can take a really long time before eventually arriving
    at the target loss value. It is important to set an optimal learning rate to ensure
    that we reach our learning goal in a reasonable amount of time. In our example,
    we set the learning rate at 0.001.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 学习是渐进的。学习率告诉优化器向解决方案迈出多大的步伐；换句话说，最小化损失的位置。迈出太大的步伐，我们最终会过度摆动并超过目标。迈出太小的步伐，可能需要很长时间才能最终到达目标损失值。设置一个最佳学习率是很重要的，以确保我们在合理的时间内达到学习目标。在我们的例子中，我们将学习率设置为0.001。
- en: Metric
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 度量
- en: Choose a metric to judge the performance of the trained model. Accuracy is a
    good explainable metric, especially when the classes are not imbalanced (i.e.,
    roughly equal amounts of data for each class). Note that this metric is not related
    to the `loss` function and is mainly used for reporting and not as feedback for
    the model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个度量标准来评估训练模型的性能。准确率是一个很好的可解释度量标准，特别是当类别不平衡时（即每个类别的数据量大致相等）。请注意，这个度量标准与`loss`函数无关，主要用于报告，而不是作为模型的反馈。
- en: 'In the following piece of code, we create the custom model using the `model_maker`
    function that we wrote earlier. We use the parameters described here to customize
    this model further for our task of cats versus dogs:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码片段中，我们使用之前编写的`model_maker`函数创建自定义模型。我们使用这里描述的参数进一步定制这个模型，以适应我们的猫狗任务：
- en: '[PRE7]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You might have noticed the term epoch in the preceding code. One epoch represents
    a full training step where the network has gone over the entire dataset. One epoch
    may consist of several minibatches.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了前面代码中的时代这个术语。一个时代代表了一个完整的训练步骤，网络已经遍历整个数据集。一个时代可能包含几个小批次。
- en: Start Training
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开始训练
- en: Run this program and let the magic begin. If you don’t have a GPU, brew a cup
    of coffee while you wait—it might take 5 to 10 minutes. Or why wait, when you
    can run the notebooks of this chapter (posted on GitHub) on Colab with a GPU runtime
    for free?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个程序，让魔法开始吧。如果你没有GPU，在等待时可以煮杯咖啡——可能需要5到10分钟。或者为什么要等待，当你可以在Colab上免费使用GPU运行本章的笔记本时呢？
- en: 'When complete, notice that there are four statistics: `loss` and `acc` on both
    the training and validation data. We are rooting for `val_acc:`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，请注意有四个统计数据：训练数据和验证数据上的`loss`和`acc`。我们期待`val_acc:`
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: All it took was 5 seconds in the very first epoch to reach 90% accuracy on the
    validation set, with just 500 training images. Not bad! And by the 10th step,
    we observe about 97% *validation accuracy*. That’s the power of transfer learning.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个时代仅用了5秒就在验证集上达到了90%的准确率，仅用了500张训练图片。不错！到第10步时，我们观察到约97%的*验证准确率*。这就是迁移学习的力量。
- en: Let us take a moment to appreciate what happened here. With just 500 images,
    we were able to reach a high level of accuracy in a matter of a few seconds and
    with very little code. In contrast, if we did not have a model previously trained
    on ImageNet, getting an accurate model might have needed training time anywhere
    between a couple of hours to a few days, and tons more data.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间欣赏这里发生的事情。仅仅用500张图片，我们就能在几秒钟内达到高水平的准确性，而且代码量很少。相比之下，如果我们没有一个在ImageNet上预先训练过的模型，要获得一个准确的模型可能需要几个小时到几天的训练时间，以及更多的数据。
- en: That’s all the code we need to train a state-of-the-art classifier on any problem.
    Place data into folders with the name of the class, and change the corresponding
    values in the configuration variables. In case our task has more than two classes,
    we should use `categorical_crossentropy` as the `loss` function and replace the
    `activation` function in the last layer with `softmax`. [Table 3-2](part0005.html#deciding_the_loss_and_activation_type_ba)
    illustrates this.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要在任何问题上训练一流分类器的所有代码。将数据放入以类名命名的文件夹中，并更改配置变量中的相应值。如果我们的任务有两个以上的类别，我们应该使用`categorical_crossentropy`作为`loss`函数，并将最后一层的`activation`函数替换为`softmax`。[表3-2](part0005.html#deciding_the_loss_and_activation_type_ba)说明了这一点。
- en: Table 3-2\. Deciding the loss and activation type based on the task
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表3-2\. 根据任务决定损失和激活类型
- en: '| **Classification type** | **Class mode** | **Loss** | **Activation on the
    last layer** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **分类类型** | **类模式** | **损失** | **最后一层的激活** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 or 2 classes | binary | binary_crossentropy | sigmoid |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 1或2个类 | 二进制 | 二元交叉熵 | sigmoid |'
- en: '| Multiclass, single label | categorical | categorical_crossentropy | softmax
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 多类别，单标签 | 分类 | 分类交叉熵 | softmax |'
- en: '| Multiclass, multilabel | categorical | binary_crossentropy | sigmoid |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 多类别，多标签 | 分类 | 二元交叉熵 | sigmoid |'
- en: 'Before we forget, save the model that you just trained so that we can use it
    later:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们忘记之前，保存刚刚训练的模型，以便稍后使用：
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Test the Model
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'Now that we have a trained model, we might eventually want to use it later
    for our application. We can now load this model anytime and classify an image.
    `load_model`, as its name suggests, loads the model:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个经过训练的模型，我们可能最终想要稍后在我们的应用程序中使用它。我们现在可以随时加载这个模型并对图像进行分类。`load_model`，顾名思义，加载模型：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let’s try loading our original sample images and see what results we get:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试加载我们的原始样本图像，看看我们得到什么结果：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Printing the value of the probability, we see that it is 0.996\. This is the
    probability of the given image belonging to the class “1,” which is a dog. Because
    the probability is greater than 0.5, the image is predicted as a dog.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 打印概率值，我们看到它是0.996。这是给定图像属于类别“1”（狗）的概率。因为概率大于0.5，所以图像被预测为狗。
- en: That’s all that we need to train our own classifiers. Throughout this book,
    you can expect to reuse this code for training with minimal modifications. You
    can also reuse this code in your own projects. Play with the number of epochs
    and images, and observe how it affects the accuracy. Also, we should play with
    any other data we can find online. It doesn’t get easier than that!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们训练自己分类器所需要的全部内容。在本书中，您可以期望在进行最少修改的情况下重复使用此代码进行训练。您也可以在自己的项目中重复使用此代码。尝试调整时代和图像的数量，并观察它如何影响准确性。此外，我们应该尝试使用我们可以在网上找到的任何其他数据。没有比这更容易的了！
- en: Analyzing the Results
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析结果
- en: With our trained model, we can analyze how it’s performing on the validation
    dataset. Beyond the more straightforward accuracy metrics, looking at the actual
    images of mispredictions should give an intuition as to whether the example was
    truly challenging or whether our model is not sophisticated enough.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们训练过的模型，我们可以分析它在验证数据集上的表现。除了更直接的准确性指标之外，查看误判的实际图像应该能让我们直观地了解这个例子是否真正具有挑战性，或者我们的模型还不够复杂。
- en: 'There are three questions that we want to answer for each category (cat, dog):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个问题我们想要为每个类别（猫、狗）回答：
- en: Which images are we most confident about being a cat/dog?
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些图像我们最有信心是猫/狗？
- en: Which images are we least confident about being a cat/dog?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些图像我们最不确定是猫/狗？
- en: Which images have incorrect predictions in spite of being highly confident?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些图像尽管非常自信，但预测错误？
- en: 'Before we get to that, let’s get predictions over the entire validation dataset.
    First, we set the pipeline configuration correctly:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这之前，让我们对整个验证数据集进行预测。首先，我们正确设置管道配置：
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we get the predictions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们进行预测：
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To make our analysis easier, we make a dictionary storing the image index to
    the prediction and ground truth (the expected prediction) for each image:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的分析更容易，我们创建一个字典，存储每个图像的索引到预测和实际值（预期预测）：
- en: '[PRE14]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For the next two code blocks, we provide boilerplate code, which we reuse regularly
    throughout the book.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的两个代码块，我们提供了常用的样板代码，我们在整本书中经常重复使用。
- en: 'The following is the signature of the helper function we’ll use to find the
    images with the highest/lowest probability value for a given category. Additionally,
    we will be using another helper function, - `display`(), to output the images
    as a grid on-screen:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们将使用的辅助函数的签名，用于查找具有给定类别的最高/最低概率值的图像。此外，我们将使用另一个辅助函数，- `display`()，将图像以网格形式输出到屏幕上：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This function is defined the book’s Github website (see [*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)),
    at *code/chapter-3*).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数在本书的Github网站上定义（请参阅[*http://PracticalDeepLearning.ai*](http://PracticalDeepLearning.ai)，位于*code/chapter-3*）。
- en: 'Now the fun starts! Which images are we most confident contain dogs? Let’s
    find images with the highest prediction probability (i.e., closest to 1.0; see
    [Figure 3-9](part0005.html#images_with_the_highest_probability_of_c)) with the
    predicted class dog (i.e., 1):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在开始有趣的部分！哪些图像我们最有信心包含狗？让我们找到预测概率最高的图像（即最接近1.0；参见[图3-9](part0005.html#images_with_the_highest_probability_of_c)中概率最高的图像）与预测类别狗（即1）：
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![Images with the highest probability of containing dogs](../images/00083.jpeg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![概率最高的包含狗的图像](../images/00083.jpeg)'
- en: Figure 3-9\. Images with the highest probability of containing dogs
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. 概率最高的包含狗的图像
- en: 'These images are indeed very dog-like. One of the reasons the probability is
    so high may be the fact that the images contain multiple dogs, as well as clear,
    unambiguous views. Now let’s try to find which images we are least confident contain
    dogs (see [Figure 3-10](part0005.html#images_with_the_lowest_probability_of_co)):'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像确实非常像狗。概率如此之高的原因之一可能是图像中包含了多只狗，以及清晰、明确的视图。现在让我们尝试找出我们最不确定包含狗的图像（请参见[图3-10](part0005.html#images_with_the_lowest_probability_of_co)）：
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Images with the lowest probability of containing dogs](../images/00040.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![概率最低的包含狗的图像](../images/00040.jpeg)'
- en: Figure 3-10\. Images with the lowest probability of containing dogs
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 概率最低的包含狗的图像
- en: To repeat, these are the images our classifier is most unsure of containing
    a dog. Most of these predictions are at the tipping point (i.e., 0.5 probability)
    to be the majority prediction. Keep in mind the probability of being a cat is
    just slightly smaller, around 0.49\. Compared to the previous set of images, the
    animals appearing in these images are often smaller and less clear. And these
    images often result in mispredictions—only 2 of the 10 images were correctly predicted.
    One possible way to do better here is to train with a larger set of images.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下，这些是我们的分类器最不确定包含狗的图像。这些预测大多处于临界点（即0.5概率）成为主要预测。请记住，作为猫的概率只是略小一些，大约在0.49左右。与之前一组图像相比，这些图像中出现的动物通常更小，更不清晰。这些图像通常导致错误预测——只有10张图像中的2张被正确预测。在这里做得更好的一种可能方法是使用更大的图像集进行训练。
- en: If you are concerned about these misclassifications, worry not. A simple trick
    to improve the classification accuracy is to have a higher threshold for accepting
    a classifier’s results, say 0.75\. If the classifier is unsure of an image category,
    its results are withheld. In [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b),
    we look at how to find an optimal threshold.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您担心这些错误分类，不用担心。提高分类准确性的一个简单技巧是对接受分类器结果的阈值设定更高，比如0.75。如果分类器对图像类别不确定，其结果将被保留。在[第5章](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)中，我们将看看如何找到最佳阈值。
- en: 'Speaking of mispredictions, they are obviously expected when the classifier
    has low confidence (i.e., near 0.5 probability for a two-class problem). But what
    we don’t want is to mispredict when our classifier is really sure of its predictions.
    Let’s check which images the classifier is confident contain dogs in spite of
    them being cats (see [Figure 3-11](part0005.html#images_of_cats_with_the_highest_probabil)):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 说到错误预测，当分类器信心较低时（即两类问题的概率接近0.5时），显然会出现错误预测。但我们不希望的是在我们的分类器对其预测非常确信时出现错误预测。让我们看看分类器确信包含狗的图像，尽管它们实际上是猫（参见[图3-11](part0005.html#images_of_cats_with_the_highest_probabil)）：
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![Images of cats with the highest probability of containing dogs](../images/00001.jpeg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![具有最高概率包含狗的猫的图像](../images/00001.jpeg)'
- en: Figure 3-11\. Images of cats with the highest probability of containing dogs
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-11。具有最高概率包含狗的猫的图像
- en: Hmm…turns out half of these images contain both cats and dogs, and our classifier
    is correctly predicting the dog category because they are bigger in size in these
    images. Thus, it’s not the classifier but the data that is incorrect here. This
    often happens in large datasets. The remaining half often contains unclear and
    relatively smaller objects (but ideally we want lower confidence for these difficult-to-identify
    images).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯...结果是这些图像中有一半包含猫和狗，我们的分类器正确地预测了狗类别，因为在这些图像中它们的大小更大。因此，这里不是分类器有问题，而是数据有问题。这在大型数据集中经常发生。另一半通常包含不清晰和相对较小的对象（但理想情况下，我们希望对这些难以识别的图像具有较低的置信度）。
- en: Repeating the same set of questions for the cat class, which images are more
    cat-like (see [Figure 3-12](part0005.html#images_with_the_highest_probab_c-id00002))?
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对猫类重复相同一组问题，哪些图像更像猫（参见[图3-12](part0005.html#images_with_the_highest_probab_c-id00002)）？
- en: '[PRE19]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Images with the highest probability of containing cats](../images/00288.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![具有最高概率包含猫的图像](../images/00288.jpeg)'
- en: Figure 3-12\. Images with the highest probability of containing cats
  id: totrans-198
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-12。具有最高概率包含猫的图像
- en: Interestingly, many of these have multiple cats. This affirms our previous hypothesis
    that multiple clear, unambiguous views of cats can give higher probabilities.
    On the other hand, which images are we most unsure about containing cats (see
    [Figure 3-13](part0005.html#images_with_the_lowest_probabili-id00001))?
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，其中许多图像中有多只猫。这证实了我们之前的假设，即多个清晰、明确的猫的视图可以给出更高的概率。另一方面，哪些图像我们对包含猫最不确定（参见[图3-13](part0005.html#images_with_the_lowest_probabili-id00001)）？
- en: '[PRE20]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![Images with the lowest probability of containing cats](../images/00248.jpeg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![具有最低概率包含猫的图像](../images/00248.jpeg)'
- en: Figure 3-13\. Images with the lowest probability of containing cats
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-13。具有最低概率包含猫的图像
- en: As seen previously, the key object size is small, and some of the images are
    quite unclear, meaning that there is too much contrast in some cases or the object
    is too bright, something not in line with most of the training images. For example,
    the camera flash in the eighth (dog.6680) and tenth (dog.1625) images in [Figure 3-13](part0005.html#images_with_the_lowest_probabili-id00001)
    makes the dog difficult to recognize. The sixth image contains a dog in front
    of a sofa of the same color. Two images contain cages.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前所见，关键对象的大小较小，有些图像相当不清晰，这意味着在某些情况下对比度太高，或者对象太亮，这与大多数训练图像不符。例如，第八张（dog.6680）和第十张（dog.1625）图像中的相机闪光灯使得狗难以识别。第六张图像包含一只狗站在同色的沙发前。两张图像包含笼子。
- en: Lastly, which images is our classifier mistakenly sure of containing cats (see
    [Figure 3-14](part0005.html#images_of_dogs_with_highest_probability))?
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的分类器错误地确信包含猫的图像是哪些（参见[图3-14](part0005.html#images_of_dogs_with_highest_probability)）？
- en: '[PRE21]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Images of dogs with the highest probability of containing cats](../images/00138.jpeg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![具有最高概率包含猫的狗的图像](../images/00138.jpeg)'
- en: Figure 3-14\. Images of dogs with the highest probability of containing cats
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-14。具有最高概率包含猫的狗的图像
- en: These mispredictions are what we want to reduce. Some of them are clearly wrong,
    whereas others are understandably confusing images. The sixth image (dog.4334)
    in [Figure 3-14](part0005.html#images_of_dogs_with_highest_probability) seems
    to be incorrectly labeled as a dog. The seventh and tenth images are difficult
    to distinguish against the background. The first and tenth lack enough texture
    within them to give the classifier enough identification power. And some of the
    dogs are too small, like the second and fourth.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这些错误预测是我们想要减少的。其中一些显然是错误的，而另一些则是令人困惑的图像。在[图3-14](part0005.html#images_of_dogs_with_highest_probability)中的第六张图（dog.4334）似乎被错误标记为狗。第七和第十张图在背景上难以区分。第一和第十张图内部缺乏足够的纹理，无法给分类器足够的识别能力。而一些狗太小，比如第二和第四张。
- en: Going over the various analyses, we can summarize that mispredictions can be
    caused by low illumination, unclear, difficult-to-distinguish backgrounds, lack
    of texture, and smaller occupied area with regard to the image.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过各种分析，我们可以总结出，错误预测可能是由于低照明、不清晰、难以区分的背景、缺乏纹理以及相对于图像的较小占用区域而引起的。
- en: Analyzing our predictions is a great way to understand what our model has learned
    and what it’s bad at, and highlights opportunities to enhance its predictive power.
    Increasing the size of the training examples and more robust augmentation will
    help in improving the classification. It’s also important to note that showing
    real-world images to our model (images that look similar to the scenario where
    our app will be used) will help improve its accuracy drastically. In [Chapter 5](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b),
    we make the classifier more robust.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 分析我们的预测是了解我们的模型学到了什么以及它擅长什么的好方法，并突出了增强其预测能力的机会。增加训练示例的大小和更强大的数据增强将有助于提高分类的准确性。还要注意，向我们的模型展示真实世界的图像（看起来类似于我们的应用程序将要使用的场景）将极大地提高其准确性。在[第5章](part0007.html#6LJU3-13fa565533764549a6f0ab7f11eed62b)中，我们使分类器更加健壮。
- en: Further Reading
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: To help understand neural networks and CNNs better, [our website](http://PracticalDeepLearning.ai)
    features a learning guide which includes recommended resources like video lectures,
    blogs, and, more interestingly, interactive visual tools which allow you to play
    with different scenarios in the browser without the need to install any packages.
    If you’re a first-time learner of deep learning, we highly recommend this guide
    in order to strengthen your foundational knowledge. It covers the theory that
    you will need to build the intuition to solve future problems. We use Google’s
    TensorFlow Playground ([Figure 3-15](part0005.html#building_a_neural_network_in_tensorflow))
    for neural networks and Andrej Karpathy’s ConvNetJS ([Figure 3-16](part0005.html#defining_a_cnn_and_visualizing_output_of))
    for CNNs.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解神经网络和CNN，[我们的网站](http://PracticalDeepLearning.ai)提供了一个学习指南，其中包括推荐的资源，如视频讲座、博客，以及更有趣的是，交互式可视化工具，让您可以在浏览器中玩不同的场景，而无需安装任何软件包。如果您是深度学习的初学者，我们强烈推荐这个指南，以加强您的基础知识。它涵盖了您需要建立直觉以解决未来问题的理论。我们使用Google的TensorFlow
    Playground（[图3-15](part0005.html#building_a_neural_network_in_tensorflow)）进行神经网络和Andrej
    Karpathy的ConvNetJS（[图3-16](part0005.html#defining_a_cnn_and_visualizing_output_of)）进行CNN。
- en: '![Building a neural network in TensorFlow Playground](../images/00238.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![在TensorFlow Playground中构建神经网络](../images/00238.jpeg)'
- en: Figure 3-15\. Building a neural network in TensorFlow Playground
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-15. 在TensorFlow Playground中构建神经网络
- en: '![Defining a CNN and visualizing output of each layer during training in ConvNetJS.](../images/00130.jpeg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![定义CNN并在ConvNetJS中训练期间可视化每个层的输出。](../images/00130.jpeg)'
- en: Figure 3-16\. Defining a CNN and visualizing the output of each layer during
    training in ConvNetJS
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16. 定义CNN并在ConvNetJS中训练期间可视化每个层的输出
- en: We additionally have a short guide in [Appendix A](part0021.html#K0RQ3-13fa565533764549a6f0ab7f11eed62b),
    which summarizes convolutional neural networks, as a ready reference.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在[附录A](part0021.html#K0RQ3-13fa565533764549a6f0ab7f11eed62b)中还有一个简短的指南，总结了卷积神经网络，作为一个方便的参考。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced the concept of transfer learning. We reused a
    pretrained model to build our own cats versus dogs classifier in under 30 lines
    of code and with barely 500 images, reaching state-of-the-art accuracy in a few
    minutes. By writing this code, we also debunk the myth that we need millions of
    images and powerful GPUs to train our classifier (though they help).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了迁移学习的概念。我们重用了一个预训练模型，在不到30行代码和几乎500张图片的情况下构建了自己的猫狗分类器，在几分钟内达到了最先进的准确性。通过编写这段代码，我们也揭穿了一个神话，即我们需要数百万张图片和强大的GPU来训练我们的分类器（尽管它们有帮助）。
- en: Hopefully, with these skills, you might be able to finally answer the age-old
    question of who let the dogs out.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 希望通过这些技能，您可能最终能够回答一个古老的问题，即谁放出了狗。
- en: In the next couple of chapters, we use this learning to understand CNNs in more
    depth and take the model accuracy to the next level.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将利用这些知识更深入地理解CNN，并将模型准确性提升到更高水平。
