- en: 3 Keras and data retrieval in TensorFlow 2
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 Keras 和 TensorFlow 2 中的数据检索
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的内容
- en: Different APIs for building models in Keras
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 中用于构建模型的不同 API
- en: Retrieving and manipulating persisted data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检索和操作持久化数据
- en: We have explored the details of the low-level TensorFlow API, such as defining
    tf.Variable objects and tf.Tensor objects, which can be used to store things like
    numbers and strings. We also looked at some of the commonly used functionality
    provided in TensorFlow in the form of tf.Operation. Finally, we looked at some
    complex operations, such as matrix multiplication and convolution, in detail.
    If you analyze any standard deep neural network, you will see that it is made
    from standard mathematical operations such as matrix multiplication and convolution.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了低级 TensorFlow API 的细节，比如定义 tf.Variable 对象和 tf.Tensor 对象，这些对象可以用来存储数字和字符串等。我们还查看了
    TensorFlow 提供的一些常用功能，如 tf.Operation。最后，我们详细讨论了一些复杂的操作，比如矩阵乘法和卷积。如果你分析任何标准的深度神经网络，你会发现它由矩阵乘法和卷积等标准数学操作构成。
- en: However, if you were to implement these networks using the low-level TensorFlow
    API, you’d find yourself replicating these operations in code many times, costing
    you valuable hours and making the code unmaintainable. But the good news is that
    you don’t have to. TensorFlow provides a submodule called Keras that takes care
    of this problem, and this is the focus of this chapter. Keras is a sub-library
    in TensorFlow that hides building blocks and provides a high-level API for developing
    machine learning models. In this chapter, we will see that Keras has several different
    APIs to choose from, depending on the complexity of your solution.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你要使用低级别的 TensorFlow API 实现这些网络，你会发现自己在代码中多次复制这些操作，这将耗费宝贵的时间，并使代码难以维护。但好消息是你不需要这样做。TensorFlow
    提供了一个名为 Keras 的子模块，它解决了这个问题，这也是本章的重点。Keras 是 TensorFlow 中的一个子库，它隐藏了构建模块，并为开发机器学习模型提供了高级
    API。在本章中，我们将看到 Keras 提供了几种不同的 API，可以根据解决方案的复杂性来选择使用。
- en: 'We will conclude this chapter by discussing another important aspect of machine
    learning: feeding data to models. Typically, we need to retrieve data from the
    disk (or web) and clean and process the data before feeding it to the model. We
    will discuss several different data retrieval facilities in TensorFlow such as
    the tf.data and tensorflow-datasets APIs and how they simplify reading and manipulating
    data that eventually feeds into models.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过讨论机器学习的另一个重要方面来结束本章：向模型提供数据。通常，我们需要从磁盘（或网络）中检索数据，并在将其提供给模型之前清理和处理数据。我们将讨论
    TensorFlow 中几种不同的数据检索工具，如 tf.data 和 tensorflow-datasets API，以及它们如何简化读取和操作最终输入模型的数据。
- en: 3.1 Keras model-building APIs
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 Keras 模型构建 API
- en: 'You are developing a flower species classifier as part of a hackathon. Your
    team is going to create several different variations of multilayer perceptron
    to compare their performance against a flower species identification data set.
    The goal is to train the models that can output the flower species given several
    measurements of the flowers. The models you have to develop are as follows:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 作为黑客马拉松的一部分，您正在开发一个花卉物种分类器。您的团队将创建几个不同的多层感知器变体，以便将它们的性能与花卉物种识别数据集进行比较。目标是训练能够在给定花卉的多个测量值的情况下输出花卉物种的模型。您需要开发的模型如下：
- en: '*Model A*—A model that learns only from the provided features (baseline)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型 A*——仅从提供的特征中学习的模型（基线）'
- en: '*Model B*—A model that uses the principal components of the features in addition
    to the features themselves (details discussed in section 3.1.3)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型 B*——除了使用特征本身外，还使用特征的主成分（详见3.1.3节）'
- en: '*Model C*—A model that uses an unorthodox hidden layer computation, which uses
    a multiplicative bias, in addition to the additive bias, not typically found in
    neural networks (details discussed in section 3.1.4)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型 C*——一个使用非常规隐藏层计算的模型，它使用了一个乘法偏差，除了传统的加法偏差之外，这在神经网络中通常是找不到的（详见3.1.4节）'
- en: You are planning to use Keras, and you know it offers multiple model-building
    APIs. In order to provide the results quickly, you need to know which Keras API
    to use for which model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您计划使用 Keras，并且知道它提供了多个模型构建 API。为了快速提供结果，您需要知道在哪个模型中使用哪个 Keras API。
- en: 'Keras ([https://keras.io/](https://keras.io/)) initially started as a high-level
    API that can use multiple low-level backends (e.g., TensorFlow, Theano) and allow
    developers to build machine learning models easily. In other words, Keras hides
    the gory details of low-level operations and provides an intuitive API with which
    you can build models with a few lines of code. Since TensorFlow 1.4, Keras has
    been integrated into TensorFlow ([https://www.tensorflow.org/guide/keras/overview](https://www.tensorflow.org/guide/keras/overview)).
    You can import Keras using import tensorflow.keras. Keras has three main APIs:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Keras ([https://keras.io/](https://keras.io/)) 最初是作为一个高级API启动的，可以使用多个低级后端（例如，TensorFlow、Theano），并允许开发人员轻松构建机器学习模型。换句话说，Keras隐藏了低级操作的细节，并提供了一个直观的API，您可以用几行代码构建模型。自从TensorFlow
    1.4以来，Keras已经集成到了TensorFlow中（[https://www.tensorflow.org/guide/keras/overview](https://www.tensorflow.org/guide/keras/overview)）。您可以使用`import
    tensorflow.keras`导入Keras。Keras有三个主要的API：
- en: Sequential
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序式
- en: Functional
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数式
- en: Sub-classing
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子类化
- en: The Sequential API is the easiest to use. However, it is a very constrictive
    API that only allows you to create a network that starts with one input, go through
    a sequence of layers, and end with one input. Next, the functional API requires
    more work to use. But it also provides more flexibility, such as having multiple
    inputs, parallel layers, and multiple outputs. Finally, the sub-classing API can
    be identified as the most difficult to wield. The idea is to create a Python object
    that represents your model or a layer in your model while using the low-level
    functionality provided by TensorFlow to achieve what’s needed. Let’s briefly go
    over how you can use these APIs. But we won’t stop there; we will look at these
    APIs in more detail in the coming chapters. Figure 3.1 highlights the main differences
    between the APIs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序式API是最容易使用的。然而，它是一个非常受限制的API，只允许您创建一个以一个输入开始，经过一系列层，以及以一个输出结束的网络。接下来，函数式API需要更多的工作才能使用。但它也提供了更多的灵活性，例如具有多个输入、并行层和多个输出。最后，子类化API可以被认为是最难驾驭的。其思想是创建一个代表您的模型或模型中的一层的Python对象，同时使用TensorFlow提供的低级功能来实现所需的功能。让我们简要地介绍一下如何使用这些API。但我们不会止步于此；在接下来的章节中，我们将更详细地了解这些API。图3.1突出显示了API之间的主要区别。
- en: '![03-01](../../OEBPS/Images/03-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![03-01](../../OEBPS/Images/03-01.png)'
- en: Figure 3.1 Sequential, functional, and sub-classing APIs in comparison.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 顺序式、函数式和子类化API的比较。
- en: Here, for model A we will use the Sequential API, as it is the simplest. To
    implement model B, which will have two input layers, we will use the functional
    API. Finally, to implement model C, for which we will need to implement a custom
    layer, we will use the sub-classing API.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，对于模型A，我们将使用顺序式API，因为它是最简单的。要实现模型B，其中将有两个输入层，我们将使用函数式API。最后，要实现模型C，其中我们需要实现一个自定义层，我们将使用子类化API。
- en: 3.1.1 Introducing the data set
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 引入数据集
- en: 'Say you decided to use a popular machine learning data set known as the Iris
    data set ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    This data set records sepal length, sepal width, petal length, and petal width
    for several different species of Iris flowers: Iris-setosa, Iris-versicolor, and
    Iris-virginica. For each flower, we have the sepal length/width and the petal
    length/width. As you can see, each input has four features, and each input can
    belong to one of three classes. To start, let’s download the data, do some quick
    analysis on it, and put it in a format that we can readily use for model training.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您决定使用一个名为鸢尾花数据集（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)）的流行机器学习数据集。这个数据集记录了几种不同种类的鸢尾花（Iris-setosa、Iris-versicolor和Iris-virginica）的萼片长度、萼片宽度、花瓣长度和花瓣宽度。对于每朵花，我们都有萼片长度/宽度和花瓣长度/宽度。正如您所看到的，每个输入都有四个特征，每个输入都可以属于三个类别之一。首先，让我们下载数据，对其进行快速分析，并将其格式化为我们可以方便地用于模型训练的格式。
- en: 'Initially, you need to make sure the environment is set up and the libraries
    are installed, as outlined in appendix A. Next, open the Jupyter notebook found
    at Ch03-Keras-and-Data-Retrieval/3.1.Keras_APIs.ipynb. Now, as shown in the code
    found in the notebook, we need to import the requests library for downloading
    data, pandas for manipulating that data, and, of course, TensorFlow:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要确保环境设置正确，并且已安装所需的库，如附录A中所述。接下来，打开位于Ch03-Keras-and-Data-Retrieval/3.1.Keras_APIs.ipynb的Jupyter笔记本。现在，如笔记本中的代码所示，我们需要导入requests库来下载数据，导入pandas来操作该数据，当然，还有TensorFlow：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now we will download the data and save the data to a file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将下载数据并将数据保存到文件中：
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then read the data using pandas library’s read_csv() function ([http://mng.bz/j2Op](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.xhtml)):'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 pandas 库的 read_csv() 函数读取数据 ([http://mng.bz/j2Op](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.xhtml))：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, iris_df is a pandas DataFrame ([http://mng.bz/Wxaw](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xhtml)).
    In its simplest form, a data frame can be thought as an informative matrix organized
    into rows and columns. You can inspect the first few rows of the data using the
    iris_df.head() command, which produces the following result:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，iris_df 是一个 pandas DataFrame ([http://mng.bz/Wxaw](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.xhtml))。在最简单的形式下，数据帧可以被认为是一个按行和列组织的信息矩阵。您可以使用
    iris_df.head() 命令检查数据的前几行，其结果如下：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then, we will make some cosmetic changes to the data to make it look better.
    We will provide appropriate column names (available from the data set’s webpage)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将对数据进行一些装饰性修改，使其看起来更好。我们将提供适当的列名称（可从数据集的网页中获取）
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'and mapping the string label to an integer:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 并将字符串标签映射为整数：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We end up with the following improved pandas DataFrame in our possession:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下改进后的 pandas DataFrame：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As the last step, we will shuffle the data by and separate the data features
    as x and data labels as y. We will also center the data by subtracting the mean
    from each column, as this usually leads to better performance:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们将通过从每列中减去均值来将数据居中，因为这通常会导致更好的性能：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, print(x) will print out
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，print(x) 将输出
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note how the indices are not in order after shuffling the data. print(y) will
    output
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 在对数据进行洗牌后，索引不再按顺序排列。print(y) 将输出
- en: '[PRE9]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Shuffling the data is an important step: the data is in a very specific order,
    with each class appearing one after another. But you achieve the best results
    when data has been shuffled so that each batch presented to the network has a
    good mix of all classes found in the full data set. You can also see that we used
    a transformation on y (or labels), known as *one-hot encoding*. One-hot encoding
    converts each label to a unique vector of zeros, where a single element is one.
    For example, the labels 0, 1, and 2 are converted to the following one-hot encoded
    vectors:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据进行洗牌是一个重要的步骤：数据是按特定顺序排列的，每个类别都紧跟在另一个后面。但是当数据被洗牌时，每个被呈现给网络的批次都有所有类别的良好混合，这样可以获得最佳结果。您还可以看到我们对
    y（或标签）使用了一种称为 *独热编码* 的转换。独热编码将每个标签转换为唯一的零向量，其中一个元素为一。例如，标签 0、1 和 2 被转换为以下独热编码向量：
- en: 0 → [1, 0, 0]
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0 → [1, 0, 0]
- en: 1 → [0, 1, 0]
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 1 → [0, 1, 0]
- en: 2 → [0, 0, 1]
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2 → [0, 0, 1]
- en: 3.1.2 The Sequential API
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 顺序 API
- en: With the data ready to be fed in, it’s time to implement model A, the first
    neural network. The first model is quite straightforward and only needs to take
    the provided features and predict the flower species. You can use the Keras Sequential
    API, as it is the simplest, and all we need to do is stack several layers on top
    of each other sequentially. Figure 3.2 depicts the Sequential API compared to
    other APIs.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备就绪后，是时候实现模型 A 了，这是第一个神经网络。第一个模型非常简单，只需要提供的特征并预测花的种类。您可以使用 Keras 顺序 API，因为它是最简单的，我们所需要做的就是将几个层顺序堆叠在一起。图
    3.2 描述了顺序 API 与其他 API 的比较。
- en: '![03-02](../../OEBPS/Images/03-02.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![03-02](../../OEBPS/Images/03-02.png)'
- en: Figure 3.2 The Sequential API compared to other APIs (grayed out)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 顺序 API 与其他 API 的比较（被标记为灰色）
- en: 'Let’s create a network that has the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个具有以下特点的网络：
- en: An input layer of 4 nodes
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 4 个节点的输入层
- en: A 32-node hidden layer
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 32 个节点的隐藏层
- en: A 16-node hidden layer
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有 16 个节点的隐藏层
- en: A 3-node output layer
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 3 节点输出层
- en: Note The number of nodes for each layer is a hyperparameter of the model. In
    this case, we chose these values arbitrarily. But to obtain the best results,
    we should use a hyperparameter optimization algorithm ([http://mng.bz/8MJB](http://mng.bz/8MJB))
    to find the best hyperparameters for a given problem.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 每层节点数是模型的超参数。在这种情况下，我们任意选择了这些值。但为了获得最佳结果，我们应该使用一个超参数优化算法 ([http://mng.bz/8MJB](http://mng.bz/8MJB))
    来找到给定问题的最佳超参数。
- en: Before we define the model, we need to import certain layers and the sequential
    model from TensorFlow. Then you can implement this model using just a single line
    of code (see the next listing).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义模型之前，我们需要导入TensorFlow中的某些层和顺序模型。然后，您可以使用一行代码即可实现该模型（见下一个清单）。
- en: Listing 3.1 Model A implemented with the Sequential API
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 利用Sequential API实现的模型A，如3.1清单所示。
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Import necessary modules and classes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶导入必要的模块和类。
- en: ❷ Clear the TensorFlow computational graph before creating the model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❷在创建模型之前清除TensorFlow计算图。
- en: ❸ Define the model with the Sequential API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❸使用Sequential API定义模型。
- en: Let’s analyze what we just did. You can create a sequential model using the
    Sequential object and then pass a sequence of layers, such as the Dense layer.
    A layer encapsulates typical reusable computations you can find in a neural network
    (e.g., hidden layer computation, convolution operations).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下我们刚刚做的。您可以使用Sequential对象创建一个顺序模型，然后传递一个层的序列，例如Dense层。一个层封装了神经网络中可以找到的典型的可重复使用的计算（例如隐藏层计算，卷积操作）。
- en: 'The Dense layer offers the core computation that happens in a fully connected
    network (i.e., going from an input (x) to a hidden output (h) using *h* *=* *activation*(*xW*
    *+* *b*)). The Dense layer has two important parameters: the number of hidden
    units and the nonlinear activation. By stacking a set of Dense layers, you have
    a multilayer, fully connected network. We are building the network using the following
    layers:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Dense层提供了全连接网络中所发生的核心计算（即通过 *h* *=* *activation*(*xW* *+* *b*)从输入（x）到隐藏输出（h））。Dense层有两个重要参数：隐藏单元的数量和非线性激活函数。通过堆叠一组Dense层，您可以构建一个多层的全连接网络。我们正在使用以下层构建网络：
- en: Dense(32, activation='relu', input_shape=(4,))
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dense(32, activation='relu', input_shape=(4,))
- en: Dense(16, activation='relu')
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dense(16, activation='relu')
- en: Dense(3, activation='softmax')
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dense(3, activation='softmax')
- en: In the first Dense layer you can see that an additional parameter called input_shape
    has been passed. input_shape is a key attribute in any model you create with TensorFlow.
    It is imperative that you know the exact shape of the input you want to pass to
    a model because the output of all the layers that follow depends on the shape
    of the input. In fact, certain layers can only process certain input shapes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个Dense层中，可以看到传递了一个额外的参数input_shape。input_shape是使用TensorFlow创建的任何模型的关键属性。您必须确切地知道要传递给模型的输入的形状，因为随后的所有层的输出都取决于输入的形状。实际上，某些层只能处理某些特定的输入形状。
- en: In this example, we are saying the input will be of shape [None, 4]. Though
    we have only specified 4 in the shape, Keras automatically adds an unspecified
    (i.e., None) dimension to the input_shape, which represents the batch dimension
    of the input. As you probably already know, deep neural networks process data
    in batches (i.e., more than a single example at once). The other dimension (of
    size 4) is the feature dimension, meaning that the network can accept an input
    that has four features in it. Having the batch dimension as None leaves the batch
    dimension unspecified, allowing you to pass any arbitrary number of examples at
    model training/ inference time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们说输入的形状将是[None, 4]。虽然我们只在形状中指定了4，但Keras会自动添加一个未指定的（即None）维度到input_shape中，它表示输入的批次维度。正如您可能已经知道的，深度神经网络以批次的方式处理数据（即一次处理多个示例）。另一个尺寸（大小为4）是特征维度，意味着网络可以接受具有四个特征的输入。将批次维度设为None将批次维度未指定，允许您在模型训练/推断时传递任意数量的示例。
- en: 'Another important aspect of a layer is the nonlinear activation used in the
    layer. Here, we can see that the first two layers use ReLU (rectified linear units)
    activation. It is a very simple yet powerful activation that’s prevalent in feed-forward
    models. ReLU does the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个层的另一个重要方面是层中使用的非线性激活函数。在这里，我们可以看到前两个层使用了ReLU（修正线性单元）激活函数。它是前馈模型中非常简单但功能强大的激活函数。ReLU具有以下功能：
- en: '*y* = max (0, *x*)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = max (0, *x*)'
- en: The final layer has a softmax activation. As previously discussed, softmax activation
    normalizes the final scores of the last layer (i.e., logits) to a valid probability
    distribution. Specifically,
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一层使用了softmax激活函数。正如之前讨论的，softmax激活函数将最后一层（即logits）的得分归一化为一个有效的概率分布。具体来说，
- en: '![03_02a](../../OEBPS/Images/03_02a.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![03_02a](../../OEBPS/Images/03_02a.png)'
- en: As an example, assume the final layer without the softmax activation produced
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以一个示例为例，假设最后一层没有使用softmax激活函数产生了
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Applying the softmax normalization converts these values to
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 应用softmax归一化将这些值转换为
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that the model is defined, we need to perform a crucial step, known as *model
    compilation*, if we are to successfully use it. For our model we will use
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经定义好了，我们需要执行一个关键步骤，称为*模型编译*，如果我们要成功地使用它的话。对于我们的模型，我们将使用
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we are setting the model up with a loss function, optimizer, and metric.
    The loss function says how good or bad the model is doing on the given data (e.g.,
    categorical cross-entropy). The lower the loss, the better. Along with that loss
    function, we use an optimizer, which knows how to change the weights and biases
    of the model in such a way that the loss is reduced. Here, we chose the loss categorical_crossentropy
    ([http://mng.bz/EWej](http://mng.bz/EWej)), which typically works well for multiclass
    classification problems and the optimizer adam ([https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)),
    which is a common choice due to its remarkable performance in a variety of problems.
    We can also optionally define metrics to keep an eye on the model (e.g., model
    accuracy). Finally, we can inspect the model you just created using
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了模型的损失函数、优化器和度量标准。损失函数表示模型在给定数据上的表现如何（例如，分类交叉熵）。损失越低，模型就越好。除了损失函数之外，我们还使用了一个优化器，它知道如何改变模型的权重和偏差，以使损失减少。在这里，我们选择了损失函数categorical_crossentropy（[http://mng.bz/EWej](http://mng.bz/EWej)），这通常在多类别分类问题中效果良好，以及优化器adam（[https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)），由于其在各种问题中的出色表现，是一个常见的选择。我们还可以选择性地定义度量标准来关注模型（例如，模型准确率）。最后，我们可以使用以下方法检查您刚刚创建的模型
- en: '[PRE14]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: which outputs
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The model summary clearly shows the number of layers, type of layers, output
    shape of each layer, and number of parameters in each layer. Let’s train this
    model to classify various iris flowers using the data set we prepared earlier.
    We train a Keras model using the convenient fit() function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型摘要清晰地显示了层数、每个层的类型、每个层的输出形状以及每个层的参数数量。让我们使用之前准备好的数据集来训练这个模型，以对各种鸢尾花进行分类。我们使用方便的fit()函数来训练一个Keras模型：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The fit() function accepts many different arguments:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: fit()函数接受许多不同的参数：
- en: X—Data features
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: X—数据特征
- en: Y—Data labels (one-hot encoded)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Y—数据标签（独热编码）
- en: batch size (optional)—Number of data points in a single batch
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理大小（可选）—单个批次中的数据点数量
- en: epochs (optional)—Number of times repeating the data set during model training
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: epochs（可选）—模型训练期间重复数据集的次数
- en: 'The values, such as batch_size and epochs, have been chosen empirically. If
    you run the previous code, you will get the following result:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 像batch_size和epochs这样的值是经验性地选择的。如果你运行前面的代码，你将得到以下结果：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It looks like our mini project was reasonably successful, as we observe a steady
    increase of the training accuracy (“acc”) up to 74% in just 25 epochs. However,
    it is not advisable to rely only on the training accuracy to decide if a model
    has performed better. There are various techniques to do so, which we will review
    in the coming chapters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的小型项目相当成功，因为我们观察到训练准确率（“acc”）在只有25个epochs的情况下稳步增长到了74%。然而，仅仅依靠训练准确率来决定一个模型是否表现更好是不明智的。有各种技术可以做到这一点，我们将在接下来的章节中进行回顾。
- en: Reproducibility in machine learning
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的可重现性
- en: 'Reproducibility is an important concept in machine learning. Reproducibility
    means that you can run an experiment, publish the results, and ensure that someone
    interested in your research can reproduce the results. It also means that you
    will get the same result across multiple trials. If you look at the notebook ch02/1.Tensorflow_
    Fundamentals.ipynb, you will see one such measure we have taken to make sure the
    results are consistent across multiple trials. You will see the following code
    in the “Library imports and some setups” section:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 可重现性是机器学习中的一个重要概念。可重现性意味着你可以运行一个实验，发布结果，并确保对你的研究感兴趣的人可以复现结果。它还意味着你将在多次试验中得到相同的结果。如果你看一下笔记本ch02/1.Tensorflow_
    Fundamentals.ipynb，你会看到我们已经采取的一项措施，以确保结果在多次试验中保持一致。你将在“Library imports and some
    setups”部分看到以下代码：
- en: '[PRE18]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The random seed is a common element that affects the reproducibility of the
    research as neural networks ubiquitously use random initializations. By fixing
    the seed, you make sure you will get the same random number sequence every time
    you run your code. This means that the weight and bias initializations of your
    model will be the same across multiple trials. This results in the same accuracy
    values given the other conditions are not changed.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 随机种子是影响研究可重复性的一个常见因素，因为神经网络普遍使用随机初始化。通过固定种子，你可以确保每次运行代码时都会得到相同的随机数序列。这意味着在多次试验中，模型的权重和偏置初始化是相同的，前提是其他条件没有改变。
- en: To make sure that your code is producing consistent results, make sure you call
    the fix_random_seed function (by running the first code cell) when you are trying
    out the code exercises.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你的代码能够产生一致的结果，请在尝试代码练习时调用 fix_random_seed 函数（通过运行第一个代码单元格）。
- en: 3.1.3 The functional API
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 函数式 API
- en: Now it’s time to implement the second model (i.e., Model B) that uses principal
    components as an extra set of inputs. The hope is that this additional input (the
    principal components) will provide additional features to the model, which will
    improve model performance. Principal components are extracted using an algorithm
    known as *principal component analysis* (PCA). PCA is a dimensionality reduction
    technique that will project high-dimensional data to a lower-dimensional space
    while trying to preserve the variance present in the data. Now you need to create
    a model that takes two different input feature sets.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候实现第二个模型（即，模型 B）了，该模型使用主成分作为额外的输入。希望这个额外输入（主成分）能为模型提供额外的特征，从而提高模型的性能。主成分是使用一种称为*主成分分析*（PCA）的算法提取出来的。PCA
    是一种降维技术，它会将高维数据投影到一个较低维度的空间中，同时努力保留数据中存在的方差。现在你需要创建一个模型，该模型接受两个不同的输入特征集。
- en: 'You no longer can use the Sequential API as it is only designed to handle sequential
    models (i.e., single input layer going through a sequence of layers to produce
    a single output). Here, we have two different inputs: the raw features of flowers
    and the PCA features. That means two layers work in parallel to produce two different
    hidden representations, concatenate that, and finally produce the class probabilities
    for the inputs, as highlighted in figure 3.3\. The functional API is a great choice
    for these kind of models, as it can be used to define models with multiple inputs
    or multiple outputs.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能再使用 Sequential API，因为它只能处理顺序模型（即，单输入层通过一系列层产生单输出）。在这里，我们有两个不同的输入：花卉的原始特征和PCA特征。这意味着两个层以并行方式工作，产生两个不同的隐藏表示，并将它们连接起来，最后为输入产生类别概率，如图3.3所示。函数式
    API 对于这种类型的模型是一个很好的选择，因为它可以用于定义具有多个输入或多个输出的模型。
- en: '![03-03](../../OEBPS/Images/03-03.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![03-03](../../OEBPS/Images/03-03.png)'
- en: Figure 3.3 The functional API compared to other APIs (grayed out)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 函数式 API 与其他 API 的对比（灰色块为无法使用的功能）
- en: 'Let’s get started. First, we need to import the following layer and model objects,
    as these will make the core of our model:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。首先，我们需要导入以下层和模型对象，因为它们将成为我们模型的核心：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we need to create two Input layers (for the raw input features and the
    PCA features):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建两个 Input 层（用于原始输入特征和 PCA 特征）：
- en: '[PRE20]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The Input layer for the raw input features will have four feature columns, whereas
    the Input layer for the PCA features will have two feature columns (as we are
    only keeping the first two principal components). If you look back at how we defined
    the model using the Sequential API, you will notice we didn’t use an Input layer.
    This is automatically added when using the Sequential API. However, when using
    the functional API, we need to explicitly specify the Input layers we need to
    include in our model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 原始输入特征的 Input 层将有四个特征列，而 PCA 特征的 Input 层将只有两个特征列（因为我们只保留了前两个主成分）。如果回顾一下我们如何使用
    Sequential API 定义模型，你会注意到我们没有使用 Input 层。但在使用函数式 API 时，我们需要明确指定我们需要包含在模型中的 Input
    层。
- en: 'With the two Input layers defined, we can now compute individual hidden representations
    for those layers:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了两个 Input 层后，我们现在可以计算这些层的单独隐藏表示：
- en: '[PRE21]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, out1 represents the hidden representation of inp1 (i.e., raw features)
    and out2 is the hidden representation of inp2 (i.e., PCA features). We then concatenate
    the two hidden representations:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，out1 表示 inp1 的隐藏表示（即原始特征），out2 是 inp2 的隐藏表示（即 PCA 特征）。然后我们连接这两个隐藏表示：
- en: '[PRE22]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Let’s delve into what happens when you use the Concatenate layer in more detail.
    The Concatenate layer simply concatenates two or more inputs along a given axis.
    In this example, we have two inputs to the Concatenate layer (i.e., [None, 16]
    and [None, 16]) and want to concatenate them along the second axis (i.e., axis=1).
    Remember that Keras adds an additional batch dimension to the input/output tensors
    when you specify the shape argument. This operation results in a [None, 32]-sized
    tensor. From this point on, you only have a single sequence of layers. We will
    define a 16-node Dense layer with relu activation and finally an output layer
    that has three nodes with softmax normalization:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解在使用 Concatenate 层时会发生什么。Concatenate 层只是沿着给定的轴连接两个或多个输入。在此示例中，我们有两个输入传递给
    Concatenate 层（即 [None, 16] 和 [None, 16]），并希望沿着第二个轴（即 axis=1）进行连接。请记住，当您指定 shape
    参数时，Keras 会向输入/输出张量添加一个额外的批次维度。这个操作的结果是一个大小为 [None, 32] 的张量。从这一点开始，您只有一个序列的层。我们将定义一个具有
    relu 激活函数的 16 节点 Dense 层，最后是一个具有 softmax 归一化的三节点输出层：
- en: '[PRE23]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We need to do one extra step: create a Model object that says what the inputs
    and outputs are. As of now, we have a bunch of layers and no Model object. Finally,
    we compile the model as we did before. We will choose categorical_crossentropy
    as the loss and adam as the optimizer, as we did before. We will also monitor
    the training accuracy:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做一步额外的工作：创建一个 Model 对象，说明输入和输出是什么。现在，我们有一堆层和没有 Model 对象。最后，我们像之前一样编译模型。我们选择
    categorical_crossentropy 作为损失函数，adam 作为优化器，像之前一样。我们还将监控训练准确率：
- en: '[PRE24]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The full code for this model is provided in the following listing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的完整代码在以下清单中提供。
- en: Listing 3.2 Model B implemented with the Keras functional API
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.2 使用 Keras 函数式 API 实现的模型 B
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Making sure we are clearing out the TensorFlow graph
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保清除 TensorFlow 图
- en: ❷ The two input layers. One input layer has four features; the other has two.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 两个输入层。一个输入层具有四个特征，另一个输入层具有两个特征。
- en: ❸ The two parallel hidden layers
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 两个并行隐藏层
- en: '❹ The concatenation layer that combines two parallel outputs: out1 and out2'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 负责将两个并行输出 out1 和 out2 进行拼接的连接层
- en: ❺ The model definition
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 模型定义
- en: ❻ Compiling the model with a loss, an optimizer, and a metric
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用损失函数、优化器和评估指标编译模型
- en: Now you can print the model summary
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以打印模型的摘要了
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which gives
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的结果为
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: What do you think about this summary representation? Can you tell what kind
    of a model it is by looking at it? Unfortunately, no. Though we have parallel
    layers in our model, the summary looks like we have a sequence of layer-processing
    inputs and outputs one after another. Can we obtain a better representation than
    this? Yes, we can!
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个摘要表示，你觉得怎么样？你能从这个摘要中推断出它是什么样的模型吗？很遗憾，不能。虽然我们的模型有并行层，但是摘要看起来似乎我们有一系列按顺序处理输入和输出的层。我们有没有办法获得比这更好的表示呢？是的，我们有！
- en: Keras also offers the ability to visualize the model as a network diagram. You
    can easily do this with
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 还提供了以网络图的形式可视化模型的能力。您可以使用下面的代码实现：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If you run this command on a Jupyter notebook, you will have the inline output
    of the following graph (figure 3.4). It is now much clearer to see what’s going
    on in our model.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在 Jupyter 笔记本上运行此命令，您将获得以下图形的内联输出（图 3.4）。现在我们的模型的运行情况更加清晰了。
- en: '![03-04](../../OEBPS/Images/03-04.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![03-04](../../OEBPS/Images/03-04.png)'
- en: Figure 3.4 An illustration of the model we created with the functional API.
    You can see the parallel input layers and hidden layers at the top. The final
    output layer is at the bottom.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4 使用函数式 API 创建的模型示例。可以在顶部看到并行的输入层和隐藏层。最终的输出层位于底部。
- en: If you need to save this diagram to a file, simply do
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将此图保存到文件中，只需执行以下操作：
- en: '[PRE29]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: If you need to see input/output sizes in addition to the layer names and types,
    you can do that by setting the show_shapes parameter to True
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在层的名称和类型之外查看输入/输出大小，可以通过将 show_shapes 参数设置为 True 来实现
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: which will return figure 3.5.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回图 3.5。
- en: '![03-05](../../OEBPS/Images/03-05.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![03-05](../../OEBPS/Images/03-05.png)'
- en: Figure 3.5 Keras model plot with show_shapes=True
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 使用 show_shapes=True 绘制的 Keras 模型图
- en: 'Remember that we have two inputs, original features (x) and the first two principal
    components of x (let’s call it x_pca). You can compute the first two principal
    components as follows (using the scikit-learn library):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住我们有两个输入，原始特征（x）和 x 的前两个主成分（我们称之为 x_pca）。您可以如下计算前两个主成分（使用 scikit-learn 库）：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: PCA is already implemented in scikit-learn. You define a PCA object and pass
    the value 2 to the n_components argument. You also fix the random seed to ensure
    consistency across trials. Then you can call the method fit_transform(x) to get
    the final PCA features. You can train this model as you did before by calling
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 已经在 scikit-learn 中实现。你定义一个 PCA 对象，并将值 2 传递给 n_components 参数。你也固定了随机种子，以确保在各个试验中保持一致性。然后你可以调用
    fit_transform(x) 方法来获得最终的 PCA 特征。你可以像之前一样训练这个模型，调用
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Sadly, you will not see much of an accuracy improvement. The results will be
    on par with what you achieved earlier. In the given code example, you would have
    around 6% accuracy improvement when using this model. However, you will see that
    this gap will become smaller and smaller if you increase the number of epochs.
    This is mostly because adding PCA features doesn’t really add much value. We are
    reducing four dimensions to two, which is unlikely to result in better features
    than what we already have. Let’s try our luck in the next exercise.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 遗憾的是，你不会看到很大的准确率提升。结果将与您之前达到的相当。在给定的代码示例中，使用这个模型时你会有大约 6% 的准确率提升。然而，你会发现，如果增加迭代次数，这个差距会变得越来越小。这主要是因为添加
    PCA 特征并没有真正增加多少价值。我们将四个维度减少到两个，这不太可能产生比我们已经拥有的更好的特征。让我们在下一个练习中试试运气。
- en: 3.1.4 The sub-classing API
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 子类化 API
- en: Back in the research lab, it is a bit disheartening to see that adding principal
    components did not improve the results. However, the team is impressed with your
    knowledge of exactly which API to use for a given model. A team member suggested
    a final model. Currently a dense layer computes its output using
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回到研究实验室，看到添加主成分并没有改善结果有点令人沮丧。然而，团队对于你对于在给定模型中使用哪个 API 的了解却印象深刻。一位团队成员建议了一个最终模型。当前，密集层是通过以下方式计算其输出的
- en: '*h* = α(*xW + b*)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*h* = α(*xW + b*)'
- en: where α is some nonlinearity. You want to see if results can be improved by
    adding another bias (i.e., in addition to the additive bias, we add a multiplicative
    bias) so that the equation becomes
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 α 是某种非线性。你想看看是否通过添加另一个偏差（即，除了加性偏差外，我们添加了一个乘法偏差）可以改善结果，使得方程变为
- en: '*h* = α([*xW + b*] × *b*[mul])'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '*h* = α([*xW + b*] × *b*[mul])'
- en: This is where layer sub-classing will save the day, as there is no prebuilt
    layer in Keras that readily offers this functionality. The final API offered by
    Keras is the sub-classing API (figure 3.6), which will allow us to define the
    required computations as a unit of computation (i.e., a layer) and reuse it with
    ease when defining a model. Sub-classing comes from the software engineering concept
    of *inheritance*. The idea is that you have a super class that provides general
    functionality for a type of object (e.g., a Layer class), and you sub-class (or
    inherit) from that layer to create a more specific layer that achieves a specific
    functionality.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是层子类化会拯救一切的地方，因为在 Keras 中没有预先构建的层能够提供这种功能。Keras 提供的最终 API 是子类化 API（见图 3.6），它将允许我们将所需的计算定义为一个计算单元（即，一个层），并在定义模型时轻松重用它。子类化来自软件工程概念中的*继承*。其思想是你有一个提供某种对象一般功能的超类（例如，一个
    Layer 类），然后你从该层中派生（或继承），创建一个更具体的层，实现特定功能。
- en: '![03-06](../../OEBPS/Images/03-06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![03-06](../../OEBPS/Images/03-06.png)'
- en: Figure 3.6 Sub-classing API compared to other APIs (grayed out)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 子类化 API 与其他 API 的比较（已灰显）
- en: The sub-classing API is drastically different from the sequential and functional
    APIs. Here, you are creating a Python class that defines the underlying operations
    of a layer or a model. In this book we will focus on sub-classing layers (i.e.,
    not models). In my opinion, there will be more instances where you subclass a
    layer than a model because layer sub-classing is more commodious and can be needed
    in cases where you have a single model or multiple models. However, model sub-classing
    is only required if you are creating larger composite models that consist of many
    smaller models. It is also worthwhile to note that once you learn layer sub-classing,
    it’s relatively easy to extend to model sub-classing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 子类化 API 与顺序 API 和函数 API 有着截然不同的风格。在这里，你正在创建一个 Python 类，该类定义了层或模型的基本操作。在本书中，我们将专注于子类化层（即不包括模型）。在我看来，更多的情况下你会对层进行子类化而不是模型，因为层的子类化更加方便，可能在你只有一个模型或多个模型的情况下需要。然而，只有当你创建由许多较小模型组成的更大的复合模型时，才需要模型的子类化。值得注意的是，一旦你学会了层的子类化，扩展到模型的子类化相对容易。
- en: 'When sub-classing a layer, there are three important functions that you need
    to override from the Layer base class you inherit from:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当子类化层时，有三个重要的函数需要从你继承的 Layer 基类中重写：
- en: __init__()—Initializes the layer with any parameters it accepts
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: __init__() — 使用任何它接受的参数初始化层
- en: build()—Where the parameters of the model will be created
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: build() — 模型的参数将在这里创建
- en: call()—Defines the computations that need to happen during the forward pass
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: call() — 定义了正向传播期间需要进行的计算
- en: Here’s how you would write our new layer. We will call our custom layer MulBiasDense
    appropriately. Notice how this layer inherits from the base layer Layer found
    in the tensorflow.keras.layers submodule.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你会写的新层。我们将适当地称呼我们的自定义层为 MulBiasDense。注意这一层是如何继承自位于 tensorflow.keras.layers
    子模块中的基础层 Layer 的。
- en: Listing 3.3 Sub-classing a new layer with Keras
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 使用 Keras 子类化新层
- en: '[PRE33]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Defines various hyperparameters required to define the layer
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义了定义层所需的各种超参数
- en: ❷ Defines all the parameters in the layer as tf.Variable objects. self.b_mul
    represents the multiplicative bias.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将层中的所有参数定义为 tf.Variable 对象。self.b_mul 代表了乘法偏置。
- en: ❸ Defines the computation that needs to happen when data is fed to the layer
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义了在向层馈送数据时需要进行的计算
- en: 'First, we have the __init__() function. There are two parameters for the layer:
    the number of hidden units and the type of activation. The activation defaults
    to None, meaning that if unspecified, there will be no nonlinear activation (i.e.,
    only a linear transformation):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有 __init__() 函数。层有两个参数：隐藏单元的数量和激活类型。激活默认为 None，意味着如果未指定，则没有非线性激活（即仅进行线性转换）：
- en: '[PRE34]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we implement the build() function, a significant puzzle piece of sub-classing.
    All the parameters (e.g., weights and biases) are created within this function:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实现 build() 函数，这是子类化中的一个重要的拼图。所有参数（例如权重和偏置）都是在这个函数内创建的：
- en: '[PRE35]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here, the parameters w, b, and b_mul refer to *W*, *b*, and *b*[mul] in the
    equation. For each parameter, we provide the shape, an initializer, and a Boolean
    to indicate trainability. The initializer ''glorot_uniform'' ([http://mng.bz/N6A7](http://mng.bz/N6A7))
    used here is a popular neural network initializer. Finally, we need to write the
    call() function, which defines how the inputs are going to be transformed to produce
    an output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数 w、b 和 b_mul 分别指代方程中的 *W*、*b* 和 *b*[mul]。对于每个参数，我们提供了形状、初始化器和一个布尔值以指示可训练性。此处使用的初始化器
    'glorot_uniform'（[http://mng.bz/N6A7](http://mng.bz/N6A7)）是一种流行的神经网络初始化器。最后，我们需要编写
    call() 函数，它定义了输入将如何转换为输出：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'There it is: our first subclassed layer. It is worth noting that there are
    several other functions you need to be aware of when it comes to subclassing layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样：我们的第一个子类化层。值得注意的是，在子类化层时，你需要了解的其他几个函数还有：
- en: compute_output_shape()—Typically, Keras will automatically infer the shape of
    the output of the layer. But, if you do too many complex transformations, Keras
    might lose track, and you will need to explicitly define what the output shape
    is using this function.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: compute_output_shape() — 通常，Keras 会自动推断出层的输出形状。但是，如果你进行了太多复杂的转换，Keras 可能会迷失方向，你将需要使用这个函数明确地定义输出形状。
- en: get_config()—If you plan to save your model to disk after training, you need
    to implement this function, which returns a dictionary of the parameters taken
    in by the layer.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: get_config() - 如果您计划在训练后将模型保存到磁盘，则需要实现此函数，该函数返回由图层使用的参数的字典。
- en: With the new layer defined, you can use the functional API as before to create
    a model, as the following listing shows.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了新的层后，可以像以下清单展示的那样使用函数式 API 创建模型。
- en: Listing 3.4 Model C implemented with the Keras sub-classed API
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 3.4 使用 Keras 子类化 API 实现的模型 C
- en: '[PRE37]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Importing necessary modules and classes
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入必要的模块和类
- en: ❷ Making sure we are clearing out the TensorFlow graph
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确保我们清除 TensorFlow 图
- en: ❸ Defining the input layer
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义输入层
- en: ❹ Defining two layers with the new sub-classed layer MulBiasDense
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用新的子类化层 MulBiasDense 定义两个层
- en: ❺ Defining the softmax output layer
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义 softmax 输出层
- en: ❻ Defining the final model
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义最终模型
- en: ❼ Compiling the model with a loss function, an optimizer, and accuracy as metrics
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用损失函数、优化器和准确率作为指标编译模型
- en: Unfortunately, in our experiments, none of the architectural improvements we
    tried delivered a significantly better result. But you have managed to impress
    your colleagues by knowing which API to use for which model, enabling the group
    to have the results ready for the paper deadline. Table 3.1 further summarizes
    main advantages and disadvantages of the APIs we discussed.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在我们的实验中，我们尝试的所有架构改进都没有带来明显的改进。但是，您通过知道针对哪个模型使用哪个 API 使同事们感到印象深刻，使小组能够在提交论文的截止日期前准备好结果。表3.1
    进一步总结了我们讨论的 API 的主要优缺点。
- en: Table 3.1 Pros and cons of using various Keras APIs
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 使用不同 Keras APIs 的优缺点
- en: '| Sequential API | Pros | Models implemented with the Sequential API are easy
    to understand and are concise. |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Sequential API | Pros | 使用顺序 API 实现的模型易于理解、简洁。 |'
- en: '| Cons | Cannot implement models having complex architectural characteristics
    such as multiple inputs/outputs. |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Cons | 无法实现具有多输入/输出等复杂架构特性的模型。 |'
- en: '| Functional API | Pros | Can be used to implement models with complex architectural
    elements such as multiple inputs/outputs. |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Functional API | Pros | 可用于实现具有多输入/输出等复杂架构元素的模型。 |'
- en: '| Cons | The developer needs to manually connect various layers correctly and
    create a model. |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Cons | 开发人员需要手动正确连接各种层并创建模型。 |'
- en: '| Sub-classing API | Pros | Can create custom layers and models that are not
    provided as standard layers. |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Sub-classing API | Pros | 可以创建不作为标准层提供的自定义层和模型。 |'
- en: '| Cons | Requires thorough understanding of low-level functionality provided
    by TensorFlow. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Cons | 需要深入理解 TensorFlow 提供的底层功能。 |'
- en: '| Due to the user-defined nature, it can lead to instabilities and difficulties
    in debugging. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 由于用户定义的性质，可能会导致不稳定性和调试困难。 |'
- en: In the next section, we will discuss different ways you can import and ingest
    data in TensorFlow.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论您可以在 TensorFlow 中导入和摄入数据的不同方式。
- en: Exercise 1
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 练习1
- en: Say you need to create a fully connected neural network that has a single input
    layer and two output layers. Which API you think is the most suitable for this
    task?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您需要创建一个具有单个输入层和两个输出层的全连接神经网络。您认为哪个 API 最适合这项任务？
- en: 3.2 Retrieving data for TensorFlow/Keras models
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 获取 TensorFlow/Keras 模型的数据
- en: So far, we have looked at how to implement various models with different Keras
    APIs. At this point, you should be comfortable with knowing which API to use (or
    sometimes which API *not* to use) when you see the architecture of a model. Moving
    forward, we will learn about reading data to train these models using TensorFlow/Keras.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过了如何使用不同的 Keras APIs 实现各种模型。此时，您应该已经知道何时使用哪种 API（有时甚至知道不该使用哪种API）来查看模型的架构。接下来，我们将学习如何使用
    TensorFlow/Keras 读取数据来训练这些模型。
- en: Let’s assume that you recently joined a startup as a data scientist who is experimenting
    with software encompassing a machine learning model to identify flower species
    (using images). They already have a custom-written data pipeline that can take
    a batch of images and a batch of labels and train a model. However, this data
    pipeline is quite obscure and difficult to maintain. You’re tasked with implementing
    a replacement data pipeline that is easy to understand and maintain. This is a
    golden opportunity to impress your boss by quickly prototyping a data pipeline
    using TensorFlow.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您最近加入了一家初创公司，作为一名数据科学家，正在尝试使用包含机器学习模型的软件来识别花的品种（使用图像）。他们已经有一个可以接受一批图像和一批标签并训练模型的自定义数据管道。然而，这个数据管道相当隐晦且难以维护。您的任务是实现一个易于理解和维护的替代数据管道。这是一个通过使用
    TensorFlow 快速原型设计数据管道来给您的老板留下深刻印象的绝佳机会。
- en: 'A model doesn’t have any value unless it has been trained with data. As more
    (quality) data means better performance, it is important to feed data to the model
    in a scalable and efficient manner. It’s time to explore features of TensorFlow
    that allow you to create input pipelines that achieve this. There are two popular
    alternatives to retrieving data:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 除非经过数据训练，否则模型没有任何价值。更多（高质量）的数据意味着更好的性能，因此将数据以可伸缩和高效的方式提供给模型非常重要。现在是时候探索 TensorFlow
    的特性，以创建实现此目的的输入管道。有两种流行的获取数据的替代方法：
- en: The tf.data API
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tf.data API
- en: Keras data generators
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras 数据生成器
- en: The data set you’ll be working with (downloaded from [http://mng.bz/DgVa](http://mng.bz/DgVa))
    contains a collection of 210 flower images (in .png format) and a CSV (comma-separated
    value) file that contains the filename and label.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您将要处理的数据集（从 [http://mng.bz/DgVa](http://mng.bz/DgVa) 下载）包含一个包含文件名和标签的 CSV（逗号分隔值）文件以及一个包含
    210 个花卉图像（.png 格式）的集合。
- en: Note There is also a third method, which is to use a Python package to access
    popular machine learning data sets. This package is known as the tensorflow-datasets.
    This means that this method works only if you want to use a data set that is already
    supported by the package.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 还有第三种方法，那就是使用 Python 包访问流行的机器学习数据集。这个包被称为 tensorflow-datasets。这意味着只有当您想要使用包已支持的数据集时，此方法才有效。
- en: It’s time to crack some knuckles and get to implementing the data pipeline.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候伸展一下手指，开始实现数据管道了。
- en: 3.2.1 tf.data API
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 tf.data API
- en: Let’s see what an input pipeline might look like. For example, an input pipeline
    for your image classification task might look like figure 3.7\. Initially, the
    integer labels are read from a text file (stored as [filename, label] records).
    Next, the images corresponding to the filenames are read and resized to a constant
    height and width. The labels are then converted to a one-hot encoded representation.
    One-hot encoded representation converts an integer to a vector of zeros and ones.
    Then the images and one-hot encoded labels are zipped together to keep the correct
    correspondence between images and their respective labels. This data now can be
    fed directly to a Keras model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看输入管道可能是什么样子。例如，用于您的图像分类任务的输入管道可能看起来像图 3.7。首先，从文本文件中读取整数标签（存储为 [文件名、标签]
    记录）。接下来，读取与文件名对应的图像并将其调整为固定的高度和宽度。然后，将标签转换为 one-hot 编码表示。One-hot 编码表示将整数转换为由零和一组成的向量。然后，将图像和
    one-hot 编码标签压缩在一起，以保持图像与其相应标签之间的正确对应关系。现在，这些数据可以直接馈送到 Keras 模型中。
- en: '![03-07](../../OEBPS/Images/03-07.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![03-07](../../OEBPS/Images/03-07.png)'
- en: Figure 3.7 The input pipeline that you’ll be developing using the tf.data API
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7 您将使用 tf.data API 开发的输入管道
- en: 'In our data set, we have a collection of flower images and a CSV file that
    contains the filename and the corresponding label. We will perform the following
    steps in order to create the data pipeline:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中，我们有一组花卉图像和一个包含文件名及其对应标签的 CSV 文件。我们将按照以下步骤创建数据管道：
- en: Read CSV file as a tf.data.Dataset.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 CSV 文件读取为 tf.data.Dataset。
- en: Extract filenames and labels as separate data sets.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件名和标签作为单独的数据集提取出来。
- en: Read the image files corresponding to the filenames in the filename data set.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取与文件名数据集中的文件名对应的图像文件。
- en: Decode the image data and convert it to a float32 tensor.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码图像数据并将其转换为 float32 张量。
- en: Resize the images to 64 × 64 pixels.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像调整为 64 × 64 像素。
- en: Convert labels to one-hot encoded vectors.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签转换为 one-hot 编码向量。
- en: Zip the image data set and the one-hot vector data sets.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像数据集和 one-hot 向量数据集压缩在一起。
- en: Batch the data set in batches of five samples.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分批为五个样本的批次。
- en: 'In order to read the CSV file as a data set entity, we will use the convenient
    tf.data .experimental.CsvDataset object. You might see that this is, in fact,
    an experimental object. This means it has not been tested as extensively as other
    functionality in the tf.data API and might break in certain instances. But for
    our small and simple example there won’t be any issues:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将 CSV 文件读取为一个数据集实体，我们将使用方便的 tf.data.experimental.CsvDataset 对象。您可能会发现，实际上，这是一个实验性的对象。这意味着它的测试程度没有
    tf.data API 中的其他功能那么多，并且在某些情况下可能会出现问题。但对于我们的小而简单的示例，不会出现任何问题：
- en: '[PRE38]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The tf.data.experimental.CsvDataset object expects two mandatory arguments:
    one or more filenames and a default record, which will be used as the default
    if a record is corrupted or unreadable. In our case, the default record is an
    empty filename (“”) and the label -1\. You can print some of the records from
    tf.data.Dataset by calling'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: tf.data.experimental.CsvDataset 对象需要两个强制参数：一个或多个文件名和一个默认记录，如果记录损坏或不可读，将使用默认记录。在我们的案例中，默认记录是一个空文件名（“”）和标签
    -1。您可以通过调用 tf.data.Dataset 打印一些记录
- en: '[PRE39]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here, take() is a function that takes a number as the argument and returns
    that many records from the data set. This will output the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，take() 是一个函数，它以数字作为参数，并从数据集中返回那么多的记录。这将输出以下内容：
- en: '[PRE40]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'If you remember, the flower_labels.csv file contains two columns: filenames
    and the corresponding labels. You can see in the data set output that each tuple
    carries two elements: the filename and the label. Next, we will split these two
    columns as two separate data sets. This can easily be done using the map() function,
    which applies a given function across all the records in a data set:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，flower_labels.csv 文件包含两列：文件名和相应的标签。您可以在数据集输出中看到，每个元组都包含两个元素：文件名和标签。接下来，我们将这两列拆分为两个单独的数据集。这可以很容易地通过使用
    map() 函数来完成，该函数将一个给定的函数应用于数据集中的所有记录：
- en: '[PRE41]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Lambda expressions
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 表达式
- en: 'Lambda expressions are a great tool that enables you to have anonymous functions
    in the code. Just like normal functions, they take in arguments and return some
    output. For example, the following function will add two given values (x and y):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 表达式是一个很棒的工具，它使您可以在代码中使用匿名函数。就像普通函数一样，它们接受参数并返回一些输出。例如，以下函数将添加两个给定值（x
    和 y）：
- en: '[PRE42]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Lambda expressions are a great way to write functions if they are used only
    once and thus require no name. Learning to use lambda expressions effectively
    will keep your code clean and succinct.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 表达式是一种很好的写函数的方式，如果它们只被使用一次，因此不需要名称。学会有效使用 lambda 表达式将使您的代码清晰而简洁。
- en: 'Here, we use a succinct lambda expression to tell the map() function what we
    want to achieve. We can now focus on fetching the image data. In order to do that,
    we will again use the map() function. But this time, we will write a separate
    function defining what needs to happen:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用简洁的 lambda 表达式告诉 map() 函数我们想要实现什么。现在，我们可以专注于获取图像数据。为了做到这一点，我们将再次使用 map()
    函数。但这一次，我们将编写一个单独的函数来定义需要发生的事情：
- en: '[PRE43]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'To get the image tensors from the filename, all we need to do is apply this
    function to all filenames in the fname_ds:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要从文件名中获取图像张量，我们所需要做的就是将该函数应用于 fname_ds 中的所有文件名：
- en: '[PRE44]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'With the image data set read, let’s convert the label data to one-hot encoded
    vectors:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 随着图像数据集的读取，让我们将标签数据转换为独热编码向量：
- en: '[PRE45]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'In order to train an image classifier, we need two items: an image and a label.
    We do have both of these as two separate data sets. However, we need to combine
    them into one data set in order to ensure consistency. For example, if we need
    to shuffle data, it is immensely important to have the data sets combined into
    one to avoid different randomly shuffled states, which will destroy the image-to-label
    correspondence in the data. The tf.data.Dataset.zip() function lets you do this
    easily:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练图像分类器，我们需要两个项目：一个图像和一个标签。我们确实有这两个作为两个单独的数据集。但是，我们需要将它们合并为一个数据集，以确保一致性。例如，如果我们需要对数据进行洗牌，将数据集合并成一个非常重要，以避免不同的随机洗牌状态，这将破坏数据中的图像到标签的对应关系。tf.data.Dataset.zip()
    函数让您可以轻松地做到这一点：
- en: '[PRE46]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We’ve done lots of work. Let’s recap:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经做了大量工作。让我们回顾一下：
- en: Read a CSV file as a tf.data.Dataset, which contains filenames and labels
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取一个包含文件名和标签的 CSV 文件作为 tf.data.Dataset
- en: Separated file names (fname_ds) and labels (label_ds) into two separate data
    sets
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将文件名（fname_ds）和标签（label_ds）分开为两个单独的数据集
- en: Loaded images from file names as a data set (images_ds) while doing some preprocessing
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件名加载图像作为数据集（images_ds）同时进行一些预处理
- en: Converted labels to one-hot encoded vectors
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标签转换为独热编码向量
- en: Created a combined data set using the zip() function
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 zip() 函数创建了一个组合数据集
- en: 'Let’s take a moment to see what we have created. A tf.data.Dataset behaves
    like a normal python iterator. This means that you can iterate through items easily
    using a loop (e.g., for/while) and also use functions such as next() to get items.
    Let’s see how we can iterate data in a for loop:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间看看我们创建了什么。tf.data.Dataset 的行为类似于普通的 python 迭代器。这意味着你可以使用循环（例如 for/while）轻松地迭代项，也可以使用
    next() 等函数获取项。让我们看看如何在 for 循环中迭代数据：
- en: '[PRE47]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'This will return the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这会返回以下内容：
- en: '[PRE48]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As you can see, item is a tuple, the first element being the image tensor (of
    size 64 × 64 × 3) and the second being a one-hot encoded vector (of size 10).
    There’s some more work to be done. First, let’s shuffle the data set to make sure
    we are not introducing any consistent ordering of data when feeding to the model:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，item 是一个元组，第一个元素是图像张量（大小为 64 × 64 × 3），第二个元素是一个独热编码向量（大小为 10）。还有一些工作要做。首先，让我们对数据集进行洗牌，以确保在馈送给模型之前不引入任何有序数据：
- en: '[PRE49]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The buffer_size argument serves an important purpose. It specifies, at run time,
    how many elements are loaded to memory for the shuffling. In this case, the input
    pipeline will load 20 records to memory and randomly sample from that when you
    iterate the data. A larger buffer_size will provide better randomization but will
    increase the memory requirement. Next, we will look at how to create a batch of
    data from the data set.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: buffer_size 参数起着重要作用。它在运行时指定了加载到内存中用于洗牌的元素数量。在本例中，输入管道将加载 20 条记录到内存中，并在迭代数据时从中随机抽样。较大的
    buffer_size 可以提供更好的随机化，但会增加内存需求。接下来，我们将讨论如何从数据集中创建数据批次。
- en: 'Remember that we said Keras adds a batch dimension automatically when you specify
    either input_shape (Sequential API) or the shape (functional API) when creating
    a model. That’s how deep networks process data: as batches of data (i.e., not
    individual samples). Therefore, it is important to batch data before you feed
    it to the model. For example, if you use a batch size of 5, you will get a 5 ×
    64 × 64 × 3 image tensor and a 5 × 10 labels tensor if you iterate the previous
    data set. With tf.data.Dataset, API batching data is quite straightforward:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们说过 Keras 在创建模型时，如果指定了 input_shape（Sequential API）或 shape（functional API），会自动添加批次维度。这就是深度网络处理数据的方式：作为数据批次（即，不是单个样本）。因此，在将数据馈送到模型之前进行批处理非常重要。例如，如果使用批次大小为
    5，如果迭代之前的数据集，你将得到一个大小为 5 × 64 × 64 × 3 的图像张量和一个大小为 5 × 10 的标签张量。使用 tf.data.Dataset
    API 对数据进行批处理非常简单：
- en: '[PRE50]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You can print one element of this using
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下方式打印其中一个元素：
- en: '[PRE51]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: which will show
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 运行这个命令后，你将得到以下结果：
- en: '[PRE52]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: That’s the end of this exercise. The next listing shows what the final code
    looks like.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是本练习的结束。下面的代码展示了最终的代码的样子。
- en: Listing 3.5 tf.data Input pipeline for the flower images data set
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 代码清单3.5 tf.data 用于花朵图像数据集的输入管道
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ Reading the data from the CSV file using TensorFlow
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 TensorFlow 从 CSV 文件中读取数据。
- en: ❷ Separating out the filenames and integer labels to two data set objects
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将文件名和整数标签分开为两个数据集对象
- en: ❸ Reading in the images from filenames
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从文件名中读取图像
- en: ❹ Converting the integer labels to one-hot encoded labels
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将整数标签转换为独热编码标签
- en: ❺ Combining the images and labels into a single data set
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将图像和标签合并为一个数据集
- en: ❻ Shuffling and batching data, preparing it for the model
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对数据进行洗牌和分批处理，为模型做准备。
- en: Note that you won’t be able to use the models we created during the Iris data
    set exercise, as those are fully connected networks. We need convolutional neural
    networks for processing image data. To get your hands dirty, there is a very simple
    convolutional neural network model provided in the exercise notebook 3.2.Creating_Input_
    Pipelines.ipynb in the Ch03-Keras-and-Data-Retrieval folder. Don’t worry about
    the various layers and their parameters used here. We will discuss convolutional
    neural networks in detail in the next chapter.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你无法使用我们在鸢尾花数据集练习中创建的模型，因为那些是全连接网络。我们需要使用卷积神经网络来处理图像数据。为了让你有所了解，练习笔记本3.2.Creating_Input_
    Pipelines.ipynb中提供了一个非常简单的卷积神经网络模型。不用担心这里使用的各种层和它们的参数，我们将在下一章详细讨论卷积神经网络。
- en: '[PRE54]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Using this input pipeline, you can conveniently feed data to an appropriate
    model using
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此输入管道，你可以方便地使用适当的模型馈送数据：
- en: '[PRE55]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Once you run this command, you’ll get the following:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，你将获得以下结果：
- en: '[PRE56]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: With some great results achieved quickly in your very first week on the job,
    you walk proudly up to your boss and demonstrate the work you have done. He is
    quite impressed with the clarity and efficiency of the pipeline you have built.
    However, you begin to wonder, can I do a better job with Keras data generators?
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在你上任的第一个星期里迅速取得了一些很好的成果，你自豪地走到老板面前展示你所做的工作。他对你建立的流程的清晰性和高效性感到非常印象深刻。然而，你开始思考，我能用
    Keras 数据生成器做得更好吗？
- en: Exercise 2
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 2
- en: Imagine you have a labels data set called labels_ds (i.e., a sequence of integer
    labels), and there are corrupted labels with the value -1\. Can you write a lambda
    function and use that with the tf.Dataset.map() function to remove these labels?
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下你有一个标签数据集叫 labels_ds（即一个整数标签序列），并且有一些值为 -1 的损坏标签。你能写一个 lambda 函数并将其与 tf.Dataset.map()
    函数一起使用来删除这些标签吗？
- en: 3.2.2 Keras DataGenerators
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 Keras 数据生成器
- en: 'Another avenue for fetching image data is to use a data generator provided
    in Keras. Currently, Keras provides two data generators:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个获取图像数据的途径是使用 Keras 提供的数据生成器。目前，Keras 提供了两个数据生成器：
- en: '[PRE57]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Though not as customizable as the tf.data API, these generators still provide
    a quick and easy way to feed data into a model. Let’s see how we can use the ImageDataGenerator
    to feed this data to the model. The ImageDataGenerator ([http://mng.bz/lxpB](http://mng.bz/lxpB))
    has a very long list of allowed parameters. Here, we will only focus on how we
    can adapt ImageDataGenerator to read the data we have.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然不像 tf.data API 那样可定制，但这些生成器仍然提供了一种快速简便的方式将数据输入模型。我们来看看如何使用 ImageDataGenerator
    将这些数据提供给模型。ImageDataGenerator ([http://mng.bz/lxpB](http://mng.bz/lxpB)) 有一个非常长的允许参数列表。在这里，我们只关注如何使
    ImageDataGenerator 适应我们所拥有的数据。
- en: 'Then, to fetch data, Keras ImageDataGenerator offers the flow_from_dataframe()
    function. This function is ideal for us, as we have a CSV file that contains filenames
    and their associated labels, which can be represented as a pandas DataFrame. Let’s
    start with some variable definitions:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了获取数据，Keras ImageDataGenerator 提供了 flow_from_dataframe() 函数。这个函数对我们来说非常理想，因为我们有一个包含文件名和它们关联标签的
    CSV 文件，可以表示为一个 pandas DataFrame。让我们从一些变量定义开始：
- en: '[PRE58]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we’ll define an ImageDataGenerator with default parameters:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用默认参数定义一个 ImageDataGenerator：
- en: '[PRE59]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now we can use the flow_from_dataframe() function:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 flow_from_dataframe() 函数：
- en: '[PRE60]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We first load the CSV file, which contains two columns: file (filenames) and
    label (integer label). With that, we call the flow_from_dataframe() function,
    along with the following important parameters:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载包含两列的 CSV 文件：file（文件名）和 label（整数标签）。接下来，我们调用 flow_from_dataframe() 函数，同时还有以下重要参数：
- en: dataframe—The data frame that contains label information
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataframe—包含标签信息的数据框
- en: directory—The directory to locate images
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: directory—定位图像的目录
- en: x_col—The name of the column in the data frame that contains filenames
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x_col—数据框中包含文件名的列的名称
- en: y_col—The name of the column containing the labels
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y_col—包含标签的列的名称
- en: class_mode—The nature of the labels (since we have the raw label, class_mode
    is set to raw)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: class_mode—标签的性质（由于我们有原始标签，class_mode 设置为原始）
- en: You can see what the first sample looks like by running
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过运行下面的代码来查看第一个样本是什么样子的
- en: '[PRE61]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: which will output
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE62]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Again, with a batch size of 5, you see a batch of images (i.e., of size 5 ×
    64 × 64 × 3) and a one-hot encoded batch of labels (of size 5 × 6) generated as
    a tuple. The full code looks like the following listing.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，使用批量大小为 5，你会看到一个图像批（即大小为 5 × 64 × 64 × 3）和一个 one-hot 编码的标签批（大小为 5 × 6）生成为一个元组。完整的代码如下所示。
- en: Listing 3.6 Keras ImageDataGenerator for the flower image data set
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6 Keras ImageDataGenerator 用于花卉图像数据集
- en: '[PRE63]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: ❶ Importing necessary modules
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入必要的模块
- en: ❷ Defining the data directory
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义数据目录
- en: ❸ Defining the ImageDataGenerator to process the images and labels
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义 ImageDataGenerator 来处理图像和标签
- en: ❹ Defining the labels by reading the CSV as a data frame
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过读取 CSV 文件作为数据框来定义标签
- en: ❺ Reading the images and labels from the filenames and labels in the data frame
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 从数据框中的文件名和标签读取图像和标签
- en: This looks even better than the previous pipeline. In just three lines of code,
    you have created a data pipeline. You have definitely impressed your boss with
    your knowledge, and you are on track for a quick promotion.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来比之前的流程更好。你仅用三行代码就创建了一个数据流程。你的知识肯定让你的老板印象深刻，你正在走上快速晋升的道路。
- en: We will discuss the parameters of the ImageDataGenerator, as well as some of
    the other data retrieval functions this supports, in detail in a later chapter.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节详细讨论ImageDataGenerator的参数以及它支持的其他数据检索函数。
- en: However, it is important to keep in mind that concise is not always better.
    Usually, concise means that what you can achieve with that method is limited.
    And that is true for the tf.data API and Keras data generators. The tf.data API,
    despite requiring a bit more work than the Keras data generator, is much more
    flexible (and can be made efficient) than Keras data generators.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，要记住简洁并不总是好的。通常，简洁意味着你可以通过这种方法实现的功能有限。对于tf.data API和Keras数据生成器来说也是如此。tf.data
    API尽管需要比Keras数据生成器更多的工作，但比Keras数据生成器更灵活（并且可以提高效率）。
- en: 3.2.3 tensorflow-datasets package
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 tensorflow-datasets 包
- en: The easiest way to retrieve data in TensorFlow is to use the tensorflow-datasets
    ([https://www.tensorflow.org/datasets/overview](https://www.tensorflow.org/datasets/overview))
    package. However, a key limitation is that tensorflow-datasets only supports a
    set of defined data sets, unlike the tf.data API or Keras data generators, which
    can be used to feed data from a custom data set. This is a separate package and
    is not a part of the official TensorFlow package. And if you have set up the Python
    environment as instructed, you already have this package installed in your environment.
    If not, you can easily install this by executing
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中检索数据的最简单方法是使用tensorflow-datasets ([https://www.tensorflow.org/datasets/overview](https://www.tensorflow.org/datasets/overview))
    包。然而，一个关键的限制是tensorflow-datasets只支持一组定义好的数据集，而不像tf.data API或Keras数据生成器可以用于从自定义数据集中获取数据。这是一个单独的包，不是官方TensorFlow包的一部分。如果你按照说明设置了Python环境，你已经在你的环境中安装了这个包。如果没有，你可以通过执行以下命令轻松安装它：
- en: '[PRE64]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'in your virtual Python environment’s terminal (e.g., Anaconda command prompt).
    To make sure the package is installed correctly, run the following line in your
    Jupyter notebook and make sure you don’t get any errors:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的虚拟Python环境的终端（例如，Anaconda命令提示符）中执行上述命令。为了确保软件包安装正确，运行以下行在你的Jupyter笔记本中，确保没有出现任何错误：
- en: '[PRE65]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: tensorflow-datasets provides a plethora of data sets under many different categories.
    You can find a comprehensive list of what’s available at [https://www.tensorflow.org/datasets/catalog](https://www.tensorflow.org/datasets/catalog).
    Table 3.2 also outlines some popular data sets available in tensorflow-datasets.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: tensorflow-datasets提供了许多不同类别的数据集。你可以在[https://www.tensorflow.org/datasets/catalog](https://www.tensorflow.org/datasets/catalog)找到一个全面的可用列表。表3.2还概述了一些在tensorflow-datasets中可用的热门数据集。
- en: Table 3.2 Several data sets available in tensorflow-datasets
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 tensorflow-datasets中可用的几个数据集
- en: '| **Data type** | **Dataset name** | **Task** |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| **数据类型** | **数据集名称** | **任务** |'
- en: '| Audio | librispeech | Speech recognition |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| Audio | librispeech | 语音识别 |'
- en: '| ljspeech | Speech recognition |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| ljspeech | 语音识别 |'
- en: '| Images | caltech101 | Image classification |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| Images | caltech101 | 图像分类 |'
- en: '| cifar10 and cifar100 | Image classification |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| cifar10和cifar100 | 图像分类 |'
- en: '| imagenet2012 | Image classification |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| imagenet2012 | 图像分类 |'
- en: '| Text | imdb_reviews | Sentiment analysis |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Text | imdb_reviews | 情感分析 |'
- en: '| tiny_shakespeare | Language modelling |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| tiny_shakespeare | 语言模型 |'
- en: '| wmt14_translate | Machine translation |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| wmt14_translate | 机器翻译 |'
- en: 'Let’s use tensorflow-datasets to retrieve the cifar10 data set, a widely used
    image classification data set that has images (RGB images of size 32 × 32) belonging
    to 10 categories (e.g., automobile, ship, cat, horse, etc.). First, let’s make
    sure it’s available as a data set. Execute the following on your Jupyter notebook:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用tensorflow-datasets来检索cifar10数据集，这是一个广泛使用的图像分类数据集，其中包含属于10个类别（例如汽车、船、猫、马等）的32×32大小的RGB图像。首先，让我们确保它作为一个数据集可用。在Jupyter笔记本上执行以下操作：
- en: '[PRE66]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We can see that cifar10 is one of those data sets, as expected. Let’s load
    the data set using the tfds.load() function. When you initially call this method,
    TensorFlow will first download the data set and then load it for you:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到cifar10是其中一个数据集，正如我们所期望的那样。让我们使用tfds.load()函数加载数据集。当你首次调用这个方法时，TensorFlow会先下载数据集，然后为你加载它：
- en: '[PRE67]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'When it is successfully downloaded, look at what information is available in
    the (info) variable:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当它成功下载后，查看（info）变量中可用的信息：
- en: '[PRE68]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'It’s quite informative. We now know that there are 60,000 32 × 32 color images
    that belong to 10 classes. The data set is split into 50,000 (training) and 10,000
    (testing). Let’s now look at the data variable:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有信息量。我们现在知道有 60,000 个 32 × 32 的彩色图像属于 10 个类别。数据集分为 50,000（训练）和 10,000（测试）。现在让我们看看数据变量：
- en: '[PRE69]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: We can see that it is a dictionary with keys 'train' and 'test', and each key
    has a tf.data.Dataset. Luckily, we have studied how tf.data.Dataset works, so
    we can race forward to understand how to prepare data. Let’s look at the training
    data. You can access this training data set using
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它是一个包含键“train”和“test”的字典，每个键都有一个 tf.data.Dataset。幸运的是，我们已经学习了 tf.data.Dataset
    的工作原理，所以我们可以快速了解如何准备数据。让我们看一下训练数据。你可以通过以下方式访问这个训练数据集。
- en: '[PRE70]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'However, if you try to iterate this data set, you will notice that the data
    has not been batched. In other words, data is retrieved a single sample at a time.
    But, as we have said many times, we need data in batches. And the fix is simple:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你尝试迭代这个数据集，你会注意到数据并没有被分批。换句话说，数据是一次检索一个样本。但是，正如我们已经说过很多次的那样，我们需要批量数据。修复方法很简单：
- en: '[PRE71]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Now, to see what a batch of data looks like in train_ds, you can execute the
    following:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了看一下 train_ds 中的一批数据是什么样子，你可以执行以下操作：
- en: '[PRE72]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: This will output
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出
- en: '[PRE73]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'It will be a dictionary with three keys: id, image, and label. id is a unique
    ID for each training record. image will have a tensor of size 16 × 32 × 32 × 3,
    whereas label will have a tensor of size 16 (i.e., integer labels). When passing
    a tf.data.Dataset to a Keras model, the model expects the data set object to produce
    a tuple (x,y), where x would be a batch of images and y would be the labels (e.g.,
    one-hot encoded). Therefore, we need to write one additional function that will
    put data into the correct format:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 它将是一个包含三个键的字典：id、image 和 label。id 是每个训练记录的唯一标识。image 将有一个大小为 16 × 32 × 32 ×
    3 的张量，而 label 将有一个大小为 16 的张量（即整数标签）。当将 tf.data.Dataset 传递给 Keras 模型时，模型期望数据集对象产生一个元组
    (x,y)，其中 x 是一批图像，y 是标签（例如，one-hot 编码）。因此，我们需要编写一个额外的函数，将数据放入正确的格式：
- en: '[PRE74]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'With that simple transformation, you can feed this data set to a model as follows:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个简单的转换，你可以将这个数据集馈送给一个模型，方法如下：
- en: '[PRE75]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'This is amazing work. Now you know three different ways to retrieve data for
    your models: the tf.data API, Keras data generators, and the tensorflow-datasets
    package. We will conclude our discussion about Keras APIs and different data import
    APIs here.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这是令人惊讶的工作。现在你知道了为模型检索数据的三种不同方法：tf.data API、Keras 数据生成器和 tensorflow-datasets
    包。我们将在这里结束对 Keras API 和不同数据导入 API 的讨论。
- en: Exercise 3
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3
- en: Can you write a line of code to import the caltech101 data set? After you do
    that, explore this data set.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 你能写一行代码导入 caltech101 数据集吗？在你这样做之后，探索这个数据集。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Keras, now integrated into TensorFlow, provides several high-level model-building
    APIs: the Sequential API, functional API and sub-classing API. These APIs have
    different pros and cons.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras，现在已经集成到 TensorFlow 中，提供了几种高级模型构建 API：串行 API、功能 API 和子类化 API。这些 API 有不同的优缺点。
- en: The Sequential API is the easiest to use but can only be used to implement simple
    models.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 串行 API 是使用最简单的，但只能用于实现简单的模型。
- en: The functional and sub-classing APIs can be difficult to use but enable developers
    to implement complex models.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能和子类化 API 可能难以使用，但允许开发人员实现复杂的模型。
- en: 'TensorFlow encompasses several methods for retrieving data: the tf.data API,
    Keras data generators, and tensorflow-datasets. tf.data.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 包含几种获取数据的方法：tf.data API、Keras 数据生成器和 tensorflow-datasets。tf.data。
- en: An API provides the most customizable way to feed data to a model but requires
    more work to fetch data.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 提供了向模型提供数据的最可定制方式，但需要更多的工作来获取数据。
- en: tensorflow-datasets is the easiest to use but is limited as it only supports
    a limited set of data sets.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensorflow-datasets 是使用最简单的，但是它有限，因为它只支持有限的数据集。
- en: Answers to exercises
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1:** The functional API. As there are two output layers, we cannot
    use the Sequential API. There is no need to use the sub-classing API, as everything
    we need can be done using Keras layers.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1:** 功能 API。由于有两个输出层，我们不能使用串行 API。没有必要使用子类化 API，因为我们需要的一切都可以使用 Keras 层完成。'
- en: '**Exercise 2:** labels_ds.map(lambda x: x if x != -1). You can also use the
    tf.Dataset .filter() method (i.e., labels_ds.filter(lambda x: x != -1)).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2:** labels_ds.map(lambda x: x if x != -1). 你也可以使用 tf.Dataset .filter()
    方法（即 labels_ds.filter(lambda x: x != -1)）。'
- en: '**Exercise 3:** tfds.load("caltech101", with_info=True)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3:** tfds.load("caltech101", with_info=True)'
