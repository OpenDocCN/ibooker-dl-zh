- en: 11 Creating an authorship identification program
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 创建一个作者身份识别程序
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Writing an authorship identification program using top-down design
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自顶向下设计编写作者身份识别程序
- en: Learning about refactoring code and why you would do it
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解代码重构以及为什么你会这样做
- en: 'In chapter 7, we learned about problem decomposition and top-down design when
    we wrote our Spelling Suggestions program. Here, we’re going to take top-down
    design to the next level and solve a much larger problem. We’re still doing the
    same thing as in chapter 7: dividing a problem into subproblems, and further dividing
    those subproblems into sub-subproblems as needed. And, just like before, we’re
    looking to design functions with a small number of parameters that return a meaningful
    and useful result to their caller. It’s also a good sign if we’re able to design
    functions that are called by multiple other functions—that helps reduce code repetition!'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们在编写拼写建议程序时学习了问题分解和自顶向下的设计。在这里，我们将把自顶向下的设计提升到下一个层次，解决一个更大的问题。我们仍然在做第7章中同样的工作：将问题分解为子问题，并在需要时进一步将那些子问题分解为更小的子子问题。而且，就像之前一样，我们希望设计出具有少量参数且能返回对调用者有意义的和有用的结果的函数。如果我们能够设计出被多个其他函数调用的函数，那也是一个好兆头——这有助于减少代码重复！
- en: We’re including this chapter because we wanted to provide a more authentic example
    than the Spelling Suggestions problem we solved in chapter 7\. We hope our example
    here is motivating and feels like a real problem that you could imagine yourself
    wanting to solve.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包括这一章节，是因为我们希望提供一个比第7章中解决的拼写建议问题更真实的例子。我们希望这里的例子能够激励人心，感觉像是一个你可能会想要解决的问题。
- en: In this chapter, we’re going to write a program that tries to identify the unknown
    author of a mystery book. It’ll be an example of a program that uses artificial
    intelligence (AI) to make a prediction. We couldn’t resist the opportunity to
    include an AI example in a book about programming with AI!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将编写一个尝试识别神秘书籍未知作者的程序。这将是使用人工智能（AI）进行预测的程序的一个例子。我们无法抗拒在关于编程与AI的书籍中包含一个AI例子的机会！
- en: 11.1 Authorship identification
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 作者身份识别
- en: 'This problem is based on an assignment created by our colleague Michelle Craig
    [1]. Let’s start by taking a look at these two book excerpts:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题基于我们同事米歇尔·克雷格[1]创建的作业。让我们先看看这两段书籍摘录：
- en: '*Excerpt 1*—I have not yet described to you the most singular part. About six
    years ago—to be exact, upon the 4^(th) of May 1882—an advertisement appeared in
    the Times asking for the address of Miss Mary Morstan and stating that it would
    be to her advantage to come forward. There was no name or address appended. I
    had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess. By her advice I published my address in the advertisement column.
    The same day there arrived through the post a small card-board box addressed to
    me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed. Since then, every year upon the same date there has always appeared
    a similar box, containing a similar pearl, without any clue as to the sender.
    They have been pronounced by an expert to be of a rare variety and of considerable
    value. You can see for yourselves that they are very handsome.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摘录1*——我还没有向你描述最独特的一部分。大约六年前——确切地说，是在1882年5月4日——泰晤士报上出现了一则广告，要求提供玛丽·莫斯坦小姐的地址，并表示她出面将对她有利。广告中没有附上姓名或地址。当时我刚刚以家庭教师身份加入了塞西尔·福雷斯特夫人的家庭。在她的建议下，我在广告栏中发布了我的地址。同一天，通过邮局寄来一个小纸板箱，上面写着我收，我发现里面装有一颗非常大的、光泽的珍珠。里面没有附上任何文字。从那时起，每年的同一天，总会出现类似的箱子，里面装着类似的珍珠，没有任何关于发送者的线索。专家们认为这些珍珠是罕见品种，价值不菲。你们自己可以看到，它们非常漂亮。'
- en: '*Excerpt 2*—It was the Dover Road that lay on a Friday night late in November,
    before the first of the persons with whom this history has business. The Dover
    Road lay, as to him, beyond the Dover mail, as it lumbered up Shooter’s Hill.
    He walked up hill in the mire by the side of the mail, as the rest of the passengers
    did; not because they had the least relish for walking exercise, under the circumstances,
    but because the hill, and the harness, and the mud, and the mail, were all so
    heavy, that the horses had three times already come to a stop, besides once drawing
    the coach across the road, with the mutinous intent of taking it back to Blackheath.
    Reins and whip and coachman and guard, however, in combination, had read that
    article of war which forbade a purpose otherwise strongly in favour of the argument,
    that some brute animals are endued with Reason; and the team had capitulated and
    returned to their duty.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*摘录2*——那是11月底的一个周五夜晚，多佛路就在与这个历史有关的第一批人之前。对于他来说，多佛路位于多佛邮件之外，因为邮件车笨拙地爬上射手山。他和其他乘客一样沿着邮件车旁边的泥泞小路上山；并不是因为他们在这种情况下对步行锻炼有丝毫的兴趣，而是因为山丘、马具、泥泞和邮件车都如此沉重，以至于马匹已经三次停下来，而且一次还把马车拉到路上，意图把它拉回布莱克希思。然而，缰绳、鞭子、车夫和守卫联合起来，读到了那条禁止有强烈支持论点的文章，即某些动物被赋予了理性；于是马队屈服了，并返回了它们的职责。'
- en: Suppose we asked you whether these two excerpts were likely written by the same
    author. One reasonable assumption you might make is that different authors write
    differently, and that these differences would show up in metrics that we could
    calculate about their texts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们问你这两段摘录是否可能是由同一作者写的。你可能做出的一个合理假设是，不同的作者有不同的写作风格，这些差异会在我们可以计算其文本的指标中体现出来。
- en: For example, whoever wrote the first excerpt seems to use quite a few short
    sentences in terms of number of words compared to the second excerpt. We find
    short sentences like “There was no name or address appended” and “No word of writing
    was enclosed” in the first excerpt; those kinds of sentences are absent from the
    second. Similarly, the sentences from the first excerpt seem to be less complex
    than those in the second; look at all of those commas and semicolons in the second
    excerpt.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，与第二段摘录相比，似乎撰写第一段摘录的人使用了相当多的短句。我们在第一段摘录中发现了像“没有附上名字或地址”和“没有附上任何文字”这样的短句；这些句子在第二段中不存在。同样，第一段摘录中的句子似乎比第二段中的句子简单；看看第二段中所有的逗号和分号。
- en: This analysis may lead you to believe that these texts are written by different
    authors, and, indeed, they are. The first is written by Sir Arthur Conan Doyle,
    and the second by Charles Dickens.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分析可能会让你认为这些文本是由不同的作者写的，确实如此。第一段是由亚瑟·柯南·道尔爵士写的，第二段是由查尔斯·狄更斯写的。
- en: To be fair, we’ve absolutely cherry-picked these two excerpts. Doyle does use
    some long, complex sentences. Dickens does use some short ones. But, on average,
    at least for the two books that we took these excerpts from, Doyle does write
    shorter sentences than Dickens. More generally, if we look at two books written
    by different authors, we might expect to find some quantifiable differences on
    average.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，我们绝对是有选择性地挑选了这两段摘录。道尔确实使用了一些长而复杂的句子。狄更斯确实使用了一些简短的句子。但是，至少对于我们从中摘录这两段摘录的两本书来说，道尔的句子平均来说比狄更斯的短。更普遍地说，如果我们比较两位不同作者所写的两本书，我们可能会期望在平均意义上找到一些可以量化的差异。
- en: Suppose that we have a bunch of books whose authors we know. We have one written
    by Doyle, one written by Dickens, and so on. Then, along comes a mystery book.
    Oh no! We don’t know who wrote it! Is it a lost Sherlock Holmes story from Doyle?
    A lost *Oliver Twist* sequel from Dickens? We want to find out who that unknown
    author is, and to do that, we’ll turn to a basic AI technique.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一系列我们知道作者的书。我们有一本道尔写的，一本狄更斯写的，等等。然后，出现了一本神秘的书。哦不！我们不知道是谁写的！这是道尔的失落《福尔摩斯探案故事》？狄更斯的失落《雾都孤儿》续集？我们想知道这位未知作者是谁，为了做到这一点，我们将转向一种基本的AI技术。
- en: Our strategy will be to come up with a *signature* for each author, using one
    of the books we know they wrote. We’ll refer to these signatures as *known signatures*.
    Each of these signatures will capture metrics about the book text, such as the
    average number of words per sentence and the average sentence complexity. Then,
    we’ll come up with a signature for the mystery book with an unknown author. We’ll
    call this the *unknown signature*. We’ll look through all the known signatures,
    comparing each one to our unknown signature. We’ll use whichever is the closest
    as our guess for the unknown author.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略将是为每位作者找到一个 *签名*，使用我们知道他们写过的其中一本书。我们将把这些签名称为 *已知签名*。每个这样的签名都将捕捉关于书籍文本的指标，例如每句话的平均单词数和平均句子复杂度。然后，我们将为未知作者的神秘书籍找到一个签名。我们将称之为
    *未知签名*。我们将查看所有已知签名，将每个签名与我们的未知签名进行比较。我们将使用最接近的一个作为我们对未知作者的猜测。
- en: Of course, we have no idea if the unknown author is really one of the authors
    whose signatures we have. It could be a completely new author, for example. Even
    if the unknown author *is* one of the authors whose signature we have, we still
    might end up guessing wrong. After all, maybe the same author writes books in
    different styles (giving their books very different signatures), or we simply
    fail to capture what is most salient about how each of our authors writes. Indeed,
    we’re not after an industry-strength author identification program in this chapter.
    Still, considering the difficulty of this task, we think it’s impressive how well
    the approach that we’ll show you here works.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们并不知道这个未知的作者是否真的是我们拥有的签名作者之一。它可能是一个全新的作者，例如。即使这个未知的作者*确实是*我们拥有的签名作者之一，我们仍然可能猜错。毕竟，也许同一个作者以不同的风格写作书籍（给他们的书籍带来非常不同的签名），或者我们未能捕捉到我们每位作者写作中最显著的特点。实际上，在本章中，我们并不是追求一个行业级的作者识别程序。尽管如此，考虑到这个任务的难度，我们认为我们将向您展示的方法效果非常好。
- en: Machine learning
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 机器学习
- en: Authorship identification, as we’re doing here, is a *machine learning*(ML)
    task. ML is a branch of AI designed to help computers “learn” from data in order
    to make predictions. There are various forms of ML; the one we’re using here is
    called supervised learning. In supervised learning, we have access to training
    data, which consists of objects and their known categories (or labels). In our
    case, our objects are book texts, and the category for each book is the author
    who wrote it. We can train (i.e., learn) on the training set by calculating features—average
    number of words per sentence, average sentence complexity, and so on—for each
    book. Later, when we’re provided a book whose author we don’t know, we can use
    what we learned in the training to make our prediction (or guess).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里所做的那样，作者识别是一个 *机器学习*(ML) 任务。机器学习是人工智能的一个分支，旨在帮助计算机“学习”数据以便做出预测。机器学习有多种形式；我们在这里使用的是监督学习。在监督学习中，我们可以访问训练数据，它由对象及其已知的类别（或标签）组成。在我们的案例中，我们的对象是书籍文本，每本书的类别是写这本书的作者。我们可以通过计算每本书的特征（例如，每句话的平均单词数、平均句子复杂度等）来在训练集上训练（即学习）。后来，当我们得到一本我们不知道作者的书时，我们可以使用我们在训练中学到的知识来做出预测（或猜测）。
- en: 11.2 Authorship identification using top-down design
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 使用自顶向下设计进行作者识别
- en: Alright, we want to “write a program to determine the author of a book.” This
    seems like a daunting task, and it would be if we were trying to do this in one
    shot, using a single function. But just like in our Spelling Suggestions example
    in chapter 7, we’re not going to do that. We’re going to systematically break
    this problem down into subproblems that we can solve.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们想要“编写一个程序来确定一本书的作者。”这似乎是一个艰巨的任务，如果我们试图一次性完成，使用一个单独的函数，确实会是这样。但就像在第7章中我们的拼写建议示例中做的那样，我们不会这么做。我们将系统地把这个问题分解成我们可以解决的子问题。
- en: 'In chapter 7, we solved the Spelling Suggestions problem by using the model
    of reading input, processing that input, and producing an output result. We can
    think about our authorship identification program as following that model as well:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们通过读取输入、处理输入并产生输出结果的方式来解决拼写建议问题。我们可以把我们的作者识别程序看作也是遵循这个模型：
- en: '*Input step* —For the input step, we need to ask the user for the filename
    of the mystery book.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入步骤* — 对于输入步骤，我们需要询问用户神秘书籍的文件名。'
- en: '*Process step* —For the process step, we need to figure out the signature for
    the mystery book (that’s the unknown signature), as well as the signature for
    each of the books whose author we know (those are the known signatures). Creating
    the signature for each book is commonly called the training phase in ML. We also
    need to compare the unknown signature to each known signature to figure out which
    known signature is closest. These comparisons are the prediction phase in ML.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*过程步骤* — 对于过程步骤，我们需要确定神秘书籍的签名（这是未知签名），以及我们知道作者每本书的签名（这些是已知签名）。为每本书创建签名通常被称为机器学习中的训练阶段。我们还需要将未知签名与每个已知签名进行比较，以确定哪个已知签名最接近。这些比较是机器学习中的预测阶段。'
- en: '*Output step* —For the output step, we need to report to the user the unknown
    signature that’s closest to the known signature.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出步骤* — 对于输出步骤，我们需要向用户报告最接近已知签名的未知签名。'
- en: That is, to solve our overall authorship identification problem, we need to
    solve these three subproblems. We’re starting our top-down design!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，为了解决我们的整体作者识别问题，我们需要解决这三个子问题。我们开始自顶向下的设计！
- en: We’ll name our top-level function `make_guess`. In it, we’ll solve each of the
    three subproblems we identified.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将顶层函数命名为`make_guess`。在其中，我们将解决我们确定的三个子问题。
- en: 'For the input step, we’re simply asking the user for a filename. That’s something
    we can do in a small number of lines of code, so we probably don’t need a separate
    function for that. The output step seems similar: assuming that we already know
    which known signature is closest, we can just report that to the user. By contrast,
    the process step looks like a lot of work, and we’ll certainly want to break that
    subproblem down further. That’s what we’ll do next.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入步骤，我们只是要求用户输入一个文件名。这可以通过少量代码完成，所以我们可能不需要为这个步骤创建一个单独的函数。输出步骤看起来类似：假设我们已经知道哪个已知签名最接近，我们可以直接向用户报告。相比之下，过程步骤看起来需要做很多工作，我们当然希望进一步分解这个子问题。这就是我们接下来要做的。
- en: 11.3 Breaking down the process subproblem
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 分解过程子问题
- en: We’ll name our overall process function `process_data`. It will take the mystery
    book filename and the name of a directory of known-author books as parameters
    and return the name of the closest known signature.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将整体过程函数命名为`process_data`。它将接受神秘书籍的文件名和已知作者书籍目录的名称作为参数，并返回最接近的已知签名的名称。
- en: 'Looking at our description for the process step, it seems that we have three
    subproblems to solve here:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们对该过程步骤的描述，似乎我们在这里有三个子问题需要解决：
- en: '*Figure out the signature for the mystery book.* That’s our unknown signature.
    We’ll name this function `make_signature`.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定神秘书籍的签名*。这是我们未知的签名。我们将这个函数命名为`make_signature`。'
- en: '*Figure out the signature for each of the books whose author we know.* These
    are our known signatures. We’ll name this function `get_all_signatures`.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*确定已知作者每本书的签名*。这些是我们的已知签名。我们将这个函数命名为`get_all_signatures`。'
- en: '*Compare the unknown signature to each known signature to figure out which
    known signature is closest.* Because close signatures will have small differences,
    we’ll name this function `lowest_score`.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将未知签名与每个已知签名进行比较，以确定哪个已知签名最接近*。因为接近的签名会有小的差异，我们将这个函数命名为`lowest_score`。'
- en: We’ll work on our top-down design for each of these subproblems in turn. Figure
    11.1 shows a diagram of what we have so far.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将依次为这些子问题制定自顶向下的设计方案。图11.1展示了我们目前的设计图。
- en: '![figure](../Images/11-1.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-1.png)'
- en: Figure 11.1 Functions diagram with the three subtasks of `process_data`
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.1 `process_data`的三个子任务函数图
- en: 11.3.1 Figuring out the signature for the mystery book
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3.1 确定神秘书籍的签名
- en: The function for this task, `make_signature`, will take the text for our book
    as a parameter and return the book’s signature. At this point, we need to decide
    on the features that we’ll use to determine the signature for each book. Let’s
    break this down by thinking back to the previous example passages. We noticed
    there are differences in the authors’ passages based on the complexity and length
    of sentences. You may have also suspected that the authors may differ in the length
    of words used and ways they use words (e.g., some authors may be more repetitive
    than others). As such, we’ll want some features to be based on the structure of
    the author’s sentences, and we’ll want others based on the words that the author
    uses. We’ll look at each of these in detail.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`make_signature`将接受我们书籍的文本作为参数，并返回书籍的签名。在这个阶段，我们需要决定我们将使用哪些特征来确定每本书的签名。让我们通过回顾之前的例子段落来分析这个问题。我们注意到，根据句子的复杂性和长度，作者之间的段落存在差异。你可能也怀疑过，作者在使用的单词长度和用词方式上可能存在差异（例如，一些作者可能比其他作者更重复）。因此，我们希望有一些特征基于作者句子的结构，而其他特征基于作者使用的单词。我们将详细探讨这些特征。
- en: Features related to the structure of the author’s sentences
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与作者句子结构相关的特征
- en: 'In our earlier Doyle versus Dickens example, we talked about using the average
    number of words per sentence as one feature. We can calculate this by dividing
    the total number of words by the total number of sentences. For example, consider
    the following text:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的道尔对狄更斯例子中，我们讨论了使用每句话的平均单词数作为一个特征。我们可以通过将总单词数除以总句子数来计算这个值。例如，考虑以下文本：
- en: The same day there arrived through the post a small card-board box addressed
    to me, which I found to contain a very large and lustrous pearl. No word of writing
    was enclosed.
  id: totrans-42
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 就在同一天，通过邮局寄来一个小纸板箱，上面写着我的名字，我发现里面有一个非常巨大而光亮的珍珠。没有附上任何字条。
- en: If you count the words and sentences, you should find that there are 32 words
    (card-board counts as one word) and two sentences, so we’ll calculate the average
    words per sentence as 32/2 = 16\. This will be the*average number of words per
    sentence*feature.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计算单词和句子，你应该会发现有32个单词（纸板算作一个单词）和两个句子，因此我们将计算每句话的平均单词数为32/2 = 16。这将是我们所说的*每句话的平均单词数*特征。
- en: 'We also noticed that the complexity of sentences may vary between authors (i.e.,
    some authors have sentences with many more commas and semicolons compared to other
    authors), so it makes sense to use that as another feature. More complex sentences
    have more phrases, which are coherent pieces of sentences. Breaking a sentence
    into its component phrases is a tough challenge in its own right, and although
    we could try to do it more accurately, we’ll settle here for a simpler rule of
    thumb. Namely, we’ll say that a phrase is separated from other phrases in the
    sentence by a comma, semicolon, or colon. Looking at the previous text again,
    we find that there are three phrases. The first sentence has two phrases: “The
    same day there arrived through the post a small card-board box addressed to me”
    and “which I found to contain a very large and lustrous pearl.” The second sentence
    has no commas, semicolons, or colons, so it has only one phrase. As there are
    three phrases and two sentences, we’d say that the sentence complexity for this
    text is 3/2 = 1.5\. This will be the *average sentence complexity* feature.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还注意到，句子的复杂度可能在作者之间有所不同（即，一些作者的句子比其他作者有更多的逗号和分号），因此将其作为另一个特征是有意义的。更复杂的句子有更多的短语，这些短语是连贯的句子片段。将一个句子分解为其组成部分短语是一项艰巨的挑战，尽管我们可以尝试做得更准确，但在这里我们将采用一个更简单的经验法则。也就是说，我们将说一个短语是通过逗号、分号或冒号与其他短语分开的。再次查看之前的文本，我们发现有三个短语。第一个句子有两个短语：“The
    same day there arrived through the post a small card-board box addressed to me”和“which
    I found to contain a very large and lustrous pearl.”第二个句子没有逗号、分号或冒号，所以只有一个短语。由于有三个短语和两个句子，我们会说这个文本的句子复杂度为3/2
    = 1.5。这将是我们所说的*平均句子复杂度*特征。
- en: We hope that these sentence-level features intuitively make sense as things
    we could use to differentiate how authors write. Next, let’s start looking at
    the ways that authors may differ in their use of words.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这些句子级别的特征直观上是有意义的，可以作为区分作者写作风格的东西。接下来，让我们开始探讨作者在用词上可能存在的差异。
- en: Features related to the author’s word selection
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与作者词汇选择相关的特征
- en: 'You can probably think of your own metrics for word-level features, but we’ll
    use three here that in our experience work well. First, it’s likely that some
    authors use shorter words on average than other authors. To that end, we’ll use
    the average word length, which is just the average number of letters per word.
    Let’s consider this sample text that we created:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能可以想到自己的单词级特征指标，但我们将使用三个在我们经验中效果很好的指标。首先，可能有些作者的平均单词长度比其他作者短。为此，我们将使用平均单词长度，这仅仅是每个单词的平均字母数。让我们考虑我们创建的这个样本文本：
- en: A pearl! Pearl! Lustrous pearl! Rare. What a nice find.
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一颗珍珠！珍珠！光泽的珍珠！稀有。多么好的发现。
- en: If you count the letters and words, you should find that there are 41 letters
    and 10 words. (Don’t count punctuation as letters here.) So, we’ll calculate the
    average word length as 41/10 = 4.1\. This will be the *average word length* feature.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你数一下字母和单词，你应该会发现有41个字母和10个单词。（在这里不要把标点符号算作字母。）所以，我们将计算平均单词长度为41/10 = 4.1。这将是*平均单词长度*的特征。
- en: 'Second, it may be that some authors use words more repetitively than others.
    To capture this, we’ll take the number of different words that the author uses
    and divide it by the total number of words. For our previous sample text, there
    are only seven different words used: *a*, *pearl*, *lustrous*, *rare*, *what*,
    *nice*, and *find*. There are 10 words in all, so our calculation for this metric
    would be 7/10 = 0.7\. This will be the*different words divided by total words*feature.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，可能有些作者比其他作者更频繁地使用相同的单词。为了捕捉这一点，我们将使用作者使用的不同单词的数量除以总单词数。对于我们之前的样本文本，只有七个不同的单词被使用：*一个*，*珍珠*，*光泽的*，*稀有的*，*什么*，*好的*，和*找到的*。总共有十个单词，所以这个指标的计算结果是7/10
    = 0.7。这将是*不同单词除以总单词数*的特征。
- en: 'Third, it may be that some authors tend to use many words a single time, whereas
    other authors tend to use words multiple times. To calculate this one, we’ll take
    the number of words used exactly once and divide it by the total number of words.
    For our sample text, there are five words that are used exactly once: *lustrous*,
    *rare*, *what*, *nice*, and *find*. There are 10 words in all, so our calculation
    for this metric would be 5/10 = 0.5\. This will be the *number of words used exactly
    once divided by total words*feature.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，可能有些作者倾向于一次性使用很多单词，而其他作者则倾向于多次使用相同的单词。为了计算这个指标，我们将使用一次的单词数量除以总单词数。对于我们的样本文本，有五个单词是只使用了一次：*光泽的*，*稀有的*，*什么*，*好的*，和*找到的*。总共有十个单词，所以这个指标的计算结果是5/10
    = 0.5。这将是*使用一次的单词数除以总单词数*的特征。
- en: In all, we have five features that will make up each signature. We’ll need to
    store those numbers together in a single value, so we’ll end up using a list of
    five numbers for each signature.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，我们有五个特征将构成每个签名。我们需要将这些数字存储在一个单独的值中，所以我们将为每个签名使用五个数字的列表。
- en: 'Let’s dig into how we’ll implement each of these features, starting with the
    word-level ones and proceeding to the sentence-level ones. We’ll go in this order:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解我们将如何实现每个特征，从单词级特征开始，然后到句子级特征。我们将按以下顺序进行：
- en: Average word length
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均单词长度
- en: Different words divided by total words
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同单词除以总单词数
- en: Words used exactly once divided by total words
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一次的单词数除以总单词数
- en: Average number of words per sentence
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每句平均单词数
- en: Average sentence complexity
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均句子复杂度
- en: For each one, we’ll ultimately end up writing a function. We have an updated
    diagram with function names for each of these five new functions that will help
    us implement `make_signature` in figure 11.2\. Do we need to further break down
    these problems, or are they OK as is? Let’s see!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每一个，我们最终将编写一个函数。我们有一个更新后的图，其中包含每个这五个新函数的函数名，这将帮助我们实现图11.2中的`make_signature`。我们需要进一步分解这些问题，还是它们就这样可以？让我们看看！
- en: '![figure](../Images/11-2.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-2.png)'
- en: Figure 11.2 Functions diagram with the additional five subtasks of `make_signature`
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.2 函数图，包含`make_signature`的额外五个子任务
- en: Average word length
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均单词长度
- en: The function for this task, `average_word_length`, will take the text of the
    book as a parameter and return the average word length. We might start solving
    this task by using the split method on the text. As a reminder, the `split` method
    is used to split a string into a list of its pieces. By default, `split` will
    split around spaces. The book text is a string, and if we split around spaces,
    we’ll get its words! That’s exactly what we need here. We can then loop through
    that list of words, counting up the number of letters and number of words.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 用于此任务的函数`average_word_length`将接受书籍的文本作为参数，并返回平均单词长度。我们可能通过在文本上使用split方法来开始解决这个问题。提醒一下，split方法用于将字符串分割成其各个片段的列表。默认情况下，split会在空格周围分割。书籍文本是一个字符串，如果我们围绕空格分割，我们就会得到其单词！这正是我们需要的。然后我们可以遍历这个单词列表，计算字母数和单词数。
- en: That’s a good start, but we need to be a little more careful here because we
    don’t want to end up counting nonletters as letters. For example, “pearl” has
    five letters. But so does “pearl.” or “pearl!!” or “(pearl)”. Aha—this sounds
    like a subtask to us! Namely, we can divide out the subtask of cleaning up a word
    into its own function to be used by `average_word_length`. We’ll call that cleanup
    function `clean_word`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是个不错的开始，但我们需要更加小心，因为我们不希望最终将非字母字符计为字母。例如，“pearl”有五个字母。但“pearl.”、“pearl!!”或“(pearl)”也是如此。啊哈——这听起来像是我们的一项子任务！具体来说，我们可以将清理单词的子任务划分为一个单独的函数，用于`average_word_length`。我们将称这个清理函数为`clean_word`。
- en: There’s another benefit to having our `clean_word` function, and that’s to help
    us identify when a “word” is actually not a word. For example, suppose one of
    our “words” in the text is . . . . When we pass this to `clean_word`, we’ll get
    an empty string back. That signifies that this in fact isn’t a word at all, so
    we won’t count it as such.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`clean_word`函数还有一个好处，那就是帮助我们识别一个“单词”实际上不是一个单词的情况。例如，假设我们的文本中的一个“单词”是……当我们将其传递给`clean_word`时，我们会得到一个空字符串。这表示这实际上根本不是一个单词，所以我们不会将其计为一个单词。
- en: Different words divided by total words
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同单词与总单词的比例
- en: The function for this task, `different_to_total`, will take the text of the
    book as a parameter and will return the number of different words used divided
    by the total number of words.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 用于此任务的函数`different_to_total`将接受书籍的文本作为参数，并将不同单词的数量除以总单词数。
- en: As with `average_word_length`, we need to be careful to count only letters,
    not punctuation. But wait—we just talked about a `clean_word` function that we
    needed for `average_word_length`. We can use that function here as well! In fact,
    we’ll use `clean_ word` in most of our five feature tasks. This is the sign of
    a useful general-purpose function! Our top-down design is going well. We can see
    how the `clean_word` function will be called by both functions in our updated
    function diagram in figure 11.3.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 与`average_word_length`一样，我们需要小心只计算字母，而不是标点符号。但是等等——我们刚刚讨论了`clean_word`函数，我们需要的`average_word_length`。我们也可以在这里使用该函数！实际上，我们将在我们五个特征任务中的大多数任务中使用`clean_word`。这是有用的一般用途函数的标志！我们的自顶向下的设计进展顺利。我们可以看到`clean_word`函数如何在图11.3中更新的功能图中被两个函数调用。
- en: '![figure](../Images/11-3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-3.png)'
- en: Figure 11.3 Functions diagram with two functions, both using our `clean_word`
    function to help
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.3包含两个函数的功能图，这两个函数都使用了我们的`clean_word`函数来帮助
- en: There’s one extra complication here, though, and it involves words like *pearl*,
    *Pearl*, and *PEARL*. We want to consider those to be the same words, but if we
    simply use string comparisons, they will be treated as different words. One solution
    here is to split this off into another subproblem to convert a string to all lowercase.
    We could also think of this as another part of cleaning up a word, right along
    with removing the punctuation. We’ll go with this second option. What we’ll do,
    then, is make our `clean_word` function not only remove punctuation but also convert
    the word to lowercase.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个额外的复杂性，涉及到像*pearl*、*Pearl*和*PEARL*这样的单词。我们希望将这些视为相同的单词，但如果我们简单地使用字符串比较，它们将被视为不同的单词。这里的解决方案之一是将这个子问题分离出来，将其转换为全小写的字符串。我们也可以将其视为清理单词的另一个部分，与去除标点符号一样。我们将选择第二个选项。那么，我们将使我们的`clean_word`函数不仅去除标点符号，还将单词转换为小写。
- en: You might wonder whether we need to split off another subtask here, one that
    determines the number of different words. You could do that, and it wouldn’t be
    a mistake to do so. However, if we persevere without doing that, we’ll see that
    the function remains quite manageable without this additional subtask. Practice
    and experience over time will help you anticipate when a task needs to be further
    broken down.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道我们是否需要在这里拆分另一个子任务，一个确定不同单词数量的子任务。你可以这样做，这样做并不错误。然而，如果我们坚持不这样做，我们会发现函数在没有这个额外子任务的情况下仍然相当易于管理。随着时间的推移，实践和经验将帮助你预测何时需要进一步分解任务。
- en: Words used exactly once divided by total words
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一次使用的单词数除以总单词数
- en: 'The function for this task, `exactly_once_to_total`, will take the text of
    the book as a parameter and will return the number of words used exactly once
    divided by the total number of words. We’re going to need our `clean_word` function
    here as well for reasons similar to why we needed it in the prior two tasks: to
    make sure we’re working only with letters, not punctuation. Again, while we could
    split out a subtask to determine the number of words that are used exactly once,
    we’ll find that it doesn’t take much Python code to do this, so we’ll just leave
    this task alone without splitting it further.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`exactly_once_to_total`将接受书籍的文本作为参数，并返回一次使用的单词数除以总单词数。我们还需要在这里使用`clean_word`函数，原因与我们在前两个任务中需要它的原因类似：确保我们只处理字母，而不是标点符号。同样，虽然我们可以拆分一个子任务来确定一次使用的单词数量，但我们会发现用Python代码做这件事并不需要太多，所以我们不会进一步拆分这个任务。
- en: Average number of words per sentence
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每句的平均单词数
- en: The function for this task, `average_sentence_length`, will take the text of
    the book as a parameter and will return the average number of words per sentence.
    To split our text into words for the previous three tasks, we can use the string
    split method. How do we split our text into sentences? Is there a string method
    for that?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务的函数`average_sentence_length`将接受书籍的文本作为参数，并返回每句的平均单词数。为了将我们的文本拆分为单词以进行前三个任务，我们可以使用字符串拆分方法。我们如何将文本拆分为句子？有没有一个字符串方法可以做到这一点？
- en: Unfortunately, there isn’t. For that reason, it will be helpful to split out
    a task to break our text string into sentences. We’ll call the function for that
    subtask `get_ sentences`. The `get_sentences` function will take the text of the
    book as a parameter and will return a list of sentences from the text.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，没有。因此，将任务拆分为将文本字符串拆分成句子的任务将是有帮助的。我们将为这个子任务命名函数为`get_sentences`。`get_sentences`函数将接受书籍的文本作为参数，并从文本中返回一个句子列表。
- en: What’s a sentence? We’ll define a sentence as text that is separated by a period
    (.), question mark (?), or exclamation point (!). This rule, while convenient
    and simple, is going to make mistakes. For example, how many sentences are in
    this text?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是句子？我们将句子定义为由句号（.）、问号（?）或感叹号（!）分隔的文本。这个规则虽然方便且简单，但会犯错误。例如，这个文本中有多少个句子？
- en: I had at that time just entered the family of Mrs. Cecil Forrester in the capacity
    of governess.
  id: totrans-79
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在那时，我只是以家庭教师的身份加入了塞西尔·福雷斯特夫人的家庭。
- en: The answer is one. Our program, though, is going to pull out two sentences,
    not one. It’ll get tricked by the word *Mrs.*, which has a period at the end.
    If you continue with authorship identification past this chapter, you could work
    on making your rules more robust or, use sophisticated natural language processing
    (NLP) software to do even better. For our purposes, however, we’ll be content
    with this rule that sometimes gets sentences wrong because most of the time we’ll
    get them right. If we’re only wrong once in a while, the errors won’t have an
    appreciable effect on our metric.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是一。然而，我们的程序将提取两个句子，而不是一个。它会被单词*Mrs.*欺骗，该单词在结尾处有一个句号。如果你继续在本章之后进行作者识别，你可以尝试使你的规则更加健壮，或者使用复杂的高级自然语言处理（NLP）软件来做得更好。然而，对于我们来说，我们将满足于这个有时会出错但大多数时候会正确的规则。如果我们偶尔出错，这些错误对我们的指标的影响将不会很大。
- en: Average sentence complexity
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每句的平均复杂度
- en: We’ll name the function for this task `average_sentence_complexity`. It will
    take the text of a sentence as a parameter and return a measure of the sentence
    complexity.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个任务命名的函数是`average_sentence_complexity`。它将接受句子的文本作为参数，并返回句子复杂度的度量。
- en: As we discussed previously, we’re interested in quantifying sentence complexity
    using the number of phrases in a sentence. Much as we used punctuation to separate
    sentences from each other, we’ll use different punctuation to separate phrases
    from each other. Namely, we’ll say that a phrase is separated by a comma (,),
    semicolon (;), or colon (:).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，我们感兴趣的是使用句子中的短语数量来量化句子复杂性。就像我们使用标点符号来分隔句子一样，我们将使用不同的标点符号来分隔短语。具体来说，我们将说短语是由逗号（,）、分号（;）或冒号（:）分隔的。
- en: It would be nice to have a subtask to break a sentence into its phrases, just
    like we had a subtask to break text into its sentences. Let’s make that happen!
    We’ll call the function for that subtask `get_phrases`. The `get_phrases` function
    will take a sentence of the book as a parameter and return a list of phrases from
    the sentence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个子任务来将句子分解成短语，就像我们有一个子任务来将文本分解成句子一样，那会很棒！我们将把这个子任务命名为 `get_phrases`。`get_phrases`
    函数将接受一本书的句子作为参数，并返回句子中的短语列表。
- en: Let’s pause for a moment and think about what we’re doing with our `get_sentences`
    and `get_phrases` functions. They’re both quite similar, come to think of it.
    All that distinguishes them is the characters that they use to make the splits.
    `get_sentences` cares about periods, question marks, and exclamation points, whereas
    `get_phrases` cares about commas, semicolons, and colons. We see an opportunity
    for a parent task that would simplify both of these tasks!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，思考一下我们正在用 `get_sentences` 和 `get_phrases` 函数做什么。回想起来，它们非常相似。它们之间的区别仅在于它们用来进行分割的字符。`get_sentences`
    关注的是句号、问号和感叹号，而 `get_phrases` 关注的是逗号、分号和冒号。我们看到一个机会，可以创建一个父任务来简化这两个任务！
- en: Namely, imagine that we had a `split_string` function that took two parameters,
    the text and a string of separator characters, and it returned a list of pieces
    of text separated by any of the separators. We could then call it with `'.?!'`
    to split into sentences and `',;:'` to split into phrases. That would make both
    `get_sentences` and `get_phrases` easier to implement and reduce code duplication.
    This is a win!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，想象一下我们有一个名为 `split_string` 的函数，它接受两个参数，即文本和分隔符字符的字符串，并返回由任何分隔符分隔的文本片段列表。然后我们可以用
    `'.?!'` 来分割成句子，用 `',;:'` 来分割成短语。这将使 `get_sentences` 和 `get_phrases` 的实现更加容易，并减少代码重复。这是一个胜利！
- en: At this point, we’ve fully fleshed out the functions necessary to support the
    higher-level function `make_signature`, as reflected in figure 11.4\. We’ll next
    turn to the `get_all_signatures` function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经完全实现了支持高级函数 `make_signature` 所需的所有函数，如图 11.4 所示。接下来，我们将转向 `get_all_signatures`
    函数。
- en: '![figure](../Images/11-4.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-4.png)'
- en: Figure 11.4 Functions diagram with all the supporting functions for the `make_signature`
    function complete
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.4 函数图，其中包含 `make_signature` 函数的所有支持函数已全部完成
- en: Figuring out each known signature
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 确定每个已知签名
- en: We just worked hard to break down our `make_signature` function into five main
    tasks, one for each feature of our signatures. We designed that function to determine
    the unknown signature—the signature for the mystery text whose author we’re trying
    to identify.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚努力将 `make_signature` 函数分解为五个主要任务，每个任务对应于我们签名的一个特性。我们设计该函数是为了确定未知签名——即试图识别的神秘文本的签名。
- en: Our next task is to figure out the signature for each of the books for which
    we know the author. In the resources for this book, under the ch11 folder, you’ll
    find a directory called `known_authors`. In there, you’ll find several files,
    each named as an author. Each file contains a book written by that author. For
    example, if you open Arthur_Conan_Doyle.txt, you’ll find the text of the book
    *A Study in Scarlet* by Arthur Conan Doyle. We need to determine the signature
    for each of these files.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来的任务是确定我们已知作者的所有书籍的签名。在这本书的资源中，在 ch11 文件夹下，你会找到一个名为 `known_authors` 的目录。在那里，你会找到几个文件，每个文件都按作者命名。每个文件都包含那位作者所写的书籍。例如，如果你打开
    Arthur_Conan_Doyle.txt，你会找到亚瑟·柯南·道尔的书籍《血字的研究》的文本。我们需要确定这些文件中每个文件的签名。
- en: Amazingly, we have far less work to do to solve this problem than it may seem.
    That’s because we can use that same `make_signature` function, the one we designed
    to determine the signature of the mystery book, to also determine the signature
    for any known book!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，我们解决这个问题的实际工作量远小于表面上看起来那么多。这是因为我们可以使用那个相同的 `make_signature` 函数，即我们为确定神秘书籍的签名而设计的函数，来同样确定任何已知书籍的签名！
- en: We’ll name the function for this task `get_all_signatures`. It wouldn’t make
    sense for this function to take the text of one book as a parameter because it’s
    supposed to be able to get the signature for all of our known books. Rather, it
    will take a directory of known books as a parameter. Its behavior will be to loop
    through the files in that directory, calculating the signature for each one.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个任务的函数命名为`get_all_signatures`。这个函数接受一本书的文本作为参数是没有意义的，因为它应该能够获取我们所有已知书籍的签名。相反，它将接受一个已知书籍的目录作为参数。它的行为将是遍历该目录中的文件，计算每个文件的签名。
- en: We need the function to tell us which signature goes with which book. In other
    words, we need it to associate each book with its corresponding signature. This
    kind of association is precisely why Python has dictionaries! We’ll therefore
    have this function return a dictionary, where the keys are names of files, and
    the values are the corresponding signature. Our function diagram didn’t need any
    *new* functions to support the `get_all_signatures` function, so our updated diagram
    in figure 11.5 just shows how `get_all_signatures` calls `make_signature`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来告诉我们哪个签名对应哪本书。换句话说，我们需要它将每本书与其对应的签名关联起来。这种关联正是Python有字典的原因！因此，我们将让这个函数返回一个字典，其中键是文件名，值是对应的签名。我们的函数图不需要任何新的函数来支持`get_all_signatures`函数，所以图11.5中更新的图只显示了`get_all_signatures`如何调用`make_signature`。
- en: '![figure](../Images/11-5.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-5.png)'
- en: Figure 11.5 Functions diagram updated for `get_all_signatures` to call `make_signature`
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.5 `get_all_signatures`调用`make_signature`的函数图更新
- en: Finding closest known signature
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 寻找最接近的已知签名
- en: 'Let’s recap what we’ve designed so far:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下到目前为止我们设计的：
- en: We’ve designed our `make_signature` function to get us the unknown signature
    for the mystery book.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经设计了`make_signature`函数来为我们获取神秘书籍的未知签名。
- en: We’ve designed our `get_all_signatures` function to get us all of our known
    signatures.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经设计了`get_all_signatures`函数来获取我们所有的已知签名。
- en: 'Now, we need to design a function that tells us which of those known signatures
    is best; that is, which known signature is closest to our unknown signature. Each
    of our signatures will be a list of five numbers giving the quantity for each
    of our five features. The order of these numbers will be the same order we used
    before: average word length, different words divided by total words, words used
    exactly once divided by total words, average number of words per sentence, and
    average sentence complexity.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要设计一个函数来告诉我们那些已知的签名中哪一个是最合适的；也就是说，哪一个已知的签名与我们的未知签名最接近。我们的每个签名都将是一个包含五个数字的列表，表示我们五个特征的量。这些数字的顺序将与之前使用的顺序相同：平均单词长度、不同单词数除以总单词数、仅使用一次的单词数除以总单词数、每句话的平均单词数和平均句子复杂性。
- en: Suppose that we have two signatures. The first one is `[4.6,` `0.1,` `0.05,`
    `10,` `2]`which means that the average word length for that book is 4.6, the different
    words divided by total words is 0.1, and so on. The second signature is `[4.3,`
    `0.1,` `0.04,` `16,` `4]`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个签名。第一个是`[4.6, 0.1, 0.05, 10, 2]`，这意味着这本书的平均单词长度是4.6，不同单词数除以总单词数是0.1，等等。第二个签名是`[4.3,
    0.1, 0.04, 16, 4]`。
- en: There are many ways to get an overall score giving the difference between signatures.
    The one we’ll use will give us a difference score for each feature, and then we’ll
    add up those scores to get our overall score.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以得到一个总分，表示签名之间的差异。我们将使用的方法将为每个特征提供一个差异分数，然后我们将把这些分数加起来得到总分。
- en: 'Let’s look at the values of each signature for the first feature: 4.6 and 4.3\.
    If we subtract those, we get a difference of 4.6 – 4.3 = 0.3\. We could use 0.3
    as our answer for this feature, but it turns out to work better if we *weight*
    each difference using a different weight. Each weight gives the importance of
    that feature. We’ll use some weights(`[11,` `33,` `50,` `0.4,` `4]`) that in our
    experience have proven to work well. You might wonder where the heck these weights
    come from. But note that there’s no magic about them: in working with our students
    over the years, we’ve just found that these weights seem to work out. This would
    be only a starting point for a stronger authorship identification program. When
    doing this type of research, people routinely *tune* their training, which means
    to adjust weights to obtain stronger results.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第一个特征每个签名的值：4.6 和 4.3。如果我们减去这些值，我们得到差值 4.6 – 4.3 = 0.3。我们可以用 0.3 作为这个特征的答案，但结果证明如果我们使用不同的权重来*加权*每个差值，效果会更好。每个权重都表示该特征的重要性。我们将使用一些经验证明效果良好的权重（`[11,`
    `33,` `50,` `0.4,` `4]`）。你可能想知道这些权重从何而来。但请注意，它们并没有什么神奇之处：在多年的学生工作中，我们发现这些权重似乎效果不错。这将是更强大的作者识别程序的一个起点。在进行这类研究时，人们通常会*调整*他们的训练，这意味着调整权重以获得更好的结果。
- en: When we say that we’re using weights of `[11,` `33,` `50,` `0.4,` `4]`, it means
    that we’ll multiply the difference on the first feature by 11, the difference
    on the second feature by 33, and so on. So, rather than getting a difference of
    0.3 for the first feature, we’ll get 0.3 × 11 = 3.3.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说我们使用权重 `[11,` `33,` `50,` `0.4,` `4]` 时，这意味着我们将第一个特征的差值乘以 11，第二个特征的差值乘以
    33，依此类推。因此，对于第一个特征，我们不会得到 0.3 的差值，而是得到 0.3 × 11 = 3.3。
- en: We need to be careful with features like the fourth one, where the difference
    is negative. We don’t want to start with 10 – 16 = –6 because that’s a negative
    number, and that would *undo* some of the positive difference from other features.
    Instead, we need to first make this number positive and then multiply it by its
    weight. Removing the negative sign from a number is known as taking the absolute
    value, and the absolute value is denoted as `abs`. The full calculation for this
    fourth feature, then, is abs(10 – 16) × 0.4 = 2.4.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要小心处理第四个这样的特征，其中差值是负数。我们不希望从 10 – 16 = –6 开始，因为这是一个负数，这会*抵消*其他特征的一些正差值。相反，我们首先需要使这个数字为正，然后乘以它的权重。从一个数字中移除负号称为取绝对值，绝对值表示为
    `abs`。因此，这个第四个特征的完整计算是 abs(10 – 16) × 0.4 = 2.4。
- en: Table 11.1 gives the calculation for each feature. If we add up all five scores,
    we get an overall score of 14.2.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 给出了每个特征的计算方法。如果我们把所有五个分数加起来，我们得到一个总分 14.2。
- en: Table 11.1 Calculating the difference between two signatures
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.1 计算两个签名之间的差异
- en: '| Feature Number | Value of Feature in Signature 1 | Value of Feature in Signature
    2 | Weight of Feature | Contribution of Feature |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 特征编号 | 签名 1 中特征值 | 签名 2 中特征值 | 特征权重 | 特征贡献 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1  | 4.6  | 4.3  | 11  | abs(4.6 – 4.3) × 11 = 3.3  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 1  | 4.6  | 4.3  | 11  | abs(4.6 – 4.3) × 11 = 3.3  |'
- en: '| 2  | 0.1  | 0.1  | 33  | abs(0.1 – 0.1) × 33 = 0  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 0.1  | 0.1  | 33  | abs(0.1 – 0.1) × 33 = 0  |'
- en: '| 3  | 0.05  | 0.04  | 50  | abs(0.05 – 0.04) × 50 = .5  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 0.05  | 0.04  | 50  | abs(0.05 – 0.04) × 50 = .5  |'
- en: '| 4  | 10  | 16  | 0.4  | abs(10 – 16) × 0.4 = 2.4  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 10  | 16  | 0.4  | abs(10 – 16) × 0.4 = 2.4  |'
- en: '| 5  | 2  | 4  | 4  | abs(2 – 4) × 4 = 8  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 5  | 2  | 4  | 4  | abs(2 – 4) × 4 = 8  |'
- en: '| Sum  |  |  |  | 14.2  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Sum  |  |  |  | 14.2  |'
- en: 'Remember where we are in the top-down design: we need a function that tells
    us which known signature is best. Well, now we know how to compare two signatures
    and get the score for that comparison. We’ll want to make that comparison between
    the unknown signature and each known signature to determine which known signature
    is best. The lower the score, the closer the signatures; the higher the score,
    the more different the signatures are. As such, we’ll want to ultimately choose
    the signature with the lowest comparison score.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们在自顶向下的设计中的位置：我们需要一个函数来告诉我们哪个已知签名是最好的。现在我们知道如何比较两个签名并得到该比较的分数。我们希望将未知签名与每个已知签名进行比较，以确定哪个已知签名最好。分数越低，签名越接近；分数越高，签名越不同。因此，我们最终会选择比较分数最低的签名。
- en: 'We’ll name the function for this task `lowest_score`. It will take three parameters:
    a dictionary mapping author names to their known signatures, an unknown signature,
    and a list of weights. The function will return the signature that has the lowest
    comparison score with our unknown signature.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为这个任务命名的函数为`lowest_score`。它将接受三个参数：一个将作者名字映射到他们已知签名的字典、一个未知签名和权重列表。该函数将返回与我们的未知签名比较分数最低的签名。
- en: Think about the work that this function will need to do. It needs to loop through
    the known signatures. We can do that with a `for` loop—no need for a subtask there.
    It will need to compare the unknown signature to the current known signature.
    Oh! That’s a subtask right there, embodying the scoring mechanism that we outlined
    in table 11.1\. We’ll name the function for that subtask `get_score`. Our `get_score`
    function will take two signatures to compare and the list of weights and return
    the score for the comparison between these two signatures.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这个函数需要完成的工作。它需要遍历已知的签名。我们可以用`for`循环来完成这个任务——不需要子任务。它需要将未知签名与当前已知签名进行比较。哦！那是一个子任务，体现了我们在表11.1中概述的评分机制。我们将为这个子任务命名的函数为`get_score`。我们的`get_score`函数将接受两个要比较的签名和权重列表，并返回这两个签名之间的比较分数。
- en: 11.4 Summary of our top-down design
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 我们自顶向下设计的总结
- en: We did it! We’ve broken down our original big problem into several smaller problems
    that are amenable to being implemented as a function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们做到了！我们已经将原始的大问题分解成几个更小的问题，这些问题可以作为一个函数来实现。
- en: Figure 11.6 depicts all the work that we’ve done during the process of decomposing
    the problem. Remember, we started with a `make_guess` function, which will solve
    the overall problem. To help us with `make_guess`, we created a `process_data`
    function that will do some of the work for `make_guess`. To help `process_data`,
    we created three more functions, `make_signature`, `get_all_signatures`, and `lowest_score`,
    that each have their own helper functions, and so forth. Having sketched out the
    functions we’ll need to solve our problem, our next step will be to implement
    them.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6展示了我们在分解问题的过程中所做的工作。记住，我们从一个`make_guess`函数开始，这个函数将解决整体问题。为了帮助`make_guess`，我们创建了一个`process_data`函数，它将为`make_guess`做一些工作。为了帮助`process_data`，我们又创建了三个更多函数，`make_signature`、`get_all_signatures`和`lowest_score`，每个函数都有自己的辅助函数，等等。在勾勒出解决我们问题的所需函数后，我们的下一步将是实现它们。
- en: '![figure](../Images/11-6.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-6.png)'
- en: Figure 11.6 Full functions diagram for `make_guess`
  id: totrans-125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.6 `make_guess`的完整函数图
- en: 11.5 Implementing our functions
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 实现我们的函数
- en: Now we’re ready to ask Copilot to implement each function that we need. We designed
    our functions by starting from the top—the biggest problem—and working down to
    smaller problems. But remember from chapter 7 that this isn’t the order that we
    implement the functions; instead, we implement the functions in the opposite order,
    from bottom to top (or right to left in figure 11.6).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好让Copilot实现我们需要的每个函数。我们设计函数的方式是从上到下——从最大的问题开始，逐步细化到更小的问题。但请记住，从第7章我们知道这不是实现函数的顺序；相反，我们是从下到上（或如图11.6所示从右到左）实现函数的。
- en: Just as in our example in chapter 7, we’re not going to focus much on testing,
    prompt engineering, debugging, or code reading. We do encourage you to run doctest
    on the docstring tests that we’ve provided, and further encourage you to add additional
    tests for each function.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第7章的例子中一样，我们不会过多关注测试、提示工程、调试或代码阅读。我们鼓励你运行我们提供的文档字符串测试的doctest，并进一步鼓励你为每个函数添加额外的测试。
- en: 11.5.1 clean_word
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.1 clean_word
- en: We’ll start with our `clean_word` function. As usual, we provide the function
    header (the `def` line) and docstring, and we let Copilot fill in the code. We
    also provide some annotations to briefly illustrate how the code works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从`clean_word`函数开始。像往常一样，我们提供函数头（`def`行）和文档字符串，并让Copilot填写代码。我们还提供了一些注释，简要说明代码的工作原理。
- en: Remember that we want our `clean_word` function to remove punctuation that might
    show up around the word and to convert the word to lowercase. But we don’t want
    to mess with punctuation in the middle of the word, such as the “-” in *card-board*.
    We’ve written the docstring to make clear what we want.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们希望`clean_word`函数能够移除可能出现在单词周围的标点符号，并将单词转换为小写。但我们不希望干扰单词中间的标点符号，比如*card-board*中的“-”。我们已经编写了文档字符串来清楚地说明我们的需求。
- en: Listing 11.1 Clean words for analysis
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.1 分析用的清洁单词
- en: '[PRE0]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Converts the word to lowercase'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将单词转换为小写'
- en: '#2 Uses the string module to strip punctuation from ends'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用字符串模块从两端去除标点符号'
- en: When working on our password functions in chapter 3, we saw Copilot using the
    string module, and we see Copilot doing that again here. We know from our work
    in chapter 3 that this won’t work unless we import string first, so add
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第 3 章处理密码函数时，我们看到了 Copilot 使用字符串模块，在这里我们又看到了 Copilot 在这样做。我们知道从第 3 章的工作中，除非我们首先导入字符串，否则这不会起作用，所以添加
- en: '[PRE1]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: above this function as we’ve done in the following listing.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 上述函数如以下列表所示。
- en: Listing 11.2 Clean words for analysis, complete
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.2 分析用的清洁单词，已完成
- en: '[PRE2]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This completes the `clean_word` function, so we can mark this as complete in
    our functions diagram in figure 11.7.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了 `clean_word` 函数，因此我们可以在图 11.7 的函数图中标记为完成。
- en: '![figure](../Images/11-7.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-7.png)'
- en: Figure 11.7 Full functions diagram with `clean_word` now finished
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.7 完整的函数图，`clean_word` 现已完成
- en: 11.5.2 average_word_length
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.2 average_word_length
- en: 'Now let’s tackle the first of our five signature feature functions: `average_word_length`.
    It needs to determine the average number of letters per word, but we don’t want
    to count surrounding punctuation as letters nor include words that don’t have
    any letters. We want to use our `clean_word` function here, as shown in the following
    listing. As always, we’ve written the docstring in a way that we hope directs
    Copilot to make these decisions.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来处理我们五个标志性功能函数中的第一个：`average_word_length`。它需要确定每个单词的平均字母数，但我们不想将周围的标点符号算作字母，也不包括没有字母的单词。我们想在这里使用
    `clean_word` 函数，如以下列表所示。和往常一样，我们编写了文档字符串，希望它能指导 Copilot 做出这些决定。
- en: Listing 11.3 Average word length
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.3 平均单词长度
- en: '[PRE3]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Splits string into its words'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将字符串拆分为其单词'
- en: '#2 total will count the total number of letters across all words.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 总计将计算所有单词中的字母总数。'
- en: '#3 count will count the number of words.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 count 将计算单词的数量。'
- en: '#4 Loops through each word'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历每个单词'
- en: '#5 Copilot calls clean_word for us!'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 协作者调用 clean_word 为我们服务！'
- en: '#6 Considers this word only if it isn’t empty'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 仅考虑不为空的单词'
- en: '#7 Adds number of letters in word'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 添加单词中的字母数'
- en: '#8 Adds 1 to count this word'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 将 1 添加到计数以记录这个单词'
- en: '#9 Returns number of letters divided by number of words'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 返回字母数与单词数的比例'
- en: You’ll notice in the doctest here that we’ve split our string over two lines,
    ending the first line with a \ character. The reason we did this is that the string
    wouldn’t otherwise fit on one line in the book. We also needed to keep the second
    line without any indentation; otherwise, doctest would use that indentation as
    spaces in the string. On your computer, you can type the string on a single line
    and not worry about the \ or lack of indentation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在这个 doctest 中，我们将字符串分成了两行，第一行以 \ 字符结束。我们这样做的原因是，如果不这样做，字符串将无法在书的一行中显示。我们还需要保持第二行没有任何缩进；否则，doctest
    会将那个缩进作为字符串中的空格。在你的电脑上，你可以将字符串放在一行中输入，不必担心 \ 或缩进。
- en: We can now mark `average_word_length` as done in our updated figure (figure
    11.8). Although satisfying, marking these off in the figure one by one might be
    a bit too much noise, so we’ll revisit the figure only periodically going forward.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在更新的图（图 11.8）中标记 `average_word_length` 为完成。尽管令人满意，但一个接一个地在图中标记这些可能会有些过于嘈杂，所以我们将定期回顾这个图。
- en: '![figure](../Images/11-8.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-8.png)'
- en: Figure 11.8 Full functions diagram with `average_word_length` now finished
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.8 完整的函数图，`average_word_length` 现已完成
- en: 11.5.3 different_to_total
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.3 different_to_total
- en: This is the second of our signature features. We need this one to calculate
    the number of different words used divided by the total number of words. Again,
    we don’t want surrounding punctuation or empty words.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的第二个标志性功能。我们需要这个功能来计算不同单词的使用次数与总单词数的比例。同样，我们不想包括周围的标点符号或空单词。
- en: Listing 11.4 Different words divided by total number of words
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.4 不同单词与总单词数的比例
- en: '[PRE4]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Splits string into its words'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将字符串拆分为其单词'
- en: '#2 total will count the total number of nonempty words.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 总计将计算非空单词的总数。'
- en: '#3 A set of the unique words found'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 找到的一组唯一单词'
- en: '#4 Copilot again calls clean_word for us!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 协作者再次为我们调用 clean_word！'
- en: '#5 A set is like a list but doesn’t accept duplicates.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 集合就像列表，但不接受重复项。'
- en: '#6 Returns the number of different words divided by total number of words'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 如果这个单词不为空，则考虑这个单词'
- en: This code uses a Python set, rather than a Python list, to store the unique
    words that it finds. A set is similar to a list except that it doesn’t accept
    duplicates, so if you try to add the same word multiple times, it retains only
    one copy of the word. That’s why we can use `len(unique)` in the return statement
    to get the total number of unique words.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用Python集合而不是Python列表来存储它找到的唯一单词。集合与列表类似，但不会接受重复项，所以如果你尝试多次添加相同的单词，它只会保留一个单词的副本。这就是为什么我们可以在返回语句中使用`len(unique)`来获取唯一单词的总数。
- en: We don’t need to know any more about Python sets to continue here. But if you’re
    interested in learning more, you can ask Copilot for a code explanation and/or
    do a Google search for information on Python sets.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们不需要了解更多的Python集合知识。但如果你有兴趣了解更多，可以向Copilot请求代码解释，或者通过Google搜索Python集合的相关信息。
- en: 11.5.4 exactly_once_to_total
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.4 exactly_once_to_total
- en: We’re ready to implement our third signature feature. This one calculates the
    number of words used exactly once divided by the total number of words, as shown
    in the following listing. We provided the prompt to Copilot and received the following
    function.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备实现第三个签名功能。这个功能计算使用一次的单词数与总单词数的比值，如下所示。我们向Copilot提供了提示，并收到了以下函数。
- en: 'Listing 11.5 Fraction of words used just once: Try 1'
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5 使用一次的单词占比：尝试1
- en: '[PRE5]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Removing a word from the unique set?'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从唯一集合中移除单词？'
- en: When we skim that code, it looks like `unique` is a set that keeps track of
    the unique words. At the end, we divide that number of unique words by the total
    number of words.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们浏览这段代码时，看起来`unique`是一个记录唯一单词的集合。最后，我们将唯一单词的数量除以总单词数。
- en: However, run our test case using doctest, and you’ll see that our code fails
    the test. We need the result to be 0.5, but our code is unfortunately giving us
    0.6\. Our code is wrong. We need to stay vigilant!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用doctest运行我们的测试用例，你会发现我们的代码没有通过测试。我们需要的结果是0.5，但遗憾的是，我们的代码给出了0.6。我们的代码是错误的。我们需要保持警惕！
- en: Let’s review the code to see why it was wrong. Imagine that a word is used three
    times in a text. The first time our code finds it, the code will add it to `unique`.
    The second time, it will *remove* it from `unique`. That’s fine so far because
    the word isn’t being used exactly once anymore, so it shouldn’t be in there. But
    then the third time, it will add that word back to`unique`! So, our code is going
    to count some words as being used exactly once when in fact they were used more
    than once. We looked through the list that appears when you press Ctrl-Enter and
    found this alternate version, which is correct, as shown in the following listing.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下代码，看看为什么它是错误的。想象一下一个单词在文本中使用了三次。第一次我们的代码找到它时，代码会将其添加到`unique`中。第二次，它会从`unique`中*移除*它。到目前为止一切正常，因为这个词不再只使用一次了，所以它不应该在那里。但是第三次，它会将这个词再次添加到`unique`中！所以，我们的代码将会把一些单词计为只使用了一次，而实际上它们被使用了多次。我们查看了按下Ctrl-Enter时出现的列表，并找到了这个替代版本，如下所示，它是正确的。
- en: 'Listing 11.6 Fraction of words used just once: Try 2'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.6 使用一次的单词占比：尝试2
- en: '[PRE6]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Keeps track of all words we’ve seen'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 跟踪我们看到的所有单词'
- en: '#2 Keeps track of words used exactly once—what we ultimately care about'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 跟踪使用一次的单词——这是我们最终关心的'
- en: '#3 We’re seeing the word again.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 我们再次看到了这个单词。'
- en: '#4 So the word has to be removed from once.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 所以这个单词必须从`once`中移除。'
- en: '#5 We’ve now seen this word.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 我们现在已经看到了这个单词。'
- en: '#6 So far, the word is used exactly once.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 到目前为止，这个单词只使用了一次。'
- en: '#7 Returns the number of words used exactly once divided by the total number
    of words'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 返回使用一次的单词数与总单词数的比值'
- en: This code is tricky! To understand it, start by focusing on the `else` code.
    That’s the code that runs the first time we see each word. That word gets added
    to both the `unique` and `once` sets. It’s the `once` set that’s going to keep
    track for us of the words used exactly once.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码有点棘手！要理解它，首先关注`else`代码。这是第一次看到每个单词时运行的代码。那个单词会被添加到`unique`和`once`集合中。`once`集合将为我们跟踪使用一次的单词。
- en: 'Now imagine that we see a word for a second time. The `if` code is going to
    run when this happens because the word is already in `unique` (we added it there
    the first time we saw this word). Now, because we’ve seen the word more than once,
    we need it gone from the `once` set. That’s exactly what the `if` code does: it
    uses `once.discard(word)` to remove the word from `once`.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，我们第二次看到一个单词。当单词已经在`unique`中时（我们第一次看到这个单词时添加到那里），`if`代码将会运行。现在，因为我们已经看到了这个单词多次，我们需要从`once`集合中移除它。这正是`if`代码所做的：它使用`once.discard(word)`从`once`中移除单词。
- en: To summarize, the first time we see a word, it gets added to `once`. When we
    see it again, it gets removed from `once` with no way to ever have that word added
    back to `once`. The `once` set is correctly tracking the words used exactly once.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当我们第一次看到某个单词时，它会添加到`once`中。当我们再次看到它时，它会从`once`中移除，并且永远无法再次添加到`once`中。`once`集合正确地跟踪了只使用过一次的单词。
- en: 11.5.5 split_string
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.5 split_string
- en: We’ve finished our three word-level signature feature functions. Before we can
    move on to our two sentence-level signature feature functions, we need to write
    `get_ sentences`. But to write `get_sentences`, we first need `split_string`,
    which is what we’ll work on now.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了三个单词级别的签名特征函数。在我们能够继续到两个句子级别的签名特征函数之前，我们需要编写`get_sentences`。但为了编写`get_sentences`，我们首先需要`split_string`，这正是我们现在要工作的。
- en: 'Our `split_string` function is supposed to be able to split a string around
    any number of separators. It inherently has nothing to do with sentences or phrases.
    We’ve included one docstring test to highlight this fact: even though we’re going
    to use it to split sentences and phrases, it’s more general than that. Look at
    the following listing.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`split_string`函数应该能够根据任意数量的分隔符分割字符串。它本质上与句子或短语无关。我们包含了一个docstring测试来强调这一点：尽管我们将使用它来分割句子和短语，但它比这更通用。看看下面的列表。
- en: Listing 11.7 Split a string around separators
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.7 分割字符串周围的分隔符
- en: '[PRE7]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '******#1 A better variable name would be all_strings.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '******#1 更好的变量名应该是all_strings。'
- en: '#2 A better variable name would be current_string.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 更好的变量名应该是current_string。'
- en: '#3 Current string ends here.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 当前字符串在这里结束。'
- en: '#4 Removes any space from beginning and end of current string'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从当前字符串的开始和结束处移除任何空格'
- en: '#5 If the current string isn’t empty . . .'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果当前字符串不为空 . . .'
- en: '#6 . . . saves this as one of the split strings.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 . . . 将其保存为分割字符串之一。'
- en: '#7 Clears the current string to get ready for the next one'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 清除当前字符串，为下一个字符串做准备'
- en: '#8 Adds to the current string (don’t split yet)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 向当前字符串添加内容（尚未分割）'
- en: '#9 Handles the final split string by adding if not empty******  ******You might
    be curious about the code after the `for` loop and before the `return` statement.
    It seems to be duplicating some of the code from within the `for` loop, so what’s
    it doing there? This code is there because the loop only adds a split string to
    our list of strings when it finds one of the separator characters. If the text
    doesn’t end with a separator character, the loop won’t add the final split string.
    The code below the loop ensures that this final split string isn’t lost.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 处理最终的分割字符串，如果它不为空******  ******您可能对`for`循环之后和`return`语句之前的代码感到好奇。它似乎在复制`for`循环内部的某些代码，那么它在那里做什么呢？这段代码之所以存在，是因为当循环找到分隔符字符时，它才会将分割字符串添加到我们的字符串列表中。如果文本不以分隔符字符结尾，循环就不会添加最终的分割字符串。循环下面的代码确保这个最终的分割字符串不会丢失。'
- en: It’s been a little while since we updated our diagram with functions we’ve completed.
    Time for an update! This also serves as a reminder that we’re finishing functions
    from the bottom up (right-to-left in the diagram). As such, figure 11.9 has our
    functions completed so far.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们更新了包含我们已完成函数的图表以来已经有一段时间了。是时候更新了！这也提醒我们，我们是从底部向上（在图表中从右到左）完成函数的。因此，图11.9显示了到目前为止我们已完成的函数。
- en: '![figure](../Images/11-9.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-9.png)'
- en: Figure 11.9 Full functions diagram updated with `different_to_total`, `exactly_once_to_total`,
    and `split_string` now finished
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.9 完整函数图更新了`different_to_total`、`exactly_once_to_total`和`split_string`现在已完成
- en: 11.5.6 get_sentences
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.6 get_sentences
- en: In our top-down design, we kicked most of the work for `get_sentences` off to
    the `split_string` function. Therefore, what we’re hoping for is that Copilot
    implements `get_sentences` by making a suitable call to `split_string`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的自顶向下设计中，我们将`get_sentences`的大部分工作委托给了`split_string`函数。因此，我们希望Copilot通过调用`split_string`来实施`get_sentences`。
- en: We don’t want our sentences to have any space at the beginning or end, and we
    don’t want any empty sentences. We initially had these requirements in the docstring,
    but even without them Copilot correctly calls `split_string`, and that function
    handles these edge cases anyway.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望我们的句子开头或结尾有任何空格，也不希望有任何空句子。我们最初在文档字符串中提出了这些要求，但即使没有它们，Copilot也能正确地调用`split_string`函数，并且该函数无论如何都会处理这些边缘情况。
- en: Listing 11.8 Return list of sentences in text
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.8 从文本中返回句子列表
- en: '[PRE8]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Calls split_string with the sentence separator characters'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用句子分隔符调用split_string'
- en: 11.5.7 average_sentence_length
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.7 平均句子长度
- en: Now that we have `get_sentences`, we can proceed with `average_sentence_length`,
    our fourth signature feature. This one calculates the average number of words
    per sentence. We provided the prompt to Copilot and received the function shown
    in the following listing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了`get_sentences`函数，我们可以继续进行`average_sentence_length`，这是我们第四个签名特征。这个特征计算每个句子的平均单词数。我们向Copilot提供了提示，并收到了以下列表中显示的函数。
- en: Listing 11.9 Average number of words per sentence
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.9 每个句子的平均单词数
- en: '[PRE9]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Gets a list of sentences'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取句子列表'
- en: '#2 Loops through the sentences'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历句子'
- en: '#3 Breaks current sentence into its words'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将当前句子分解为其单词'
- en: '#4 Loops through sentence’s words'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 遍历句子的单词'
- en: '#5 Returns number of words divided by number of sentences'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回单词数除以句子数'
- en: 11.5.8 get_phrases
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.8 获取短语
- en: Much liked we needed `get_sentences` before we could implement `average_sentence_
    length`, we need `get_phrases` before we can implement `average_sentence_complexity`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们需要`get_sentences`函数才能实现`average_sentence_length`一样，我们需要`get_phrases`函数才能实现`average_sentence_complexity`。
- en: As with `get_sentences`, we expect Copilot to call `split_string` to get the
    phrases. That is what it has done for us here, as shown in the following listing.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与`get_sentences`函数一样，我们期望Copilot调用`split_string`来获取短语。这正是它为我们所做的事情，如下列所示。
- en: Listing 11.10 Return list of phrases from a sentence
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.10 从句子中返回短语列表
- en: '[PRE10]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Calls split_string with the phrase separator characters'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用短语分隔符调用split_string'
- en: 11.5.9 average_sentence_complexity
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.9 平均句子复杂度
- en: With `get_phrases` completed, we can now prompt for an implementation of `average_
    sentence_complexity`. The code is shown in the following listing.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成`get_phrases`后，我们现在可以提示实现`average_sentence_complexity`。代码如下列所示。
- en: Listing 11.11 Average number of phrases per sentence
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.11 每个句子的平均短语数
- en: '[PRE11]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**#1 We changed a period to a comma to make this 5/4 = 1.25.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 我们将一个句号改为逗号，使其成为5/4 = 1.25。'
- en: '#2 Gets a list of sentences'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 获取句子列表'
- en: '#3 Loops through the sentences'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 遍历句子'
- en: '#4 Gets a list of phrases in the current sentence'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取当前句子的短语列表'
- en: '#5 Adds the number of phrases in the current sentence'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 添加当前句子的短语数'
- en: '#6 Returns the number of phrases divided by the number of sentences**  **We’re
    really coming along now! We’ve finished all the functions needed to create `make_signature`,
    as shown in figure 11.10.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 返回短语数除以句子数**  **我们现在真的在进步！我们已经完成了创建`make_signature`所需的所有函数，如图11.10所示。'
- en: '![figure](../Images/11-10.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-10.png)'
- en: Figure 11.10 Full functions diagram updated to show that we’re now ready to
    write `make_signature`
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.10 完整函数图已更新，显示我们现在可以编写`make_signature`
- en: 11.5.10 make_signature
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.10 创建签名
- en: We’ve written nine functions to this point, and while they’re all important,
    we may feel a little unsatisfied right now because we’re not even dealing with
    text signatures yet. We’ve got some functions that clean words, split strings
    in various ways, and calculate individual features of signatures, but no function
    to make a full signature.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经编写了九个函数，虽然它们都很重要，但我们现在可能感到有点不满意，因为我们甚至还没有处理文本签名。我们有一些函数可以清理单词，以各种方式分割字符串，并计算签名的单个特征，但没有一个函数可以创建完整的签名。
- en: That changes now because we’re finally ready to implement `make_signature` to
    give us the signature for a text. This function will take the text of a book and
    return a list of five numbers, each of which is the result of calling one of our
    five feature functions.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在情况有所改变，因为我们终于准备实现`make_signature`来为我们提供文本的签名。这个函数将接受一本书的文本，并返回一个包含五个数字的列表，每个数字都是调用我们五个特征函数之一的结果。
- en: Listing 11.12 Numeric signature for the text
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.12 文本的数字签名
- en: '[PRE12]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '******#1 Each of our five feature functions is called.******  ******Notice
    that this function can be implemented as nothing more than a call to each of our
    five feature functions. It’s important to pause now to think about just how messy
    this function would have been without having done a solid top-down design first.
    The code for all five of the functions that we’re calling here would have had
    to be in a single function, with all of their own variables and calculations mingled
    together into a real mess. Lucky for us, we’re using top-down design! Our function
    is therefore easier for us to read and easier to convince ourselves that it’s
    doing the right thing.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '******#1 我们五个特征函数都被调用了。******  ******请注意，这个函数可以简单地通过调用我们五个特征函数来实现。现在停下来思考一下，如果没有先进行良好的自顶向下的设计，这个函数会多么混乱。我们在这里调用的所有五个函数的代码都必须在一个函数中，它们各自的变量和计算混合在一起，形成了一团糟。幸运的是，我们使用了自顶向下的设计！因此，我们的函数更容易阅读，也更容易让我们相信自己正在做正确的事情。'
- en: 11.5.11 get_all_signatures
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.11 get_all_signatures
- en: Our `process_data` function has three subtasks for us to implement. We just
    finished with the first one (`make_signature`), so now we’ll move on to its second
    subtask, which is our `get_all_signatures` function.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`process_data`函数有三个子任务需要我们实现。我们刚刚完成了第一个（`make_signature`），所以现在我们将继续进行其第二个子任务，即我们的`get_all_signatures`函数。
- en: From now on, we’ll assume that your working directory has your code and that
    it also has the subdirectory of books that we’ve provided. We need this function
    to return the signature for each file in our directory of known authors. We’re
    hoping for Copilot to call `make_signature` here to make this function far simpler
    than it otherwise would be.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们假设你的工作目录包含你的代码，并且它还包含我们提供的书籍子目录。我们需要这个函数为我们目录中的每个已知作者文件返回签名。我们希望Copilot在这里调用`make_signature`，使这个函数比其他方式简单得多。
- en: Copilot did do that for us, but the code we got still had two problems. Our
    initial code is shown in the following listing.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Copilot确实为我们做了这件事，但我们得到的代码仍然有两个问题。我们的初始代码如下所示。
- en: 'Listing 11.13 Obtain all signatures from known authors: Try 1'
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.13 从已知作者处获取所有签名：尝试 1
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**#1 Our dictionary, initially empty, maps filenames to signatures.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 我们最初为空的字典将文件名映射到签名。**'
- en: '#2 Loops through each file in the known authors directory'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历已知作者目录中的每个文件'
- en: '#3 Opens the current file'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 打开当前文件'
- en: '#4 Reads all text from the file'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 读取文件中的所有文本'
- en: '#5 Makes the signature for text and stores it in the dictionary**  **Try running
    this function from the Python prompt as'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 为文本创建签名并将其存储在字典中**  **尝试从Python提示符运行此函数作为'
- en: '[PRE14]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'and you’ll get the following error:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到以下错误：
- en: '[PRE15]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The error is telling us that the function is trying to use a module named os,
    but we don’t have this module available. This module is built-in to Python, and
    we know what to do in this case: import it! That is, we need to add'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 错误告诉我们，该函数试图使用名为os的模块，但我们没有这个模块。这个模块是Python内置的，我们知道在这种情况下要做什么：导入它！也就是说，我们需要添加
- en: '[PRE16]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'above this function. After that, we still get an error:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在此函数上方。之后，我们仍然得到一个错误：
- en: '[PRE17]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You might be wondering what a `UnicodeDecodeError` is. You could google it or
    ask ChatGPT if you’re interested in a technical explanation. What we need to know
    is that each file that we open is encoded in a specific way, and Python has chosen
    the wrong encoding to try to read this file.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道`UnicodeDecodeError`是什么。如果你对技术解释感兴趣，可以谷歌搜索或向ChatGPT提问。我们需要知道的是，我们打开的每个文件都是用特定的方式编码的，而Python选择了错误的编码来尝试读取这个文件。
- en: We can, however, direct Copilot to fix it by adding a comment near the top of
    our function. (When you encounter errors like these, you can try placing a comment
    directly above the erroneous code that was generated. Then, once you delete the
    incorrect code, Copilot can often generate new code that is correct.) Once we
    do that, all is well, as shown in the following listing.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以通过在函数顶部附近添加注释来指导Copilot修复它。（当你遇到这些错误时，你可以在生成的错误代码上方直接放置一个注释。然后，一旦你删除了错误的代码，Copilot通常可以生成新的正确代码。）一旦我们这样做，一切都会好起来，如下所示。
- en: 'Listing 11.14 Obtain all signatures from known authors: Try 2'
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.14 从已知作者处获取所有签名：尝试 2
- en: '[PRE18]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**#1 This prompt tells Copilot to fix the error we saw previously.**  **Now,
    if you run this function, you should see a dictionary of authors and their signatures,
    like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 这个提示告诉Copilot修复我们之前看到的错误。** **现在，如果你运行这个函数，你应该会看到一个作者和他们的签名的字典，如下所示：**'
- en: '[PRE19]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For simplicity, we haven’t added a test in the docstring for this function.
    If we did, however, we would create a fake, small book, along the lines of what
    we did in our second example in chapter 6\. We’d like to proceed here with our
    overall purpose of function decomposition, though, so we’ll leave that exercise
    to you if you’d like to pursue that. As shown in figure 11.11, we’ve gotten two
    `process_data` subtasks out of the way. Let’s keep going!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们没有在这个函数的文档字符串中添加测试。如果我们这样做，我们会创建一个假的、小的书籍，类似于我们在第 6 章第二个示例中所做的。不过，我们在这里想继续我们的函数分解的整体目的，所以如果你愿意继续这个练习，我们可以留给你。如图
    11.11 所示，我们已经完成了两个 `process_data` 子任务。让我们继续前进！
- en: '![figure](../Images/11-11.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/11-11.png)'
- en: Figure 11.11 Full functions diagram updated to show that `make_signature` and
    `get_all_signatures` are finished
  id: totrans-275
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.11 完整函数图更新以显示 `make_signature` 和 `get_all_signatures` 已完成
- en: 11.5.12 get_score
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.12 获取分数
- en: Let’s implement `get_score`, where we need to encode the way that we compare
    signatures. Remember the whole thing where we find the difference on each feature,
    multiply it by a weight, and then add everything together into an overall score?
    That’s what we want `get_score` to do.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实现 `get_score`，我们需要编码比较签名的方式。记得我们之前在每个特征上找到差异，乘以权重，然后将所有这些加在一起得到一个总分？这正是我们希望
    `get_score` 做的。
- en: 'It would be a challenge to explain this formula in the docstring. And we’re
    not even sure that it should go there: a docstring is supposed to explain how
    someone can use your function, not how it works internally. And, arguably, users
    of our function won’t care about this specific formula anyway. What we can do
    is use a general docstring, without our specific formula, and see what Copilot
    does with it. Here we go in the following listing.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档字符串中解释这个公式将是一个挑战。我们甚至不确定它是否应该放在那里：文档字符串应该解释如何使用你的函数，而不是它内部的工作方式。而且，可以说，我们的函数用户不会关心这个特定的公式。我们可以做的是使用一个通用的文档字符串，不包含我们的特定公式，看看
    Copilot 会如何处理它。以下是在以下列表中的内容。
- en: Listing 11.15 Compare two signatures
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.15 比较两个签名
- en: '[PRE20]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**#1 These weights, [11, 33, 50, 0.4, 4], worked well for us.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**#1 这些权重 [11, 33, 50, 0.4, 4] 对我们来说效果很好。'
- en: '#2 Loops through each signature index'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 遍历每个签名索引'
- en: '#3 Adds the weighted difference to score**  **Copilot has implemented exactly
    the formula that we wanted. Now, before we start thinking that Copilot mind-melded
    us or anything like that, remember that the formula we’ve used here is a very
    common metric for comparing signatures. Many students and other programmers over
    the years have implemented authorship identification using this very formula.
    Copilot is just giving that back to us because it occurs so often in its training
    data. If Copilot happened to give us a different formula, we could have tried
    to describe what we want in a comment or, failing that, changed the code ourselves
    to get what we want.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将加权差异加到分数**  **Copilot 实现了我们想要的公式。现在，在我们开始认为 Copilot 与我们心灵相通或类似的事情之前，记住我们在这里使用的公式是用于比较签名的非常常见的指标。多年来，许多学生和其他程序员都使用这个公式实现了作者识别。Copilot
    只是将其反馈给我们，因为它在训练数据中非常常见。如果 Copilot 碰巧给了我们一个不同的公式，我们可以尝试在注释中描述我们想要的，或者如果失败了，我们可以自己更改代码以得到我们想要的结果。'
- en: 11.5.13 lowest_score
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.13 最低分
- en: Our `lowest_score` function will finally wrap up everything we need to implement
    `process_data`. The `get_score` function that we just implemented gives us the
    score between any two signatures. Our `lowest_score` function is going to call
    `get_score` once for each known signature to compare the unknown signature to
    each known signature. It will then return the known signature that has the lowest
    score with the unknown signature, as shown in the following listing.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `lowest_score` 函数最终将汇总我们实现 `process_data` 所需要的一切。我们刚刚实现的 `get_score` 函数为我们提供了任何两个签名之间的分数。我们的
    `lowest_score` 函数将针对每个已知签名调用一次 `get_score`，将未知签名与每个已知签名进行比较。然后，它将返回与未知签名分数最低的已知签名，如下所示。
- en: Listing 11.16 Closest known signature
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.16 最接近的已知签名
- en: '[PRE21]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '****#1 Using variables in the doctest to make the test itself easier to read'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '****#1 在 doctest 中使用变量以使测试本身更容易阅读'
- en: '#2 This line is easier to read because we’re using our variables.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 这一行更容易阅读，因为我们使用了我们的变量。'
- en: '#3 Loops through each author name'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 遍历每个作者名称'
- en: '#4 Gets a score for comparing this known signature to the unknown signature'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取将此已知签名与未知签名进行比较的分数'
- en: '#5 If this is the first comparison or we’ve found a lower score . . .'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 如果这是第一次比较，或者我们已经找到了一个更低的分数...'
- en: '#6 . . . this stores both the best key and score for that key.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 . . . 这个存储了该键的最佳键和分数。'
- en: '#7 lowest[0] is the best key.****  ****The first parameter, `signatures_dict`,
    is a dictionary that maps names of authors to their known signatures. That will
    ultimately come from the `get_all_signatures` function. The second parameter,
    `unknown_signature`, will ultimately come from calling `make_signature` on the
    mystery book. The third parameter, `weights`, will be hard-coded by us when we
    call this function.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 lowest[0]是最佳键。****  ****第一个参数`signatures_dict`是一个将作者名称映射到其已知签名的字典。这最终将来自`get_all_signatures`函数。第二个参数`unknown_signature`最终将来自在神秘书籍上调用`make_signature`的结果。第三个参数`weights`将是我们调用此函数时硬编码的。'
- en: 11.5.14 process_data
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.14 process_data
- en: Only two functions to go! One of them is `process_data`—it feels like it took
    us forever, but we’re finally ready for it.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 只剩下两个函数了！其中一个是`process_data`——它感觉我们花了很长时间，但我们终于准备好了。
- en: 'Our `process_data` function is going to take two parameters in the following
    listing: the filename of a mystery book and the directory of known-author books.
    It will return the author that we think wrote the mystery book.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在下面的列表中`process_data`函数将接受两个参数：神秘书籍的文件名和已知作者书籍的目录。它将返回我们认为写了神秘书籍的作者。
- en: Listing 11.17 Signature closest to the mystery author
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.17 最接近神秘作者的特征
- en: '[PRE22]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Gets all the known signatures'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 获取所有已知签名'
- en: '#2 Copilot uses our prior work to get the encoding right this time.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Copilot使用我们之前的工作来正确地获取编码。'
- en: '#3 Reads text of the mystery book'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 读取神秘书籍的文本'
- en: '#4 Gets the unknown signature'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 获取未知签名'
- en: '#5 Returns the signature with the lowest comparison score'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回具有最低比较分数的特征'
- en: Again, notice how much we’re relying on our earlier functions. This massively
    useful `process_data` function is now really nothing more than a carefully sequenced
    list of function calls.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意我们有多么依赖我们之前的功能。这个极其有用的`process_data`函数现在实际上不过是一个精心编排的函数调用列表。
- en: In the book resources for this chapter, we’ve included a few unknown author
    files, for example, unknown1.txt and unknown2.txt. Those should be in your current
    working directory along with your code (and the subdirectory of known author files).
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的书籍资源中，我们包括了一些未知作者文件，例如unknown1.txt和unknown2.txt。这些文件应该与你的代码（以及已知作者文件的子目录）一起位于你的当前工作目录中。
- en: 'Let’s call `process_data` to guess who wrote `''unknown1.txt''`:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用`process_data`来猜测谁写了`'unknown1.txt'`：
- en: '[PRE23]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Our program guesses that Arthur Conan Doyle wrote unknown1.txt. And if you peek
    at the text of unknown1.txt by opening the file, you’ll see that our guess is
    right. The book is called *The Sign of the Four*, which is a well-known Arthur
    Conan Doyle book.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的程序猜测亚瑟·柯南·道尔写了unknown1.txt。如果你通过打开文件查看unknown1.txt的文本，你会发现我们的猜测是正确的。这本书叫做《四个签名》，是亚瑟·柯南·道尔的一本知名作品。
- en: 11.5.15 make_guess
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5.15 make_guess
- en: To guess the author of a book, we currently need to type the Python code to
    run `process_data`. That’s not very friendly to users; it would be nice if we
    could run the program and have it ask us which mystery book file we want to work
    with.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 要猜测一本书的作者，我们目前需要输入Python代码来运行`process_data`。这对用户来说不是很友好；如果我们能运行程序并让它询问我们想要处理哪个神秘书籍文件，那就太好了。
- en: We’ll put that finishing touch on our program by implementing `make_guess`,
    our top-most function! This function will ask the user for a filename of a mystery
    book, get the best guess using `process_data`, and tell the user about that guess,
    as shown in the following listing.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过实现`make_guess`，我们最高级别的函数，来给我们的程序添加最后一笔！这个函数将询问用户神秘书籍的文件名，使用`process_data`获取最佳猜测，并告诉用户关于这个猜测的信息，如下所示。
- en: Listing 11.18 Interacts with user and guesses text’s author
  id: totrans-313
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.18 与用户交互并猜测文本的作者
- en: '[PRE24]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '#1 Asks the user for the filename of the mystery book'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 询问用户神秘书籍的文件名'
- en: '#2 Calls process_data to do all the work and report our guess'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 调用process_data来完成所有工作并报告我们的猜测'
- en: This completes all the functions from our diagram! Figure 11.12 shows that we’ve
    checked off every function from the bottom to the very top of our diagram.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们图中的所有函数！图11.12显示我们已经从底部到顶部检查了图中的每个函数。
- en: '![figure](../Images/11-12.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/11-12.png)'
- en: Figure 11.12 All the required functions for `make_guess` are now complete!
  id: totrans-319
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图11.12 `make_guess`所需的所有功能现在都已完整！
- en: 'If you have all of our code in your Python file, you’ll be able to run it to
    guess the author of a mystery book after you add the following line of code at
    the bottom of that file:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将所有代码都放在你的Python文件中，你将在文件底部添加以下代码行后能够运行它来猜测神秘书籍的作者：
- en: '[PRE25]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For example, here’s what happens when we run our program and type `unknown1.txt`
    as the unknown book:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当我们运行程序并输入`unknown1.txt`作为未知书籍时，会发生以下情况：
- en: '[PRE26]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It correctly tells us that unknown1.txt is written by Arthur Conan Doyle! Try
    running it for each of the other unknown book files that we’ve provided. How many
    of those does it guess correctly? Which ones does it get wrong?
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 它正确地告诉我们unknown1.txt是由亚瑟·柯南·道尔写的！尝试为我们提供的其他每个未知书籍文件运行它。有多少个它猜对了？哪些猜错了？
- en: Congratulations! You’ve completed your first real-world top-down design. And
    look at what we’ve managed to accomplish—an authorship identification program
    that any beginning programmer should be proud of. Your program uses AI to learn
    how individual authors write (do they use shorter or longer words on average,
    shorter or longer sentences on average, etc.?) by using the text of books in its
    training data. It then applies that learning to make a prediction on a mystery
    book by determining which author the mystery book most closely emulates—very cool!
    We managed to solve a very difficult problem, and we did it by breaking down the
    problem and letting Copilot write the code for each of the subproblems.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了你的第一个真实世界的自顶向下设计。看看我们取得了什么成就——一个任何初学者程序员都应该为之自豪的作者识别程序。你的程序使用AI通过使用训练数据中的书籍文本来学习个别作者是如何写作的（他们平均使用较短或较长的单词，平均使用较短或较长的句子等？）。然后，它将这种学习应用到对神秘书籍的预测中，通过确定神秘书籍最接近哪个作者来做出预测——非常酷！我们解决了一个非常困难的问题，我们通过分解问题并让Copilot为每个子问题编写代码来做到了这一点。
- en: 11.6 Going further
  id: totrans-326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 进一步探索
- en: After people do a top-down design, they often see opportunities to refactor
    their code, which means making the code cleaner or better organized without changing
    its behavior. It’s possible to refactor our program in several ways. For example,
    you might notice that many of our signature feature functions split the string
    into words and then ignore empty words. This task (returning a list of nonempty
    words from a string) could be split off into its own subtask function, which would
    further simplify any function that calls it.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在人们完成自顶向下的设计之后，他们通常会看到重构代码的机会，这意味着在不改变其行为的前提下，使代码更加整洁或更有组织性。我们可以以几种方式重构我们的程序。例如，你可能会注意到我们许多签名功能函数将字符串拆分成单词，然后忽略空单词。这个任务（从字符串中返回非空单词列表）可以拆分成一个独立的子任务函数，这将进一步简化任何调用它的函数。
- en: We might also decide that weights should be passed to `process_data`, rather
    than hard-coding the weights in that function. The weights would then be hard-coded
    in `make_guess`, moving the decision higher in the function hierarchy and therefore
    making it easier to find and change if needed.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可能决定应该将权重传递给`process_data`函数，而不是在该函数中硬编码权重。然后，权重将在`make_guess`函数中硬编码，将决策提升到函数层次结构中的更高位置，因此如果需要的话，更容易找到和更改。
- en: It’s also possible to improve the program in terms of its features or efficiency.
    For features, right now, our program simply prints its best guess for the mystery
    book author. But we don’t know anything about that guess. Was there a second author
    that was very close to the one that was guessed? If so, we might want to know
    that. More generally, we might want to know the top few guesses rather than just
    the top guess. That way, we have useful information about who the author might
    be even if the top guess happens to be wrong. These are additional features that
    we could add to our program.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 还有可能在程序的功能或效率方面进行改进。就功能而言，目前我们的程序只是简单地打印出对神秘书籍作者的最好猜测。但我们对那个猜测一无所知。是否有另一个作者与猜测的作者非常接近？如果是这样，我们可能想知道这一点。更普遍地说，我们可能想知道前几个猜测，而不仅仅是最好的猜测。这样，即使最好的猜测是错误的，我们也有关于作者可能是谁的有用信息。这些都是我们可以添加到程序中的额外功能。
- en: For efficiency, let’s think about that `get_all_signatures` function again.
    That function does a lot of work! If we have five books in our known directory,
    then it will read each of the five files and calculate each signature. Big deal,
    right? It’s only five files, and computers are really fast. But imagine if we
    had 100 files or 10,000 files. It may be acceptable to do all that work as a one-time-only
    thing, but that’s not what our program does. In fact, every time we run the program
    to get a guess for the author of a mystery book, it runs that `get_all_signatures`
    function, which means re-creating those signatures every single time. That’s a
    huge amount of wasted effort; it would be nice if we could just store those signatures
    somewhere, never having to calculate them again. Indeed, if we were to redesign
    the code for efficiency, a first step would be to ensure that the signature for
    a known text is only computed once and reused thereafter.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高效率，让我们再次思考那个`get_all_signatures`函数。这个函数做了很多工作！如果我们已知目录中有五本书，那么它将读取这五个文件并计算每个签名。这有什么大不了的？只有五个文件，而计算机真的很快。但想象一下，如果我们有100个文件或10,000个文件。可能一次只做所有这些工作是可以接受的，但我们的程序并不是这样做的。实际上，每次我们运行程序来猜测神秘书籍的作者时，它都会运行那个`get_all_signatures`函数，这意味着每次都要重新创建那些签名。这是一大笔浪费的努力；如果我们可以将这些签名存储在某个地方，以后再也不用计算它们，那将很棒。确实，如果我们为了效率而重新设计代码，第一步就是确保已知文本的签名只计算一次，之后可以重复使用。
- en: That’s exactly what tools like Copilot do! OpenAI trained GitHub Copilot just
    once on a huge corpus of code. That took thousands or millions of computer hours.
    But now that the training is done, it can keep writing code for us without having
    to train from scratch every time. The idea of doing the training once and then
    using that training for many subsequent predictions is a common paradigm throughout
    all of ML.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是像Copilot这样的工具所做的事情！OpenAI只对大量代码语料库训练了GitHub Copilot一次。这需要成千上万或数百万的计算机小时。但现在训练完成之后，它可以为我们的代码编写提供帮助，而无需每次都从头开始训练。一次训练然后使用该训练进行许多后续预测的想法是机器学习中的常见范式。
- en: 11.7 Exercises
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.7 练习
- en: Which of the following isn’t a step in the AI-based authorship identification
    process described in this chapter?
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪一项不是本章描述的基于AI的作者识别过程中的步骤？
- en: Calculating the average word length of the mystery book
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神秘书籍的平均词长
- en: Comparing the mystery book’s signature to known signatures
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将神秘书籍的签名与已知签名进行比较
- en: Asking the user for the filename of the mystery book
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 询问用户神秘书籍的文件名
- en: Finding the total number of pages in the mystery book
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查找神秘书籍的总页数
- en: 'Build a classifier that can distinguish between spam and non-spam (ham) emails
    based on email content. Use features like word frequency, presence of certain
    keywords, and email length. Here are the steps you’ll need to take:'
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立一个可以根据邮件内容区分垃圾邮件和非垃圾邮件（ham）的分类器。使用诸如词频、某些关键词的存在以及邮件长度等特征。以下是你需要采取的步骤：
- en: Collect a dataset of spam and non-spam emails. You can find publicly available
    datasets online, such as the Enron spam dataset.
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集垃圾邮件和非垃圾邮件的数据集。你可以在网上找到公开可用的数据集，例如Enron垃圾邮件数据集。
- en: Preprocess the emails (remove stop words, punctuation, etc.).
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理邮件（移除停用词、标点符号等）。
- en: Extract features (e.g., word counts, presence of certain words).
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取特征（例如，词数、某些单词的存在）。
- en: Train a classifier using our labeled data (supervised learning). A simple and
    effective choice for the classifier is the Naïve Bayes classifier (feel free to
    use a Python library to help you).
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的标记数据（监督学习）训练一个分类器。一个简单而有效的分类器选择是朴素贝叶斯分类器（你可以自由使用Python库来帮助你）。
- en: Test the classifier with a separate set of emails to check its accuracy.
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用单独的邮件集测试分类器以检查其准确性。
- en: 'In this exercise, you’ll create a simple text generation program using n-grams.
    N-grams are contiguous sequences of *n* items from a given sample of text or speech.
    You’ll use these n-grams to generate new text that mimics the style of the input
    text. The key idea is to build a model that is trained to know which words commonly
    follow other words (i.e., “cat eats” makes sense, “tissue eats” does not) and
    then, among the possible choices, randomly select the next one. Feel free to look
    up n-grams for more information. Here are the steps you’ll need to take:'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个练习中，你将创建一个简单的基于n-gram的文本生成程序。n-gram是从给定文本或语音样本中连续的*n*个项的序列。你将使用这些n-gram来生成模仿输入文本风格的新的文本。关键思想是构建一个模型，该模型经过训练，知道哪些词通常跟在哪些词后面（例如，“猫吃”是有意义的，“纸巾吃”则没有意义），然后，在可能的选项中随机选择下一个词。如有需要，可以查阅n-gram以获取更多信息。以下是你需要采取的步骤：
- en: 'Choose input text that you can load into Python. You can use something like:
    “Pride and Prejudice” by Jane Austen.'
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择可以加载到Python中的输入文本。你可以使用类似“简·奥斯汀的《傲慢与偏见》”的东西。
- en: Preprocess the text by converting it to lowercase and removing punctuation.
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将其转换为小写并删除标点符号来预处理文本。
- en: Create n-grams from the input text. An n-gram is a contiguous sequence of *n*
    items from a given text. For simplicity, we’ll use bigrams (*n* = 2) in this example.
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入文本中创建n-gram。n-gram是从给定文本中连续的*n*个项的序列。为了简单起见，在这个例子中我们将使用二元组（*n* = 2）。
- en: Use the generated n-grams to produce new text. Start with a random n-gram, and
    keep adding new words based on the n-gram model until the desired length is reached.
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用生成的n-gram来生成新的文本。从一个随机的n-gram开始，根据n-gram模型不断添加新词，直到达到所需的长度。
- en: Summary
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Top-down design becomes more and more critical as the complexity of our programs
    increase.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着我们程序复杂性的增加，自顶向下的设计变得越来越重要。
- en: Author identification is the process of guessing the author of a mystery book.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者识别是猜测神秘书籍作者的过程。
- en: We can use features about words (e.g., average word length) and sentences (e.g.,
    average number of words per sentence) to characterize how each known author writes.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用关于单词（例如，平均单词长度）和句子（例如，每句平均单词数）的特征来描述每个已知作者是如何写作的。
- en: Machine learning is an important area of computer science that investigates
    how machines can learn from data and make predictions.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习是计算机科学的一个重要领域，研究机器如何从数据中学习并做出预测。
- en: In supervised learning, we have some training data in the form of objects (e.g.,
    books) and their categories (who wrote each book). We can learn from that data
    to make predictions about new objects.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在监督学习中，我们有某些以对象（例如，书籍）及其类别（谁写了每本书）形式存在的训练数据。我们可以从这些数据中学习，以对新对象做出预测。
- en: A signature consists of a list of features, one signature per object.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 签名由一系列特征组成，每个对象一个签名。
- en: Refactoring code means to improve the design of the code (e.g., by reducing
    code repetition).************************
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代码重构意味着改进代码的设计（例如，通过减少代码重复）。************************
