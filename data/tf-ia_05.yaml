- en: 4 Dipping toes in deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 涉足深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing and training fully connected neural networks using Keras
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 实现和训练全连接神经网络
- en: Implementing and training convolutional neural networks to classify images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和训练卷积神经网络以对图像进行分类
- en: Implementing and training a recurrent neural network to solve a time-series
    problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现和训练递归神经网络以解决时间序列问题
- en: In chapter 3, you learned about the different model-building APIs provided by
    TensorFlow and their advantages and disadvantages. You also learned about some
    of the options in TensorFlow to retrieve and manipulate data. In this chapter,
    you will learn how to leverage some of that to build deep neural networks and
    use them to solve problems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章，您了解了 TensorFlow 提供的不同模型构建 API 及其优缺点。您还了解了 TensorFlow 中一些检索和操作数据的选项。在本章中，您将学习如何利用这些知识来构建深度神经网络，并使用它们来解决问题。
- en: '*Deep learning* is a broad term that has many different algorithms under its
    wings. Deep learning algorithms come in many different flavors and colors and
    can be classified by many criteria: the type of data they consume (e.g., structured
    data, images, time-series data), depth (shallow, deep, and very deep), and so
    on. The main types of deep networks we are going to discuss and implement are
    as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*是一个广泛的术语，它包含许多不同的算法。深度学习算法有许多不同的类型和颜色，可以根据许多标准进行分类：它们消耗的数据类型（例如，结构化数据、图像、时间序列数据）、深度（浅层、深层和非常深层）等等。我们将要讨论和实现的主要深度网络类型如下：'
- en: Fully connected networks (FCNs)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接网络（FCNs）
- en: Convolutional neural networks (CNNs)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）
- en: Recurrent neural networks (RNNs)
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 递归神经网络（RNNs）
- en: Being able to comfortably implement these neural networks is a key skill to
    be successful in the field, whether you are a graduate student, a data scientist,
    or a research scientist. This knowledge directly extends to becoming skillful
    in implementing more complex deep neural networks that deliver state-of-the-art
    performance in various problem domains.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 能够熟练实现这些神经网络是在该领域取得成功的关键技能，无论你是研究生、数据科学家还是研究科学家。这些知识直接延伸到如何熟练实现更复杂的深度神经网络，这些网络在各种问题领域提供了最先进的性能。
- en: 'In chapter 2, we discussed FCN and various operations in CNNs, such as convolution
    and pooling operations. In this chapter, you will see the FCNs again, as well
    as a holistic implementation of CNNs showing how convolution and pooling operations
    coalesce to form a CNN. Finally, you will learn about a new type of model: RNNs.
    RNNs are typically used to solve time-series problems, where the task is to learn
    patterns in data over time so that, by looking at the past patterns, we can leverage
    them to forecast the future. We will also see how RNNs are used to solve an exciting
    real-world time-series problem.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 2 章中，我们讨论了 FCN 和 CNN 中的各种操作，例如卷积和池化操作。在本章中，您将再次看到 FCNs，以及 CNNs 的整体实现，展示了卷积和池化操作如何合并形成
    CNN。最后，您将了解一个新类型的模型：RNNs。RNNs 通常用于解决时间序列问题，其中的任务是学习数据随时间变化的模式，以便通过查看过去的模式来预测未来。我们还将看到
    RNNs 如何用于解决一个有趣的现实世界时间序列问题。
- en: 4.1 Fully connected networks
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 全连接网络
- en: '*You* *have found some precious photos of your grandmother while going through
    some storage boxes you found in the attic. Unfortunately, they have seen better
    days. Most of the photos are scratched, smudged, and even torn. You know that
    recently deep networks have been used to restore old photos and videos. In the
    hope of restoring these photos, you decide to implement an image restoration model*
    using TensorFlow. You will first develop a model that can restore corrupted images
    of handwritten digits, as this data set is readily available, in order to understand
    the model and the training process. You believe an autoencoder model (a type of
    FCN) would be a great starting point. This autoencoder will have the following
    specifications:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*当您在阁楼找到一些存储盒时，里面有一些珍贵的祖母的照片。不幸的是，它们已经过时了。大多数照片都被划痕、污迹和甚至撕裂了。您知道最近已经使用了深度网络来恢复旧照片和视频。希望能恢复这些照片，您决定使用
    TensorFlow 实现图像恢复模型*。您首先将开发一个可以恢复手写数字损坏图像的模型，因为这个数据集是readily available，以便了解模型和训练过程。您认为自动编码器模型（一种
    FCN）将是一个很好的起点。这个自动编码器将具有以下规格：'
- en: Input layer with 784 nodes
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 784 个节点的输入层
- en: A hidden layer with 64 nodes, having ReLU activation
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 64 个节点的隐藏层，采用 ReLU 激活
- en: A hidden layer with 32 nodes, having ReLU activation
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 32 个节点的隐藏层，使用 ReLU 激活函数
- en: A hidden layer with 64 nodes, having ReLU activation
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 64 个节点的隐藏层，使用 ReLU 激活函数
- en: An output layer with 784 nodes with tanh activation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含 784 个节点的输出层，使用 tanh 激活函数
- en: Hyperparameter optimization for deep learning
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的超参数优化
- en: You might have noticed that when defining neural networks, we are choosing structural
    hyperparameters (e.g., number of units in hidden layers) somewhat arbitrarily.
    These values have, in fact, been chosen empirically through a few rounds of trial
    and error.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在定义神经网络时，我们选择结构超参数（例如，隐藏层中的单元数）有些是凭空选择的。实际上，这些值是通过几轮试错经验选择的。
- en: Typically, in machine learning, these hyperparameters are chosen using a principled
    approach, such as hyperparameter optimization. But hyperparameter optimization
    is an expensive process that needs to evaluate hundreds of models with different
    hyperparameter choices to choose the best set of hyperparameters. This makes it
    very difficult to use for deep learning methods, as these methods usually deal
    with large, complex models and large amounts of data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在机器学习中，这些超参数是使用基于原则的方法选择的，例如超参数优化。但是，超参数优化是一个昂贵的过程，需要评估具有不同超参数选择的数百个模型，以选择最佳的超参数集。这使得它非常难以用于深度学习方法，因为这些方法通常涉及大型、复杂的模型和大量的数据。
- en: 'Therefore, in deep learning, you will commonly see the following trends, in
    order to limit the time spent on hyperparameter optimization:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在深度学习中，为了限制在超参数优化上花费的时间，你通常会看到以下趋势：
- en: Optimizing a subset of hyperparameters to limit the exploration space (e.g.,
    type of activation instead of number of hidden units, regularization parameters,
    etc.)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化一部分超参数以限制探索空间（例如，激活类型而不是隐藏单元数量，正则化参数等）。
- en: Using robust optimizers, early stopping, learning rate decay, and so on, which
    are designed to reduce or prevent overfitting
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用健壮的优化器、早停、学习率衰减等方法，旨在减少或预防过拟合
- en: Using model specifications from published models that have delivered state-of-the-art
    performance
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已发表的模型规范，这些模型提供了最先进的性能
- en: Following rules of thumb such as reducing the output size as you go deeper into
    the network
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遵循一些经验法则，例如随着网络深入减少输出大小
- en: In this chapter, we will use model architectures chosen empirically. The focus
    of this chapter is to show how a given architecture can be implemented using TensorFlow
    2 and not to find the architectures themselves.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用经验选择的模型架构。本章的重点是展示如何使用 TensorFlow 2 实现给定的架构，而不是找到架构本身。
- en: Let’s examine the data we’ll use to implement the FCN.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们将用于实现 FCN 的数据。
- en: 4.1.1 Understanding the data
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 理解数据
- en: For this scenario, we will use the MNIST digit data set, a simple data set that
    contains the black-and-white images of hand-written digits and the corresponding
    labels representing the digits. Each image has a single digit and goes from 0-9\.
    Therefore, the data set has 10 different classes. Figure 4.1 shows several samples
    from the data set along with the digit it represents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，我们将使用 MNIST 数字数据集，这是一个简单的数据集，包含手写数字的黑白图像以及表示数字的对应标签。每个图像都有一个数字，从 0 到
    9。因此，数据集有 10 个不同的类别。图 4.1 显示了数据集中的几个样本及其表示的数字。
- en: '![04-01](../../OEBPS/Images/04-01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![04-01](../../OEBPS/Images/04-01.png)'
- en: Figure 4.1 Sample digit images. Each image contains a number from 0 to 9.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 样本数字图像。每个图像包含一个从 0 到 9 的数字。
- en: 'In TensorFlow, you can load the MNIST data set with a single line. Loading
    this data set has become an integral part of various machine learning libraries
    (including TensorFlow) due to its extremely common usage:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中，你可以用一行代码加载 MNIST 数据集。由于其极为常见的用法，加载此数据集已成为各种机器学习库（包括 TensorFlow）的重要组成部分：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The load_data() method returns two tuples: training data and testing data.
    Here, we will only use the training images (i.e., x_train) data set. As we covered
    earlier, this is an unsupervised task. Because of that, we will not need the labels
    (i.e., y_train) of the images to complete this task.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_data()`方法返回两个元组：训练数据和测试数据。在这里，我们只会使用训练图像（即，x_train）数据集。正如我们之前介绍的，这是一个无监督任务。因此，我们不需要图像的标签（即，y_train）来完成这个任务。'
- en: Better than MNIST?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 比 MNIST 好吗？
- en: It should be noted that, due to advancements in the field of computer vision
    over the last decade, MNIST is considered too easy, where a test accuracy of more
    than 92% can be achieved with a simple logistic regression model ([http://mng.bz/j2l9](http://mng.bz/j2l9))
    and a 99.84% accuracy with a state-of-the-art model ([http://mng.bz/d2Pv](http://mng.bz/d2Pv)).
    Furthermore, it’s being overused in the computer vision community. Because of
    this, a new data set known as Fashion-MNIST ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))
    has emerged. This is a black-and-white data set containing images belonging to
    10 classes. Instead of digits, it contains images of various fashion categories
    (e.g., T-shirt, sandal, bag, etc.), which poses a much harder problem than recognizing
    digits.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，由于过去十年计算机视觉领域的进展，MNIST被认为过于简单，简单的逻辑回归模型就可以实现超过92%的测试准确率（[http://mng.bz/j2l9](http://mng.bz/j2l9)），而最先进的模型则可以达到99.84%的准确率（[http://mng.bz/d2Pv](http://mng.bz/d2Pv)）。此外，它在计算机视觉社区中被过度使用。因此，一个名为Fashion-MNIST（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）的新数据集应运而生。这是一个包含属于10个类别的图像的黑白数据集。与数字不同，它包含各种时尚类别的图像（例如T恤、凉鞋、包等），这比识别数字要困难得多。
- en: You can print x_train and y_train to understand those arrays a bit better using
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以打印x_train和y_train来更好地了解这些数组，使用
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will produce
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And do the same for y_train:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对y_train执行相同的操作：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will give
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then we will do some basic data preprocessing. We will normalize all samples
    in the data set by bringing their pixel values from [0, 255] to [-1, 1]. This
    is done by subtracting 128 and dividing the result by 128 element-wise for all
    pixels. This is important because the final layer of the autoencoder has a tanh
    activation, which ranges between (-1, 1). Tanh is a nonlinear activation function
    like the sigmoid function, and for a given input, *x* is computed as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将进行一些基本的数据预处理。我们将通过将它们的像素值从[0, 255]归一化到[-1, 1]来规范化数据集中的所有样本。这是通过减去128并逐元素除以128来完成的。这很重要，因为自动编码器的最后一层具有tanh激活函数，其取值范围为(-1,
    1)。tanh是一个非线性激活函数，类似于sigmoid函数，对于给定的输入*x*，计算如下：
- en: '![04_01a](../../OEBPS/Images/04_01a.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![04_01a](../../OEBPS/Images/04_01a.png)'
- en: 'Therefore, we need to make sure we feed something to the model that is within
    the range of values that can be produced by the final layer. Also, if you look
    at the shape of x_train, you will see that it has a shape of (60000, 28, 28).
    The autoencoder takes a one-dimensional input, so we need to reshape the image
    to a one-dimensional vector of size 784\. Both these transformations can be achieved
    by the following line:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们需要确保向模型提供的内容在最终层可以生成的值范围内。另外，如果您查看x_train的形状，您将看到它的形状为(60000, 28, 28)。自动编码器接受一维输入，因此我们需要将图像重塑为大小为784的一维向量。这两种转换可以通过以下行来实现:'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, reshape([-1, 784]) will unwrap the two-dimensional images (size 28 × 28)
    in the data set to a single dimensional vector (size 784). When reshaping, you
    do not need to provide all the dimensions of the reshaped tensor. If you provide
    the sizes of all dimensions except one, NumPy can still infer the size of the
    missing dimension as it knows the dimensions of the original tensor. The dimension
    that you want NumPy to infer is denoted by -1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，reshape([-1, 784])将数据集中的二维图像（大小为28×28）展开为一个单一维度的向量（大小为784）。在进行重塑时，您不需要提供重塑张量的所有维度。如果您仅提供除一个维度外的所有维度的大小，NumPy仍然可以推断出缺失维度的大小，因为它知道原始张量的维度。您希望NumPy推断的维度用-1表示。
- en: 'You might be wondering, “These images look crisp and clean. How on earth can
    we train our model to restore corrupted images?” That’s a very easy fix. All we
    need to do is synthesize a corresponding corrupted set of images from the original
    set of images. For that, we will define the generate_masked_inputs(...) function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想：“这些图像看起来清晰干净。我们如何训练模型来恢复损坏的图像？” 这很容易解决。我们只需要从原始图像中合成一组相应的损坏图像集。为此，我们将定义generate_masked_inputs(...)函数：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This function will set pixels randomly (with 50% probability) to zero. But
    let’s inspect what we are doing in more detail. First, we will give the option
    to set a random seed so that we can deterministically change the generated random
    masks. We are creating a mask of 1s and 0s using the binomial distribution, which
    is the same size as norm_x_train. In simple words, the binomial distribution represents
    the probability of heads (1) or tails (0) if you flip a coin several times. The
    binomial distribution has several important parameters:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将随机（有50%的概率）将像素设置为零。但让我们更详细地检查我们正在做什么。首先，我们将提供设置随机种子的选项，以便我们可以确定性地改变生成的随机掩码。我们使用二项分布创建一个与norm_x_train大小相同的1和0的掩码。简单来说，二项分布表示如果你多次抛硬币，出现正面（1）或反面（0）的概率。二项分布有几个重要参数：
- en: N—Number of trials
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N—试验次数
- en: P—Probability of a success (1)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: P—成功的概率（1）
- en: Size—The number of tests (i.e., trial sets)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Size—测试的数量（即，试验集）
- en: Here, we have x.shape tests and one trial in each with a 50% success probability.
    Then this mask is multiplied element-wise with the original tensor. This will
    result in black pixels randomly distributed over the image (figure 4.2).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有x.shape个测试，在每个测试中有一个50%的成功概率。然后将此掩码与原始张量进行逐元素相乘。这将导致随机分布在图像上的黑色像素（图4.2）。
- en: '![04-02](../../OEBPS/Images/04-02.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![04-02](../../OEBPS/Images/04-02.png)'
- en: Figure 4.2 Some of the synthetically corrupted images
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 一些合成损坏的图像
- en: Next, let’s discuss the fully connected network we’ll be implementing. It’s
    called an autoencoder model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们讨论我们将要实现的全连接网络。它被称为自动编码器模型。
- en: 4.1.2 Autoencoder model
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 自动编码器模型
- en: Both the autoencoder model and the multilayer perceptron (MLP) model (from chapter
    1) are FCNs. These are called FCNs because all the input nodes are connected to
    all the output nodes, in every layer of the network. Autoencoders operate in a
    similar way to the multilayer perceptron. In other words, the computations (e.g.,
    forward pass) you see in an autoencoder are exactly the same as in an MLP. However,
    the final objectives of the two are different. An MLP is trained to solve a supervised
    task (e.g., classifying flower species), whereas an autoencoder is trained to
    solve an unsupervised task (e.g., reconstructing the original image, given a corrupted/noisy
    image). Let’s now delve into what an autoencoder actually does.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器模型和多层感知机（MLP）模型（来自第1章）都是全连接网络（FCN）。之所以称为FCN，是因为网络中的每一层都将所有输入节点连接到所有输出节点。自动编码器的操作方式与多层感知机类似。换句话说，在自动编码器中看到的计算（例如，正向传播）与MLP中完全相同。然而，两者的最终目标不同。MLP被训练来解决监督任务（例如，分类花的品种），而自动编码器被训练来解决无监督任务（例如，在给定损坏/嘈杂图像的情况下重建原始图像）。现在让我们深入了解自动编码器实际上是做什么的。
- en: Supervised versus unsupervised learning
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习与无监督学习
- en: In supervised learning, a model is trained using a labeled data set. Each input
    (e.g., image/audio/movie review) has a corresponding label (e.g., object class
    for images, sentiment of the review) or continuous value(s) (e.g., bounding boxes
    of an object for images). Some examples of supervised tasks are image classification,
    object detection, speech recognition, and sentiment analysis.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，模型使用带标签的数据集进行训练。每个输入（例如，图像/音频/电影评论）都有一个相应的标签（例如，图像的对象类别、评论的情感）或连续值（例如，图像对象的边界框）。监督任务的一些示例包括图像分类、目标检测、语音识别和情感分析。
- en: In unsupervised learning, the models are trained using unlabeled data (e.g.,
    images/audio/text extracted from websites without any labeling). The training
    process varies significantly depending on the final expected outcome. For example,
    autoencoders are trained to reconstruct images as a pretraining step for an image-based
    supervised learning task. Some examples of unsupervised tasks are image reconstruction,
    image generation using generative adversarial networks, text clustering, and language
    modeling.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，模型使用未标记的数据进行训练（例如，从网站提取的没有任何标签的图像/音频/文本）。训练过程根据最终预期结果而显著变化。例如，自动编码器被训练为重建图像，作为基于图像的监督学习任务的预训练步骤。无监督任务的一些示例包括图像重构、使用生成对抗网络生成图像、文本聚类和语言建模。
- en: '![04-03](../../OEBPS/Images/04-03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![04-03](../../OEBPS/Images/04-03.png)'
- en: Figure 4.3 A simple autoencoder with one layer for compression and another layer
    for reconstruction. The black and white rectangles in the input image are the
    pixels present in the image.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 一个简单的自动编码器，其中一个层用于压缩，另一个层用于重构。输入图像中的黑色和白色矩形是图像中存在的像素。
- en: 'Figure 4.3 depicts a simple autoencoder with two layers. An autoencoder has
    two phases in its functionality:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3描绘了一个具有两层的简单自编码器。自编码器在其功能上有两个阶段：
- en: '*Compression phase*—Compresses a given image (i.e., the corrupted image) to
    a compressed hidden (i.e., latent) representation'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*压缩阶段*—将给定图像（即损坏的图像）压缩为压缩的隐藏（即潜在）表示。'
- en: '*Reconstruction phase*—Reconstructs the original image from the hidden representation'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*重构阶段*—从隐藏表示中重构原始图像。'
- en: In the compression phase, a compressed hidden representation is computed as
    follows
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩阶段，计算压缩的隐藏表示如下所示。
- en: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*h*[1] = *ReLU*(*xW*[1] + *b*[1])'
- en: where *W*[1],*b*[1] are the weights and biases of the first compression layer
    and *h*[1] is the final hidden representation of the layer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*W*[1]，*b*[1]是第一压缩层的权重和偏置，*h*[1]是层的最终隐藏表示。
- en: 'Similarly, we compute the output of the reconstruction layers:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们计算重构层的输出：
- en: '*ŷ* = *ReLU*(*h*[1] *W*[2] + *b*[2])'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *ReLU*(*h*[1] *W*[2] + *b*[2])'
- en: This is known as the forward pass, as you are going from the input to the output.
    Then you compute a loss (e.g., mean squared error [MSE]) between the expected
    output (i.e., target) and the prediction. For example, mean squared error for
    a single image is computed as
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为前向传播，因为您从输入到输出。然后，您计算预期输出（即目标）和预测之间的损失（例如，均方误差[MSE]）。例如，单个图像的均方误差计算为
- en: '![04_03a](../../OEBPS/Images/04_03a.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![04_03a](../../OEBPS/Images/04_03a.png)'
- en: where *D* is the dimensionality of the data (784 in our example), *y*[j] is
    the *j*^(th) pixel in our image, and (*ŷ*[j]) is the *j*^(th) pixel of the predicted
    image. We compute this loss for each batch of images and optimize the model parameters
    to minimize the computed loss. This is known as the backward pass.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*D*是数据的维度（在我们的示例中为784），*y*[j]是我们图像中的第*j*个像素，(*ŷ*[j])是预测图像的第*j*个像素。我们为每批图像计算此损失，并优化模型参数以最小化计算的损失。这被称为向后传递。
- en: You can have an arbitrary number of compression and reconstruction layers. In
    our assignment, we need to have two compression layers and two reconstruction
    layers (see the next listing).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以有任意数量的压缩和重构层。在我们的任务中，我们需要两个压缩层和两个重构层（见下一个列表）。
- en: Listing 4.1 The denoising autoencoder model
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 去噪自编码器模型。
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Defining four Dense layers, three with ReLU activation and one with tanh activation
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义四个稠密层，其中三个使用ReLU激活，一个使用tanh激活。
- en: ❷ Compiling the model with a loss function and an optimizer
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用损失函数和优化器编译模型。
- en: ❸ Printing the summary
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印摘要。
- en: 'Let’s go over what we did in more detail. The first thing you should notice
    is that we used the Keras Sequential API for this task. This makes sense as this
    is a very simple deep learning model. Next, we added four Dense layers. The first
    Dense layer takes an input with 784 features and produces a 64-elements-long vector.
    Then the second layer takes the 64-elements-long vector and produces a 32-elements-long
    vector. The third dense layer takes the 32-elements-long vector and produces a
    64-elements-long vector, passing it on to the final layer, which produces a 784-elements-long
    vector (i.e., size of the input). The first three layers have ReLU activation,
    and the last layer has a tanh activation, as the last layer needs to produce values
    between (-1, 1). Let’s remind ourselves how the ReLU and tanh activations are
    computed:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论我们所做的事情。您应该注意到的第一件事是，我们在这个任务中使用了Keras Sequential API。这是有道理的，因为这是一个非常简单的深度学习模型。接下来，我们添加了四个稠密层。第一个稠密层接受具有784个特征的输入，并产生一个64元素的向量。然后第二层接受64元素的向量并产生一个32元素的向量。第三个稠密层接受32元素的向量并产生一个64元素的向量，将其传递给最终层，该层产生一个784元素的向量（即输入的大小）。前三层使用ReLU激活，最后一层使用tanh激活，因为最后一层需要产生在(-1,
    1)之间的值。让我们再次提醒自己如何计算ReLU和tanh激活：
- en: '*ReLU*(*x*) = max (0, *x*)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*ReLU*(*x*) = max (0, *x*)'
- en: '![04_03b](../../OEBPS/Images/04_03b.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![04_03b](../../OEBPS/Images/04_03b.png)'
- en: 'Finally, we compile the model using the mean squared error as the loss function
    and adam as the optimizer. The model we just described has the specifications
    we defined at the beginning of the section. With the model defined, you can now
    train the model. You will train the model for 10 epochs with batches of size 64:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用均方误差作为损失函数，使用adam作为优化器编译模型。我们刚刚描述的模型具有我们在本节开头定义的规格。有了定义好的模型，现在您可以训练模型了。您将使用64个大小的批次训练模型10个时期：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The masked inputs we generated become the input, and the original images will
    be the ground truth. When you train the model, you will see a loss that goes down
    over time:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的遮罩输入成为输入，原始图像将成为地面真相。当你训练模型时，你会看到随时间推移损失下降：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It seems the error (i.e., loss value) has gone down from approximately 0.15
    to roughly 0.078\. This is a strong indication that the model is learning to reconstruct
    images. You can get similar results by setting the seed using the fix_random_seed(...)
    function we used in chapter 2 (provided in the notebook). Note that for this task
    we cannot define a metric like accuracy, as it is an unsupervised task.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来误差（即，损失值）从大约 0.15 下降到大约 0.078。这是模型正在学习重建图像的一个强有力的指示。你可以通过设置种子来获得类似的结果，使用我们在第二章中使用的
    fix_random_seed(...) 函数（提供在笔记本中）。请注意，对于这个任务，我们无法定义像准确度这样的指标，因为这是一个无监督的任务。
- en: Denoising autoencoders
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自动编码器
- en: 'Normally an autoencoder maps a given input to a small latent space and then
    back to the original input space to reconstruct the original images. However,
    here we use the autoencoder for a special purpose: to restore original images
    or denoise original images. Such autoencoders are known as *denoising*. Read more
    about denoising autoencoders at [http://mng.bz/WxyX](http://mng.bz/WxyX).'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自动编码器将给定的输入映射到一个小的潜在空间，然后再返回到原始输入空间以重建原始图像。然而，在这里，我们将自动编码器用于一个特殊目的：还原原始图像或去噪原始图像。这样的自动编码器被称为
    *去噪*。在[http://mng.bz/WxyX](http://mng.bz/WxyX)上阅读更多关于去噪自动编码器的信息。
- en: 'Let’s now see what the trained model can do! It should now be able to decently
    restore an image of a corrupted digit. And to make things interesting, let’s make
    sure we generate a mask that the training data has not seen:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看训练好的模型能做什么！它现在应该能够还原一个受损数字的图像了。为了让事情变得有趣，让我们确保我们生成的遮罩是训练数据没有见过的：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we will be using the first 10 images in the data set to test out the model
    we just trained. However, we are making sure that the random mask is different
    by changing the seed. You can display some information about y_pred using
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用数据集中的前 10 张图像来测试我们刚刚训练的模型。然而，我们通过更改种子确保了随机遮罩不同。你可以使用以下代码显示关于 y_pred
    的一些信息
- en: '[PRE11]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: which will give
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将会给出
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finally, you can visualize what the model does by plotting the images (the code
    provided in the notebook). Figure 4.4 illustrates the corrupted images (top row)
    and the outputs of the model (bottom row). Though you are not yet restoring real-world
    photos of your grandmother, this a great start, as you now know the approach to
    follow.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以通过绘制图像来可视化模型的作用（在笔记本中提供的代码）。图 4.4 说明了损坏的图像（顶行）和模型的输出（底行）。虽然你还没有恢复你祖母的真实照片，但这是一个很好的开始，因为现在你知道了要遵循的方法。
- en: '![04-04](../../OEBPS/Images/04-04.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![04-04](../../OEBPS/Images/04-04.png)'
- en: Figure 4.4 Images restored by the model. It seems our model is doing a good
    job.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 模型恢复的图像。看起来我们的模型做得很好。
- en: You might be wondering, “What do autoencoders help you to achieve in general?”
    Autoencoders are a great tool for learning unsupervised features from unlabeled
    data, which is handy when solving more interesting downstream tasks like image
    classification. When autoencoders are trained on an unsupervised task, they learn
    useful features for other tasks (e.g., image classification). Therefore, training
    an autoencoder model to classify images will get you to a well-performing model
    faster and with less labeled data than training a model from scratch. As you are
    probably aware, there’s much more unlabeled data in the world than labeled data,
    as labeling usually requires human intervention, which is time-consuming and expensive.
    Another use of autoencoders is that the hidden representation it produces can
    be used as a low-dimensional proxy to cluster the images.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，“自动编码器通常能帮你实现什么？”自动编码器是从未标记数据中学习无监督特征的好工具，这在解决更有趣的下游任务时非常方便，比如图像分类。当自动编码器在无监督任务上进行训练时，它们学习了其他任务（例如，图像分类）的有用特征。因此，训练一个自动编码器模型来对图像进行分类将比从头开始训练模型更快地获得性能良好的模型，并且所需的标记数据更少。正如你可能知道的，世界上的未标记数据要比标记数据多得多，因为标记通常需要人为干预，这是耗时且昂贵的。自动编码器的另一个用途是它产生的隐藏表示可以用作聚类图像的低维代理。
- en: In this section, you learned about the autoencoder model, which is a type of
    FCN and is used to reconstruct/restore damaged images in an unsupervised manner.
    This is a great way to leverage copious amounts of unlabeled data to pretrain
    models, which becomes useful in more downstream interesting tasks (e.g., image
    classification). You first learned the architecture and then how to implement
    an autoencoder model with the Keras Sequential API. Finally, you trained the model
    on a hand-written image data set (MNIST) to reconstruct the images in the data
    set. During the training process, to ensure the model was learning, you monitored
    the loss to make sure it decreased over time. Finally, you used the model to predict
    restorations of corrupted images and ensured the model was performing well.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了自动编码器模型，它是一种 FCN 类型，用于以无监督的方式重构/恢复损坏的图像。这是一种利用大量未标记数据来预训练模型的好方法，这在更下游的有趣任务（如图像分类）中非常有用。你首先学习了架构，然后学习了如何使用
    Keras Sequential API 实现自动编码器模型。最后，你对手写图像数据集（MNIST）进行了模型训练以重构数据集中的图像。在训练过程中，为了确保模型在学习，你监控了损失以确保随着时间的推移减少。最后，你使用模型预测了损坏图像的恢复，并确保模型表现良好。
- en: 'In the next section, we will discuss a different type of deep learning network
    that has revolutionized the field of computer vision: CNNs.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论一种不同类型的深度学习网络，它彻底改变了计算机视觉领域：CNN。
- en: Exercise 1
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 1
- en: Implement an autoencoder model that takes in a 512-elements-long vector. The
    network has a 32-node layer, a 16-node layer, and finally an output layer. In
    total, there are three layers. All these layers have the sigmoid activation.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实现一个接受 512 元素长向量的自动编码器模型。网络有一个 32 节点层，一个 16 节点层，最后是一个输出层。总共有三层。所有这些层都具有 sigmoid
    激活。
- en: 4.2 Convolutional neural networks
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 卷积神经网络
- en: You have been working at a startup as a data scientist trying to model traffic
    congestion on the road. One important model in the company’s solution is building
    a model to predict whether a vehicle is present, given a patch or image, as a
    part of a larger plan. You plan to develop a model first on the cifar-10 data
    set and see how well it classifies vehicles. This is a great idea, as it will
    give a rough approximation of the feasibility of the idea while spending minimal
    time and money on labeling custom data. If we can achieve good accuracy on this
    data set, that is a very positive sign. You have learned that CNNs are great for
    computer vision tasks. So, you are planning to implement a CNN.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你一直在一家初创公司担任数据科学家，试图对道路上的交通拥堵建模。公司解决方案中的一个重要模型是构建一个模型，以预测在给定的图像或图像块中是否存在车辆，作为更大计划的一部分。你计划首先在
    cifar-10 数据集上开发一个模型，并查看它在分类车辆方面的效果如何。这是一个很好的主意，因为它将在最小的时间和金钱上为自定义数据标记提供一个粗略的近似值。如果我们在这个数据集上能够达到较高的准确度，那是一个非常积极的信号。你了解到
    CNN 对于计算机视觉任务非常有效。因此，你计划实现一个 CNN。
- en: 4.2.1 Understanding the data
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 理解数据
- en: We will use is the cifar-10 data set. We briefly looked at this data set in
    the previous chapter, and it is a great cornerstone for this task. It has various
    vehicles (e.g., automobile, truck) and other objects (e.g., dog, cat) as classes.
    Figure 4.5 illustrates some of the classes and corresponding samples for them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用的是 cifar-10 数据集。我们在上一章节简要地查看过这个数据集，它是这项任务的重要基石。它包含各种交通工具（如汽车、卡车）和其他物体（如狗、猫）作为类别。图
    4.5 展示了一些类别及其对应的样本。
- en: '![04-05](../../OEBPS/Images/04-05.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![04-05](../../OEBPS/Images/04-05.png)'
- en: Figure 4.5 Sample images from cifar-10 data set along with their labels
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 cifar-10 数据集的样本图像及其标签
- en: The data set consists of 50,000 training instances and 10,000 testing instances.
    Each instance is a 32 × 32 RGB image. There are 10 different classes of objects
    in this data set.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 50,000 个训练实例和 10,000 个测试实例。每个实例是一个 32 × 32 的 RGB 图像。这个数据集中有 10 个不同的对象类别。
- en: 'Let’s first load the data by executing the following line:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过执行以下行来加载数据：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: print(data) will yield
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 print(data) 将产生
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you explore the data a bit, you will realize that
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微探索一下数据，你会意识到
- en: Images are provided with the data type as unsigned eight-bit integers.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像以无符号的八位整数类型提供。
- en: Labels are provided as integer labels (i.e., not one-hot encoded).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签以整数标签提供（即，未进行 one-hot 编码）。
- en: 'Therefore, we will write a very simple function to convert the images to data
    type float32 (to make the data type consistent with the model parameters) and
    labels to one-hot encoded vectors:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将编写一个非常简单的函数，将图像转换为float32数据类型（使数据类型与模型参数一致），并将标签转换为独热编码向量：
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we will create a batched data set by applying this function to all
    the training data:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过将此函数应用于所有训练数据来创建一个批处理数据集：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can again look at the data with
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次查看数据
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: which will produce
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now our data is ready to be fed to a model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据准备好输入模型了。
- en: 4.2.2 Implementing the network
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 实现网络
- en: 'To classify these images, we will employ a CNN. CNNs have gained a stellar
    reputation for solving computer vision tasks and are a popular choice for image-related
    tasks for two main reasons:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对这些图像进行分类，我们将采用CNN。CNN以解决计算机视觉任务而闻名，并且是处理图像相关任务的流行选择，原因有两个主要方面：
- en: CNNs process the images while preserving their spatial information (i.e., while
    keeping the height and width dimensions as is), while a fully connected layer
    will need to unwrap the height and width dimensions to a single dimension, losing
    precious locality information.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN在处理图像时保留它们的空间信息（即保持高度和宽度维度不变），而全连接层则需要将高度和宽度维度展开为一个单一维度，从而丢失宝贵的局部信息。
- en: Unlike a fully connected layer where every input is connected to every output,
    the convolution operation shifts a smaller kernel over the entire image, demanding
    only a handful of parameters in a layer, making CNNs very parameter efficient.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同于全连接层，其中每个输入都连接到每个输出，卷积操作将一个较小的核移动到整个图像上，每层只需少量参数，使得CNN非常高效。
- en: 'A CNN consists of a set of interleaved convolution and pooling layers followed
    by several fully connected layers. This means there are three main types of layers
    in a CNN:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CNN由一组交错的卷积和池化层以及几个全连接层组成。这意味着CNN中有三种主要类型的层：
- en: Convolution layers
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Pooling layers
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Fully connected layers
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全连接层
- en: A convolution layer consists of several filters (i.e., convolution kernels)
    that are convolved over the image to produce a *feature map*. The feature map
    is a representation of how strongly a given filter is present in the image. For
    example, if the filter represents a vertical edge, the feature map represents
    where (and how strongly) in the image vertical edges are present. As another example,
    think of a neural network that is trained to identify faces. A filter might represent
    the shape of an eye and activate the corresponding area of the output highly when
    an eye is present in a given image (figure 4.6). We will discuss the convolution
    operation in more detail later in the chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个卷积层由多个滤波器（即卷积核）组成，这些滤波器在图像上进行卷积以生成*特征图*。特征图是一个表示给定滤波器在图像中存在程度的表示。例如，如果滤波器表示垂直边缘，则特征图表示图像中垂直边缘存在的位置（以及强度）。再举一个例子，想象一个训练用于识别人脸的神经网络。一个滤波器可能表示眼睛的形状，并且在给定图像中存在眼睛时会高度激活相应区域的输出（见图4.6）。我们将在本章后面更详细地讨论卷积操作。
- en: '![04-06](../../OEBPS/Images/04-06.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![04-06](../../OEBPS/Images/04-06.png)'
- en: Figure 4.6 The result of a convolution operation at a very abstract level. If
    we have an image of a human face and a convolution kernel that represents the
    shape/color of an eye, then the convolution result can be roughly thought of as
    a heatmap of where that feature (i.e., the eyes) are present in the image.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 卷积操作的结果，非常抽象。如果我们有一个人脸图像和一个表示眼睛形状/颜色的卷积核，那么卷积结果可以粗略地被认为是该特征（即眼睛）在图像中存在的热图。
- en: Another important characteristic of the convolution layers is that the deeper
    you go in the network (i.e., further away from the input), the more high-level
    features the layers learn. Going back to our face recognition example, the lower
    layers might learn various edges present; the next layer, the shape of an eye,
    ear, and a nose; the next layer, how two eyes are positioned, the alignment of
    the nose and mouth; and so on (figure 4.7).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的另一个重要特性是，网络越深（即离输入越远），层学习的高级特征就越多。回到我们的人脸识别例子，较低的层可能学习到各种边缘的存在；下一层学习到眼睛、耳朵和鼻子的形状；下一层学习到两只眼睛的位置、鼻子和嘴巴的对齐等（见图4.7）。
- en: '![04-07](../../OEBPS/Images/04-07.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![04-07](../../OEBPS/Images/04-07.png)'
- en: 'Figure 4.7 Features learned by a convolutional neural network. The lower layers
    (closest to the input) are learning edges/lines, whereas the upper layers (furthest
    from input) are learning higher-level features. (Source: [http://mng.bz/8MPg](http://mng.bz/8MPg))'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 卷积神经网络学习到的特征。较低的层（离输入最近）学习到的是边缘/线条，而较高的层（离输入最远）学习到的是更高级的特征。（来源：[http://mng.bz/8MPg](http://mng.bz/8MPg)）
- en: Next, the pooling layer takes in feature maps generated by a convolution layer
    and reduces their height and width dimensions. Why is it useful to reduce the
    height and width of the feature maps? It helps the model be translation invariant
    during the machine learning task. For instance, if the task is image classification,
    even if the objects appear several pixels offset from what was seen during training,
    the network is still able to identify the object.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，池化层接收卷积层生成的特征图并减少它们的高度和宽度维度。为什么减少特征图的高度和宽度有用？它帮助模型在机器学习任务中具有平移不变性。例如，如果任务是图像分类，即使物体在训练期间看到的几个像素偏移，该网络仍然能够识别出物体。
- en: Finally, to get the final probability distribution, you have several fully connected
    layers. But you might have suspected an issue we face here. A convolution/pooling
    layer produces a three-dimensional output (i.e., height, width, and channel dimensions).
    But a fully connected layer accepts a one-dimensional input. How do we connect
    the three-dimensional output of a convolution/pooling layer to a one-dimensional
    fully connected layer? There’s a simple answer to this problem. You squash all
    three dimensions into a single dimension. In other words, it is analogous to unwrapping
    a two-dimensional RGB image to a one-dimensional vector. This provides the fully
    connected layer with a one-dimensional input. Finally, a softmax activation is
    applied to the outputs of the final fully connected layer (i.e., scores of the
    network) to obtain a valid probability distribution. Figure 4.8 depicts a simple
    CNN.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，为了获得最终的概率分布，你有多个全连接层。但你可能已经怀疑我们在这里面临的问题。卷积/池化层产生三维输出（即高度、宽度和通道维度）。但全连接层接受一维输入。我们如何将卷积/池化层的三维输出连接到一维的全连接层呢？这个问题有一个简单的答案。你将所有三个维度压缩成一个维度。换句话说，这类似于将二维的
    RGB 图像展开成一维向量。这为全连接层提供了一维输入。最后，对最后一个全连接层的输出（即网络的得分）应用 softmax 激活，以获得有效的概率分布。图
    4.8 描述了一个简单的 CNN。
- en: '![04-08](../../OEBPS/Images/04-08.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![04-08](../../OEBPS/Images/04-08.png)'
- en: Figure 4.8 A simple CNN. First, we have an image with height, width, and channel
    dimensions, followed by a convolution and pooling layer. Finally, the last convolution/pooling
    layer output is flattened and fed to a set of fully connected layers.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 一个简单的 CNN。首先，我们有一个带有高度、宽度和通道维度的图像，然后是一个卷积和池化层。最后，最后一个卷积/池化层的输出被展平，并输入到一组全连接层中。
- en: With a good understanding of what a CNN comprises, we will create the following
    CNN using the Keras Sequential API. However, if you run this code, you will get
    an error. We will investigate and fix this error in the coming sections (see the
    next listing).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过深入了解 CNN 的构成，我们将使用 Keras Sequential API 创建以下 CNN。然而，如果你运行此代码，你将会收到一个错误。我们将在接下来的部分调查并修复这个错误（参见下一个列表）。
- en: Listing 4.2 Defining a CNN with the Keras Sequential API
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.2 使用 Keras Sequential API 定义 CNN
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Clearing any existing Keras states (e.g., models) to start fresh
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清除任何现有的 Keras 状态（例如模型）以重新开始
- en: ❷ Defining a convolution layer; it takes parameters like filters, kernel_size,
    strides, activation, and padding.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义卷积层；它接受过滤器、内核大小、步幅、激活和填充等参数。
- en: ❸ Before feeding the data to a fully connected layer, we need to flatten the
    output of the last convolution layer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在将数据输入全连接层之前，我们需要展平最后一个卷积层的输出。
- en: ❹ Creating an intermediate fully connected layer
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建一个中间的全连接层
- en: ❺ Final prediction layer
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 最终预测层
- en: 'You can see that the network consists of three convolution layers and two fully
    connected layers. Keras provides all the layers you need to implement a CNN. As
    you can see, it can be done in a single line of code for our image classification
    network. Let’s explore what is happening in this model in more detail. The first
    layer is specified as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，该网络由三个卷积层和两个全连接层组成。Keras 提供了你实现 CNN 所需的所有层。如你所见，我们的图像分类网络只需一行代码即可完成。让我们更详细地探索一下这个模型中发生了什么。第一层定义如下：
- en: '[PRE20]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Hyperparameters of convolutional neural networks
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络的超参数
- en: In the CNN network from listing 4.2, filters, kernel_size, and strides of the
    Conv2D layers, the number of hidden units in the Dense layers (except the output
    layer) and the activation function are known as the hyperparameters of the model.
    Ideally, these hyperparameters need to be selected using a hyperparameter optimization
    algorithm, which would run hundreds (if not thousands) of models with different
    hyperparameter values and choose the one that maximizes a predefined metric (e.g.,
    model accuracy). However, here we have chosen the values for these hyperparameters
    empirically and will not be using hyperparameter optimization.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表4.2中的CNN网络中，Conv2D层的filters、kernel_size和strides，Dense层（除了输出层）中的隐藏单元数以及激活函数被称为模型的超参数。理想情况下，这些超参数需要使用超参数优化算法进行选择，该算法会运行数百（如果不是数千）个具有不同超参数值的模型，并选择最大化预定义度量（例如，模型准确性）的那个。然而，这里我们已经根据经验选择了这些超参数的值，并且不会使用超参数优化。
- en: 'First, the Conv2D layer is the Keras implementation of the 2D convolution operation.
    As you’ll remember from chapter 1, we achieved this using the tf.nn.convolution
    operation. The Conv2D layer executes the same functionality under the hood. However,
    it hides some of the complexities met when using the tf.nn.convolution operation
    directly (e.g., defining the layer parameters explicitly) There are several important
    arguments you need to provide to this layer:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Conv2D层是2D卷积操作的Keras实现。正如您在第1章中记得的那样，我们使用了tf.nn.convolution操作来实现这一点。Conv2D层在幕后执行相同的功能。但是，它隐藏了一些直接使用tf.nn.convolution操作时遇到的复杂性（例如，显式定义层参数）。您需要为这一层提供几个重要的参数：
- en: filters—The number of output channels that will be present in the output.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤器—输出中将存在的通道数。
- en: kernel_size—The convolution window size on the height and width dimensions,
    in that order.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核大小—高度和宽度维度上的卷积窗口大小，按顺序。
- en: strides—Represents how many pixels are skipped on height and with dimensions
    (in that order) every time the convolution window shifts on the input. Having
    a higher value here helps to reduce the size of the convolution output quickly
    as you go deeper.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长—表示在每次卷积窗口在输入上移动时，跳过的高度和宽度像素数量（按顺序）。在这里有较高的值有助于随着深入，快速减小卷积输出的尺寸。
- en: activation—The nonlinear activation of the convolution layer.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活—卷积层的非线性激活。
- en: padding—Type of padding used for the border while performing the convolution
    operation. Padding borders gives more control over the size of the output.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充—在执行卷积操作时用于边界的填充类型。填充边界可以更好地控制输出的大小。
- en: input_shape—A three-dimensional tuple representing the input size on (height,
    width, channels) dimensions, in that order. Remember that Keras adds an unspecified
    batch dimension automatically when specifying the shape of the data using this
    argument.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: input_shape—表示（高度，宽度，通道）维度上的输入大小的三维元组。请记住，当使用此参数指定数据的形状时，Keras会自动添加一个未指定的批量维度。
- en: Let’s now go over the convolution function and its parameters in more detail.
    We already know that the convolution operation shifts a convolution window (i.e.,
    a kernel) over the image, while taking the sum of an element-wise product between
    the kernel and the portion of the image that overlaps the kernel at a given time
    (figure 4.9). Mathematically, the convolution operation can be stated as follows
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地介绍卷积函数及其参数。我们已经知道，卷积操作将一个卷积窗口（即一个核）在图像上移动，同时取得与图像部分与核重叠的元素的乘积之和（图4.9）。从数学上讲，卷积操作可以表示为
- en: '![04_08a](../../OEBPS/Images/04_08a.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![04_08a](../../OEBPS/Images/04_08a.png)'
- en: where *x* is a *n* × *n* input matrix, *f* is a *m* × *m* filter, and *y* is
    the output.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*x*是*n* × *n*的输入矩阵，*f*是*m* × *m*的过滤器，*y*是输出。
- en: '![04-09](../../OEBPS/Images/04-09.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![04-09](../../OEBPS/Images/04-09.png)'
- en: Figure 4.9 The computations that happen in the convolution operation while shifting
    the window
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 在移动窗口时进行卷积操作的计算
- en: 'Apart from the computations that take place during the convolution operation,
    there are four important hyperparameters that affect the size and values produced
    when using the Conv2D layer in Keras:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了卷积操作期间发生的计算外，在使用Keras中的Conv2D层时产生的大小和值时，还有四个重要的超参数：
- en: Number of filters
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滤波器数量
- en: Kernel height and width
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核高度和宽度
- en: Kernel stride (height and width)
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核步长（高度和宽度）
- en: Type of padding
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充类型
- en: The first aspect we will discuss is the number of filters in the layer. Typically,
    a single convolution layer has multiple filters. For example, think of a neural
    network that is trained to identify faces. One of the layers in the network might
    learn to identify the shape of an eye, shape of a nose, and so on. Each of these
    features might be learned by a single filter in the layer.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个方面是层中滤波器的数量。通常，单个卷积层有多个滤波器。例如，想象一个训练用于识别人脸的神经网络。网络中的一个层可能会学习识别眼睛的形状，鼻子的形状等等。每个这些特征可能由层中的单个滤波器学习。
- en: The convolution layer takes an image, which is a three-dimensional tensor of
    some height, width, and channels. For example, if the image is an RGB image, there
    will be three channels. If the image is a grayscale image, the number of channels
    will be one. Then, convolving this tensor with n number of filters will result
    in a three-dimensional output of some height, width, and n channels. This is shown
    in figure 4.10\. When used in a CNN, the filters are the parameters of a convolution
    layer. These filters are initialized randomly, and over time they evolve to become
    meaningful features that help solve the task at hand.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层接收一个图像，这是一个具有某些高度、宽度和通道的三维张量。例如，如果图像是 RGB 图像，则会有三个通道。如果图像是灰度图像，则通道数将为一。然后，将该张量与
    n 个滤波器卷积将导致一个具有某些高度、宽度和 n 个通道的三维输出。这在图 4.10 中显示。在 CNN 中使用时，滤波器是卷积层的参数。这些滤波器被随机初始化，随着时间的推移，它们会演变成有助于解决手头任务的有意义特征。
- en: As we have said before, deep neural networks process data in batches. CNNs are
    no exception. You can see that we have set the input_shape parameter to (32, 32,
    3), where an unspecified batch dimension is automatically added, making it (None,
    32, 32, 3). The unspecified dimension is denoted by None, and it means that the
    model can take any arbitrary number of items on that dimension. This means that
    a batch of data can have 3, 4, 100, or any number of images (as the computer memory
    permits) at run time while feeding data to the model. Therefore, the input/output
    of a Conv2D layer is, in fact, a four-dimensional tensor with a batch, height,
    width, and channel dimension. Then the filters will be another four-dimensional
    tensor with a kernel height, width, incoming channel, and outgoing channel dimension.
    Table 4.1 summarizes this information.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，深度神经网络以批量方式处理数据。CNN也不例外。您可以看到，我们将 input_shape 参数设置为 (32, 32, 3)，其中自动添加了一个未指定的批量维度，使其为
    (None, 32, 32, 3)。未指定的维度用 None 表示，意味着模型可以在该维度上取任意数量的项目。这意味着在向模型提供数据时，一个数据批次可以有
    3、4、100 或任意数量的图像（根据计算机内存的情况）。因此，Conv2D 层的输入/输出实际上是一个四维张量，具有批量、高度、宽度和通道维度。然后，滤波器将是另一个四维张量，具有核高度、宽度、输入通道和输出通道维度。表
    4.1 总结了这些信息。
- en: Table 4.1 The dimensionality of the input, filters, and the output of a Conv2D
    layer
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 卷积层的输入、滤波器和输出的维度
- en: '|  | **Dimensionality** | **Example** |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | **维度** | **示例** |'
- en: '| Input | [batch size, height, width, in channels] | [32, 64, 64, 3] (i.e.,
    a batch of 32, 64 × 64 RGB images) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | [批量大小，高度，宽度，输入通道] | [32, 64, 64, 3]（即，一批 32 个，64 × 64 的 RGB 图像） |'
- en: '| Convolution filters | [height, width, in channels, out channels] | [5, 5,
    3, 16] (i.e., 16 convolution filters of size 5 × 5 with 3 incoming channels) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 卷积滤波器 | [高度，宽度，输入通道，输出通道] | [5, 5, 3, 16]（即，大小为 5 × 5 的 16 个输入通道的卷积滤波器） |'
- en: '| Output | [batch size, height, width, out channels] | [32, 64, 64, 16] (i.e.,
    a batch of 32, 64 × 64 × 16 tensors) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 输出 | [批量大小，高度，宽度，输出通道] | [32, 64, 64, 16]（即，一批 32 个，64 × 64 × 16 的张量） |'
- en: Figure 4.10 depicts how the inputs and outputs look in a convolution layer.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 描述了卷积层中输入和输出的外观。
- en: '![04-10](../../OEBPS/Images/04-10.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![04-10](../../OEBPS/Images/04-10.png)'
- en: Figure 4.10 A computation of a convolution layer with multiple filters (randomly
    initialized). We left the batch dimension of the tensor representation to avoid
    clutter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 多个滤波器（随机初始化）的卷积层的计算。我们保留了张量表示的批量维度以避免混乱。
- en: 'Next, kernel height and width are the size of the filter on height and width
    dimensions. Figure 4.11 depicts how different kernel sizes lead to different outputs.
    Typically, when implementing CNNs, we keep kernel height and width equal. With
    that, we will refer to both the height and width dimensions of the kernel generally
    as the *kernel size*. We can compute the output size as a function of the kernel
    and input size as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，内核的高度和宽度是在高度和宽度维度上的滤波器大小。图 4.11 描述了不同内核大小导致不同输出的情况。通常，在实现 CNN 时，我们保持内核的高度和宽度相等。因此，我们将内核的高度和宽度维度统称为*内核大小*。我们可以将输出大小计算为内核和输入大小的函数，如下所示：
- en: '*size*(*y*) = *size*(*x*) - *size*(*f*) + 1'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*size*(*y*) = *size*(*x*) - *size*(*f*) + 1'
- en: For example, if the image is a 7 × 7 matrix and the filter is a 3 × 3 matrix,
    then the output will be a (7 - 3 + 1, 7 - 3 + 1) = 5 × 5 matrix. Or, if the image
    is a 7 × 7 matrix and the filter is a 5 × 5 matrix, then the output will be a
    3 × 3 matrix.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果图像是一个 7 × 7 的矩阵，滤波器是一个 3 × 3 的矩阵，那么输出将是一个 (7 - 3 + 1, 7 - 3 + 1) = 5 ×
    5 的矩阵。或者，如果图像是一个 7 × 7 的矩阵，滤波器是一个 5 × 5 的矩阵，那么输出将是一个 3 × 3 的矩阵。
- en: '![04-11](../../OEBPS/Images/04-11.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![04-11](../../OEBPS/Images/04-11.png)'
- en: Figure 4.11 Convolution operation with a kernel size of 2 and kernel size of
    3\. Increasing the kernel size leads to a reduced output size.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 使用 2 和 3 的内核大小的卷积操作。增加内核大小会导致减小输出大小。
- en: From a modeling perspective, increasing the kernel size (i.e., filter size)
    translates to an increased number of parameters. Typically, you should try to
    reduce the number of parameters in your network and target smaller-sized kernels.
    Having small kernel sizes encourages the model to learn more robust features with
    a small number of parameters, leading to better generalization of the model.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从建模的角度来看，增加内核大小（即滤波器大小）意味着增加参数的数量。通常，您应该尝试减少网络中的参数数量并针对较小的内核大小。使用小内核大小鼓励模型使用少量参数学习更健壮的特征，从而更好地泛化模型。
- en: 'The next important parameter is the stride. Just like the kernel size, the
    stride has two components: height and width. Intuitively, the stride defines how
    many pixels/ values you skip while shifting the convolution operation. Figure
    4.12 illustrates the difference between having stride = 1 (i.e., no stride versus
    stride = 2). As before, we can specify the output size as a function of the input
    size, kernel size, and stride:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个重要参数是步幅。与内核大小类似，步幅有两个组成部分：高度和宽度。直觉上，步幅定义了在进行卷积操作时跳过多少像素/值。图 4.12 说明了没有步幅和步幅
    = 2 之间的区别。与之前一样，我们可以将输出大小指定为输入大小、内核大小和步幅的函数：
- en: '![04_11a](../../OEBPS/Images/04_11a.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![04_11a](../../OEBPS/Images/04_11a.png)'
- en: '![04-12](../../OEBPS/Images/04-12.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![04-12](../../OEBPS/Images/04-12.png)'
- en: Figure 4.12 Convolution operation with stride = 1 (i.e., no stride) versus stride
    = 2\. An increased stride leads to a smaller output.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 步幅为 1（即无步幅）与步幅为 2 的卷积操作。增加步幅会导致较小的输出。
- en: From a modeling perspective, striding is beneficial, as it helps you to control
    the amount of reduction you need in the output. You might have noticed that, even
    without striding, you still get an automatic dimensionality reduction during convolution.
    However, when using striding, you can control the reduction you want to gain without
    affecting the kernel size.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从建模的角度来看，步幅是有益的，因为它帮助您控制输出中需要减少的量。您可能已经注意到，即使在没有步幅的情况下，卷积过程中仍会自动减少维度。但是，当使用步幅时，您可以控制要获得的减少而不影响内核大小。
- en: Finally, padding decides what happens near the borders of the image. As you
    have already seen, when you convolve an image, you don’t get a same-sized output
    as the input. For example, if you have a 4 × 4 matrix and a 2 × 2 kernel, you
    get a 3 × 3 output (i.e., following the equation *size*(*y*) = *size*(*x*) - *size*(*f*
    ) + 1 we saw earlier, where *x* is the input size and *f* is the filter size).
    This automatic dimensionality reduction creates an issue when creating deep models.
    Specifically, it limits the number of layers you can have, as at some point the
    input will become a 1 × 1 pixel due to this automatic dimension reduction. Consequentially,
    this will create a very narrow bottleneck in passing information to the fully
    connected layers that follow, causing massive information loss.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，填充决定了图像边界附近发生的情况。正如你已经看到的，当你对图像进行卷积时，你得不到与输入尺寸相同的输出。例如，如果你有一个4 × 4的矩阵和一个2
    × 2的核，你会得到一个3 × 3的输出（即，根据我们之前看到的方程*size*(*y*) = *size*(*x*) - *size*(*f* ) + 1，其中*x*是输入尺寸，*f*是滤波器尺寸）。这种自动降维会在创建深度模型时产生问题。具体来说，它限制了你可以拥有的层数，因为在某些时候，输入会由于这种自动尺寸减小而变成1
    × 1像素。因此，这将在将信息传递给随后的全连接层时创建一个非常窄的瓶颈，导致大量信息丢失。
- en: 'You can use padding to alleviate this issue. With padding, you create an imaginary
    border of zeros around the image, such that you get the same-sized output as the
    input. More specifically, you append a border that is a *size*(*f* ) - 1-thick
    border of zeros in order to get an output of same size as the input. For example,
    if you have an input of size 4 × 4 and a kernel of size 2 × 2, then you would
    apply a border of size 2 - 1 = 1 vertically and horizontally. This means that
    the kernel is essentially processing a 5 × 5 input (i.e., (4 + 1) × (4 + 1)-sized
    input), resulting in a 4 × 4-sized output. This is called *same padding*. Note
    that it does not always have to be zeros that you are padding. Though currently
    not supported in Keras, there are different padding strategies (some examples
    are available here: [https://www.tensorflow.org/api_docs/python/tf/pad](https://www.tensorflow.org/api_docs/python/tf/pad)),
    such as padding with'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用填充来缓解这个问题。通过填充，你在图像周围创建一个零边框，以便获得与输入相同大小的输出。更具体地说，你在周围附加一个大小为*size*(*f*
    ) - 1的零边框，以获得与输入相同大小的输出。例如，如果你有一个大小为4 × 4的输入和一个大小为2 × 2的核，那么你将垂直和水平应用大小为2 - 1
    = 1的边框。这意味着核实际上正在处理一个5 × 5的输入（即，(4 + 1) × (4 + 1)-大小的输入），结果是一个4 × 4的输出。这被称为*same
    padding*。注意，你填充的不总是零。虽然目前Keras不支持，但有不同的填充策略（一些示例可以在这里找到：[https://www.tensorflow.org/api_docs/python/tf/pad](https://www.tensorflow.org/api_docs/python/tf/pad)），例如填充
- en: A constant value
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常量值
- en: A reflection of the input
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入的反射
- en: The nearest value
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最近的值
- en: If you don’t apply padding, that is called *valid padding*. Not applying padding
    leads to the standard convolution operation we discussed earlier. The differences
    in padding are shown in figure 4.13.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不应用填充，那就是*valid padding*。不应用填充会导致我们之前讨论过的标准卷积操作。填充的差异如图4.13所示。
- en: '![04-13](../../OEBPS/Images/04-13.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![04-13](../../OEBPS/Images/04-13.png)'
- en: Figure 4.13 Valid versus same padding. Valid padding leads to a reduced output
    size, whereas same padding results in an output with equal dimensions to the input.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 有效填充与相同填充。有效填充导致输出尺寸减小，而相同填充导致输出与输入尺寸相等。
- en: 'With that, we conclude our discussion about various hyperparameters of the
    Conv2D layer. Now let’s circle back to the network we implemented. Unfortunately
    for you, if you try to run the code we discussed, you will be presented with a
    somewhat cryptic error like this:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个，我们结束了对Conv2D层的各种超参数的讨论。现在让我们回到我们实现的网络。不幸的是，如果你尝试运行我们讨论过的代码，你会看到一个有些晦涩的错误，就像这样：
- en: '[PRE21]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'What have we done wrong here? It seems TensorFlow is complaining about a negative
    dimension size while trying to compute the output of a convolution layer. Since
    we have learned all about how to compute the size of the output under various
    circumstances (e.g., with stride, with padding, etc.), we will compute the final
    output of the convolution layers. We have the following layers:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做错了什么？TensorFlow似乎在尝试计算卷积层输出时抱怨负尺寸。由于我们已经学会了在各种情况下计算输出大小（例如，带有步幅，带有填充等），我们将计算卷积层的最终输出。我们有以下层：
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We are starting with an input of size 32 × 32 × 3\. Then, after the convolution
    operation, which has 16 filters, a kernel size of 9, and stride 2, we get an output
    of size (height and width)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从尺寸为32×32×3的输入开始。然后，经过具有16个过滤器、卷积核尺寸为9和步幅为2的卷积操作后，我们得到一个尺寸为（高度和宽度）的输出。
- en: '[PRE23]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we focus only on the height and width dimensions. The next layer has
    32 filters, a kernel size of 7, and no stride:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们只关注高度和宽度维度。下一层有32个过滤器，卷积核尺寸为7，没有步幅：
- en: '[PRE24]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This layer produces an output of size
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 该层产生一个尺寸为的输出。
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The final convolution layer has 64 filters, a kernel size of 7, and no stride
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的卷积层有64个过滤器，卷积核尺寸为7，没有步幅。
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: which will produce an output of size
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生一个尺寸为的输出。
- en: 6 - 7 + 1= 0
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 6 - 7 + 1 = 0
- en: We figured it out! With our chosen configuration, our CNN is producing an invalid
    zero-sized output. The term *negative dimension* in the error refers to an output
    with invalid dimensions (i.e., less than one) being produced. The output always
    needs to be greater than or equal to 1.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们找到了解决办法！通过我们选择的配置，我们的 CNN 产生了一个无效的零尺寸输出。错误中的“负尺寸”一词指的是产生具有无效尺寸（即小于1）的输出。输出总是需要大于或等于1。
- en: Let’s correct this network by making sure the outputs will never have negative
    dimensions. Furthermore, we will introduce several interleaved max-pooling layers
    to the CNN, which helps the network to learn translation-invariant features (see
    the next listing).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过确保输出永远不会具有负尺寸来修正这个网络。此外，我们将在 CNN 中引入几个交错的最大池化层，这有助于网络学习平移不变特征（参见下一列表）。
- en: Listing 4.3 The corrected CNN model that has positive dimensions
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 已修正的具有正尺寸的 CNN 模型。
- en: '[PRE27]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ The first convolution layer. The output size reduces from 32 to 16.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个卷积层。输出尺寸从32减小到16。
- en: ❷ The first max-pooling layer. The output size reduces from 16 to 8.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个最大池化层。输出尺寸从16减小到8。
- en: ❸ The second convolution layer. The output size stays the same as there is no
    stride.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第二个卷积层。由于没有步幅，输出尺寸保持不变。
- en: ❹ The second max-pooling layer. The output size reduces from 8 to 4.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第二个最大池化层。输出尺寸从8减小到4。
- en: ❺ Squashing the height, width, and channel dimensions to a single dimension
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将高度、宽度和通道维度压缩为单一维度。
- en: ❻ The two intermediate Dense layers with ReLU activation
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 两个中间的具有 ReLU 激活函数的密集层。
- en: ❼ The final output layer with softmax activation
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 softmax 激活的最终输出层。
- en: 'Max-pooling is provided by the tensorflow.keras.layers.MaxPool2D layer. The
    hyperparameters of this layer are very similar to tensorflow.keras.layers.Conv2D:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化由 tensorflow.keras.layers.MaxPool2D 层提供。该层的超参数与 tensorflow.keras.layers.Conv2D
    非常相似：
- en: pool_size—This is analogous to the kernel size parameter of the Conv2D layer.
    It is a tuple representing (window height, window width), in that order.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: pool_size——这类似于 Conv2D 层的卷积核尺寸参数。它是一个表示（窗口高度，窗口宽度）的元组，按照那个顺序。
- en: Strides—This is analogous to the strides parameter of the Conv2D layer. It is
    a tuple representing (height stride, width stride), in that order.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步幅——这类似于 Conv2D 层的步幅参数。它是一个表示（高度步幅，宽度步幅）的元组，按照那个顺序。
- en: Padding—Padding can be same or valid and has the same effect as it has in the
    Conv2D layer.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充——填充可以是 same 或 valid，并且具有与 Conv2D 层中相同的效果。
- en: 'Let’s analyze the changes we made to our CNN:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下我们对 CNN 所做的更改：
- en: We used padding='same' for all the Conv2D and MaxPool2D layers, meaning that
    there won’t be any automatic reduction of the output size. This eliminates the
    risk of mistakenly going into negative dimensions.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们对所有 Conv2D 和 MaxPool2D 层都使用了 padding='same'，这意味着不会自动减小输出尺寸。这消除了意外进入负尺寸的风险。
- en: We used stride parameters to control the reduction of the output size as we
    go deeper into the model.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用步幅参数来控制随着模型深入而输出尺寸的减小。
- en: You can follow the output sizes in listing 4.1 and make sure that the output
    will never be less than or equal to zero for the input images we have.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照列表4.1中的输出尺寸，并确保对于我们拥有的输入图像，输出永远不会小于或等于零。
- en: After the Conv2D and MaxPool2D layers, we have to have at least one Dense layer,
    as we are solving an image classification task. To get the final prediction probabilities
    (i.e., the probabilities of a given input belonging to the output classes), a
    Dense layer is essential. But before having a Dense layer, we need to flatten
    our four-dimensional output (i.e., [batch, height, width, channel] shaped) of
    the Conv2D or MaxPool2D layers to a two-dimensional input (i.e., [batch, features]
    shaped) to the Dense layer. That is, except for the batch dimension, everything
    else gets squashed to a single dimension. For this, we use the tensorflow.keras.layers.Flatten
    layer provided by Keras. For example, if the output of our last Conv2D layer was
    [None, 4, 4, 64], then the Flatten layer will flatten this output to a [None,
    1024]-sized tensor. Finally, we add three Dense layers, where the first two dense
    layers have 64 and 32 output nodes and an activation of type ReLU. The final Dense
    layer will have 10 nodes (1 for each class) and a softmax activation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在Conv2D和MaxPool2D层之后，我们必须至少有一个全连接层，因为我们正在解决图像分类任务。为了获得最终的预测概率（即给定输入属于输出类的概率），一个全连接层是必不可少的。但在拥有全连接层之前，我们需要将Conv2D或MaxPool2D层的四维输出（即[batch,
    height, width, channel]形状）展平为全连接层的二维输入（即[batch, features]形状）。也就是说，除了批处理维度之外，其他所有维度都被压缩为单个维度。为此，我们使用由Keras提供的tensorflow.keras.layers.Flatten层。例如，如果我们最后一个Conv2D层的输出是[None,
    4, 4, 64]，那么Flatten层将这个输出展平为一个[None, 1024]大小的张量。最后，我们添加三个全连接层，其中前两个全连接层具有 64 和
    32 个输出节点，并且使用ReLU类型的激活函数。最后一个全连接层将有 10 个节点（每个类一个）和 softmax 激活函数。
- en: Performance bottleneck of CNNs
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: CNN的性能瓶颈
- en: Typically, in a CNN the very first Dense layer after the convolution/pooling
    layers is considered a *performance bottleneck*. This is because this layer will
    usually contain a large proportion of the parameters of the network. Assume you
    have a CNN where the last pooling layer produces an 8 × 8 × 256 output followed
    by a Dense layer with 1,024 nodes. This Dense layer would contain 16,778,240 (more
    than 16 million) parameters. If you don’t pay attention to the first Dense layer
    of the CNN, you can easily run into out-of-memory errors while running the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在CNN中，卷积/池化层之后的第一个全连接层被认为是*性能瓶颈*。这是因为这一层通常包含网络参数的很大一部分。假设您有一个CNN，其中最后一个池化层产生一个
    8 × 8 × 256 的输出，后面是一个具有 1,024 个节点的全连接层。这个全连接层将包含 16,778,240（超过 1600 万）个参数。如果您不注意CNN的第一个全连接层，您很容易在运行模型时遇到内存不足的错误。
- en: 'It’s time to test our first CNN on the data. But before that we have to compile
    the model with appropriate parameters. Here, we will monitor the training accuracy
    of the mode:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候在数据上测试我们的第一个CNN了。但在此之前，我们必须用适当的参数编译模型。在这里，我们将监视模型的训练准确率：
- en: '[PRE28]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Finally, you can use the training data we created earlier and train the model
    on the data by calling
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以使用我们之前创建的训练数据，并通过调用数据训练模型。
- en: '[PRE29]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You should get the following output:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '[PRE30]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It seems we are getting good at training accuracies (denoted by acc) and creating
    a steady reduction of the training loss (denoted by loss) for the task of identifying
    vehicles (around 72.2% accuracy). But we can go for far better accuracies by employing
    various techniques, as you will see in later chapters. This is very promising
    news for the team, as this means they can continue working on their full solution.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们在训练准确率（以acc表示）方面做得不错，并且在识别车辆任务的训练损失（以loss表示）上保持稳定的降低（约为 72.2% 的准确率）。但是通过采用各种技术，我们可以获得更好的准确率，您将在后面的章节中看到。对于团队来说，这是一个非常令人兴奋的消息，因为这意味着他们可以继续努力完成他们的全面解决方案。
- en: In this section, we looked at CNNs. CNNs work extremely well, especially in
    computer vision problems. In this instance, we looked at using a CNN to classify
    images to various classes (e.g., animals, vehicles, etc.) as a feasibility study
    for a model’s ability to detect vehicles. We looked at the technical aspects of
    the CNN in detail, while scrutinizing various operations like convolution and
    pooling, as well as the impact of the parameters associated with these operations
    (e.g., window size, stride, padding). We saw that if we do not pay attention to
    how the output changes while using these parameters, it can lead to errors in
    our code. Next, we went on to fix the error and train the model on the data set.
    Finally, we saw that the model showed promising results, quickly reaching for
    a training accuracy above 70%. Next, we will discuss RNNs, which are heavily invested
    in solving time-series problems.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们研究了CNNs。CNNs在解决计算机视觉问题时表现得非常好。在这个实例中，我们研究了使用CNN对图像进行分类到各种类别（例如，动物、车辆等）作为模型检测车辆能力的可行性研究。我们详细研究了CNN的技术方面，同时仔细检查了各种操作，如卷积和池化，以及与这些操作相关的参数的影响（例如，窗口大小、步长、填充）。我们发现，如果我们在使用这些参数时不注意输出的变化，可能会导致代码错误。接下来，我们修复了错误并在数据集上训练了模型。最后，我们发现模型显示出有希望的结果，迅速达到了70%以上的训练准确度。接下来，我们将讨论RNNs，它们在解决时间序列问题方面投入了大量的投资。
- en: Exercise 2
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2
- en: 'Consider the following network:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下网络：
- en: '[PRE31]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: What is the final output size (ignoring the batch dimension)?
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的输出大小是多少（忽略批次维度）？
- en: '4.3 One step at a time: Recurrent neural networks (RNNs)'
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 一步一步地：递归神经网络（RNNs）
- en: You are working for a machine learning consultant for the National Bureau of
    Meteorology. They have data for CO2 concentration over the last three decades.
    You have been tasked with developing a machine learning model that predicts CO2
    concentration for the next five years. You are planning to implement a simple
    RNN that takes a sequence of CO2 concentrations (in this case, the values from
    the last 12 months) and predicts the next in the sequence.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 您是国家气象局的机器学习顾问。他们拥有过去三十年的CO2浓度数据。您被委托开发一个机器学习模型，预测未来五年的CO2浓度。您计划实现一个简单的RNN，它接受CO2浓度序列（在本例中，过去12个月的值）并预测序列中的下一个值。
- en: It can be clearly seen that what we have in front of us is a time series problem.
    This is quite different from the tasks we have been solving so far. In previous
    tasks, one input did not depend on the previous inputs. In other words, you considered
    each input to be *i.i.d* (independent and identically distributed) inputs. However,
    in this problem, that is not the case. The CO2 concentration today will depend
    on what the CO2 concentration was over the last several months.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，我们面对的是一个时间序列问题。这与我们以往解决的任务非常不同。在以前的任务中，一个输入不依赖于先前的输入。换句话说，您认为每个输入都是 *i.i.d*（独立同分布）的输入。然而，在这个问题中，情况并非如此。今天的CO2浓度将取决于过去几个月的CO2浓度。
- en: Typical feed-forward networks (i.e., fully connected networks, CNNs) cannot
    learn from time series data without special adaptations. However, there is a special
    type of neural network that is designed to learn from time series data. These
    networks are generally known as RNNs. RNNs not only use the current input to make
    a prediction, but also use the *memory* of the network from past time steps, at
    a given time step. Figure 4.14 depicts how a feed-forward network and an RNN differ
    in predicting CO2 concentration over the months. As you can see, if you use a
    feed-forward network, it has to predict the CO2 level for the next month based
    *only* on the previous month, whereas an RNN looks at all the previous months.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的前馈网络（即全连接网络、CNNs）在没有特殊适应的情况下无法从时间序列数据中学习。然而，有一种特殊类型的神经网络专门设计用于从时间序列数据中学习。这些网络通常被称为RNNs。RNNs不仅使用当前输入进行预测，而且在给定时间步长时还使用网络的*记忆*，从过去的时间步长。图4.14描述了前馈网络和RNN在预测几个月内CO2浓度时的差异。正如您所看到的，如果您使用前馈网络，它必须仅基于上个月来预测下个月的CO2水平，而RNN则会查看所有以前的月份。
- en: '![04-14](../../OEBPS/Images/04-14.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![04-14](../../OEBPS/Images/04-14.png)'
- en: Figure 4.14 The operational difference between a feed-forward network and an
    RNN in terms of a CO2 concentration-level prediction task
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 以CO2浓度水平预测任务为例，前馈网络和RNN之间的操作差异
- en: 4.3.1 Understanding the data
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 理解数据
- en: 'The data set is very simple (downloaded from [https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv](https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv)).
    Each datapoint has a date (YYYY-MM-DD format) and a floating-point value representing
    the CO2 concentration in CSV format. The data is provided to us as a CSV file.
    Let’s download the file as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集非常简单（从[https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv](https://datahub.io/core/co2-ppm/r/co2-mm-gl.csv)下载）。每个数据点都有一个日期（YYYY-MM-DD
    格式）和一个浮点值，表示 CSV 格式中的 CO2 浓度。数据以 CSV 文件的形式提供给我们。让我们按如下方式下载文件：
- en: '[PRE32]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can easily load this data set using pandas:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 pandas 轻松加载这个数据集：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can see what the data looks like, using the head() operation, which
    will provide the first few entries in the data frame:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看一下数据的样子，使用 head() 操作，它将提供数据框中的前几个条目：
- en: '[PRE34]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will give something like figure 4.15.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到类似图 4.15 的东西。
- en: '![04-15](../../OEBPS/Images/04-15.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![04-15](../../OEBPS/Images/04-15.png)'
- en: Figure 4.15 Sample data in the data set
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 数据集中的示例数据
- en: 'In this data set, the only two columns we are interested in are the Date column
    and the Average column. Out of these, the Date column is important for visualization
    purposes only. Let’s set the Date column as the index of the data frame. This
    way, when we plot data, the *x*-axis will be automatically annotated with the
    corresponding date:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们唯一感兴趣的两列是日期列和平均列。其中，日期列仅用于可视化目的。让我们将日期列设置为数据框的索引。这样，当我们绘制数据时，*x* 轴将自动注释相应的日期：
- en: '[PRE35]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We can now visualize the data (figure 4.16) by creating a line plot with
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过创建一条线图来可视化数据（图 4.16）：
- en: '[PRE36]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![04-16](../../OEBPS/Images/04-16.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![04-16](../../OEBPS/Images/04-16.png)'
- en: Figure 4.16 CO2 concentration plotted over time
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.16 CO2 浓度随时间变化的图示
- en: 'The obvious features of the data are that it has an upward trend and short
    repetitive cycles. Let’s see what sort of improvements we can do to this data.
    The clear upward trend the data is showing poses a problem. This means that the
    data is not distributed in a consistent range. The range increases as we go further
    and further down the timeline. If you feed data as it is to the model, usually
    the model will underperform, because any new data the model has to predict is
    out of the range of the data it saw during training. But if you forget the absolute
    values and think about this data relative to the previous value, you will see
    that it moves between a very small range of values (appx -2.0 to +1.5). In fact,
    we can test this idea easily. We will create a new column called Average Diff,
    which will contain the relative difference between two consecutive time steps:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的明显特征是它呈上升趋势和短周期性重复。让我们看看我们可以对这些数据做什么样的改进。数据明显的上升趋势构成了一个问题。这意味着数据在一个一致的范围内没有分布。随着时间线的推移，范围不断增加。如果你把数据直接输入模型，通常模型的性能会下降，因为模型必须预测的任何新数据都超出了训练期间看到的数据范围。但是如果你忘记绝对值，思考这些数据与前一个值的相对关系，你会发现它在一个非常小的值范围内波动（大约为-2.0到+1.5）。事实上，我们可以很容易地测试这个想法。我们将创建一个名为
    Average Diff 的新列，其中将包含两个连续时间步之间的相对差异：
- en: '[PRE37]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: If you do a data.head() at this stage, you will see something similar to table
    4.2.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个阶段执行 data.head()，你会看到类似表 4.2 的东西。
- en: Table 4.2 Sample data in the data set after introducing the Average diff column
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 引入平均差异列后数据集中的示例数据
- en: '| **Date** | **Decimal data** | **Average** | **Trend** | **Average diff**
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| **日期** | **十进制日期** | **平均值** | **趋势** | **平均差异** |'
- en: '| 1980-01-01 | 1980.042 | 338.45 | 337.83 | 0.00 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 1980-01-01 | 1980.042 | 338.45 | 337.83 | 0.00 |'
- en: '| 1980-02-01 | 1980.125 | 339.15 | 338.10 | 0.70 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 1980-02-01 | 1980.125 | 339.15 | 338.10 | 0.70 |'
- en: '| 1980-031-01 | 1980.208 | 339.48 | 338.13 | 0.33 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 1980-03-01 | 1980.208 | 339.48 | 338.13 | 0.33 |'
- en: '| 1980-04-01 | 1980.292 | 339.87 | 338.25 | 0.39 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 1980-04-01 | 1980.292 | 339.87 | 338.25 | 0.39 |'
- en: '| 1980-05-01 | 1980.375 | 340.30 | 338.78 | 0.43 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 1980-05-01 | 1980.375 | 340.30 | 338.78 | 0.43 |'
- en: Here, we are subtracting a version of the Average column, where values are shifted
    forward by one time step, from the original average column. Figure 4.17 depicts
    this operation visually.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们正在从原始平均列中减去一个平移一个时间步长的值的版本的平均列。图 4.17 在视觉上描述了这个操作。
- en: '![04-17](../../OEBPS/Images/04-17.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![04-17](../../OEBPS/Images/04-17.png)'
- en: Figure 4.17 Transformations taking place going from the original Average series
    to the Average Diff series
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.17 从原始平均系列到平均差异系列的转换
- en: Finally, we can visualize how the values behave (figure 4.18) using the data["Average
    Diff"].plot(figsize=(12,6)) line.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以可视化值的行为（图 4.18）使用 data["Average Diff"].plot(figsize=(12,6)) 行。
- en: '![04-18](../../OEBPS/Images/04-18.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![04-18](../../OEBPS/Images/04-18.png)'
- en: Figure 4.18 Relative change of values (i.e., Average[t]-Average[t-1]) of CO2
    concentration over time
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.18 CO2 浓度值的相对变化（即，Average[t]-Average[t-1]）随时间的变化
- en: Can you see the difference? From an ever-increasing data stream, we have gone
    to a stream that changes within a short vertical span. The next step is creating
    batches of data for the model to learn. How do we create batches of data for a
    time series problem? Remember, we cannot just randomly sample data naively, as
    each input depends on its predecessors.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看到区别吗？从不断增长的数据流中，我们已经转变成了在短时间内发生变化的数据流。下一步是为模型创建数据批处理。我们如何为时间序列问题创建数据批处理呢？请记住，我们不能简单地随机采样数据，因为每个输入都取决于其前序输入。
- en: Let’s assume we want to use 12 past CO2 concentration values (i.e., 12 time
    steps) to predict the current CO2 concentration value. The number of time steps
    is a hyperparameter you must choose carefully. In order to choose this hyperparameter
    confidently, you must have a solid understanding of the data and the memory limitations
    of the model you are using.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要使用过去 12 个 CO2 浓度值（即 12 个时间步）来预测当前的 CO2 浓度值。时间步数是一个必须仔细选择的超参数。为了自信地选择这个超参数，你必须对数据和所使用的模型的内存限制有扎实的了解。
- en: We first randomly choose a position in the sequence and take the 12 values from
    that point on as the inputs and the 13^(th) value as the output we’re interested
    in predicting so that the total sequence length (n_seq) you sample at a time is
    13\. If you do this process 10 times, you will have a batch of data with 10 elements.
    As you can see, this process exploits the randomness while preserving the temporal
    characteristics of the data, and while feeding data to the model. Figure 4.19
    visually describes this process.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先随机选择序列中的一个位置，并从该位置开始取 12 个值作为输入，并将第 13 个值作为我们感兴趣的要预测的输出，以便每次采样的总序列长度（n_seq）为
    13。如果你这样做 10 次，你将得到一个具有 10 个元素的数据批处理。正如你所看到的，这个过程利用了随机性，同时保留了数据的时间特性，并向模型提供数据。图
    4.19 对这个过程进行了可视化描述。
- en: '![04-19](../../OEBPS/Images/04-19.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
  zh: '![04-19](../../OEBPS/Images/04-19.png)'
- en: Figure 4.19 Batching time series data. n_seq represents the number of time steps
    we see at a given time to create a single input and an output.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19 批处理时间序列数据。n_seq 表示我们在给定时间内看到的时间步数，以创建单个输入和输出。
- en: To do this in Python, let’s write a function that gives the data at all positions
    as a single data set. In other words, this function returns all possible consecutive
    sequences with 12 elements as x and the corresponding next value for each sequence
    as y. It is possible to perform the shuffling while feeding this data to the model,
    as the next listing shows.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中执行此操作，让我们编写一个函数，以单个数据集的形式给出所有位置的数据。换句话说，该函数返回所有可能的具有 12 个元素的连续序列作为
    x，并将每个序列的相应下一个值作为 y。在将数据提供给模型时可以执行洗牌操作，如下一个清单所示。
- en: Listing 4.4 The code for generating time-series data sequences for the model
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 4.4 用于为模型生成时间序列数据序列的代码
- en: '[PRE38]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Extracting a sequence of values n_seq long
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提取长度为 n_seq 的值序列
- en: ❷ Extracting the next value in the sequence as the output
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将序列中的下一个值提取为输出
- en: ❸ Combining everything into an array
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将所有内容组合成一个数组
- en: 4.3.2 Implementing the model
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 实现模型
- en: 'With a good understanding of the data, we can start implementing the network.
    We will implement a network that has the following:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 了解数据后，我们可以开始实现网络。我们将实现一个具有以下内容的网络：
- en: A rnn layer with 64 hidden units
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 64 个隐藏单元的 rnn 层
- en: A Dense layer with 64 hidden units and a ReLU activation
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有 64 个隐藏单元和 ReLU 激活的密集层
- en: A Dense layer with a single output and a linear activation
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有单输出和线性激活的密集层
- en: '[PRE39]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that the hyperparameters of the network (e.g., number of hidden units)
    have been chosen empirically to work well for the given problem. The first layer
    is the most crucial component of the network, as it is the element that makes
    it possible to learn from time series data. The SimpleRNN layer encapsulates the
    functionality shown in figure 4.20.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，网络的超参数（例如，隐藏单元的数量）已经经验性地选择，以便在给定问题上良好地工作。第一层是网络中最关键的组件，因为它是从时间序列数据中学习的要素。SimpleRNN
    层封装了图 4.20 中所示的功能。
- en: '![04-20](../../OEBPS/Images/04-20.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![04-20](../../OEBPS/Images/04-20.png)'
- en: Figure 4.20 The functionality of a SimpleRNN cell. The cell goes from one input
    to another while producing a memory at every time step. The next step consumes
    the current input as well as the memory from the previous time step.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 SimpleRNN单元的功能。该单元在每个时间步长产生一个内存，从一个输入到另一个输入。下一步会消耗当前输入以及上一个时间步长的内存。
- en: The computations that happen in an RNN are more sophisticated than in an FCN.
    An RNN goes from one input to the other in the input sequence (i.e., x1, x2, x3)
    in the given order. At each step, the recurrent layer produces an output (i.e.,
    o1, o2, o3) and passes the hidden computation (h0, h1, h2, h3) to the next time
    step. Here, the first hidden state (h0) is typically set to zero.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在RNN中发生的计算比在FCN中更复杂。RNN按给定顺序（即x1、x2、x3）从一个输入到另一个输入。在每个步骤中，递归层产生一个输出（即o1、o2、o3），并将隐藏计算（h0、h1、h2、h3）传递到下一个时间步长。在这里，第一个隐藏状态（h0）通常设为零。
- en: At a given time step, the recurrent layer computes a hidden state, just like
    a Dense layer. However, the specific computations involved are bit more complex
    and are out of the scope of this book. The hidden state size is another hyperparameter
    of the recurrent layer. The recurrent layer takes the current input as well as
    the previous hidden state computed by the cell. A larger-sized hidden state helps
    to maintain more memory but increases the memory requirement of the network. As
    the hidden state is dependent on itself from the previous time step, these networks
    are called RNNs.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的时间步长上，递归层计算一个隐藏状态，就像Dense层一样。然而，涉及的具体计算更加复杂，超出了本书的范围。隐藏状态的大小是递归层的另一个超参数。递归层接受当前输入以及细胞计算的先前隐藏状态。更大尺寸的隐藏状态有助于保持更多内存，但增加了网络的内存需求。由于隐藏状态依赖于上一个时间步长的自身，这些网络被称为RNNs。
- en: The algorithm used for SimpleRNN
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SimpleRNN的算法
- en: The computations mimicked by the SimpleRNN layer are also known as *Elman networks*.
    To learn more about specific computations taking place in recurrent layers, you
    can read the paper “Finding Structure in Time” by J.L. Elman (1990). For a more
    high-level overview of later variations of RNNs and their differences, see [http://mng.bz/xnJg](http://mng.bz/xnJg)
    and [http://mng.bz/Ay2g](http://mng.bz/Ay2g)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: SimpleRNN层模仿的计算也称为*Elman网络*。要了解递归层中发生的具体计算，你可以阅读J.L. Elman（1990）的论文“Finding
    Structure in Time”。要了解RNN的后续变体及其区别的更高级概述，请参阅[http://mng.bz/xnJg](http://mng.bz/xnJg)和[http://mng.bz/Ay2g](http://mng.bz/Ay2g)。
- en: By default, the SimpleRNN does not expose the hidden state to the developer
    and will be propagated between time steps automatically. For this task, we only
    need the final output produced by each time step, which is the output of that
    layer by default. Therefore, you can simply connect the SimpleRNN in the Sequential
    API to a Dense layer without any additional work.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，SimpleRNN不会将隐藏状态暴露给开发者，并且会在时间步长之间自动传播。对于这个任务，我们只需要每个时间步长产生的最终输出，这默认情况下是该层的输出。因此，你可以简单地将SimpleRNN连接到Sequential
    API中的一个Dense层，而无需进行任何额外的工作。
- en: Did you notice that we haven’t provided an input_shape to the first layer? This
    is possible, as long as you provide the data in the correct shape during model
    fitting. Keras builds the layers lazily, so until you feed data to your model,
    the model doesn’t need to know the input sizes. But it is always safer to set
    the input_shape argument in the first layer of the model to avoid errors. For
    example, in the model we defined, the first layer (i.e., the SimpleRNN layer)
    can be changed to layers.SimpleRNN(64, input_shape=x), where x is a tuple containing
    the shape of the data accepted by the model.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否注意到我们没有为第一层提供input_shape？只要你在模型拟合期间提供正确形状的数据即可。Keras会懒惰地构建层，因此在你向模型提供数据之前，模型不需要知道输入大小。但为了避免错误，最好在模型的第一层设置input_shape参数。例如，在我们定义的模型中，第一层（即SimpleRNN层）可以更改为layers.SimpleRNN(64,
    input_shape=x)，其中x是包含模型接受的数据形状的元组。
- en: 'Another important difference in this model is that it is a regression model,
    not a classification model. In a classification model, there are distinct classes
    (represented by output nodes), and we try to associate a given input with a distinct
    class (or a node). A regression model predicts a continuous value(s) as the output.
    Here, in our regression model, there is no notion of classes in the outputs, but
    a real continuous value representing CO2 concentration. Therefore, we have to
    choose the loss function appropriately. In this case, we will use mean squared
    error (MSE) as the loss. MSE is a very common loss function for regression problems.
    We will compile the rnn with the MSE loss and the adam optimizer:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的另一个重要区别是它是一个回归模型，而不是分类模型。在分类模型中，有不同的类别（由输出节点表示），我们尝试将给定的输入与不同的类别（或节点）关联起来。回归模型预测一个连续的值作为输出。在我们的回归模型中，输出中没有类的概念，而是表示
    CO2 浓度的实际连续值。因此，我们必须适当地选择损失函数。在这种情况下，我们将使用均方误差（MSE）作为损失。MSE 是回归问题的非常常见的损失函数。我们将使用
    MSE 损失和 adam 优化器编译 rnn：
- en: '[PRE40]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s cross our fingers and train our model:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们祈祷并训练我们的模型：
- en: '[PRE41]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You’ll get the following exception:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 你将得到以下异常：
- en: '[PRE42]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: It seems we have done something wrong. The line we just ran resulted in an exception,
    which says something is wrong with the dimensionality of the data given to the
    layer sequential_1 (i.e., the SimpleRNN layer). Specifically, the sequential_1
    layer expects a three-dimensional input but has a two-dimensional input. We need
    to investigate what’s happening here and solve this.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们做错了什么。我们刚刚运行的那行导致了一个异常，它说给层 sequential_1（即 SimpleRNN 层）提供的数据的维度出了问题。具体来说，sequential_1
    层期望一个三维输入，但却有一个二维输入。我们需要调查这里发生了什么，并解决这个问题。
- en: 'The problem is that the SimpleRNN (or any other sequential layer in tf.keras)
    only accepts data in a very specific format. The data needs to be three-dimensional,
    with the following dimensions, in this order:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于 SimpleRNN（或 tf.keras 中的任何其他顺序层）只接受非常特定格式的数据。数据需要是三维的，按照以下顺序的维度：
- en: Batch dimension
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 批处理维度
- en: Time dimension
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间维度
- en: Feature dimension
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征维度
- en: Even when you have a single element for any of these dimensions, they need to
    be present as a dimension of size 1 in the data. Let’s look at what the dimensionality
    of x is by printing x.shape. You will get x.shape = (429, 12). Now we know what
    went wrong. We tried to pass a two-dimensional data set when we should have passed
    a three-dimensional one. In this case, we need to reshape x into a tensor of shape
    (492, 12, 1). Let’s change our generate_data(...) function to reflect this change
    in the following listing.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于这些维度中的任何一个，你只有一个元素，它们也需要以大小为 1 的维度存在于数据中。让我们通过打印 x.shape 来查看 x 的维度。你将会得到
    x.shape = (429, 12)。现在我们知道了问题所在。我们尝试传递一个二维数据集，但我们应该传递一个三维数据集。在这种情况下，我们需要将 x 重塑为形状为
    (492, 12, 1) 的张量。让我们修改我们的 generate_data(...) 函数以反映以下清单中的这种变化。
- en: Listing 4.5 The previous generate_data() function with data in the correct shape
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 具有正确形状数据的先前 generate_data() 函数
- en: '[PRE43]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Create two lists to hold input sequences and scalar output targets.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建两个列表来保存输入序列和标量输出目标。
- en: ❷ Iterate through all the possible starting points in the data for input sequences.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历数据中所有可能的起始点，以用作输入序列。
- en: ❸ Create the input sequence and the output target at the ith position.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建第 i 个位置的输入序列和输出目标。
- en: ❹ Convert x from a list to an array and make x a 3D tensor to be accepted by
    the RNN.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将 x 从列表转换为数组，并使 x 成为 RNN 可接受的 3D 张量。
- en: 'Let’s try training our model now:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试训练我们的模型：
- en: '[PRE44]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'You should see the MSE of the model going down:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到模型的 MSE 在下降：
- en: '[PRE45]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We start with a loss of approximately 0.5 and end up with a loss of roughly
    0.015\. This is a very positive sign, as it indicates the model is learning the
    trends present in the data.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从大约 0.5 的损失开始，最终损失大约为 0.015。这是一个非常积极的迹象，因为它表明模型正在学习数据中存在的趋势。
- en: 4.3.3 Predicting future CO2 values with the trained model
  id: totrans-344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 使用经过训练的模型预测未来的 CO2 值
- en: Thus far, we have focused on classification tasks. It is much easier to evaluate
    models on classification tasks than regression tasks. In classification tasks
    (assuming a balanced data set), by computing the overall accuracy on the data,
    we can get a decent representative number on how well our model is doing. In regression
    tasks it’s not so simple. We cannot measure an accuracy on regressed values, as
    the predictions are real values, not classes. For example, the magnitude of the
    mean squared loss depends on values we are regressing, which makes them difficult
    to objectively interpret. To address this, we predict the values for the next
    five years and visually inspect what the model is predicting (see the next listing).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于分类任务。对于分类任务，评估模型要比回归任务容易得多。在分类任务中（假设数据集平衡），通过计算数据的总体准确性，我们可以得到一个体现模型表现的不错的代表性数字。在回归任务中，情况并不那么简单。我们无法对回归值进行准确度测量，因为预测的是实际值，而不是类别。例如，均方损失的大小取决于我们正在回归的值，这使它们难以客观解释。为了解决这个问题，我们预测未来五年的数值，并直观地检查模型的预测情况（见下一列表）。
- en: Listing 4.6 The future CO2 level prediction logic using the trained model
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 使用训练模型的未来CO2水平预测逻辑
- en: '[PRE46]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ The first data sequence to start predictions from, which is reshaped to the
    correct shape the SimpleRNN accepts
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从中获取开始预测的第一个数据序列，重塑为SimpleRNN接受的正确形状
- en: ❷ Save the very last absolute CO2 concentration value to compute the actual
    values from the relative predictions.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存最后一个绝对CO2浓度值，以计算相对预测的实际值。
- en: ❸ Predict for the next 60 months.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预测接下来的60个月。
- en: ❹ Make a prediction using the data sequence.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用数据序列进行预测。
- en: ❺ Modify the history so that the latest prediction is included.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 修改历史记录，以包括最新的预测。
- en: ❻ Compute the absolute CO2 concentration.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算绝对CO2浓度。
- en: ❼ Update prev_true so that the absolute CO2 concentration can be computed in
    the next time step.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 更新prev_true，以便在下一个时间步骤中计算绝对CO2浓度。
- en: 'Let’s review what we have done. First, we extracted the last 12 CO2 values
    (from the Average Diff column) from our training data to predict the first future
    CO2 value and reshaped it to the correct shape the model expects the data to be
    in:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们所做的事情。首先，我们从我们的训练数据中提取最后12个CO2值（从平均差值列中）来预测第一个未来的CO2值，并将其重塑为模型期望数据的正确形状：
- en: '[PRE47]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Then, we captured the predicted CO2 values in true_vals list. Remember that
    our model only predicts the relative movement of CO2 values with respect to the
    previous CO2 values. Therefore, after the model predicts, to get the absolute
    CO2 value, we need the last CO2 value. prev_true captures this information, which
    initially has the very last value in the Average column of the data:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将预测的CO2值记录在true_vals列表中。请记住，我们的模型只预测CO2值相对于先前CO2值的相对运动。因此，在模型预测之后，为了得到绝对CO2值，我们需要最后一个CO2值。prev_true捕获了这一信息，最初包含数据的平均列中的最后一个值：
- en: '[PRE48]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now, for the next 60 months (or 5 years), we can recursively predict CO2 values,
    while making the last predicted the next input to the network. To do this we first
    predict a value using the predict(...) method provided in Keras. Then, we need
    to make sure the prediction is also a three-dimensional tensor (though it’s a
    single value). Then we modify the history variable:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，接下来的60个月（或5年），我们可以递归预测CO2值，同时使最后预测的值成为网络的下一个输入。要做到这一点，我们首先使用Keras提供的predict(...)方法预测一个值。然后，我们需要确保预测也是一个三维张量（尽管它只是一个单一值）。然后我们修改history变量：
- en: '[PRE49]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We are taking all but the first value from the history and appending the last
    predicted value to the end. Then we append the absolute predicted CO2 value by
    adding the prev_true value to p_diff:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们把历史中除了第一个值之外的所有值，并将最后预测的值附加到末尾。然后，我们通过添加prev_true值到p_diff来附加绝对预测的CO2值：
- en: '[PRE50]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Finally, we update prev_true to the last absolute CO2 value we predicted:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将prev_true更新为我们预测的最后一个绝对CO2值：
- en: '[PRE51]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: By doing this set of operations recursively, we can get the predictions for
    the next 60 months (captured in true_vals variable). If we visualize the predicted
    values, they should look like figure 4.21.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过递归执行这组操作，我们可以获得接下来60个月的预测值（保存在true_vals变量中）。如果我们可视化预测的值，它们应该看起来像图4.21。
- en: '![04-21](../../OEBPS/Images/04-21.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![04-21](../../OEBPS/Images/04-21.png)'
- en: Figure 4.21 The CO2 concentration predicted over the next five years. Dashed
    line represents the trend from the current data, and the solid line represents
    the predicted trend.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21 在接下来的五年里预测的CO2浓度。虚线代表当前数据的趋势，实线代表预测的趋势。
- en: Great work! Given the simplicity of the model, predictions look very promising.
    The model has definitely captured the annual trend of the CO2 concentration and
    has learned that the CO2 level is going to keep going up. You can now go to your
    boss and explain factually why we should be worried about climate change and dangerous
    levels of CO2 in the future. We end our discussion about different neural networks
    here.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 做得好！考虑到模型的简单性，预测看起来非常有前景。该模型肯定捕捉到了二氧化碳浓度的年度趋势，并学会了二氧化碳水平将继续上升。你现在可以去找你的老板，事实性地解释为什么我们应该担心未来气候变化和危险水平的二氧化碳。我们在这里结束了对不同神经网络的讨论。
- en: Exercise 3
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3
- en: Impressed by your work on predicting the CO2 concentration, your boss has provided
    you the data and asked you to enhance the model to predict both CO2 and temperature
    values. Keeping the other hyperparameters the same, how would you change the model
    for this task? Make sure you specify the input_shape parameter for the first layer.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 受到你在预测二氧化碳浓度方面工作的印象，你的老板给了你数据，并要求你改进模型以预测二氧化碳和温度值。保持其他超参数不变，你会如何改变模型以完成这个任务？确保指定第一层的input_shape参数。
- en: Summary
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: Fully connected networks (FCNs) are one of the most straightforward neural networks.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全连接网络（FCNs）是最简单直接的神经网络之一。
- en: FCNs can be implemented using the Keras Dense layer.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FCNs可以使用Keras Dense层来实现。
- en: Convolutional neural networks (CNNs) are a popular choice for computer vision
    tasks.
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）是计算机视觉任务的热门选择。
- en: TensorFlow offers various layers, such as Conv2D, MaxPool2D, and Flatten, that
    help us implement CNNs quickly.
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow提供了各种层，如Conv2D、MaxPool2D和Flatten，这些层帮助我们快速实现CNNs。
- en: CNNs have parameters such as kernel size, stride, and padding that must be set
    carefully. If not, this can lead to incorrectly shaped tensors and runtime errors.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs有一些参数，如卷积核大小、步幅和填充，必须小心设置。如果不小心，这可能导致张量形状不正确和运行时错误。
- en: Recurrent neural networks (RNNs) are predominantly used to learn from time-series
    data.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 循环神经网络（RNNs）主要用于学习时间序列数据。
- en: The typical RNN expects the data to be organized into a three-dimensional tensor
    with a batch, time, and feature dimension.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 典型的RNN期望数据组织成具有批次、时间和特征维度的三维张量。
- en: The number of time steps the RNN looks at is an important hyperparameter that
    should be chosen based on the data.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN看的时间步数是一个重要的超参数，应该根据数据进行选择。
- en: Answers to exercises
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: '**Exercise 1:** You can do this using the Sequential API, and you will be using
    only the Dense layer.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1：** 你可以使用Sequential API 来做到这一点，你只需要使用Dense层。'
- en: '**Exercise 2**'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: '[PRE52]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '**Exercise 3**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: '[PRE53]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
