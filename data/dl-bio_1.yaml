- en: Chapter 1\. Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 简介
- en: Biology is increasingly becoming a data-driven science, and deep learning—a
    powerful subfield of machine learning—is opening new ways to uncover patterns
    in complex, high-dimensional datasets. As these two fields converge, new opportunities
    are emerging to extract meaningful insights using modern computational tools.
    This book is a practical introduction to working at that intersection, focused
    on developing the skills and mindset needed to apply deep learning effectively
    in biological contexts.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学正日益成为一个数据驱动的科学，深度学习——机器学习的一个强大子领域——正在开辟新的途径，以揭示复杂、高维数据集中的模式。随着这两个领域的融合，新的机会正在出现，可以利用现代计算工具提取有意义的见解。这本书是一本实用的入门指南，专注于在交叉领域工作，侧重于培养在生物环境中有效应用深度学习所需的技能和心态。
- en: Getting Started
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 入门
- en: This opening chapter helps you get oriented. Before jumping into code, we walk
    through how to frame a project, evaluate your data, and avoid common pitfalls.
    A bit of structure and planning up front will make your work more reproducible,
    more flexible, and ultimately more useful and impactful.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章的开篇帮助你定位。在跳入代码之前，我们探讨了如何构建项目、评估你的数据以及避免常见的陷阱。一开始就有一点结构和规划将使你的工作更具可重复性、更灵活，最终更有用和有影响力。
- en: Deciding What Your Model Will Replace
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定你的模型将取代什么
- en: The success of a deep learning project in biology often hinges on what happens
    before you write a single line of code. It’s easy to get lost in technical details
    or spend weeks exploring data and architecture variants that don’t lead to meaningful
    outcomes. Especially in a field as interesting as this one, the temptation to
    tinker is strong. To stay focused, it helps to ask a few grounding questions up
    front.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 生物学中的深度学习项目成功往往取决于你编写第一行代码之前发生的事情。很容易迷失在技术细节中，或者花费数周时间探索那些不会导致有意义结果的数据和架构变体。特别是在这样一个有趣的领域，尝试改进的诱惑很强。为了保持专注，一开始提出几个基础问题很有帮助。
- en: 'One of the most useful is: *What existing process will my model replace or
    improve?* The most impactful projects in this field often (though not always)
    have a clear answer. Here are some examples across different domains:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 其中最有用的是：*我的模型将取代或改进现有的哪个流程？* 这个领域的最有影响力的项目通常（尽管不总是）有一个明确的答案。以下是一些不同领域的例子：
- en: 'In healthcare and drug discovery:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在医疗保健和药物发现中：
- en: Skin cancer classification models
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 皮肤癌分类模型
- en: Aim to replicate a dermatologist’s ability to visually diagnose melanoma or
    other lesions from clinical images, offering faster, more scalable screening for
    at-risk populations
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在复制皮肤科医生从临床图像中视觉诊断黑色素瘤或其他病变的能力，为高风险人群提供更快、更可扩展的筛查
- en: Pathogen detection systems
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 致病微生物检测系统
- en: Trained on sequencing data or imaging to detect bacterial or viral infections
    directly from raw clinical samples (e.g., blood, saliva, or tissue), potentially
    replacing slower, culture-based diagnostics
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在测序数据或成像数据上训练，以从原始临床样本（例如血液、唾液或组织）直接检测细菌或病毒感染，可能取代较慢的培养诊断
- en: Brain tumor segmentation models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 脑肿瘤分割模型
- en: Automate or accelerate the process of outlining tumors on MRI scans, a task
    that radiologists typically perform manually and with great time investment
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化或加速在MRI扫描上勾勒肿瘤的过程，这是放射科医生通常手动进行且耗时很大的任务
- en: Drug-target interaction prediction tools
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 药物-靶标相互作用预测工具
- en: Aim to prioritize the most promising compound-target pairs, reducing the need
    for costly wet-lab screening of massive chemical libraries
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在优先考虑最有希望的化合物-靶标对，减少对大量化学库昂贵湿实验室筛选的需求
- en: Antibiotic resistance prediction models
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 抗生素耐药性预测模型
- en: Forecast whether a given bacterial strain will resist certain treatments, helping
    clinicians select effective antibiotics more quickly
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 预测特定细菌菌株是否会抵抗某些治疗，帮助临床医生更快地选择有效的抗生素
- en: 'In molecular biology:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在分子生物学中：
- en: AlphaFold
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: AlphaFold
- en: This protein structure prediction model, in many cases, replaces the need for
    experimentally determining 3D protein shapes via expensive lab-based techniques
    like X-ray crystallography, cryo-EM, or NMR
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这个蛋白质结构预测模型，在许多情况下，取代了通过昂贵的基于实验室的技术（如X射线晶体学、冷冻电镜或核磁共振）实验确定3D蛋白质形状的需求
- en: Gene expression prediction models
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 基因表达预测模型
- en: Forecast gene activity from raw genomic sequences, offering a computational
    alternative to RNA sequencing (RNA-seq) experiments
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 从原始基因组序列预测基因活性，为RNA测序（RNA-seq）实验提供计算替代方案
- en: Variant effect prediction models
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 变体效应预测模型
- en: Help automate the interpretation of genetic mutations, supporting clinical decision
    making by prioritizing likely pathogenic variants for follow-up analysis or experimental
    validation
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助自动化基因突变的解释，通过优先考虑可能致病变异的后续分析或实验验证，支持临床决策
- en: 'And in ecology and environmental science:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在生态学和环境科学中：
- en: Acoustic species classification systems
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 声学物种分类系统
- en: Use forest sound recordings to identify animal species present, offering a scalable
    and less labor-intensive alternative to in-person biodiversity surveys
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用森林声音录音来识别存在的动物物种，提供了一种可扩展且劳动强度较低的替代方案，用于现场生物多样性调查
- en: Crop disease detection
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 作物病害检测
- en: Via drone or satellite imagery, enables early identification of plant stress,
    reducing the need for manual scouting across large fields
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过无人机或卫星图像，可以早期识别植物压力，减少在大片田野上手动巡查的需求
- en: Animal facial recognition tools
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 动物面部识别工具
- en: Track individual animals, reptiles, birds, and mammals over time without the
    need for tagging, collars, or other invasive methods
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在不需要标记、项圈或其他侵入性方法的情况下，跟踪个体动物、爬行动物、鸟类和哺乳动物随时间的变化
- en: Poaching detection systems
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 盗猎检测系统
- en: Trained on infrared or motion sensor data, can automatically flag human activity
    in protected wildlife zones, assisting conservation efforts
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在红外或运动传感器数据上训练，可以自动标记受保护野生动物区的人类活动，协助保护工作
- en: Ideally this gives you a flavor of the kinds of workflows deep learning can
    improve or even replace. Where possible, try to estimate the potential impact
    of your model—how much time, cost, or manual labor it could save, or what new
    insights it might enable. This will help you stay grounded in the real-world utility
    of your work and communicate its value to collaborators, stakeholders, or the
    public.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这可以让你了解深度学习可以改进或甚至取代的工作流程类型。在可能的情况下，尝试估计你模型的潜在影响——它可以节省多少时间、成本或人工劳动，或者它可能带来哪些新的见解。这将帮助你保持对工作的现实世界效用有清晰的认识，并向合作伙伴、利益相关者或公众传达其价值。
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: That said, not every valuable model needs to replace an existing process. Some
    open up entirely new capabilities—like generating novel biological sequences,
    uncovering hidden patterns in large datasets, or linking data types that were
    never connected before. These models might not streamline a lab task, but they
    can enable new kinds of discovery, expand what questions we can ask, or offer
    fresh ways to interpret complex systems. If your model creates something new,
    just be clear about what that is and why it matters—and be thoughtful about how
    you evaluate success when no established benchmark exists.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，并非每个有价值的模型都需要取代现有的流程。一些模型开辟了全新的能力——如生成新的生物序列、揭示大型数据集中隐藏的模式，或连接以前从未连接过的数据类型。这些模型可能不会简化实验室任务，但它们可以启用新的发现类型，扩展我们可以提出的问题，或提供解释复杂系统的新方法。如果你的模型创造了新的东西，只需清楚地说明是什么以及为什么它很重要——并且要深思熟虑地评估成功，因为没有既定的基准。
- en: Determining Your Criteria for Success
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定你的成功标准
- en: It’s important to define, as early and explicitly as possible, what success
    looks like for your project. Research can be time-consuming and open-ended, so
    clear goals help you stay focused and avoid endless tweaking—repeatedly changing
    models, architectures, or training settings without a clear hypothesis or evaluation
    plan. This kind of trial-and-error loop is common in deep learning due to the
    large number of design choices and hyperparameters. Without structure, it can
    waste time and produce results that are hard to interpret or reproduce.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是尽早明确地定义你的项目成功的样子。研究可能耗时且没有明确的终点，所以明确的目标有助于你保持专注，避免无休止的调整——反复改变模型、架构或训练设置，而没有明确的假设或评估计划。这种试错循环在深度学习中很常见，因为设计选择和超参数数量庞大。没有结构，这可能会浪费时间，并产生难以解释或再现的结果。
- en: 'Examples of success criteria include:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 成功标准的例子包括：
- en: Performance metric (e.g., accuracy, AUC, F1)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 性能指标（例如，准确率、AUC、F1）
- en: You might aim to match the performance of a human expert, achieve a correlation
    with experimental results comparable to a technical replicate, or keep the false-positive
    rate below a certain number.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能的目标是匹配人类专家的表现，实现与实验结果的可比相关性，或者将假阳性率控制在一定数量以下。
- en: Level of interpretability
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性水平
- en: In many applications, it’s important not only that a model performs well, but
    also that its decisions can be understood by domain experts. For instance, you
    may prioritize well-calibrated uncertainty estimates or interpretable feature
    attributions, especially when trust and explainability are critical.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，不仅模型的表现要好，而且其决策还需要被领域专家理解。例如，你可能需要优先考虑准确校准的不确定性估计或可解释的特征归因，尤其是在信任和可解释性至关重要的情况下。
- en: Model size or inference latency
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小或推理延迟
- en: If your model needs to operate in a resource-constrained environment (e.g.,
    smartphones or embedded devices) or meet real-time throughput targets (e.g., process
    20 frames per second), your success criterion might focus on efficiency—such as
    achieving high performance per floating point operation (FLOP), which measures
    how effectively the model uses computational resources. In such cases, metrics
    like inference time, memory usage, or energy consumption may matter more than
    raw accuracy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型需要在资源受限的环境中运行（例如，智能手机或嵌入式设备）或满足实时吞吐量目标（例如，每秒处理20帧），你的成功标准可能集中在效率上——例如，实现每浮点运算（FLOP）的高性能，这衡量了模型有效使用计算资源的能力。在这种情况下，推理时间、内存使用或能耗等指标可能比原始准确性更重要。
- en: Training time and efficiency
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练时间和效率
- en: When compute is limited—or for educational contexts—you may prioritize fast
    training or minimal hardware requirements. Since training deep learning models
    typically involves large matrix operations, they are often accelerated using graphics
    processing units (GPUs). In low-resource settings, developing a simpler model
    that trains quickly on a CPU may be a more practical goal than maximizing performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当计算资源有限或处于教育环境中时，你可能需要优先考虑快速训练或最小硬件需求。由于训练深度学习模型通常涉及大量矩阵运算，它们通常使用图形处理单元（GPU）来加速。在资源有限的环境中，开发一个在CPU上快速训练的简单模型可能比最大化性能更实际。
- en: Generalizability
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化能力
- en: In some cases, the goal is to build a model that works well across many datasets
    or tasks, rather than one that is finely tuned to a single benchmark. For example,
    *foundational models*—large models trained on broad datasets that can be adapted
    to many downstream applications—prioritize flexibility and reuse. In such settings,
    broad applicability may be more valuable than squeezing out the best possible
    performance on a specific task.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，目标是构建一个在许多数据集或任务上表现良好的模型，而不是一个针对单个基准精心调优的模型。例如，*基础模型*——在广泛数据集上训练的大模型，可以适应许多下游应用——优先考虑灵活性和重用。在这种情况下，广泛的适用性可能比在特定任务上获得最佳性能更有价值。
- en: 'Defining these goals up front helps you answer the key question: *When is the
    project done?* You’ll likely need to balance multiple criteria—but having them
    laid out early will keep your efforts aligned and your scope realistic.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在一开始就定义这些目标有助于你回答关键问题：“项目何时完成？”你可能会需要平衡多个标准，但尽早列出它们将保持你的努力一致，并使你的范围保持现实。
- en: Invest Heavily in Evaluations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在评估上投入大量精力
- en: Once you’ve defined your criteria for success, it’s time to prioritize *evaluation*.
    This means thinking carefully about precisely how you’ll measure progress—including
    what metrics you’ll use, how you’ll validate results, and which baselines you’ll
    compare against. Without a clear, well-designed evaluation strategy, even a technically
    impressive model can fail to produce meaningful conclusions.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了成功的标准，就是时候优先考虑*评估*了。这意味着仔细思考你将如何衡量进度——包括你将使用哪些指标，你将如何验证结果，以及你将比较哪些基线。如果没有明确、精心设计的评估策略，即使是一个技术上令人印象深刻的模型也可能无法产生有意义的结论。
- en: 'Strong evaluations don’t just help you measure progress. They also help you
    detect bugs, estimate task difficulty, and build intuition. The key idea is simple:
    you need a known point of comparison to understand if your model is doing anything
    meaningful.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的评估不仅有助于你衡量进度，还有助于你检测错误、估计任务难度和建立直觉。关键思想很简单：你需要一个已知的比较点来了解你的模型是否正在做有意义的事情。
- en: Tip
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'While no general rule exists, it wouldn’t be surprising if successful machine
    learning projects spent 50% of their time designing evaluation strategies and
    running baselines, 25% curating or processing data, and only 25% on model architecture.
    Without good evaluations, you’re flying blind: you won’t know whether your model
    is actually improving, what trade-offs you’re making, or even whether it’s learning
    anything meaningful at all.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然没有普遍的规则，但如果成功的机器学习项目花费50%的时间设计评估策略和运行基线，25%的时间整理或处理数据，只有25%的时间用于模型架构，这并不会令人惊讶。没有良好的评估，你就像是在盲目飞行：你不会知道你的模型是否真的在改进，你正在做出哪些权衡，甚至是否在学习任何有意义的内容。
- en: Spend time here. Evaluation isn’t just something you do at the end. It’s something
    you design at the beginning, and it guides the entire project.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里花些时间。评估不仅仅是你在项目结束时做的事情。它是在项目开始时设计的，并指导整个项目。
- en: Designing Baselines
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计基线
- en: One of the most practical evaluation tools is a strong *baseline*—a simple method
    that gives you something to beat. Good baselines help you measure progress, catch
    bugs early, and understand the difficulty of your task. Sometimes they can even
    be surprisingly hard to beat.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最实用的评估工具之一是一个强大的**基线**——一个简单的方法，让你有东西可以超越。好的基线有助于你衡量进度，早期发现错误，并理解你任务的难度。有时它们甚至可能难以超越。
- en: 'Designing good baselines requires thinking carefully about the task. Here are
    a few common baseline strategies for classification tasks:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 设计好的基线需要仔细思考任务。以下是针对分类任务的几种常见基线策略：
- en: Random prediction
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 随机预测
- en: Assign labels completely at random, with equal probability for each class. This
    tells you what performance looks like with no information at all.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 完全随机分配标签，每个类别的概率相等。这告诉你没有任何信息时的性能看起来如何。
- en: Random prediction weighted by class frequencies
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 根据类别频率加权的随机预测
- en: Sample labels randomly, but in proportion to how often they occur in the training
    data. This is useful for imbalanced datasets.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随机采样标签，但按它们在训练数据中出现的频率成比例。这对于不平衡数据集很有用。
- en: Majority class
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数类别
- en: Always predict the most common class. This can be a surprisingly hard baseline
    to beat in highly class imbalanced settings.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 总是预测最常见的类别。在高度类别不平衡的情况下，这可能是一个难以超越的基线。
- en: Nearest neighbor
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻
- en: Predict the label of the most similar example in the training data (e.g., 1-nearest
    neighbor using Euclidean distance). This is often effective when inputs are low
    dimensional or well structured.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 预测训练数据中最相似示例的标签（例如，使用欧几里得距离的1-最近邻）。当输入是低维或结构良好时，这通常很有效。
- en: 'And for regression tasks:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归任务：
- en: Mean or median of the target
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 目标的平均值或中位数
- en: Always predict the average or median target value from the training set. This
    often matches what a model would do if it’s not learning anything meaningful.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 总是预测训练集中的平均或中位数目标值。这通常与模型如果没有学习到任何有意义的内容会做的事情相匹配。
- en: Linear regression with a single feature
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 单特征线性回归
- en: Fit a line using just the strongest individual predictor (e.g., one biomarker).
    This helps gauge how much a more complex model improves over a simple signal.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用最强的单个预测因子（例如，一个生物标志物）拟合一条线。这有助于衡量比简单信号更复杂的模型改进了多少。
- en: K-nearest neighbor regression
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: K-最近邻回归
- en: Predict the target as the average (or weighted average) of the *k* most similar
    data points. This is simple to implement and often surprisingly competitive on
    structured datasets.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 预测目标为最相似的**k**个数据点的平均值（或加权平均值）。这在结构化数据集上简单易行，并且通常具有出人意料的竞争力。
- en: 'And for both:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两者：
- en: Simple heuristics
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 简单启发式方法
- en: Use straightforward rules based on domain knowledge. For example, in diagnostics,
    classify a patient as positive if a single biomarker or measurement exceeds a
    threshold. For skin cancer images, rank lesions by average pixel intensity. In
    genomics, if the task is to predict which gene a mutation affects, a simple baseline
    is to assume it affects the nearest gene in the genome.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于领域知识的简单规则。例如，在诊断中，如果单个生物标志物或测量值超过阈值，则将患者分类为阳性。对于皮肤癌图像，按平均像素强度对病变进行排序。在基因组学中，如果任务是预测突变影响的基因，一个简单的基线是假设它影响基因组中最接近的基因。
- en: Warning
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If your model can’t beat a basic baseline, something is likely off—and that’s
    useful to know. It’s a key signal to revisit your data, features, or modeling
    approach.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型无法击败基本基线，那么可能有问题——这很有用。这是重新审视你的数据、特征或建模方法的关键信号。
- en: Time-Boxing Your Project
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间盒你的项目
- en: It’s important to time-box your project—that is, set a fixed amount of time
    to work on it, after which you pause or stop regardless of the outcome. Many research
    ideas “fail” in the sense that they don’t achieve the desired metrics. That’s
    normal. All projects, even the unsuccessful ones, generate insights that inform
    future work. Time-boxing helps ensure that failed experiments still move you forward—without
    consuming unlimited time and energy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为项目设置时间限制很重要——也就是说，设定一个固定的时间来工作，之后无论结果如何都要暂停或停止。许多研究想法“失败”在意义上是它们没有达到预期的指标。这是正常的。所有项目，即使是那些不成功的项目，也会产生有助于未来工作的见解。时间分箱有助于确保失败的实验仍然能推动你前进——而不消耗无限的时间和精力。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Time-boxing doesn’t mean giving up easily—it means setting boundaries to maintain
    focus, avoid burnout, and keep making progress.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分箱并不意味着轻易放弃——这意味着设定界限以保持专注，避免疲劳，并继续取得进步。
- en: 'Here are some tips for time-boxing effectively:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有效时间分箱的技巧：
- en: 1\. Set a clear deadline
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 设定一个明确的截止日期
- en: Choose a realistic time frame (e.g., two weeks, three months) and stick to it.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个现实的时间框架（例如，两周、三个月）并坚持下去。
- en: 2\. Define checkpoints
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 定义检查点
- en: Identify intermediate milestones—like completing dataset preprocessing, training
    a baseline model, or hitting a certain accuracy—to track progress.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 确定中间里程碑——比如完成数据集预处理、训练基线模型或达到一定的准确率——以跟踪进度。
- en: 3\. Reflect at the end
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 结束时反思
- en: Take time to evaluate what worked, what didn’t, and what you learned.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间评估哪些有效，哪些无效，以及你学到了什么。
- en: Tip
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Time-boxing is also useful *within* a project. For example: “I’ll experiment
    with this new processing or modeling idea for one week, and if it doesn’t help,
    I’ll move on.”'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分箱在项目内部也很有用。例如：“我将尝试这个新的处理或建模想法一周，如果不起作用，我就继续前进。”
- en: The biggest risk with time-boxing is yourself. It’s easy to justify extensions,
    add new ideas, or convince yourself you’ll strike gold if you just try 10 more
    things. Scope creep and perfectionism are common traps. In these cases, it can
    help to talk to someone else—a collaborator, mentor, or friend—to get perspective
    and avoid spinning your wheels. A quick conversation can often cut through indecision
    or obsessiveness and help you refocus on the broader context.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 时间分箱的最大风险是你自己。很容易为延期辩护，添加新想法，或者说服自己如果再尝试10次就能找到金子。范围蔓延和完美主义是常见的陷阱。在这些情况下，与他人交谈——合作者、导师或朋友——以获得观点并避免原地打转可能会有所帮助。简短的交谈往往能消除犹豫不决或执着，帮助你重新关注更广泛的背景。
- en: Deciding Whether You Really Need Deep Learning
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 判断你是否真的需要深度学习
- en: 'This might seem like strange advice in a deep learning book, but before diving
    in, take a moment to ask yourself: *Do I actually need deep learning for this
    problem?* We’ll say it again—seriously consider simpler approaches.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这在深度学习书中可能看起来像奇怪的忠告，但在深入之前，花点时间问问自己：*我真的需要深度学习来解决这个问题吗？* 我们再说一遍——认真考虑更简单的方法。
- en: 'Deep learning models are powerful (and undeniably interesting), but they’re
    also resource intensive, complex to train, and difficult to debug. In many cases,
    traditional methods—like linear regression, decision trees, or basic statistical
    techniques—can achieve your goals with far less effort. These approaches are often:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型强大（并且不可否认地有趣），但它们也是资源密集型的，训练复杂，调试困难。在许多情况下，传统方法——如线性回归、决策树或基本统计技术——可以以远少于努力达到你的目标。这些方法通常：
- en: Easier to implement
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 更容易实现
- en: Quicker to set up and require less expertise
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 更快设置并需要更少的专长
- en: Less computationally demanding
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 计算量要求较低
- en: Can run on standard hardware with minimal training time
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在标准硬件上运行，并且训练时间最少
- en: More interpretable
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更易于理解
- en: Easier to explain, troubleshoot, and validate
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 更容易解释、故障排除和验证
- en: Carefully weigh the trade-offs. If a simpler method delivers the insights or
    performance you need, it’s often the smarter and more efficient path.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细权衡利弊。如果更简单的方法提供了你需要的见解或性能，那么它通常是更明智和更高效的途径。
- en: Ensuring That You Have Enough Good Data
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确保你有足够的好数据
- en: Deep learning models don’t just need a lot of data—they also generally need
    high-quality data. Models trained on poor data can often fail catastrophically,
    regardless of their sophistication.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型不仅需要大量数据，通常还需要高质量的数据。在较差数据上训练的模型往往可能失败得非常严重，无论它们的复杂性如何。
- en: 'Make sure you have:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你有：
- en: Sufficient quantity
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 足够的数量
- en: Deep models typically need thousands of examples or more. What counts as “enough”
    data depends on your problem and architecture. Check relevant literature for benchmarks.
    If you’re working with a small dataset, consider *transfer learning*, where you
    start from a model trained on a related task and fine-tune it on your own data.
    This approach can dramatically reduce the amount of data needed to achieve good
    performance.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 深度模型通常需要数千个或更多的示例。什么算是“足够”的数据取决于你的问题和架构。检查相关文献以获取基准。如果你正在处理小数据集，可以考虑*迁移学习*，即从一个与相关任务训练的模型开始，并在你自己的数据上微调它。这种方法可以显著减少实现良好性能所需的数据量。
- en: Sufficient quality
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 足够的质量
- en: Clean, consistent data is critical. Label errors, noise, or inconsistencies
    can seriously degrade performance. Even large language models—like those powering
    modern chat-based assistant systems—can benefit substantially from training on
    carefully curated, high-quality data. Prioritize quality checks and thoughtful
    curation.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁、一致的数据至关重要。标签错误、噪声或不一致性会严重影响性能。即使是像现代基于聊天助手系统所使用的那些大型语言模型，从精心整理的高质量数据中训练也能获得显著的好处。优先考虑质量检查和深思熟虑的整理。
- en: Assembling a Team
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组建团队
- en: 'Working alone is totally fine—but teaming up can accelerate progress, improve
    your ideas, and make the process more enjoyable. Here are some tips for finding
    great collaborators:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 单独工作完全可以接受——但团队合作可以加速进度，改进你的想法，并使整个过程更加愉快。以下是一些寻找优秀合作伙伴的建议：
- en: Engage with the community
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与社区互动
- en: Join relevant forums, online groups, or webinars to connect with others, share
    ideas, and discover potential collaborators. Communities like Reddit, Discord
    servers, X, and specialized Slack groups can be great starting points.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 加入相关的论坛、在线小组或网络研讨会，与他人建立联系，分享想法，并发现潜在的合作伙伴。像Reddit、Discord服务器、X和专门的Slack小组这样的社区可以成为很好的起点。
- en: Participate in hackathons and competitions
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 参与黑客马拉松和竞赛
- en: Platforms like Kaggle, Zindi, or local university events offer structured challenges,
    feedback, and opportunities to meet people with similar interests.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle、Zindi或当地大学活动等平台提供结构化的挑战、反馈以及与有相似兴趣的人见面的机会。
- en: Form an interdisciplinary team
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 组建跨学科团队
- en: Combining expertise from different areas often leads to stronger projects. If
    you’re a biologist, team up with someone in machine learning, and vice versa.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结合不同领域的专业知识通常会导致更强大的项目。如果你是生物学家，与机器学习领域的人合作，反之亦然。
- en: Collaborate with experts
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 与专家合作
- en: Domain experts can help shape your approach and identify blind spots early.
    Look for collaborators at conferences or workshops, or reach out to authors of
    relevant papers. Strangers are surprisingly responsive to genuine cold requests
    from interested people.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 领域专家可以帮助塑造你的方法并及早识别盲点。在会议或研讨会上寻找合作伙伴，或联系相关论文的作者。陌生人对于感兴趣的人的真诚冷请求反应惊人。
- en: 'And here are some tips for once you find people to work with:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些找到合作伙伴后可以遵循的建议：
- en: Define clear goals and roles
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 明确目标和角色
- en: When working with others, it helps to clarify responsibilities early—who’s doing
    what, what success looks like, and how decisions will be made. This avoids misunderstandings
    and keeps the project moving.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当与他人合作时，尽早明确责任有助于澄清——谁在做什么，成功的样子是什么，以及如何做出决策。这可以避免误解并使项目顺利进行。
- en: Use shared tools for collaboration
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用共享工具进行协作
- en: Version control (like Git), shared notebooks (e.g., Google Colab), and simple
    task trackers (like Notion or Trello, or just a shared Google Doc with some lists)
    can make it much easier to coordinate and stay organized.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制（如Git）、共享笔记本（例如Google Colab）和简单的任务跟踪器（如Notion或Trello，或者只是一个带有一些列表的共享Google文档）可以使协调和保持组织化变得更加容易。
- en: Support specialization
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 支持专业化
- en: Let people lean into the parts of the project they enjoy most—some may focus
    on infrastructure and software engineering, others on data curation, modeling,
    or biological interpretation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让人们投入到他们最喜欢的项目部分——有些人可能专注于基础设施和软件开发，其他人可能专注于数据整理、建模或生物解释。
- en: Start small
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 从小做起
- en: If you’re unsure about long-term compatibility, try a short project or exploration
    together first. A small, low-pressure collaboration is a great way to test the
    waters.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定长期兼容性，可以先尝试一个短期项目或探索。一个小型、低压力的合作是一个很好的测试水深的办法。
- en: Whether you’re working solo or as part of a team, the most important thing is
    to stay curious, keep learning, and take that first step.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是单独工作还是作为团队的一部分，最重要的是保持好奇心，持续学习，并迈出第一步。
- en: You Don’t Need a Supercomputer or a PhD
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你不需要超级计算机或博士学位
- en: 'There are a few common misconceptions about working in deep learning for biology
    that are worth challenging:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在生物学中从事深度学习的一些常见误解值得挑战：
- en: You need huge budgets and compute power.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要巨大的预算和计算能力。
- en: 'In an age of multimillion-dollar training runs for massive language models,
    it’s easy to assume you need vast resources. But that’s not always the case:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模语言模型数百万美元的训练运行的时代，很容易认为你需要大量的资源。但情况并不总是如此：
- en: Prototype with small models
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用小型模型进行原型设计
- en: Start small to iterate quickly. You might find that lightweight models are more
    than enough for your goals.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从小做起，快速迭代。你可能会发现轻量级模型对你的目标来说已经足够了。
- en: Use free or affordable compute
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用免费或负担得起的计算资源
- en: Platforms like Google Colab and Kaggle offer free GPU access for smaller projects.
    For more demanding workloads, cloud providers such as Amazon Web Services (AWS),
    Microsoft Azure, and Google Cloud Platform (GCP) offer scalable paid instances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 平台如Google Colab和Kaggle为小型项目提供免费的GPU访问。对于更苛刻的工作负载，云服务提供商如亚马逊网络服务（AWS）、微软Azure和谷歌云平台（GCP）提供可扩展的付费实例。
- en: Not everything is about scale
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有事情都关乎规模
- en: Many valuable projects focus on analyzing existing models rather than training
    new ones. These often require modest compute but can yield deep insight. There’s
    still a lot we don’t understand about how deep models behave.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 许多有价值的项目专注于分析现有模型，而不是训练新的模型。这些通常只需要适度的计算能力，但可以提供深刻的见解。我们对深度模型行为仍有许多不了解的地方。
- en: You need deep expertise in machine learning or biology (or both).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要机器学习或生物学（或两者）的深厚专业知识。
- en: 'Another myth is that only highly trained experts can contribute meaningfully.
    In reality:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个神话是只有高度训练的专家才能做出有意义的贡献。实际上：
- en: Better tools
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的工具
- en: Modern frameworks make it easier than ever to build and experiment with powerful
    models.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现代框架使构建和实验强大模型比以往任何时候都更容易。
- en: Open source culture
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 开源文化
- en: Freely available code and pretrained models let you learn from and build on
    existing work.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 免费可用的代码和预训练模型让你可以从现有工作中学习和构建。
- en: Educational resources
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 教育资源
- en: There’s now no shortage of tutorials, videos, and walkthroughs online to help
    you get started.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在网络上不缺乏教程、视频和操作指南，可以帮助你入门。
- en: Plenty of untapped problems
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 许多未被开发的难题
- en: Many important biological questions remain unexplored by machine learning. You
    don’t need a PhD or a Kaggle medal to work on them.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 许多重要的生物学问题尚未被机器学习探索。你不需要博士学位或Kaggle奖牌来从事这些工作。
- en: While the bleeding edge of research may require specialized knowledge and high-end
    infrastructure, there’s plenty of room in this field for curiosity, creativity,
    and new perspectives—no supercomputer required.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然研究的前沿可能需要专业知识和高端基础设施，但在这个领域仍有很大的空间供好奇心、创造力和新视角发挥作用——无需超级计算机。
- en: Tip
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'As you explore this field, you’ll almost certainly come across academic papers—whether
    you’re digging into a specific method, reading related work, or looking for project
    ideas. Both biology and machine learning papers can feel impenetrable at first:
    the language is dense, the ideas are highly condensed, and there’s often a lot
    of jargon. But remember:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当你探索这个领域时，你几乎肯定会遇到学术论文——无论你是深入研究一个特定方法，阅读相关工作，还是寻找项目想法。生物学和机器学习论文一开始可能感觉难以理解：语言密集，思想高度浓缩，而且经常有很多术语。但请记住：
- en: You’re seeing the result of months or years of work by a team of researchers—and
    encountering it for the first time.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你正在看到一支研究团队数月或数年的工作成果——并且是第一次遇到它。
- en: Reading papers is a skill, and like any skill, it improves with practice.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阅读论文是一项技能，就像任何技能一样，它通过实践而提高。
- en: Blog posts, YouTube videos, and open source projects can also be great, more
    accessible ways to learn the same concepts.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博客文章、YouTube 视频和开源项目也可以是学习相同概念的更易于获取的方式。
- en: With that background in place, let’s dive into the technical foundations of
    this book.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在具备这样的背景之后，让我们深入探讨这本书的技术基础。
- en: Technical Introduction
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术简介
- en: We’ll be using Python-based deep learning frameworks, in particular, JAX and
    Flax. JAX is a system for high-performance numerical computing and machine learning,
    and Flax is a flexible neural network library built on top of JAX. We’ll motivate
    by explaining why we’re using JAX and reviewing a few Python features that tend
    to come up in a lot of machine learning (ML) code. Then, we’ll introduce some
    foundational machine learning concepts recurring throughout, with the main one
    being how a training loop is structured.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基于Python的深度学习框架，特别是JAX和Flax。JAX是一个高性能数值计算和机器学习系统，Flax是一个构建在JAX之上的灵活神经网络库。我们将通过解释为什么我们选择JAX以及回顾一些在许多机器学习（ML）代码中经常出现的Python特性来激发动机。然后，我们将介绍一些在本书中反复出现的机器学习基础概念，主要概念是训练循环的结构。
- en: Tip
  id: totrans-160
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: As mentioned in [“Prerequisites”](preface01.html#id109), this book assumes basic
    knowledge of Python. If you’re new to Python, check out the recommended resources
    listed there first.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如[“先决条件”](preface01.html#id109)中所述，本书假设您对Python有基本了解。如果您是Python新手，请先查看那里列出的推荐资源。
- en: Finally, to avoid repetition across chapters, we’ve created a small companion
    library called `dlfb` (Deep Learning for Biology), which wraps up common utilities
    and components. We’ll reference it throughout the book.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了避免章节之间的重复，我们创建了一个名为`dlfb`（生物学的深度学习）的小型配套库，它封装了常见的实用工具和组件。本书中我们将多次引用它。
- en: Tip
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Don’t worry if some parts of this technical introduction feel unfamiliar or
    challenging at first. You’re welcome to skim or skip ahead. Many of the concepts
    will become clearer when you see them in action later in the book.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个技术介绍的部分内容一开始感觉不熟悉或具有挑战性，请不要担心。您可以随意浏览或跳过。当您在本书后面的部分看到这些概念的实际应用时，许多概念将变得更加清晰。
- en: Why JAX and Flax?
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择JAX和Flax？
- en: This book uses the JAX and Flax ecosystem. But why did we make this choice,
    when other options like PyTorch or Keras are more common?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用JAX和Flax生态系统。但为什么我们选择这个选项，而其他选项如PyTorch或Keras更为常见呢？
- en: 'First, some honesty: there is no one “best” framework. All of them can be used
    to build effective biological models, and many of the concepts in this book will
    carry over easily if you’re using PyTorch or Keras.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，有些诚实：没有一种“最佳”的框架。所有这些框架都可以用来构建有效的生物模型，并且如果使用PyTorch或Keras，本书中的许多概念将很容易迁移。
- en: 'We’ve chosen JAX/Flax primarily because:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择JAX/Flax的主要原因如下：
- en: Familiar NumPy API
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 熟悉的NumPy API
- en: JAX’s `jax.numpy` module (commonly imported as `jnp`) offers an API that so
    closely mirrors standard NumPy for array manipulation and mathematical operations
    that NumPy `np` calls can often be directly substituted with `jnp`. This means
    users already proficient with NumPy can transition to JAX with a significantly
    reduced learning curve, leveraging their existing knowledge to quickly build and
    adapt code while gaining JAX’s powerful transformations and accelerator support.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的`jax.numpy`模块（通常导入为`jnp`）提供了一个与标准NumPy在数组操作和数学运算上非常相似的API，以至于NumPy `np`调用通常可以直接用`jnp`替换。这意味着已经熟练掌握NumPy的用户可以显著降低学习曲线，利用现有知识快速构建和适应代码，同时获得JAX强大的转换和加速支持。
- en: Functional programming encourages clarity
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式编程鼓励清晰性
- en: JAX’s pure function style can reduce hidden state and make training logic more
    transparent. This fits well with the educational goals of the book—explicit is
    better than implicit.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: JAX的纯函数风格可以减少隐藏状态，使训练逻辑更加透明。这与本书的教育目标——明确优于隐晦——非常契合。
- en: Transformations are first-class
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 转换是第一类
- en: JAX provides powerful, composable transformations like `jit` (just-in-time compilation),
    `grad` (automatic differentiation), and `vmap` (vectorization) that work cleanly
    on Python functions. These tools simplify and unify many aspects of model training
    and evaluation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: JAX提供了强大的、可组合的转换，如`jit`（即时编译）、`grad`（自动微分）和`vmap`（向量化），这些转换可以在Python函数上干净地工作。这些工具简化并统一了模型训练和评估的许多方面。
- en: JAX aligns with cutting-edge research
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: JAX与前沿研究保持一致
- en: JAX has gained traction in recent machine learning research, particularly for
    biology, physics, and large-scale models. Using it here helps you align with newer
    toolchains and experiment with modern practices.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: JAX在最近的机器学习研究中获得了关注，尤其是在生物学、物理学和大规模模型方面。在这里使用它可以帮助您与更新的工具链保持一致，并尝试现代实践。
- en: Speed
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 速度
- en: JAX uses a compiler that can yield significant performance gains on specialized
    hardware such as GPUs (e.g., from NVIDIA or AMD) and TPUs (made by Google), making
    it well-suited for large-scale deep learning workloads. This compiler is based
    on XLA, a low-level system for optimizing numerical computations on accelerators.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: JAX使用一个编译器，可以在专门的硬件（如NVIDIA或AMD的GPU）和Google制造的TPU上产生显著的性能提升，这使得它非常适合大规模深度学习工作负载。这个编译器基于XLA，这是一个用于在加速器上优化数值计算的底层系统。
- en: 'That said, JAX and Flax come with trade-offs: a smaller ecosystem and APIs
    that evolve quickly (sometimes breaking things along the way). And while JAX can
    offer impressive speedups, that speed isn’t exclusive to JAX/Flax. For example,
    [Keras](https://keras.io) now supports a JAX backend, offering another option
    for users who prefer a higher-level API. If you’re already comfortable with PyTorch,
    Keras, or TensorFlow, you’re welcome to implement the ideas in this book using
    those tools—and even contribute your own version to this book’s repository.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，JAX和Flax都有其权衡：较小的生态系统和快速演变的API（有时在演变过程中会破坏某些东西）。虽然JAX可以提供令人印象深刻的加速，但这种速度并不仅限于JAX/Flax。例如，[Keras](https://keras.io)现在支持JAX后端，为更喜欢高级API的用户提供了另一种选择。如果你已经熟悉PyTorch、Keras或TensorFlow，你当然可以使用这些工具来实现本书中的想法——甚至可以将你自己的版本贡献给本书的仓库。
- en: It’s not necessary when you’re just starting out, but over time it can be helpful
    to become familiar with more than one deep learning framework. Each has strengths
    in different ecosystems—for example, we use PyTorch in Chapter 2 to extract pretrained
    embeddings from a Hugging Face model, since many models on Hugging Face are primarily
    released and maintained in PyTorch.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 当你刚开始时，这并不是必要的，但随着时间的推移，熟悉多个深度学习框架可能会有所帮助。每个框架在不同的生态系统中都有其优势——例如，我们在第二章中使用PyTorch从Hugging
    Face模型中提取预训练嵌入，因为Hugging Face上的许多模型主要是以PyTorch的形式发布和维护的。
- en: Warning
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: The deep learning field moves fast. While we use the `linen` API in Flax throughout
    this book, a newer API called `nnx` has recently emerged as the recommended way
    to build models. `linen` remains fully supported, but be aware that you may come
    across other tutorials or examples that use `nnx`, which has a slightly different
    syntax.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域发展迅速。虽然我们在这本书中始终使用Flax的`linen` API，但最近出现了一个名为`nnx`的新API，已成为构建模型的推荐方式。`linen`仍然完全受支持，但请注意，你可能会遇到使用`nnx`的其他教程或示例，其语法略有不同。
- en: We’ll introduce key JAX concepts as needed throughout the book, but we won’t
    cover the entire library in detail. For more detailed hands-on learning, check
    out the official [JAX tutorials](https://oreil.ly/Jcqtu). And if you run into
    unexpected behavior, the JAX [“sharp bits” notebook](https://oreil.ly/TcOzQ) is
    an excellent reference for common gotchas and how to avoid them.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中根据需要介绍关键的JAX概念，但不会详细涵盖整个库。为了更详细的动手学习，请查看官方[JAX教程](https://oreil.ly/Jcqtu)。如果你遇到意外行为，JAX的[“sharp
    bits”笔记本](https://oreil.ly/TcOzQ)是了解常见问题及其解决方法的绝佳参考。
- en: A note on performance
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关于性能的说明
- en: As this is an educational book, our focus is on clarity rather than peak performance.
    That means we won’t cover things like precision tuning, advanced hardware strategies,
    or distributed training. But these things can matter a lot in real-world setups.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一本教育书籍，我们的重点是清晰度而不是峰值性能。这意味着我们不会涵盖诸如精确调整、高级硬件策略或分布式训练等内容。但在现实世界的设置中，这些事情可能非常重要。
- en: 'If you’re comfortable with the basics and want to go deeper, here are a few
    areas worth exploring:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉基础知识并想进一步探索，以下是一些值得研究的地方：
- en: Numerical precision and tuning
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 数值精度和调整
- en: Many machine learning operations, especially matrix multiplications (matmuls),
    benefit from reduced-precision formats like `bfloat16`, which can significantly
    improve speed and memory usage with minimal impact on model accuracy. JAX lets
    you control the precision used for matmuls via `jax.default_matmul_precision`,
    helping you take advantage of specialized hardware like Tensor Cores (on NVIDIA
    GPUs) or matrix units (on TPUs). Lower-precision training is widely used in large-scale
    setups because it enables training larger models more efficiently and cost-effectively.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习操作，尤其是矩阵乘法（matmuls），可以从降低精度格式（如`bfloat16`）中受益，这可以在最小影响模型精度的前提下显著提高速度和内存使用。JAX允许你通过`jax.default_matmul_precision`来控制用于matmuls的精度，帮助你利用专门的硬件，如NVIDIA
    GPU上的Tensor Cores或TPU上的矩阵单元。在大型设置中，低精度训练被广泛使用，因为它可以更高效、更经济地训练更大的模型。
- en: Profiling tools like `jax.profiler` or TensorBoard
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 剖析工具，如`jax.profiler`或TensorBoard
- en: Profiling helps you identify where your code is spending time and memory so
    you can spot bottlenecks in training and optimize the most expensive operations.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 性能分析可以帮助你识别代码在时间和内存上的消耗，从而在训练中找到瓶颈并优化最昂贵的操作。
- en: Memory-efficient training techniques
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 内存高效的训练技术
- en: Methods like gradient checkpointing (remat in JAX) let you trade off computation
    for memory, allowing you to train deeper models without running out of RAM.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度检查点（JAX 中的 remat）等方法让你可以在计算和内存之间进行权衡，允许你在不耗尽 RAM 的情况下训练更深的模型。
- en: Multihost/multidevice training
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 多主机/多设备训练
- en: Training across multiple GPUs, TPUs, or even machines allows you to scale up
    models and datasets that wouldn’t fit on a single device.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个 GPU、TPU 或甚至机器上训练可以使你扩展模型和数据集，这些模型和数据集无法适应单个设备。
- en: You won’t need any of these to follow this book, but they are good to be aware
    of and are worth exploring as you grow more comfortable with the JAX ecosystem.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读这本书的过程中，你可能不需要这些内容，但了解它们是有益的，随着你对 JAX 生态系统的熟悉，它们值得探索。
- en: Python Tips
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 小贴士
- en: While this book doesn’t cover Python background knowledge in depth, this section
    highlights a few helpful Python concepts you’re likely to encounter when working
    with machine learning code in general—and with JAX and Flax in particular.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书没有深入介绍 Python 的基础知识，但本节突出了几个在处理机器学习代码时可能会遇到的有用 Python 概念——特别是与 JAX 和 Flax
    相关的。
- en: Type annotations and docstrings
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类型注解和文档字符串
- en: Python is a dynamically typed language, meaning you don’t need to declare variable
    types (such as strings or integers) explicitly. Instead, types are determined
    at runtime, which makes the language flexible—but this flexibility can also make
    bugs harder to catch, especially in larger codebases. Adding *type annotations*
    helps mitigate this by improving readability, enabling static type checks with
    tools like `mypy`, and making debugging easier.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Python 是一种动态类型语言，这意味着你不需要显式声明变量类型（如字符串或整数）。相反，类型是在运行时确定的，这使得语言灵活——但这种灵活性也可能使错误更难捕捉，尤其是在大型代码库中。添加
    *类型注解* 通过提高可读性、使用 `mypy` 等工具进行静态类型检查以及使调试更容易来减轻这一点。
- en: 'Here’s a simple function that computes the mean squared error (MSE) between
    two NumPy arrays:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的函数，用于计算两个 NumPy 数组之间的均方误差（MSE）：
- en: '[PRE0]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is an example of its use:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是它使用的一个例子：
- en: '[PRE1]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can improve this function by adding type hints to specify that the inputs
    are `np.ndarray` objects and the return type is a `float`, along with a docstring
    to explain what the function does:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过添加类型提示来改进这个函数，指定输入是 `np.ndarray` 对象，返回类型是 `float`，以及一个文档字符串来解释函数的功能：
- en: '[PRE3]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'These changes don’t affect the function’s behavior, but they offer several
    benefits:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这些更改不会影响函数的行为，但它们提供了几个好处：
- en: Clarify input and output types
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 明确输入和输出类型
- en: 'It’s immediately clear that `y_true` and `y_pred` should be NumPy arrays and
    the return value is a `float`. Note that some machine learning code goes further
    by specifying the data type within the array (e.g., `arr: NDArray[np.float64]`),
    but we will not do this in this book.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '很明显，`y_true` 和 `y_pred` 应该是 NumPy 数组，返回值是一个 `float`。请注意，一些机器学习代码会进一步指定数组中的数据类型（例如，`arr:
    NDArray[np.float64]`），但在这本书中我们不会这样做。'
- en: Enhance documentation
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 增强文档
- en: IDEs and documentation tools can provide better inline help and autocompletion.
    This can really improve productivity.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 集成开发环境（IDE）和文档工具可以提供更好的内联帮助和自动完成功能。这可以真正提高生产力。
- en: Improve readability
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 提高可读性
- en: The function is easier to understand for others (or your future self).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 函数对其他人（或你未来的自己）来说更容易理解。
- en: Enable static checking
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 启用静态检查
- en: Tools like `mypy` can catch type-related errors.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`mypy` 等工具可以捕获与类型相关的错误。'
- en: The MSE example is very simple, so adding type hints and a full docstring is
    arguably overkill—but the principle is important.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: MSE 示例非常简单，所以添加类型提示和完整的文档字符串可能是过度杀鸡用牛刀——但原则很重要。
- en: We won’t necessarily always use typing and docstrings throughout this book due
    to space constraints, but they’re good habits to adopt in your own projects. When
    we do include docstrings in the main book text, we’ll usually keep them to a single
    line, which will save a few trees in printing.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们在这本书中不一定总是使用类型注解和文档字符串，但养成良好的习惯在自己的项目中是有益的。当我们确实在正文文本中包含文档字符串时，我们通常会将其限制在一行，这样可以节省一些打印纸张。
- en: Decorators
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 装饰器
- en: '*Decorators* are functions that modify the behavior of other functions or methods.
    In machine learning and data science, decorators are often used to enhance performance,
    cache results, or log function behavior.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*装饰器*是修改其他函数或方法行为的函数。在机器学习和数据科学中，装饰器通常用于提高性能、缓存结果或记录函数行为。'
- en: Just-in-time (JIT) compilation with JAX
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用JAX进行即时（JIT）编译
- en: One of the most common decorators when working in JAX is `jax.jit`, which performs
    JIT compilation to accelerate code execution. The first run of a JIT-compiled
    function is slower because it is compiled to XLA (Accelerated Linear Algebra)
    machine code. However, subsequent calls run significantly faster.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在JAX中工作时最常用的装饰器之一是`jax.jit`，它执行JIT编译以加速代码执行。JIT编译函数的第一次运行较慢，因为它被编译为XLA（加速线性代数）机器代码。然而，后续调用运行得要快得多。
- en: 'Suppose we have a function that takes a JAX array, raises all values to the
    10th power, and sums them:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个函数，它接受一个JAX数组，将所有值提高到10次幂，并将它们相加：
- en: '[PRE4]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE5]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can speed up this function in one of two ways. Either directly:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式加快这个函数。要么直接：
- en: '[PRE6]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Or, via the Python syntactic sugar, we can apply it when defining the function
    with the `@jax.jit` decorator:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，通过Python语法糖，我们可以在使用`@jax.jit`装饰器定义函数时应用它：
- en: '[PRE8]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Output:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The output remains the same for all, but the jitted functions run much faster.
    If you are working in a Jupyter notebook, you can use `%timeit` to measure the
    execution time for a line of code (or `%%timeit` for entire cells). Try timing
    the function with and without `@jax.jit`. On a GPU, you may see an ~20× speedup.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有人来说，输出都是相同的，但JIT函数运行得要快得多。如果你在一个Jupyter笔记本中工作，你可以使用`%timeit`来测量一行代码的执行时间（或`%%timeit`用于整个单元格）。尝试对函数进行计时，带有和不带有`@jax.jit`。在GPU上，你可能看到大约20倍的速度提升。
- en: Note
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: How does `@jax.jit` work? Briefly, when you apply `@jax.jit`, JAX doesn’t run
    your function like regular Python. Instead, it first traces the function—it runs
    through it once using special tracer objects (not real data) to build a computation
    graph. This graph is a static representation of all the numerical operations performed,
    with control flow unrolled and variable shapes and types fixed.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`@jax.jit`是如何工作的？简而言之，当你应用`@jax.jit`时，JAX不会像常规Python那样运行你的函数。相反，它首先跟踪函数——它使用特殊的跟踪对象（不是真实数据）运行一次，以构建计算图。这个图是所有数值操作静态表示，控制流展开，变量形状和类型固定。'
- en: Once the graph is built, JAX compiles it using XLA (Accelerated Linear Algebra),
    a backend that generates highly optimized machine code. This compiled version
    is cached and reused whenever the function is called again with the same input
    shapes and types—leading to significant speedups.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了图，JAX会使用XLA（加速线性代数）来编译它，这是一个生成高度优化的机器代码的后端。这个编译版本会被缓存并重复使用，每当函数再次以相同的输入形状和类型被调用时——从而带来显著的加速。
- en: 'JIT compilation is powerful, but it comes with trade-offs, especially for debugging.
    This is because:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: JIT编译非常强大，但同时也伴随着权衡，尤其是在调试方面。这是因为：
- en: Python debugging tools like `print()` statements or `pdb` don’t behave as expected
    inside jitted functions.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python调试工具如`print()`语句或`pdb`在JIT函数内部的行为不符合预期。
- en: Side effects (e.g., `print()`, logging, or modifying a list) are not actually
    executed during tracing, since JAX skips anything that doesn’t affect the computation
    graph.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 副作用（例如，`print()`、日志记录或修改列表）在跟踪过程中实际上并没有被执行，因为JAX会跳过任何不影响计算图的内容。
- en: Error messages can refer to internal JAX or XLA code instead of your original
    function and can be quite cryptic.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误信息可以引用JAX或XLA的内部代码而不是你的原始函数，并且可能非常难以理解。
- en: While you can temporarily disable jitting by commenting out `@jax.jit`, this
    becomes impractical if many functions rely on JIT. Fortunately, you can globally
    disable JIT by setting the environment variable `JAX_DISABLE_JIT=True`, which
    forces all jitted functions to run normally. This is a convenient way to debug
    without rewriting your code. See the [JAX debugging documentation](https://oreil.ly/oXI97)
    for more details.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以通过取消注释`@jax.jit`来暂时禁用JIT，但如果许多函数都依赖于JIT，这就会变得不切实际。幸运的是，你可以通过设置环境变量`JAX_DISABLE_JIT=True`来全局禁用JIT，这将强制所有JIT函数正常运行。这是一种在不重写代码的情况下进行调试的便捷方式。有关更多详细信息，请参阅[JAX调试文档](https://oreil.ly/oXI97)。
- en: Preconfiguring JAX jit with partial
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 预配置JAX jit使用部分
- en: There is a common source of confusion in machine learning code regarding usage
    of `partial`, especially with JAX code. The use of `functools.partial` is to prefill
    (or “bind”) some arguments of a function, returning a new function with those
    values fixed. This is a general Python utility and is not specific to JAX or ML.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习代码中，关于`partial`的使用存在一个常见的混淆源，尤其是在JAX代码中。`functools.partial`的使用是为了预先填充（或“绑定”）函数的一些参数，返回一个具有这些值固定的新函数。这是一个通用的Python工具，并不特定于JAX或ML。
- en: 'Here, we adapt the `scale` function and create a new function, `scale_by_10`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们调整了`scale`函数并创建了一个新函数，`scale_by_10`：
- en: '[PRE10]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Output:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE11]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, `` `scale_by_10` `` is a new function that behaves like `scale(x, 10)`.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`scale_by_10`是一个新函数，其行为类似于`scale(x, 10)`。
- en: 'In the context of `JAX`, `partial` is often used to customize a decorator before
    applying it, like this: `@partial(jax.jit, static_argnums=...)`. This is a way
    to configure the `jax.jit` decorator itself. As mentioned previously, `jax.jit`
    compiles your Python function for speed. However, JAX needs to know if certain
    arguments are static. Static arguments are typically non-JAX array types (like
    integers, strings, or booleans) that control the structure of the computation
    (e.g., in if/else statements). If a static argument changes, JAX may need to recompile
    the function.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在`JAX`的上下文中，`partial`通常用于在应用装饰器之前对其进行自定义，例如：`@partial(jax.jit, static_argnums=...)`。这是配置`jax.jit`装饰器本身的一种方式。如前所述，`jax.jit`编译Python函数以提高速度。然而，JAX需要知道某些参数是否是静态的。静态参数通常是控制计算结构（例如，在if/else语句中）的非JAX数组类型（如整数、字符串或布尔值）。如果静态参数发生变化，JAX可能需要重新编译函数。
- en: 'Let’s say we want to compute a summary statistic over an array, choosing either
    the mean or the median based on a string argument `average_method`. Since this
    choice affects the control flow, JAX needs to know the value of `average_method`
    at compile time:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要计算一个数组上的汇总统计量，根据字符串参数`average_method`选择均值或中位数。由于这个选择会影响控制流，JAX需要在编译时知道`average_method`的值：
- en: '[PRE12]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: If we didn’t mark `average` as static with `static_argnums=(0,)`, JAX would
    throw an error, because it can’t trace control flow that depends on strings unless
    it knows their value ahead of time. Marking arguments as static tells JAX to compile
    a separate, specialized version of the function for each unique value of that
    static argument it encounters.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有用`static_argnums=(0,)`将`average`标记为静态，JAX会抛出错误，因为它无法跟踪依赖于字符串的控制流，除非它事先知道它们的值。将参数标记为静态告诉JAX为每个遇到的静态参数的唯一值编译一个单独的、专门的函数版本。
- en: Tip
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'A bit of a clarification on the meaning of “static” versus “dynamic”: JAX treats
    most numerical inputs (like `jax.Array`, `float`, or `int`) as dynamic, meaning
    they can vary between calls without requiring recompilation, as long as their
    shapes and types stay the same.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 关于“静态”与“动态”含义的一点点澄清：JAX将大多数数值输入（如`jax.Array`、`float`或`int`）视为动态的，这意味着只要它们的形状和类型保持不变，它们可以在调用之间变化，而无需重新编译。
- en: 'Other inputs, like strings, Python objects, or functions, are static: they
    affect control flow or can’t be traced as part of the computation graph. If you
    pass them into a jitted function, you must mark them as static using `static_argnums`
    or close over them using a closure instead (see the next section).'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 其他输入，如字符串、Python对象或函数，是静态的：它们影响控制流或不能作为计算图的一部分进行跟踪。如果您将它们传递给一个jitted函数，您必须使用`static_argnums`将其标记为静态，或者使用闭包来封装它们（参见下一节）。
- en: Closures
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 闭包
- en: 'A closure is a function that “remembers” the environment in which it was created.
    This means it can access variables from its enclosing (outer) function’s scope,
    even after that scope has finished executing:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 闭包是一个“记住”其创建环境的函数。这意味着它可以访问其封装（外部）函数的作用域中的变量，即使在该作用域执行完成后也是如此：
- en: '[PRE14]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this example, `add_five` is a closure. It “remembers” that `x` was 5 when
    `outer_function` was called.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`add_five`是一个闭包。它“记住”，当`outer_function`被调用时，`x`的值是5。
- en: Closures are used extensively in JAX-based machine learning code. Many components—like
    loss functions, regularizers, and augmentation pipelines—are parameterized by
    configuration values. Instead of passing these values as arguments (which might
    require `static_argnums` if used inside control flow), they’re often closed over.
    A little later we will see this in action when defining the JAX training loop.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于JAX的机器学习代码中，闭包被广泛使用。许多组件——如损失函数、正则化器和增强管道——由配置值参数化。而不是将这些值作为参数传递（如果它们在控制流中使用可能需要`static_argnums`），它们通常被封装起来。稍后我们将在定义JAX训练循环时看到这一点。
- en: Generators
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成器
- en: '*Generators* are functions that allow you to iterate over data lazily, yielding
    one item at a time. They are especially useful for working with large datasets
    where loading everything into memory (RAM) at once is impractical or impossible.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成器* 是允许你懒加载地遍历数据的函数，一次产生一个项目。它们在处理大型数据集时特别有用，因为一次性将所有内容加载到内存（RAM）中既不实际也不可能。'
- en: 'Here’s a simple generator function that simulates streaming batches of data:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的生成器函数，用于模拟流式传输数据批次：
- en: '[PRE16]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE17]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will be working with TensorFlow datasets (TFDSs) in some chapters. Because
    JAX doesn’t include a native data-loading library, it’s common to see hybrid setups
    using TFDS. If you have your data in NumPy arrays, you can easily create a TensorFlow
    dataset using `tf.data.Dataset.from_tensor_slices`. This allows you to integrate
    NumPy data into TensorFlow pipelines for efficient training and preprocessing.
    It provides a clean API for batching, shuffling, and prefetching (preloading data
    before it’s needed to increase training speed), which is helpful when getting
    started:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些章节中，我们将使用 TensorFlow 数据集（TFDS）。由于 JAX 不包含原生数据加载库，因此常见的是使用 TFDS 的混合设置。如果你有
    NumPy 数组格式的数据，你可以使用 `tf.data.Dataset.from_tensor_slices` 容易地创建 TensorFlow 数据集。这允许你将
    NumPy 数据集成到 TensorFlow 管道中，以实现高效的训练和预处理。它提供了一个干净的 API 用于分批、洗牌和预取（在需要之前预加载数据以增加训练速度），这对于入门很有帮助：
- en: '[PRE18]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In later chapters, we will also be writing custom data pipelines that offer
    more control.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们还将编写自定义数据管道，提供更多控制。
- en: Anatomy of a Training Loop with JAX/Flax
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 JAX/Flax 的训练循环结构
- en: While the details of machine learning projects may vary, the core structure
    of training a model remains fairly consistent. This core structure will serve
    as a foundation throughout the chapters of this book. Let’s walk through the basic
    layout of training a model using JAX and Flax—a pattern you can build upon as
    you explore more complex examples.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管机器学习项目的细节可能有所不同，但训练模型的核心理结构仍然相当一致。这个核心结构将作为本书各章节的基础。让我们通过使用 JAX 和 Flax 训练模型的基本布局来了解训练模型——这是一个你可以随着探索更复杂示例而构建的模式。
- en: Defining a dataset
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义数据集
- en: Let’s create some toy data where the target `y` values are a linear transformation
    of the `x` values with a bit of random noise added. We’ll use the relationship
    `y = 2x + 1` with Gaussian noise, as you can see in [Figure 1-1](#fig1-1).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一些玩具数据，其中目标 `y` 值是 `x` 值的线性变换，并添加了一些随机噪声。我们将使用关系 `y = 2x + 1` 并添加高斯噪声，如
    [图 1-1](#fig1-1) 所示。
- en: '[PRE20]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](assets/dlfb_0101.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0101.png)'
- en: Figure 1-1\. This scatterplot visualizes the underlying relationship that we
    want our model to learn.
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 此散点图可视化了我们希望模型学习的潜在关系。
- en: Defining a model
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义模型
- en: In Flax, we define a model by inheriting from `nn.Module`. The `@nn.compact`
    decorator allows us to define layers directly inside the `__call__` method, rather
    than in the class’s `setup()` method. This is especially useful for simple, sequential
    models.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Flax 中，我们通过从 `nn.Module` 继承来定义模型。`@nn.compact` 装饰器允许我们在 `__call__` 方法内部直接定义层，而不是在类的
    `setup()` 方法中定义。这对于简单、顺序模型特别有用。
- en: 'Here’s a minimal example, a single linear (dense) layer with one output unit
    and no activation function:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个最小示例，一个单一线性（密集）层，有一个输出单元且没有激活函数：
- en: '[PRE21]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can instantiate the model like this:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以这样实例化模型：
- en: '[PRE22]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To initialize the model’s parameters, use the `.init` method with a random
    key and a sample input. This allows Flax to infer the input and output shapes.
    Here, we pass in a dummy input with shape `[1, 1]`—one example (batch size of
    1) with one input feature, matching the shape of our toy data:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要初始化模型的参数，使用 `.init` 方法并传入一个随机键和一个样本输入。这允许 Flax 推断输入和输出形状。这里，我们传入一个形状为 `[1,
    1]` 的虚拟输入——一个示例（批大小为 1）和一个输入特征，与我们的玩具数据形状相匹配：
- en: '[PRE23]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This initializes the model’s parameters. The result is a dictionary containing
    the weights and bias for the dense layer:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 这初始化了模型的参数。结果是包含密集层权重和偏置的字典：
- en: '[PRE24]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE25]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这里：
- en: '`kernel` is the learned weight matrix (shape `[1, 1]`, since our input and
    output dimensions are both 1).'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kernel` 是学习到的权重矩阵（形状 `[1, 1]`，因为我们的输入和输出维度都是 1）。'
- en: '`bias` is the learned bias term added after the matrix multiplication.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bias` 是在矩阵乘法之后添加的学习到的偏置项。'
- en: 'Note that while the Flax API has evolved over time, the core ideas remain stable:
    defining model layers, initializing parameters, and inferring shapes from inputs.
    Moreover, even if the syntax changes, these fundamentals will carry over to nearly
    any deep learning framework.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管 Flax API 随时间发展，但其核心思想保持稳定：定义模型层、初始化参数和从输入中推断形状。此外，即使语法发生变化，这些基本原理也将几乎适用于任何深度学习框架。
- en: Note
  id: totrans-301
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Why infer shapes from input? In Flax, the shapes of the model’s weight and
    bias parameters are actually unknown until you run data through the model. That’s
    because Flax follows a *functional* style: layers don’t store input shape information
    when you define them. Instead, you provide a sample input during initialization,
    and Flax infers the necessary parameter shapes on the fly.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么从输入中推断形状？在 Flax 中，模型的权重和偏置参数的形状实际上在你通过模型运行数据之前是未知的。这是因为 Flax 遵循一种 *函数式* 风格：当你定义层时，层不会存储输入形状信息。相反，你需要在初始化时提供一个样本输入，Flax
    会动态推断必要的参数形状。
- en: Other libraries, like PyTorch or Keras, use an object-oriented style, where
    layers often remember input shapes internally. This can make model construction
    feel more automatic, but Flax’s approach gives you more control and makes model
    behavior easier to inspect and debug, especially when working with JAX’s just-in-time
    (JIT) compilation.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库，如 PyTorch 或 Keras，使用面向对象风格，其中层通常在内部记住输入形状。这可以使模型构建感觉更自动化，但 Flax 的方法给您提供了更多控制，并使模型行为更容易检查和调试，尤其是在使用
    JAX 的即时（JIT）编译时。
- en: Creating a training state
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建训练状态
- en: 'A *training state* in Flax is a container that packages together everything
    you need for training: the model’s parameters, the optimizer, and the function
    used to apply the model. Let’s build one:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Flax 中的 *训练状态* 是一个容器，它将您需要的所有训练内容打包在一起：模型的参数、优化器和用于应用模型的函数。让我们构建一个：
- en: '[PRE26]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `TrainState` object is designed to make training in Flax cleaner and more
    manageable. It holds everything needed for model updates:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrainState` 对象旨在使 Flax 中的训练更干净、更易于管理。它包含模型更新所需的一切：'
- en: '`params`'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`params`'
- en: The current model parameters.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 当前模型参数。
- en: '`tx`'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`tx`'
- en: The optimizer (in this case, Adam). “Tx” is short for transformation. In Optax
    (JAX’s optimization library), optimizers are defined as transformations of gradients.
    For example, Adam transforms raw gradients using momentum and adaptive scaling.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器（在这种情况下，Adam）。"Tx" 是 "transformation" 的缩写。在 Optax（JAX 的优化库）中，优化器被定义为梯度的转换。例如，Adam
    使用动量和自适应缩放来转换原始梯度。
- en: '`apply_fn`'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '`apply_fn`'
- en: The function that runs the model’s forward pass.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 执行模型前向传递的函数。
- en: Importantly, the training state is immutable—rather than modifying it in place,
    each update returns a new `TrainState` with the updated parameters. This functional
    style is consistent with JAX’s overall design and helps keep computations pure
    and traceable.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，训练状态是不可变的——而不是就地修改它，每次更新都会返回一个新的 `TrainState`，其中包含更新的参数。这种函数式风格与 JAX 的整体设计一致，有助于保持计算纯净和可追踪。
- en: Tip
  id: totrans-315
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Although `TrainState` is immutable and each update returns a new object, this
    doesn’t lead to memory issues. JAX reuses memory efficiently, especially inside
    `@jit`-compiled functions.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `TrainState` 是不可变的，并且每次更新都会返回一个新的对象，但这并不会导致内存问题。JAX 有效地重用内存，尤其是在 `@jit` 编译的函数内部。
- en: Defining a loss function
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义损失函数
- en: 'The loss function measures how close the model’s predictions are to the true
    targets. Here, we use MSE, which is common for regression tasks:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数衡量模型预测与真实目标之间的接近程度。在这里，我们使用 MSE，这是回归任务中常用的：
- en: '[PRE27]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note that `model.apply` works here because `model` was defined earlier and is
    available in the current scope (e.g., the same notebook or script). We don’t need
    to pass it as an argument, because the function is still pure—all variable model
    state comes from the `params` we pass in.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`model.apply` 在这里之所以有效，是因为 `model` 之前已经定义，并且当前作用域（例如，同一个笔记本或脚本）中可用。我们不需要将其作为参数传递，因为函数仍然是纯函数——所有变量模型状态都来自我们传递的
    `params`。
- en: 'Here’s how you would call this function with your current model and data:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您如何使用当前模型和数据调用此函数的方法：
- en: '[PRE28]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Output:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE29]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This computes the current loss by applying the model to the data using the (random)
    parameters stored in `state`. Since the model hasn’t learned from the data yet,
    the loss is expected to be relatively high. As training progresses, this number
    should steadily decrease—which would tell us that the model is improving at its
    task of predicting the targets from the inputs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过使用存储在 `state` 中的（随机）参数将模型应用于数据来计算当前的损失。由于模型尚未从数据中学习，损失预计会相对较高。随着训练的进行，这个数字应该稳步下降——这将告诉我们模型在预测目标方面的任务正在改进。
- en: Defining the training step
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练步骤
- en: The training step performs a forward pass, computes the loss and gradients,
    and updates the model parameters. We use `jax.jit` to compile the entire step
    for efficiency. While JAX can run on GPUs and TPUs without it, using `jit` ensures
    that the code is compiled into a single optimized graph—which runs significantly
    faster and takes full advantage of accelerator hardware.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤执行正向传递，计算损失和梯度，并更新模型参数。我们使用 `jax.jit` 来编译整个步骤以提高效率。虽然 JAX 可以在没有它的情况下在 GPU
    和 TPU 上运行，但使用 `jit` 确保代码被编译成一个单一的优化图——这运行得更快，并充分利用加速器硬件。
- en: '[PRE30]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Often, you don’t just want the gradients (to update the parameters). You also
    want to log the loss explicitly (e.g., to plot it over time). But computing gradients
    requires computing the loss anyway. Instead of doing this twice, `jax.value_and_grad`
    is a convenient utility that does both at once:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你不仅想要梯度（以更新参数）。你还想显式地记录损失（例如，以时间序列绘制它）。但是，计算梯度无论如何都需要计算损失。为了避免重复计算，`jax.value_and_grad`
    是一个方便的实用工具，它一次完成这两项操作：
- en: It evaluates the function you give it (in this case, `calculate_loss`) to get
    the loss value.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它评估你给它提供的函数（在这种情况下，`calculate_loss`），以获取损失值。
- en: It computes the gradients of that loss with respect to the parameters.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它计算该损失相对于参数的梯度。
- en: This avoids redundant computation.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这避免了重复计算。
- en: The result of our `train_step` function is a new `TrainState` with updated parameters,
    along with the current loss, which we can use to monitor training progress.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `train_step` 函数的结果是一个新的 `TrainState`，其中包含更新的参数，以及当前的损失，我们可以用它来监控训练进度。
- en: 'Note that often you will encounter the training step defined with a closure
    like so:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，通常你会遇到使用闭包定义的训练步骤，如下所示：
- en: '[PRE31]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, `state`, `x`, and `y` are closed over—they’re not part of the `calculate_loss`
    function’s input signature, making the code more compact and easier to read and
    reason about.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`state`、`x` 和 `y` 被封闭——它们不是 `calculate_loss` 函数输入签名的部分，这使得代码更加紧凑，更容易阅读和理解。
- en: Handling auxiliary outputs in the loss function
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理损失函数中的辅助输出
- en: As a slight aside (but one that comes up often), sometimes we want to return
    extra information from the loss function—for example, predictions or metrics for
    logging—without affecting the gradient computation. JAX makes this easy with the
    `has_aux=True` flag. It tells `value_and_grad` to treat everything after the loss
    as “auxiliary” and to exclude it from gradient computation.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个小插曲（但经常出现），有时我们希望从损失函数中返回额外的信息——例如，用于记录的预测或指标——而不影响梯度计算。JAX 通过 `has_aux=True`
    标志使这变得简单。它告诉 `value_and_grad` 将损失之后的所有内容视为“辅助”并排除它从梯度计算中。
- en: 'For example, let’s modify our loss function to also return predictions and
    accommodate this by using `has_aux=True` in `jax.value_and_grad` in our `train_step`:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们修改我们的损失函数以返回预测，并通过在 `train_step` 中的 `jax.value_and_grad` 使用 `has_aux=True`
    来适应这一点：
- en: '[PRE32]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Without `has_aux=True`, JAX expects the loss function to return a single scalar.
    Returning anything else (like predictions) will raise an error. By setting `has_aux=True`,
    you’re telling JAX: “only differentiate with respect to the loss; ignore any extra
    outputs, like predictions.”'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有 `has_aux=True`，JAX 预期损失函数返回一个单一的标量。返回任何其他内容（如预测）将引发错误。通过设置 `has_aux=True`，你告诉
    JAX：“仅对损失进行微分；忽略任何额外的输出，如预测。”
- en: Defining the training loop
  id: totrans-342
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义训练循环
- en: Now that all the components are in place, we can define the training loop that
    actually updates the model parameters based on the data.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有组件都已就绪，我们可以定义训练循环，该循环根据数据实际更新模型参数。
- en: 'In most machine learning workflows, training happens over either *steps* or
    *epochs*:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数机器学习工作流程中，训练是在 *步骤* 或 *epochs* 上进行的：
- en: A *step* refers to one update of the model using a single batch of data.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 *步骤* 指的是使用单个数据批次更新模型的一次操作。
- en: An *epoch* is a full pass through the entire training dataset—typically consisting
    of many steps. In our toy example, we feed the entire dataset (100 input–output
    pairs) into the model at once, without batching. This means each step is equivalent
    to a full epoch. In more realistic settings, you’d typically break the data into
    batches, resulting in many steps per epoch.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一个周期* 是整个训练数据集的一次完整遍历——通常由许多步骤组成。在我们的玩具示例中，我们一次性将整个数据集（100个输入-输出对）输入到模型中，没有分批。这意味着每一步相当于一个完整的周期。在更现实的设置中，你通常会将数据分成批次，从而在每个周期中有许多步骤。'
- en: 'Let’s now train the model:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练模型：
- en: '[PRE33]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE34]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The model converges quickly—its loss decreases rapidly to a stable, low value.
    After training, we can test how well the model has learned the underlying pattern
    by comparing its predictions to the true target values, as shown in [Figure 1-2](#fig1-2):'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 模型收敛得很快——其损失迅速降低到一个稳定、低值。经过训练后，我们可以通过将模型的预测与真实目标值进行比较来测试模型是否很好地学习了潜在的模式，如图 [图1-2](#fig1-2)
    所示：
- en: '[PRE35]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](assets/dlfb_0102.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0102.png)'
- en: Figure 1-2\. Scatterplot comparing the linear model predictions with the true
    values.
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-2\. 散点图比较线性模型预测值与真实值。
- en: We can see that after training, the model has learned to approximate the true
    function y=2x+1 very closely. The predicted outputs are nearly identical to the
    expected values, which is exactly what we want in this toy regression task.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过训练，模型已经学会了非常接近真实函数 y=2x+1 的近似。预测输出几乎与预期值相同，这正是我们在这个玩具回归任务中想要的。
- en: 'This may be a simple example, but it captures the core structure of almost
    every deep learning workflow: define a model, compute a loss, update parameters,
    and repeat. While real projects involve more complexity—batching, data pipelines,
    regularization, metrics, logging, and so on—the basic loop you’ve built here is
    the foundation of it all. You now have a solid mental model for how training works
    in JAX and Flax—and the foundation to build much more powerful systems.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是一个简单的例子，但它几乎捕捉到了每个深度学习工作流程的核心结构：定义一个模型，计算损失，更新参数，然后重复。虽然真实项目涉及更多的复杂性——批处理、数据管道、正则化、指标、日志记录等——但你在这里建立的基本循环是所有这些的基础。你现在对JAX和Flax中的训练工作有了坚实的心理模型——以及构建更强大系统的基石。
- en: Tip
  id: totrans-357
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Where to go from here? Once you’ve built a working training loop (an important
    milestone), it’s common to expand in several directions: add metrics that capture
    key aspects of model performance, split off a validation set to check generalization,
    and track progress over training time. These are all building blocks of real-world
    deep learning workflows, and we’ll walk through them in upcoming chapters.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来该往哪里走？一旦你建立了一个工作的训练循环（一个重要的里程碑），通常会在几个方向上扩展：添加能够捕捉模型性能关键方面的指标，分割出一个验证集来检查泛化能力，并在训练时间上跟踪进度。这些都是现实世界深度学习工作流程的构建块，我们将在接下来的章节中介绍它们。
- en: Machine Learning Tips
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习技巧
- en: Here’s a brief reminder of some machine learning concepts, focusing on what
    we use throughout the book. We explain these ideas in more detail as necessary
    when they come up.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这里简要回顾一些机器学习概念，重点关注我们在整本书中使用的部分。当这些概念出现时，我们会根据需要详细解释这些想法。
- en: Types of Tasks
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务类型
- en: 'In *classification*, the model predicts a label or a probability distribution
    over labels. There are three types of classification:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *分类* 中，模型预测一个标签或标签的概率分布。有三种类型的分类：
- en: Binary classification
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类
- en: Choose between two options; for example, whether a cell is healthy or not.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个选项之间进行选择；例如，一个细胞是否健康。
- en: Multiclass classification
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类
- en: Choose one label out of several; for example, based on patterns in a biological
    sample, a model might predict which part of the body it came from—like the brain,
    liver, or skin. Each sample belongs to exactly one class.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 从几个标签中选择一个；例如，基于生物样本中的模式，模型可能会预测它来自身体的哪个部分——如大脑、肝脏或皮肤。每个样本恰好属于一个类别。
- en: Multilabel classification
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签分类
- en: Choose all labels that apply, not just one. For example, when analyzing a cell
    image, the model might predict which structures are visible—such as the nucleus,
    membrane, and mitochondria. In this case, multiple labels can be true for the
    same image.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 选择所有适用的标签，而不仅仅是其中一个。例如，在分析细胞图像时，模型可能会预测哪些结构是可见的——例如细胞核、膜和线粒体。在这种情况下，同一张图像可能有多个标签是正确的。
- en: '*Regression* involves predicting a continuous value, such as the binding strength
    between two molecules.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '*回归* 涉及预测一个连续值，例如两个分子之间的结合强度。'
- en: We also use *representation learning*—learning useful embeddings or feature
    representations without direct supervision. These embeddings capture patterns
    or structure in the data, which can then be used for tasks like clustering, visualization,
    or as inputs to downstream models.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用*表示学习*——在没有直接监督的情况下学习有用的嵌入或特征表示。这些嵌入捕获数据中的模式或结构，然后可以用于聚类、可视化或作为下游模型的输入。
- en: Types of Architectures
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构类型
- en: 'In this book, we primarily use the following model architectures:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们主要使用以下模型架构：
- en: Linear models and multilayer perceptrons (MLPs)
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型和多层感知器（MLPs）
- en: These are fully connected networks that transform input vectors into output
    vectors through stacked dense layers. They are simple and widely used.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是完全连接的网络，通过堆叠密集层将输入向量转换为输出向量。它们简单且广泛使用。
- en: Convolutional neural networks (CNNs)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）
- en: CNNs apply spatial filters to model local structure, typically mapping images
    to images or feature maps. They are especially effective for image and sequence
    data.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs通过应用空间滤波器来建模局部结构，通常将图像映射到图像或特征图。它们在图像和序列数据方面特别有效。
- en: Transformers
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 传输器
- en: Model long-range dependencies in sequences using attention. They operate on
    sets or sequences and are now state of the art in many areas of biology and language
    modeling.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 使用注意力来在序列中学习长距离依赖关系。它们在集合或序列上操作，现在在许多生物学和语言建模领域都是最先进的。
- en: Graph neural networks (GNNs)
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）
- en: Operate on graph-structured data, passing messages between nodes and their neighbors.
    They are useful when data has relational or interaction-based structure.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在图结构数据上操作，在节点及其邻居之间传递消息。当数据具有关系或基于交互的结构时，它们很有用。
- en: Autoencoders
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器
- en: Learn compact representations by encoding inputs into a latent space and reconstructing
    them. They are common in unsupervised learning and denoising tasks..
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将输入编码到潜在空间并重建它们来学习紧凑的表示。它们在无监督学习和去噪任务中很常见。
- en: We’ll explain each of these as they appear throughout the book.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在整本书中解释这些内容。
- en: Dataset Splits
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集分割
- en: 'Machine learning models are typically trained using three dataset partitions:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常使用三个数据集分区进行训练：
- en: Training set
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: Used to fit the model’s parameters (its weights) by minimizing a loss function
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最小化损失函数来拟合模型的参数（其权重）
- en: Validation set
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集
- en: Used to tune *hyperparameters* (discussed next) and evaluate performance during
    development
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 用于调整*超参数*（将在下文讨论）并在开发过程中评估性能
- en: Test set
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集
- en: Held out until the end to assess the model’s final performance on truly unseen
    data, providing an estimate of how well it generalizes
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 保留到最后以评估模型在真正未见数据上的最终性能，从而提供一个关于其泛化能力的估计
- en: This split structure is good practice because it helps ensure your model generalizes
    well to new data and gives you a reliable estimate of its real-world performance.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分割结构是良好的实践，因为它有助于确保你的模型对新数据具有良好的泛化能力，并为你提供其真实世界性能的可靠估计。
- en: Hyperparameters
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'Hyperparameters control how a model is trained. They’re set before training
    begins and are not updated by the optimizer. Common examples include:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数控制模型如何进行训练。它们在训练开始之前被设置，并且不会被优化器更新。常见的例子包括：
- en: Learning rate
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率
- en: How quickly the model updates its weights
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 模型更新其权重的速度有多快
- en: Batch size
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理大小
- en: How many examples are processed together during one update
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 在一次更新中一起处理多少个示例
- en: Model size
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 模型大小
- en: The number of layers, hidden units, or attention heads
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 层数、隐藏单元或注意力头的数量
- en: Regularization
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化
- en: The dropout rate or weight decay, to prevent overfitting
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: dropout率或权重衰减，以防止过拟合
- en: We evaluate different hyperparameter settings using performance on the validation
    set. If we tune hyperparameters based on training performance alone, we risk *overfitting*—the
    model may simply memorize the training data, including noise or outliers.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用验证集上的性能来评估不同的超参数设置。如果我们仅根据训练性能调整超参数，我们可能会面临*过拟合*的风险——模型可能只是简单地记住训练数据，包括噪声或异常值。
- en: Overfitting leads to poor performance on new data. In contrast, *generalization*
    means the model has learned patterns that apply beyond the training examples.
    This is a key requirement in biological applications, where test data can often
    come from different experiments or conditions.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合会导致在新数据上的性能不佳。相比之下，*泛化*意味着模型已经学会了适用于训练示例之外的模式。这是生物应用中的一个关键要求，因为测试数据通常来自不同的实验或条件。
- en: Activation Functions
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions introduce nonlinearity into neural networks, enabling them
    to model complex relationships between inputs and outputs.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数将非线性引入神经网络，使它们能够模拟输入和输出之间的复杂关系。
- en: As data flows through a model, it’s transformed into intermediate tensors known
    as *activations*. These activations pass through a series of linear projections
    and nonlinear activation functions at each layer. The output of the final hidden
    layer is then often passed through a final activation function, which produces
    the model’s ultimate prediction.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据流经模型时，它被转换成称为*激活*的中间张量。这些激活通过每一层的线性投影和非线性激活函数传递。最终隐藏层的输出随后通常通过一个最终的激活函数，该函数产生模型的最终预测。
- en: 'Choosing the right final activation is important—it should reflect the kind
    of data you’re trying to predict. For example:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的最终激活很重要——它应该反映你试图预测的数据类型。例如：
- en: Use `sigmoid` for binary classification, where outputs should lie between 0
    and 1.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于二分类，使用`sigmoid`，其中输出应介于0和1之间。
- en: Use `softmax` for multiclass classification, where outputs represent probabilities
    across categories.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于多类分类，使用`softmax`，其中输出表示跨类别的概率。
- en: Use *no* final activation if your loss function expects raw logits (e.g., `sigmoid_cross_entropy`
    or `softmax_cross_entropy`). These loss functions apply the activation internally,
    so applying it yourself would be redundant or even harmful.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的损失函数期望原始logits（例如，`sigmoid_cross_entropy`或`softmax_cross_entropy`），则不要使用任何最终的激活。这些损失函数在内部应用激活，因此自行应用将是冗余的，甚至是有害的。
- en: Avoid ReLU or GELU as final activations when predicting real-valued outputs
    that can be negative—they will clip or distort negative values.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当预测可能为负的实值输出时，避免使用ReLU或GELU作为最终激活——它们将截断或扭曲负值。
- en: For regression tasks, no activation is often best—or use tanh or sigmoid only
    if your target values are known to be bounded (e.g., between -1 and 1 or 0 and
    1).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归任务，通常不使用激活最好——或者如果目标值已知有界（例如，在-1和1或0和1之间），则仅使用tanh或sigmoid。
- en: When in doubt, check the range of your target values, and choose an activation
    (or none) that allows the model to produce outputs in that range. The term *logits*
    is often used to describe the raw, unnormalized outputs of the final layer, especially
    before applying softmax or sigmoid. But it’s a somewhat overloaded term that is
    sometimes used more loosely—for example, to describe intermediate values passed
    into a softmax, such as attention scores in a transformer.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 当不确定时，检查目标值的范围，并选择一个允许模型在该范围内产生输出的激活（或无激活）。术语*logits*通常用于描述最终层的原始、未归一化的输出，尤其是在应用softmax或sigmoid之前。但它是一个有些过载的术语，有时被更宽松地使用——例如，描述传递给softmax的中间值，例如转换器中的注意力分数。
- en: 'The following are the most commonly used activation functions, with their shapes
    shown in [Figure 1-3](#fig1-3):'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最常用的激活函数，它们的形状在[图1-3](#fig1-3)中显示：
- en: '![](assets/dlfb_0103.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0103.png)'
- en: Figure 1-3\. Common activation functions used in deep learning.
  id: totrans-417
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 深度学习中使用的常见激活函数。
- en: 'A bit more detail on these common activation functions:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这些常见激活函数的更多细节：
- en: ReLU (rectified linear unit)
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU（修正线性单元）
- en: Outputs zero for negative inputs and the input itself for positive inputs. It
    is simple and effective, especially in deep networks.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 对于负输入和输入本身输出为零。它简单有效，尤其是在深层网络中。
- en: GELU (Gaussian error linear unit)
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: GELU（高斯误差线性单元）
- en: A smoother alternative to ReLU, often used in transformer models. It can yield
    slightly better results.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU（修正线性单元）的一个更平滑的替代品，通常用于转换器模型。它可以产生略微更好的结果。
- en: Sigmoid and tanh
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid和tanh
- en: Older activation functions that squash values into fixed ranges. Sigmoid maps
    inputs to a range between 0 and 1, while tanh maps to a range between –1 and 1\.
    They’re useful in certain settings, such as output layers for binary classification,
    but can suffer from gradient issues in deeper models.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 较旧的激活函数会将值压缩到固定范围内。Sigmoid将输入映射到0到1的范围，而tanh映射到-1到1的范围。它们在特定设置中很有用，例如二分类的输出层，但可能在深层模型中遇到梯度问题。
- en: Softmax
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Softmax
- en: Converts a vector of values into a probability distribution that sums to 1\.
    Used in the final layer of multiclass classification models. Not applied element-wise;
    it operates across the whole vector.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 将值向量转换为总和为1的概率分布。用于多类分类模型的最终层。不逐元素应用；它在整个向量上操作。
- en: In this book, we’ll usually stick to ReLU or GELU for hidden layers, and choose
    the final activation based on the prediction task.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们通常会坚持使用ReLU或GELU作为隐藏层，并根据预测任务选择最终的激活函数。
- en: Optimizers
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化器
- en: Optimizers are algorithms that adjust a model’s parameters (its weights and
    biases) to reduce the error (loss) during training. They do this using *gradient
    descent*, which computes how each parameter affects the loss and updates it to
    reduce the error.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器是调整模型参数（其权重和偏置）以减少训练过程中误差（损失）的算法。它们通过 *梯度下降* 来实现这一点，计算每个参数对损失的影响，并更新它以减少误差。
- en: We mostly use *Adam*, a widely used optimizer that adapts the learning rate
    for each parameter and combines ideas from momentum and RMSProp. It usually trains
    faster and more reliably than plain gradient descent, especially in noisy or sparse
    settings.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要使用 *Adam*，这是一个广泛使用的优化器，它为每个参数调整学习率，并结合了动量和RMSProp的思想。它通常比普通的梯度下降训练更快、更可靠，尤其是在噪声或稀疏设置中。
- en: Initialization Strategy
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始化策略
- en: Before training starts, we need to set initial values for model parameters.
    This step is more important than it sounds. Poor initialization can cause gradients
    to vanish or explode, making training unstable.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练开始之前，我们需要为模型参数设置初始值。这一步比听起来更重要。不良的初始化可能导致梯度消失或爆炸，从而使训练不稳定。
- en: We typically use Xavier (Glorot) initialization, which is designed to keep the
    scale of activations and gradients roughly stable across layers. This helps training
    proceed smoothly. By default, Flax uses Xavier initialization for most layers
    like `Dense` and `Conv`, so you don’t usually need to specify it manually.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常使用Xavier（Glorot）初始化，它旨在保持激活和梯度在层之间的规模大致稳定。这有助于训练顺利进行。默认情况下，Flax为大多数层（如`Dense`和`Conv`）使用Xavier初始化，因此你通常不需要手动指定它。
- en: Model Checkpointing
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型检查点
- en: During training, it’s often useful to periodically save your model parameters
    so that you can resume later or avoid losing progress. While production training
    pipelines often use robust checkpointing strategies (e.g., saving best-performing
    checkpoints, versioned histories, etc.), that level of complexity isn’t necessary
    for the teaching examples in this book. Instead, we provide a lightweight utility
    that saves and restores just the most recent checkpoint. This is enough to pause
    and resume training or to store the final model output for later use. You’ll see
    it used in several chapters to simplify experimentation and reduce boilerplate.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，定期保存模型参数通常很有用，这样你可以在以后恢复或避免丢失进度。虽然生产训练管道通常使用健壮的检查点策略（例如，保存最佳性能的检查点、版本化历史记录等），但在这个书籍的教学示例中，这种复杂程度并不必要。相反，我们提供了一个轻量级的实用工具，仅保存和恢复最近的检查点。这足以暂停和恢复训练，或存储最终模型输出以供以后使用。你将在几个章节中看到它的使用，以简化实验和减少样板代码。
- en: If you’re building your own training loop for production or research, consider
    upgrading to a more complete checkpointing solution—for example, using [Orbax](https://oreil.ly/aazC8),
    Flax’s newer checkpointing system.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在为生产或研究构建自己的训练循环，考虑升级到一个更完整的检查点解决方案——例如，使用 [Orbax](https://oreil.ly/aazC8)，Flax的新检查点系统。
- en: Early Stopping
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 早期停止
- en: In real-world training, it’s often a good idea to stop training when your validation
    performance stops improving. This is known as *early stopping*, which helps prevent
    overfitting and saves compute.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际训练中，当你的验证性能停止提高时停止训练通常是一个好主意。这被称为 *早期停止*，有助于防止过拟合并节省计算资源。
- en: In this book, we often show longer training runs to visualize how the loss evolves
    over time. But in practice, you’ll likely want to use early stopping when training
    your own models.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们经常展示较长的训练运行，以可视化损失随时间的变化。但在实践中，你可能会在训练自己的模型时使用早期停止。
- en: 'Flax includes a simple utility for this:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: Flax包括一个简单的实用工具来完成这个任务：
- en: '[PRE36]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You can track validation metrics and stop training when they stop improving
    for a certain number of steps (the `patience` parameter).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以跟踪验证指标，并在它们停止改善一定数量的步骤（`patience`参数）时停止训练。
- en: Selecting a Working Environment
  id: totrans-443
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择工作环境
- en: Training large neural networks often benefits from *GPU acceleration*. There
    are a few ways to access a GPU, depending on your budget, goals, and technical
    comfort level.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 训练大型神经网络通常受益于 *GPU加速*。根据你的预算、目标和技术舒适度，有多种方式可以访问GPU。
- en: One option is to use a local machine with a GPU, such as a gaming desktop or
    a workstation you’ve set up yourself. This gives you full control and avoids ongoing
    cloud costs, but it requires up-front hardware investment and setup. Another option
    is to rent GPU access from cloud providers like AWS, GCP, or Azure. These offer
    flexibility and scalability but can get expensive over time, especially if you’re
    training large models.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是使用带有GPU的本地机器，例如游戏台式机或你自己设置的工作站。这让你有完全的控制权，避免了持续云服务的费用，但需要前期硬件投资和设置。另一种选择是从AWS、GCP或Azure等云服务提供商那里租用GPU访问。这些服务提供了灵活性和可扩展性，但可能会随着时间的推移变得昂贵，尤其是如果你在训练大型模型时。
- en: For many beginners and small projects, Google Colab is a great place to start.
    It provides free, cloud-based Jupyter notebooks with GPU or TPU support and minimal
    setup.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多初学者和小型项目，Google Colab是一个很好的起点。它提供了免费、基于云的Jupyter笔记本，支持GPU或TPU，并且设置简单。
- en: In the following sections, we’ll briefly walk through your options, from interactive
    notebooks to fully customized GPU development environments.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将简要介绍你的选项，从交互式笔记本到完全定制的GPU开发环境。
- en: Selecting an Interactive Notebook
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择交互式笔记本
- en: Jupyter notebooks are a popular and powerful tool for interactive coding, making
    them ideal for running the code examples in this book. They let you write code,
    execute it in cells, visualize results, and document your work, all in one place.
    This interactivity makes it easy to experiment, debug, and iterate quickly. Common
    notebook environments include JupyterLab, VSCode with Jupyter extensions, and
    Google Colab.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter笔记本是交互式编码的一个流行且强大的工具，非常适合运行本书中的代码示例。它们让你在一个地方编写代码、在单元格中执行它、可视化结果并记录你的工作。这种交互性使得实验、调试和快速迭代变得容易。常见的笔记本环境包括JupyterLab、带有Jupyter扩展的VSCode和Google
    Colab。
- en: Google Colab in particular provides cloud-based notebooks with free access to
    GPUs and TPUs, making it a great option if you don’t have a powerful local machine.
    It runs entirely in your browser and requires no setup beyond a Google account.
    You can install libraries with commands like `!pip install jax flax optax` and
    save notebooks directly to Google Drive. To enable hardware acceleration, navigate
    to the Runtime menu. From the drop-down menu, select “Change runtime type” and
    then select a GPU or TPU.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab特别提供了基于云的笔记本，可以免费访问GPU和TPU，如果你没有强大的本地机器，这是一个很好的选择。它完全在浏览器中运行，只需一个Google账户即可使用。你可以使用如`!pip
    install jax flax optax`之类的命令安装库，并将笔记本直接保存到Google Drive。要启用硬件加速，导航到“运行”菜单。从下拉菜单中选择“更改运行时类型”，然后选择GPU或TPU。
- en: We provide all code in this book as Google Colab notebooks that you can open,
    run, and modify interactively.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这本书中提供了所有代码，以Google Colab笔记本的形式，你可以打开、运行和交互式地修改。
- en: Warning
  id: totrans-452
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Colab sessions can time out after periods of inactivity, so remember to save
    your work frequently.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: Colab会话在一段时间的不活跃后可能会超时，所以请记住经常保存你的工作。
- en: While notebooks are excellent for exploration and prototyping, they can make
    version control, debugging, and maintaining complex projects more difficult. For
    longer-term work, setting up a dedicated development environment with GPU support
    offers more control, scalability, and reproducibility.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然笔记本非常适合探索和原型设计，但它们可能会使版本控制、调试和维护复杂项目变得更加困难。对于长期工作，设置一个带有GPU支持的专用开发环境提供了更多的控制、可扩展性和可重复性。
- en: Structuring Your Code for Reuse and Debugging
  id: totrans-455
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为重用和调试结构化你的代码
- en: 'Notebooks are great for interactive use, but as you begin building your own
    projects, it’s worth thinking about how you will structure it early. Organize
    your code into clear, reusable Python modules: one for datasets, one for models,
    one for metrics and evaluation. Keeping these pieces separate makes your code
    easier to debug, scale, and reuse later.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本非常适合交互式使用，但当你开始构建自己的项目时，提前考虑如何结构化它是非常有价值的。将你的代码组织成清晰、可重用的Python模块：一个用于数据集，一个用于模型，一个用于指标和评估。将这些部分保持独立可以使你的代码更容易调试、扩展和以后重用。
- en: 'This modular approach mirrors how this book is structured. Each subsection
    of each chapter generally focuses on a specific building block: dataset, model,
    training loop, and so on. As you follow along, you might try using this structure
    in your own code. Build clean components you can plug into future projects, and
    you’ll move faster each time.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模块化方法与本书的结构相呼应。每一章的每个小节通常都专注于特定的构建块：数据集、模型、训练循环等。随着你的学习，你可能会尝试在自己的代码中使用这种结构。构建干净、可插入未来项目的组件，每次都会更快。
- en: A good dataset class in particular will help you catch issues early. There are
    plenty of reasons you might get confused during a project. If your dataset and
    metrics are cleanly separated and easy to inspect, you can at least rule them
    out as the source of the problem.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的数据集类尤其有助于你早期发现问题。在项目过程中，你可能会遇到很多让你困惑的原因。如果你的数据集和指标被干净地分离并且易于检查，你至少可以排除它们作为问题来源的可能性。
- en: Tip
  id: totrans-459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Add frequent sanity checks. Can your model overfit a tiny dataset (like 10 examples)?
    Do loss and accuracy behave as expected when you shuffle labels?
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 添加频繁的合理性检查。你的模型能否对一个小数据集（如10个示例）过拟合？当你打乱标签时，损失和准确度是否表现如预期？
- en: Plot everything. Visualizing predictions, losses, or inputs will often reveal
    issues you wouldn’t notice in the final numbers alone—like shifted targets, mislabeled
    data, empty tensors, or models that haven’t learned anything at all.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制一切。可视化预测、损失或输入通常会揭示仅从最终数字中无法注意到的问题——如移动的目标、错误标记的数据、空张量或根本没有学习的模型。
- en: Setting Up a GPU Development Environment
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置GPU开发环境
- en: As your models grow more complex, you may want to move beyond notebooks into
    a more powerful GPU-enabled setup, either locally or in the cloud. A robust development
    environment can speed up experimentation, simplify debugging, and make your workflow
    more reproducible.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的模型变得越来越复杂，你可能希望从笔记本过渡到一个更强大的、由GPU支持的设置，无论是本地还是云端。一个强大的开发环境可以加快实验速度，简化调试，并使你的工作流程更具可重复性。
- en: 'For local development, we recommend the following:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地开发，我们推荐以下设置：
- en: Use Docker with NVIDIA Docker to create containerized environments that seamlessly
    access your GPU.
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NVIDIA Docker与Docker结合创建可以无缝访问你的GPU的容器化环境。
- en: Pair it with an editor like VSCode, which supports Docker and remote development
    for a smooth workflow.
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与支持Docker和远程开发的编辑器如VSCode搭配使用，以实现流畅的工作流程。
- en: Use Git for version control to track changes, collaborate, and back up your
    work.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Git进行版本控制以跟踪更改、协作和备份你的工作。
- en: Mac users with Apple Silicon can try [`jax-metal`](https://oreil.ly/_FYSk) to
    enable GPU acceleration via Apple’s Metal backend. While it’s improving rapidly,
    not all features are fully supported yet, so expect occasional compatibility issues.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有Apple Silicon的Mac用户可以尝试使用[`jax-metal`](https://oreil.ly/_FYSk)来通过Apple的Metal后端启用GPU加速。虽然它正在迅速改进，但并非所有功能都得到完全支持，因此请预期偶尔会出现兼容性问题。
- en: Tip
  id: totrans-469
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Paid cloud services like AWS, GCP, or Azure offer on-demand GPU instances if
    you don’t have a local GPU. Alternatives like Paperspace, Lambda Labs, or RunPod
    can offer simpler setup and better value for smaller-scale projects.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有本地GPU，付费云服务如AWS、GCP或Azure提供按需GPU实例。像Paperspace、Lambda Labs或RunPod这样的替代方案可以为小型项目提供更简单的设置和更好的价值。
- en: Version Conflicts
  id: totrans-471
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 版本冲突
- en: 'Machine learning and scientific computing libraries evolve quickly—and not
    always in sync. You’ll inevitably hit version conflicts between tools like NumPy,
    JAX, Flax, PyTorch, or Hugging Face Transformers. Strive for a balance: use versions
    recent enough to benefit from improvements, but not so new that they break compatibility
    with everything else.'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习和科学计算库快速演变——并且并不总是同步。你不可避免地会遇到NumPy、JAX、Flax、PyTorch或Hugging Face Transformers等工具之间的版本冲突。努力寻求平衡：使用足够新的版本以从改进中受益，但不要新到以至于与所有其他内容都破坏兼容性。
- en: Tools like [`uv`](https://oreil.ly/_aezy)—a faster, more flexible alternative
    to `pip`—can help override incompatibilities and install packages even when their
    metadata says they’re not compatible. It’s not a permanent solution, but it can
    get you unstuck.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 像使用`uv`（https://oreil.ly/_aezy）这样的工具——`pip`的一个更快、更灵活的替代品——可以帮助覆盖不兼容性，并在其元数据表明它们不兼容的情况下安装包。这不是一个永久的解决方案，但它可以帮助你摆脱困境。
- en: If you’re working outside a notebook, always use a virtual environment (`venv`,
    `conda`, or `uv venv`) to keep dependencies isolated. Another option is to define
    your environment in a Docker container, which guarantees full reproducibility
    across machines. This setup is especially useful when working on remote GPU instances
    or in the cloud—something we’ll explain later in the book.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不在笔记本中工作，始终使用虚拟环境（`venv`、`conda`或`uv venv`）来保持依赖项的隔离。另一个选择是在Docker容器中定义你的环境，这保证了跨机器的完全可重复性。这种设置在处理远程GPU实例或在云端工作时特别有用——这一点我们将在本书的后续部分进行解释。
- en: When in doubt, check GitHub issues or forums—version mismatches are a common
    pain point for everyone, and at the very least, you can commiserate with others.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不确定时，检查GitHub问题或论坛——版本不匹配是每个人的常见痛点，至少你可以与其他人感同身受。
- en: If you’re working in a notebook, version issues can be especially tricky. Google
    Colab, for example, comes with many preinstalled packages—but these can be outdated
    or incompatible with the latest JAX/Flax stack. You can install or override versions
    directly in a cell using `!pip install`, but you may need to restart the runtime
    afterward for changes to take effect (Runtime > Restart runtime).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个笔记本中工作，版本问题可能会特别棘手。例如，Google Colab自带了许多预安装的包，但这些包可能已经过时或与最新的JAX/Flax堆栈不兼容。你可以使用`!pip
    install`直接在单元格中安装或覆盖版本，但可能需要重启运行时才能使更改生效（运行时 > 重启运行时）。
- en: A Living Document
  id: totrans-477
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个活生生的文档
- en: While this book endeavors to capture the state of the field at the time of writing,
    deep learning and biology remain fast-moving frontiers. A printed book is a static
    artifact; in contrast, frameworks evolve, APIs break, and new ideas emerge constantly.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书努力捕捉写作时的领域状态，但深度学习和生物学仍然是快速发展的前沿。一本印刷书是一个静态的物品；相比之下，框架在不断发展，API会中断，新想法不断涌现。
- en: Note
  id: totrans-479
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We’ve done our best to future-proof examples and flag potential breaking changes
    (like Flax’s transition from `linen` to `nnx`). Still, some discrepancies may
    arise over time. If you spot anything out of date—or have corrections, improvements,
    suggestions, or extensions—please let us know. The online repository can evolve
    even if the book is in a fixed state.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尽力使示例具有前瞻性，并标记出潜在的破坏性变化（例如Flax从`linen`过渡到`nnx`）。尽管如此，随着时间的推移，仍可能出现一些差异。如果你发现任何过时的内容，或者有更正、改进、建议或扩展，请告诉我们。即使书籍处于固定状态，在线存储库也可以不断进化。
- en: We also encourage you to look beyond these pages. Resources like [D2L.ai](http://D2L.ai),
    fast.ai, the JAX ecosystem, and preprints on bioRxiv and arXiv are excellent places
    to go deeper.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还鼓励你超越这些页面。像[D2L.ai](http://D2L.ai)、fast.ai、JAX生态系统以及bioRxiv和arXiv上的预印本都是深入了解的好地方。
- en: Most of all, experiment, build, break things, train bad models, and learn. The
    best way to grow in deep learning for biology is to get your hands dirty.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，进行实验，构建，打破事物，训练不良模型，并学习。在生物学的深度学习中成长的最佳方式是亲自动手。
