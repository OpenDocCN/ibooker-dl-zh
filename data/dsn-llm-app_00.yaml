- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: In the past few years, progress in the field of artificial intelligence has
    been occurring at breakneck speeds, spearheaded by advances in LLMs. It was not
    too long ago that LLMs were a nascent technology that struggled to generate a
    coherent paragraph; today they are able to solve complex mathematical problems,
    write convincing essays, and conduct long engaging conversations with humans.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几年里，人工智能领域的进步速度之快，主要是由LLMs的进步推动的。LLMs不久前还是一种新兴技术，难以生成连贯的段落；而如今，它们能够解决复杂的数学问题，撰写令人信服的论文，并与人类进行长时间的深入对话。
- en: As AI advances from strength to strength, it is rapidly being woven into the
    fabric of society, touching so many facets of our lives. Learning how to use AI
    models like LLMs effectively might be one of the most useful skills to learn this
    decade. LLMs are revolutionizing the world of software, and have made possible
    the development of applications previously considered impossible.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着人工智能从强到强的发展，它正迅速融入社会的织锦中，触及我们生活的许多方面。学习如何有效地使用LLMs等AI模型可能是这个十年中最有用的技能之一。LLMs正在改变软件世界，使得以前被认为不可能的应用程序的开发成为可能。
- en: With all the promise that LLMs bring, the reality is that they are still not
    a mature technology and have many limitations like deficiencies in reasoning,
    lack of adherence to factuality, “hallucinations”, difficulties in steering them
    toward our goals, bias and fairness issues, and so on. Despite the existence of
    these limitations, we can still harness LLMs for good use and build a variety
    of helpful applications provided we effectively address their shortcomings.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLMs（大型语言模型）带来了巨大的希望，但现实是它们仍然不是一个成熟的技术，并且存在许多限制，如推理能力不足、缺乏事实性、出现“幻觉”、难以引导它们实现我们的目标、偏见和公平性问题等。尽管存在这些限制，我们仍然可以利用LLMs进行有益的应用，并构建各种有用的应用程序，前提是我们能够有效地解决它们的不足。
- en: Plenty of software frameworks have emerged that enable rapid prototype development
    of LLM applications. However, advancing from prototypes to production-grade applications
    is a road much less traveled, and is still a very challenging task. This is where
    this book comes in—a holistic overview of the LLM landscape that provides you
    with the intuition and tools to build complex LLM applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 已经出现了许多软件框架，它们能够快速原型开发LLM应用程序。然而，从原型发展到生产级应用程序是一条很少人走的路，仍然是一个非常具有挑战性的任务。这正是这本书的作用所在——提供一个对LLM领域的全面概述，为你提供构建复杂LLM应用程序的直觉和工具。
- en: With this book, my goal is to provide you with an intuitive understanding of
    how LLMs work, the tools you have at your disposal to harness them, and the various
    application paradigms they can be built with. Unique to this book are the exercises;
    more than 80 exercises are sprinkled throughout to help you solidify your intuitions
    and sharpen your understanding of what is happening underneath the hood. While
    preparing the content of the book, I read over 800 research papers, with many
    of them referenced and linked at appropriate locations in the book, providing
    you with a jumping off point for further exploration. All in all, I am confident
    that you will come out of the book an LLM expert if you read the book in its entirety,
    complete all the exercises, and explore the recommended references.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这本书，我的目标是让你对LLMs的工作原理有一个直观的理解，以及你可以用来利用它们的工具，以及它们可以构建的各种应用范式。这本书的独特之处在于练习；书中穿插了80多个练习，帮助你巩固直觉，并加深你对底层发生事情的理解。在准备这本书的内容时，我阅读了800多篇研究论文，其中许多在书中适当位置引用并链接，为你提供了进一步探索的起点。总的来说，如果你完整地阅读这本书，完成所有练习，并探索推荐的参考文献，我坚信你将成为一个LLM专家。
- en: Who This Book Is For
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书面向的对象
- en: This book is intended for a broad audience, including software engineers transitioning
    to AI application development, machine learning practitioners and scientists,
    and product managers. Much of the content in this book is borne from my own experiments
    with LLMs, so even if you are an experienced scientist, I expect you will find
    value in it. Similarly, even if you have very limited exposure to the world of
    AI, I expect you will still find the book useful for understanding the fundamentals
    of this technology.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书面向广泛的读者，包括转向AI应用开发的软件工程师、机器学习实践者和科学家，以及产品经理。这本书的大部分内容源于我对LLMs的实验，所以即使你是经验丰富的科学家，我也期待你能在其中找到价值。同样，即使你对AI世界接触很少，我也期待你仍然能从这本书中理解这项技术的根本。
- en: The only prerequisites for this book are knowledge of Python coding and an understanding
    of basic machine learning and deep learning principles. Where required, I provide
    links to external resources that you can use to sharpen or develop your prerequisites.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的唯一先决条件是了解 Python 编码知识和对基本机器学习和深度学习原理的理解。在需要的情况下，我提供了外部资源的链接，您可以使用这些资源来提高或发展您的先决条件。
- en: How This Book Is Structured
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书结构
- en: The book is divided into 3 parts with a total of 13 chapters. The first part
    deals with understanding the ingredients of a language model. I strongly feel
    that even though you may never train a language model from scratch yourself, knowing
    what goes into making it is crucial. The second part discusses various ways to
    harness language models, be it by directly prompting the model, or by fine-tuning
    it in various ways. It also addresses limitations such as hallucinations and reasoning
    constraints, along with methods to mitigate these issues. Finally, the third part
    of the book deals with application paradigms like retrieval augmented generation
    (RAG) and agents, positioning LLMs within the broader context of an entire software
    system.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书分为 3 部分，共 13 章。第一部分涉及理解语言模型的成分。我强烈认为，即使您可能永远不会从头开始训练语言模型，了解其构成也是至关重要的。第二部分讨论了各种利用语言模型的方法，无论是直接提示模型，还是以各种方式微调它。它还解决了诸如幻觉和推理限制等问题，以及减轻这些问题的方法。最后，本书的第三部分涉及检索增强生成（RAG）和代理等应用范式，将
    LLM 定位于整个软件系统的更广泛背景下。
- en: For an extended table of contents, see my [Substack blog post](https://oreil.ly/-2zkH).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看扩展的目录，请参阅我的 [Substack 博客文章](https://oreil.ly/-2zkH)。
- en: What This Book Is Not About
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书不涉及的内容
- en: To keep the book at a reasonable length, certain topics were deemed out of scope.
    I have taken care to not cover topics that I am not confident will stand the test
    of time. This field is very fast moving, so writing a book that maintains its
    relevance over time is extremely challenging.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持本书的合理长度，某些主题被认为超出了范围。我已经注意不要涵盖那些我不确定能否经得起时间考验的主题。这个领域发展非常快，因此编写一本能够保持其相关性的书籍极具挑战性。
- en: This book focuses only on English-language LLMs and leaves out discussion on
    multilingual models for the most part. I also disagree with the notion of mushing
    all the non-English languages of the world under the “multilingual” banner. Every
    language has its own nuances and deserves its own book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本书仅关注英语语言的 LLM，大部分情况下省略了对多语言模型的讨论。我也不同意将世界上所有非英语语言都归入“多语言”这一概念。每种语言都有其独特的细微差别，都值得有自己的书籍。
- en: This book also doesn’t cover multimodal models. New models are increasingly
    multimodal, i.e., a single model supports multiple modalities like text, image,
    video, speech, etc. However, text remains the most important modality and is the
    binding substrate in these models. Thus, reading this book will still help you
    prepare for the multimodal future.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书也没有涵盖多模态模型。新的模型越来越多地是多模态的，即单个模型支持多种模态，如文本、图像、视频、语音等。然而，文本仍然是最重要的模态，并且是这些模型的结合基础。因此，阅读这本书仍然可以帮助您为多模态的未来做好准备。
- en: This book does not focus on theory or go too deep into math. There are plenty
    of other books that cover that, and I have generously linked to them where needed.
    This book contains minimal math equations and instead focuses on building intuitions.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不专注于理论或深入数学。有很多其他书籍涵盖了这些内容，我在需要的地方慷慨地提供了链接。本书包含最少的数学方程式，而是专注于建立直觉。
- en: This book contains only a rudimentary introduction to reasoning models, the
    latest LLM paradigm. At the time of the book’s writing, reasoning models are still
    in their infancy, and the jury is still out on which techniques will prove to
    be most effective.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本书仅对推理模型进行了基本的介绍，这是最新的 LLM 范式。在本书写作时，推理模型仍处于起步阶段，关于哪些技术将证明是最有效的，人们仍在争论。
- en: How to Read the Book
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何阅读本书
- en: 'The best way to consume this book is to read it sequentially, while working
    on the exercises and exploring the reference links. That said, there are a few
    alternative paths, depending on your interests:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本书的最佳方式是按顺序阅读，同时完成练习并探索参考链接。尽管如此，根据您的兴趣，还有一些替代路径：
- en: If your interest lies in understanding the LLM landscape and not necessarily
    in building applications with them, you can focus on Chapters [1](ch01.html#chapter_llm-introduction),
    [2](ch02.html#ch02), [3](ch03.html#chapter-LLM-tokenization), [4](ch04.html#chapter_transformer-architecture),
    [5](ch05.html#chapter_utilizing_llms), [10](ch10.html#ch10), and [11](ch11.html#chapter_llm_interfaces).
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你更感兴趣的是了解LLM的格局，而不是一定要用它们来构建应用程序，你可以关注第[1](ch01.html#chapter_llm-introduction)、[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[4](ch04.html#chapter_transformer-architecture)、[5](ch05.html#chapter_utilizing_llms)、[10](ch10.html#ch10)和[11](ch11.html#chapter_llm_interfaces)章。
- en: If you are a product manager seeking to understand the scope of possibilities
    for LLM applications, Chapters [1](ch01.html#chapter_llm-introduction), [2](ch02.html#ch02),
    [3](ch03.html#chapter-LLM-tokenization), [5](ch05.html#chapter_utilizing_llms),
    [8](ch08.html#ch8), [10](ch10.html#ch10), [11](ch11.html#chapter_llm_interfaces),
    [12](ch12.html#ch12), and [13](ch13.html#ch13) are a good bet.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是一名产品经理，想要了解LLM应用的可能范围，第[1](ch01.html#chapter_llm-introduction)、[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[5](ch05.html#chapter_utilizing_llms)、[8](ch08.html#ch8)、[10](ch10.html#ch10)、[11](ch11.html#chapter_llm_interfaces)、[12](ch12.html#ch12)和[13](ch13.html#ch13)章是一个不错的选择。
- en: If you are an ML scientist, then Chapters [7](ch07.html#ch07), [8](ch08.html#ch8),
    [9](ch09.html#ch09), [10](ch10.html#ch10), [11](ch11.html#chapter_llm_interfaces),
    and [12](ch12.html#ch12) will be sure to give you food-for-thought and new research
    challenges.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你是一名机器学习科学家，那么第[7](ch07.html#ch07)、[8](ch08.html#ch8)、[9](ch09.html#ch09)、[10](ch10.html#ch10)、[11](ch11.html#chapter_llm_interfaces)和[12](ch12.html#ch12)章一定会给你带来思考的食物和新研究挑战。
- en: If you want to train your own LLM from scratch, Chapters [2](ch02.html#ch02),
    [3](ch03.html#chapter-LLM-tokenization), [4](ch04.html#chapter_transformer-architecture),
    [5](ch05.html#chapter_utilizing_llms), and [7](ch07.html#ch07) will provide you
    with the foundational principles.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想要从头开始训练自己的LLM，第[2](ch02.html#ch02)、[3](ch03.html#chapter-LLM-tokenization)、[4](ch04.html#chapter_transformer-architecture)、[5](ch05.html#chapter_utilizing_llms)和[7](ch07.html#ch07)章将为你提供基础原理。
- en: Conventions Used in This Book
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用的以下排版约定：
- en: '*Italic*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`常宽`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, and keywords.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句和关键字。
- en: '**`Constant width bold`**'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**`常宽粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 显示用户应直接输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*`常宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 显示应替换为用户提供的值或由上下文确定的值的文本。
- en: Tip
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: This element signifies a tip or suggestion.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般性说明。
- en: Warning
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意。
- en: Using Code Examples
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://oreil.ly/llm-playbooks*](https://oreil.ly/llm-playbooks)下载。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*support@oreilly.com*](mailto:support@oreilly.com).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在使用代码示例时遇到技术问题或问题，请发送电子邮件至[*support@oreilly.com*](mailto:support@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书旨在帮助您完成工作。一般来说，如果这本书提供了示例代码，您可以在您的程序和文档中使用它。除非您正在复制代码的很大一部分，否则您不需要联系我们获取许可。例如，编写一个使用这本书中几个代码片段的程序不需要许可。通过引用这本书并引用示例代码来回答问题不需要许可。将这本书的大量示例代码纳入您产品的文档中则需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*Designing Large
    Language Model Applications* by Suhas Pai (O’Reilly). Copyright 2025 Suhas Pai,
    978-1-098-15050-1.”'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感激，但通常不需要注明出处。注明出处通常包括标题、作者、出版社和ISBN。例如：“*《设计大型语言模型应用》* 由苏哈斯·帕伊（O’Reilly）著。版权所有2025年苏哈斯·帕伊，978-1-098-15050-1。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您对代码示例的使用超出了合理使用或上述许可的范围，请随时通过[*permissions@oreilly.com*](mailto:permissions@oreilly.com)联系我们。
- en: O’Reilly Online Learning
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: Note
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 40多年来，[*O’Reilly Media*](https://oreilly.com)一直提供技术和商业培训、知识和见解，以帮助公司成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专长。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和200多家其他出版商的大量文本和视频。更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关这本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sebastopol, CA 95472
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969（美国或加拿大）
- en: 707-827-7019 (international or local)
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019（国际或本地）
- en: 707-829-0104 (fax)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104（传真）
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，上面列出了勘误表、示例和任何其他附加信息。您可以通过[*https://oreil.ly/designing-llm-applications-1e*](https://oreil.ly/designing-llm-applications-1e)访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)。
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)。
- en: Acknowledgments
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: They say it takes a village to raise a child; I now realize it takes a metropolis
    to write a book.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 他们常说，养育一个孩子需要整个村庄的力量；我现在意识到，写一本书则需要一个都市的力量。
- en: Firstly, I would like to thank the O’Reilly team for the meticulous professionalism
    and finesse with which they worked with me throughout the development and launch
    of the book. No wonder they are the world’s top technical book publishers. I would
    particularly like to thank Nicole Butterfield for signing me up as an author and
    Michele Cronin, the world’s best editor, whose frequent reviews ensured that the
    book developed a coherent structure. I will miss our regular check-ins! Thanks
    to Ashley Stussy, Kristen Brown, and the rest of the production team for their
    diligent work in getting the book to production.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我要感谢奥莱利团队在整个书籍的开发和发布过程中与我合作时的细致专业和优雅。难怪他们是世界上最好的技术书籍出版商。我特别想感谢妮可·巴特菲尔德，是她邀请我成为作者，以及世界最好的编辑米歇尔·克罗宁，她频繁的审阅确保了书籍发展出连贯的结构。我会怀念我们定期的检查！感谢艾什利·斯图西、克莉丝滕·布朗以及整个制作团队为将书籍投入生产所付出的辛勤工作。
- en: I am deeply thankful to my friend Amber Teng, who helped me with drawing the
    book illustrations and setting up the book’s Github repository. I am also immensely
    indebted to my technical reviewers Serena McDonnell, Yenson Lau, Susan Shu Chang,
    Gordon Gibson, and Nour Fahmy for the dozens of hours each of them spent in writing
    extremely detailed and thoughtful technical reviews. The book is so much better
    for it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常感谢我的朋友艾米伯·腾格，她帮助我绘制书籍插图并设置书籍的GitHub仓库。我也要深深地感谢我的技术审稿人塞伦娜·麦克唐纳尔、杨森·刘、苏珊·舒·张、戈登·吉布森和努尔·法赫米，他们各自花费了数十小时撰写了极其详细和深思熟虑的技术审阅。这本书因此变得更好。
- en: I am thankful to the Toronto AI ecosystem, especially the Aggregate Intellect,
    TMLS (Toronto Machine Learning Summit), and SharpestMinds communities for providing
    me with the space to engage with the community and ensure that I always had a
    finger on the pulse of the industry. Special thanks go to my friends Madhav Singhal,
    Jay Alammar, and Megan Risdal (who helped me coin the phrase “token etymology”)
    for our regular intellectually stimulating conversations on LLMs and for being
    the first readers of the book. I also want to give a shout out to my open-source
    collaborator Huu Nguyen, who I worked with on various open-source LLM projects,
    for the dozens of late night discussions on the most audacious ideas in LLM research.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我感谢多伦多AI生态系统，特别是聚合智力、TMLS（多伦多机器学习峰会）和SharpestMinds社区，为我提供了与社区互动的空间，并确保我始终能够把握行业的脉搏。特别感谢我的朋友们马达夫·辛哈尔、杰伊·阿拉马尔和梅根·里斯达尔（他们帮助我创造了“token
    etymology”这个短语），我们定期进行关于LLMs的富有启发性的对话，并且是这本书的首批读者。我还要向我的开源合作者胡安·阮致敬，我们在多个开源LLM项目上合作，对于在LLM研究中最大胆的想法上进行的数十次深夜讨论表示感谢。
- en: Writing a book while also being the cofounder of an AI startup was possible
    only due to the unwavering support of my partner in business and crime, Kris Bennatti
    (who also convinced me to remove the word “orifice” from the book). I will forever
    be in gratitude to the entire Hudson Labs team for their steadfast and consistent
    backing throughout, with a special shout out to Xiao Quan, whose steady hands
    ensured that I found the time to focus on the book. Additionally, I would like
    to thank my friends Kaaveh Shoamanesh, Abdullah Al-hayali, Zach Nguyen, Samarth
    Bhasin, Sadegh Raeisi, and Ian Yu for their moral support throughout and regularly
    checking that I was getting the right amount of sleep.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在同时作为一家AI初创公司的联合创始人写作书籍是可能的，这都得益于我的商业和犯罪伙伴，克里斯·贝纳蒂（他同时也说服我删除了书中的“orifice”一词）。我将永远感激整个哈德森实验室团队在整段时间里坚定不移和持续的支持，特别感谢小泉，他稳定的手确保了我有足够的时间专注于书籍。此外，我还要感谢我的朋友们卡夫·沙马内什、阿卜杜拉·阿尔-哈亚利、扎克·阮、萨马尔特·巴辛、萨代格·拉伊西和伊恩·余，他们在整个过程中给予我道德支持，并定期检查我是否得到了足够的睡眠。
- en: Finally, I would like to dedicate this book to my mom, Kusuma Pai, whom I simply
    refer to as “The Legend” for her lifelong sacrifices to ensure that I grew up
    and was in a position to write the book. Any success of this book should be predominantly
    credited to my mother for molding me into the person I am today.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想将这本书献给我的母亲，库苏玛·帕伊，我简单地称她为“传奇”，因为她一生的牺牲，确保了我能够长大成人并有机会写这本书。这本书的任何成功都应该主要归功于我的母亲，因为她塑造了我今天成为的人。
