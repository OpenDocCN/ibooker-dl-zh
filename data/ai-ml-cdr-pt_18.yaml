- en: Chapter 17\. Serving LLMs with Ollama
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第17章\. 使用Ollama提供LLM服务
- en: We’ve explored how to use transformers to download a model and put together
    an easy pipeline that lets you use it for inference or fine-tuning. However, I’d
    be remiss if I didn’t show you the open source Ollama project, which ties it all
    together by giving you an environment that gives you a full wrapper around an
    LLM that you can either chat with in your terminal or use as a server that you
    can HTTP POST to and read the output from.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了如何使用transformers下载模型并构建一个简单的管道，让您可以使用它进行推理或微调。然而，如果我不向您展示开源的Ollama项目，那就太遗憾了。该项目通过提供一个环境将所有这些整合在一起，您可以在终端与其聊天，或者将其用作服务器，您可以通过HTTP
    POST向其发送请求并读取输出。
- en: Technologies like Ollama will be the vanguard of the next generation of LLMs,
    which will let you have dedicated servers inside your data center or dedicated
    processes on your computer. That will make them completely private to you.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 像Ollama这样的技术将是下一代LLM的前沿，它将让您在数据中心内部拥有专用服务器或在您的计算机上拥有专用进程。这将使它们对您来说完全私密。
- en: At its core, Ollama is an open source project that simplifies the process of
    downloading, running, and managing LLMs on your computer. It also handles nonfunctional
    difficult requirements, such as memory management and model optimization, and
    it provides standardized interfaces for interaction, such as the ability to HTTP
    POST to your models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，Ollama是一个开源项目，简化了在您的计算机上下载、运行和管理LLM的过程。它还处理非功能性困难要求，例如内存管理和模型优化，并提供标准化的交互接口，例如能够通过HTTP
    POST与您的模型交互。
- en: Ollama is also a key strategic tool you should consider because it bridges the
    gap between cloud-based third-party services like GPT, Claude, and Gemini and
    locally deployed services. It goes beyond giving you a local development environment
    to giving you one that you could, for example, use within your own data center
    to serve multiple internal users.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama也是您应该考虑的关键战略工具，因为它弥合了基于云的第三方服务（如GPT、Claude和Gemini）与本地部署服务之间的差距。它不仅提供本地开发环境，还提供您可以在自己的数据中心内使用，为多个内部用户提供服务的环境。
- en: By running models locally, you can ensure the complete privacy of your data,
    eliminate network latency, and work offline. This is especially crucial in scenarios
    involving sensitive data or applications that require consistent, low-latency
    responses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在本地运行模型，您可以确保数据的完全隐私，消除网络延迟，并离线工作。这在涉及敏感数据或需要一致、低延迟响应的应用程序的场景中尤为重要。
- en: Ollama also supports a growing library of popular open source models, including
    Llama, Mistral, and Gemma, and it also supports various specialized models that
    are optimized for specific tasks. Each model can be pulled and run with simple
    commands, in a way that’s similar to how Docker containers work. The platform
    handles model quantization automatically, optimizing models to run efficiently
    on consumer hardware while maintaining good performance.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama还支持一个不断增长的流行开源模型库，包括Llama、Mistral和Gemma，它还支持针对特定任务优化的各种专用模型。每个模型都可以通过简单的命令拉取和运行，其方式类似于Docker容器的工作方式。该平台自动处理模型量化，优化模型以在消费级硬件上高效运行，同时保持良好的性能。
- en: 'In this chapter, we’ll explore Ollama in three ways: installing it and getting
    started, looking at how you can instantiate specific models and use them, and
    exploring the RESTful APIs that let you build LLM applications that preserve privacy.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将以三种方式探索Ollama：安装它并开始使用，查看您如何实例化特定模型并使用它们，以及探索RESTful API，这些API让您能够构建保护隐私的LLM应用程序。
- en: Getting Started with Ollama
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Ollama
- en: The Ollama project is hosted at [ollama.com](http://ollama.com). It’s pretty
    straightforward to get up and running, and the home screen gives download options
    for macOS, Linux, and Windows. Note also that the Windows version needs Windows
    Subsystem for Linux (WSL). For this chapter, I’m using the macOS version.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ollama项目托管在[ollama.com](http://ollama.com)。启动起来非常简单，主屏幕提供了macOS、Linux和Windows的下载选项。请注意，Windows版本需要Windows
    Subsystem for Linux (WSL)。对于本章，我使用的是macOS版本。
- en: When you navigate to the website, you’ll see a friendly welcome to download
    (see [Figure 17-1](#ch17_figure_1_1748550058907779)).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当您导航到网站时，您将看到一个友好的欢迎下载界面（见[图17-1](#ch17_figure_1_1748550058907779)）。
- en: '![](assets/aiml_1701.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![Ollama_1701.png](assets/aiml_1701.png)'
- en: Figure 17-1\. Getting started with Ollama
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图17-1\. 开始使用Ollama
- en: Once you’ve downloaded and installed Ollama, you can launch it, and you’ll see
    it in the system bar at the top of the screen. Your main interface with Ollama
    will be the command line.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你下载并安装了 Ollama，你就可以启动它，你会在屏幕顶部的系统栏中看到它。你与 Ollama 的主要界面将是命令行。
- en: 'Then, with the `ollama run` command, you can download and use models. So, for
    example, if you want to use Gemma, then from Google, you can do the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用 `ollama run` 命令，你可以下载并使用模型。例如，如果你想使用 Gemma，那么从 Google，你可以执行以下操作：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll want to be sure to note the parameters used, which you can find in the
    [model’s documentation page on Ollama](https://oreil.ly/VMLKO). While Ollama can
    and will quantize models that are optimized to run locally, it can’t perform miracles,
    and only models that will fit in your system resources—most importantly, memory—will
    work. In this case, I ran the `gemma2:2b` (2-billion parameter) version, which
    requires about 8 GB of GPU RAM. On macOS, the shared RAM with the M-Series chips
    works well, while running on an M1 Mac with 16 Gb, the Gemma 2B is fast and smooth
    with Ollama.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要确保注意使用的参数，这些参数可以在 Ollama 的[模型文档页面](https://oreil.ly/VMLKO)上找到。虽然 Ollama 可以并且会量化那些优化以本地运行的模型，但它不能创造奇迹，只有适合你系统资源——最重要的是内存——的模型才能工作。在这种情况下，我运行了
    `gemma2:2b`（20 亿参数）版本，这需要大约 8 GB 的 GPU RAM。在 macOS 上，与 M-Series 芯片的共享 RAM 工作良好，而在
    16 Gb 的 M1 Mac 上运行时，Gemma 2B 与 Ollama 一起运行得既快又流畅。
- en: You can see me chatting with Gemma in [Figure 17-2](#ch17_figure_2_1748550058907817).
    These responses took less than one second to receive on my two-year-old laptop!
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[图 17-2](#ch17_figure_2_1748550058907817)中看到我与 Gemma 的聊天。这些响应在我的两岁笔记本电脑上接收不到一秒！
- en: '![](assets/aiml_1702.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_1702.png)'
- en: Figure 17-2\. Using Ollama in a terminal
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 17-2\. 在终端中使用 Ollama
- en: It’s great to have a localized chat like this, and you can experiment with different
    models, including multimodal ones like Llama 3.2\.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有这样一个本地化的聊天真是太好了，你可以尝试不同的模型，包括像 Llama 3.2\ 这样的多模态模型。
- en: 'So, for example, you could issue the following command:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以执行以下命令：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Then, within the terminal, you could do multimodal processing. For example,
    if your terminal supported it, and you dragged and dropped an image into the terminal,
    you could ask the model what it could see in the image. The multimodal power of
    Llama would parse the image for you, and Ollama would handle all the technical
    difficulties.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在终端内，你可以进行多模态处理。例如，如果你的终端支持它，并且你将一个图像拖放到终端中，你可以询问模型它在图像中看到了什么。Llama 的多模态能力会为你解析图像，而
    Ollama 会处理所有的技术难题。
- en: 'In my case, all I had to do was give a prompt and then drag and drop the image
    onto it. So, I opened an Ollama chat window with the preceding command and then
    entered this prompt:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的情况下，我只需要给出一个提示，然后将图像拖放到上面。因此，我使用前面的命令打开了一个 Ollama 聊天窗口，并输入了这个提示：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`And then I just dragged and dropped the image into the chat window, and Ollama
    did the rest.    You can see in [Figure 17-3](#ch17_figure_4_1748550058907880)
    how detailed the results were. This is a photo I took of Osaka castle one morning
    while on a run in 2018\. While Llama couldn’t guess the date, it was able to predict
    the season based on the foliage in the image. It got everything else correct and
    gave very detailed output!  ![](assets/aiml_1703.png)  ###### Figure 17-3\. Using
    Ollama for a multimodal model    While it’s really cool to have a local LLM that
    you can chat with in a privacy-preserving way, I think the real power in Ollama
    is in using it as a server that can then be the foundation of an application.
    We’ll explore that next.`  [PRE3][PRE4]py ollama serve [PRE5]py curl http://localhost:11434/api/generate
    -d ''{ `"model"``:` `"gemma2:2b"``,`   `"prompt"``:` `"Why is the sky blue?"``,`   `"stream"``:`
    `false` `}``''` [PRE6]py[PRE7] [PRE8] {     "model":"llama3.2-vision",     "created_at":"2024-12-10T17:15:06.264497Z",     "response":"The
    image depicts Osaka Castle...",     "done":true,     "done_reason":     "stop",     "context":[128006,882,128007,271,58,...],     "total_duration":88817301209,     "load_duration":21197292,     "prompt_eval_count":19,     "prompt_eval_duration":84560000000,     "eval_count":56,     "eval_duration":4050000000
    }% [PRE9]` [PRE10][PRE11][PRE12][PRE13][PRE14][PRE15][PRE16] You are an expert
    storyteller who understands story structure, nuance, and content. Attached is
    a novel, so please evaluate this novel for storylines and suggest improvements
    that could  be made in character development, plot, and emotional  content. Be
    as verbose as needed to provide an in-depth  analysis that would help the author
    understand how their work  would be accepted. [PRE17] # Read the file content
    with open(filepath, ''r'', encoding=''utf-8'') as file:     file_content = file.read()
    [PRE18] # Prepare the request url = "http://localhost:11434/api/generate" headers
    = {"Content-Type": "application/json"} [PRE19] payload = {     "model": model,     "prompt":
    f"You are an expert storyteller who understands                  `story` `structure``,`
    `nuance``,` `and` `content``.` `Attached` `is` `a` `novel``,`                  `please`
    `evaluate` `this` `novel` `for` `storylines``,` `and` `suggest`                  `improvements`
    `that` `could` `be` `made` `in` `character` `development``,`                  `plot``,`
    `and` `emotional` `content``.` `Be` `as` `verbose` `as` `needed`                  `to`
    `provide` `an` `in``-``depth` `analysis` `that` `would` `help` `the` `author`                  `understand`
    `how` `their` `work` `would` `be` `accepted``:` `{``file_content``}``",` [PRE20]
    [PRE21]`` [PRE22] try:     # Send the request     response = requests.post(url,
    headers=headers, json=payload)     response.raise_for_status()  # Raise an exception
    for bad status        # Parse the response     result = response.json()[''response'']       #
    Return the response text from Ollama     return result   except requests.exceptions.RequestException
    as e:     raise Exception(f"Error making request to Ollama: {str(e)}") except
    json.JSONDecodeError as e:     raise Exception(f"Error parsing Ollama response:
    {str(e)}") [PRE23] def analyze_file(filepath: str, model: str = "gemma2:2b") ->
    dict: [PRE24] result = analyze_file(str(input_path)) [PRE25] **Strengths:**   *
    **Intriguing Premise:** The concept of a deadly plague originating  from space
    with potentially disastrous consequences is both  suspenseful and timely, appealing
    to a wider audience. * **Realistic Characters:**  The characters feel grounded
    despite being  involved in extraordinary events, and their personalities shine  through
    (Aisha''s determination, Soo-Kyung''s wisdom). Their connection  adds emotional
    weight. * **Suspenseful Tone:** The story builds suspense gradually. You expertly  use
    cliffhangers like the three-year deadline for the plague  to leave readers wanting
    more. * **Worldbuilding Potential:**  The mention of the moon base and potential  interstellar
    jumps introduces a rich world with possibilities  for further exploration.   [PRE26]`
    [PRE27]`` [PRE28] const PORT = process.env.PORT || 3000; app.listen(PORT, () =>
    {   console.log(`Server running on port ${PORT}`); }); [PRE29] app.post(''/analyze'',
    upload.single(''novel''), async (req, res) => {   try {     if (!req.file) {       return
    res.status(400).send(''No file uploaded'');     } [PRE30] // Generate a unique
    job ID const jobId = Date.now().toString();   // Store job status analysisJobs.set(jobId,
    { status: ''processing'' });   // Start analysis in background const fileContent
    = await fs.readFile(req.file.path, ''utf8'');   // Clean up uploaded file await
    fs.unlink(req.file.path); [PRE31] // Process in background analyzeNovel(fileContent)   .then(result
    => {     analysisJobs.set(jobId, {       status: ''completed'',       result:
    result     });   })   .catch(error => {     analysisJobs.set(jobId, {       status:
    ''error'',       error: error.message     });   }); [PRE32] async function analyzeNovel(text)
    {   try {     console.log(''Sending request to Ollama...'');     const requestBody
    = {       model: ''gemma2:2b'',       prompt: `You are an expert storyteller who
    understands        story structure, nuance, and content. Attached is a novel.        Please
    evaluate this novel for storylines and suggest improvements        that could
    be made in character development, plot, and emotional content.        Be as verbose
    as needed to provide an in-depth analysis that would help        the author understand
    how their work would be accepted:\n\n${text}`,       stream: false     }; [PRE33]
    const response = await fetch(OLLAMA_URL, {   method: ''POST'',   headers: {     ''Content-Type'':
    ''application/json'',   },   body: JSON.stringify(requestBody) }); [PRE34] if
    (!response.ok) {   throw new Error(`HTTP error! status: ${response.status}`);
    }   const data = await response.json(); return data.response; [PRE35] <h1>Novel
    Analysis Tool</h1> <div class="upload-form">   <h2>Upload your novel</h2>   <form
    id="uploadForm">     <input type="file" name="novel" accept=".txt" required>     <br>     <button
    type="submit" class="submit-button">Analyze Novel</button>   </form> </div> [PRE36]
    document.getElementById(''uploadForm'')         .addEventListener(''submit'',
    async (e) => {e.preventDefault();   [PRE37] // Upload file const formData = new
    FormData(form); const response = await fetch(''/analyze'', {   method: ''POST'',   body:
    formData }); [PRE38] if (!response.ok) throw new Error(''Upload failed'');   const
    { jobId } = await response.json();   // Poll for results while (true) {   const
    statusResponse = await fetch(`/status/${jobId}`);   if (!statusResponse.ok) throw
    new Error(''Status check failed'');     const status = await statusResponse.json();     if
    (status.status === ''completed'') {     result.textContent = status.result;     result.style.display
    = ''block'';     loading.style.display = ''none'';     break;   } else if (status.status
    === ''error'') {     throw new Error(status.error);   }     // Wait before polling
    again   await new Promise(resolve => setTimeout(resolve, 1000)); } [PRE39] node
    app.js [PRE40]` [PRE41] ``# Summary    In this chapter, we looked at how the open
    source Ollama tool gives you the ability to wrap an LLM with an easy-to-use API
    that lets you build applications with it. You saw how to install Ollama and then
    explored some scenarios with it. You also downloaded and used models like the
    simple, lightweight Gemma from Google, as well as the powerful, multimodal Llama3.2-vision
    from Meta. You explored not just chatting with them but also attaching files to
    upload. Ollama gives you an HTTP endpoint that you saw how to experiment with
    by using a `curl` command to simulate HTTP traffic. Finally, you got into writing
    a prototype of a real-world LLM-based application that analyzed contents of books,
    first as a simple Python script that proved the concept and then as a more sophisticated
    web-based application in `node.js` that used an Ollama backend and a Gemma LLM
    to do the heavy lifting!    In the next chapter, we’ll build on this and explore
    the concepts of RAG, and you’ll build apps that use local vector databases to
    enhance the knowledge of LLMs.`` [PRE42][PRE43][PRE44][PRE45]``````'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`然后我只是把图片拖放到聊天窗口中，Ollama就完成了剩下的工作。您可以在[图17-3](#ch17_figure_4_1748550058907880)中看到结果有多么详细。这是我在2018年一次晨跑时拍摄的大阪城照片。虽然Llama无法猜出日期，但它能够根据图像中的树叶预测季节。它对其他所有事情都判断正确，并给出了非常详细的输出！![assets/aiml_1703.png](assets/aiml_1703.png)
    ###### 图17-3\. 使用Ollama进行多模态模型    虽然能够以隐私保护的方式与本地LLM聊天确实很酷，但我认为Ollama真正的力量在于将其用作服务器，然后可以作为应用程序的基础。我们将在下一章探讨这一点。`
    [PRE3][PRE4]py ollama serve [PRE5]py curl http://localhost:11434/api/generate
    -d ''{ `"model"``:` `"gemma2:2b"``,`   `"prompt"``:` `"Why is the sky blue?"``,`   `"stream"``:`
    `false` `}``''` [PRE6]py[PRE7] [PRE8] {     "model":"llama3.2-vision",     "created_at":"2024-12-10T17:15:06.264497Z",     "response":"The
    image depicts Osaka Castle...",     "done":true,     "done_reason":     "stop",     "context":[128006,882,128007,271,58,...],     "total_duration":88817301209,     "load_duration":21197292,     "prompt_eval_count":19,     "prompt_eval_duration":84560000000,     "eval_count":56,     "eval_duration":4050000000
    }% [PRE9]` [PRE10][PRE11][PRE12][PRE13][PRE14][PRE15][PRE16] 你是一位精通故事讲述技巧的专家，理解故事结构、细微差别和内容。附上的是一部小说，请对该小说的故事情节进行评估，并提出在角色发展、情节和情感内容方面可以改进的建议。如有必要，请详细阐述，以便作者了解他们的作品将如何被接受。
    [PRE17] # 读取文件内容 with open(filepath, ''r'', encoding=''utf-8'') as file:     file_content
    = file.read() [PRE18] # 准备请求 url = "http://localhost:11434/api/generate" headers
    = {"Content-Type": "application/json"} [PRE19] payload = {     "model": model,     "prompt":
    f"You are an expert storyteller who understands                  `story` `structure``,`
    `nuance``,` `and` `content``.` `Attached` `is` `a` `novel``,`                  `please`
    `evaluate` `this` `novel` `for` `storylines``,` `and` `suggest`                  `improvements`
    `that` `could` `be` `made` `in` `character` `development``,`                  `plot``,`
    `and` `emotional` `content``.` `Be` `as` `verbose` `as` `needed`                  `to`
    `provide` `an` `in``-``depth` `analysis` `that would help        the author`                  `understand`
    `how` `their` `work` `would` `be` `accepted``:` `{``file_content``}``",` [PRE20]
    [PRE21]`` [PRE22] try:     # 发送请求     response = requests.post(url, headers=headers,
    json=payload)     response.raise_for_status()  # 对不良状态引发异常        # 解析响应     result
    = response.json()[''response'']       # 从Ollama返回响应文本     return result   except
    requests.exceptions.RequestException as e:     raise Exception(f"Error making
    request to Ollama: {str(e)}") except json.JSONDecodeError as e:     raise Exception(f"Error
    parsing Ollama response: {str(e)}") [PRE23] def analyze_file(filepath: str, model:
    str = "gemma2:2b") -> dict: [PRE24] result = analyze_file(str(input_path)) [PRE25]
    **优势：**   * **引人入胜的情节设定：** 来自太空的致命瘟疫及其可能带来的灾难性后果这一概念既令人紧张又具有时效性，吸引了更广泛的受众。 * **真实的人物形象：**
    尽管角色参与了非凡的事件，但他们的形象感觉真实可信，个性鲜明（艾莎的决心，苏-京的智慧）。他们的联系增加了情感分量。 * **紧张的氛围：** 故事逐渐建立紧张感。您巧妙地使用了悬念，如瘟疫三年的最后期限，让读者渴望更多。
    * **世界构建潜力：** 提到月球基地和可能的星际跳跃引入了一个充满可能性的丰富世界，为进一步的探索提供了空间。   [PRE26]` [PRE27]``
    [PRE28] const PORT = process.env.PORT || 3000; app.listen(PORT, () => {   console.log(`Server
    running on port ${PORT}`); }); [PRE29] app.post(''/analyze'', upload.single(''novel''),
    async (req, res) => {   try {     if (!req.file) {       return res.status(400).send(''No
    file uploaded'');     } [PRE30] // 生成唯一的作业ID const jobId = Date.now().toString();   //
    存储作业状态 analysisJobs.set(jobId, { status: ''processing'' });   // 在后台开始分析 const
    fileContent = await fs.readFile(req.file.path, ''utf8'');   // 清理上传的文件 await fs.unlink(req.file.path);
    [PRE31] // 在后台处理 analyzeNovel(fileContent)   .then(result => {     analysisJobs.set(jobId,
    {       status: ''completed'',       result: result     });   })   .catch(error
    => {     analysisJobs.set(jobId, {       status: ''error'',       error: error.message     });   });
    [PRE32] async function analyzeNovel(text) {   try {     console.log(''Sending
    request to Ollama...'');     const requestBody = {       model: ''gemma2:2b'',       prompt:
    `You are an expert storyteller who understands        story structure, nuance,
    and content. Attached is a novel.        Please evaluate this novel for storylines
    and suggest improvements        that could be made in character development, plot,
    and emotional content.        Be as verbose as needed to provide an in-depth analysis
    that would help        the author understand how their work would be accepted:\n\n${text}`,       stream:
    false     }; [PRE33] const response = await fetch(OLLAMA_URL, {   method: ''POST'',   headers:
    {     ''Content-Type'': ''application/json'',   },   body: JSON.stringify(requestBody)
    }); [PRE34] if (!response.ok) {   throw new Error(`HTTP error! status: ${response.status}`);
    }   const data = await response.json(); return data.response; [PRE35] <h1>Novel
    Analysis Tool</h1> <div class="upload-form">   <h2>Upload your novel</h2>   <form
    id="uploadForm">     <input type="file" name="novel" accept=".txt" required>     <br>     <button
    type="submit" class="submit-button">Analyze Novel</button>   </form> </div> [PRE36]
    document.getElementById(''uploadForm'')         .addEventListener(''submit'',
    async (e) => {e.preventDefault();   [PRE37] // 上传文件 const formData = new FormData(form);
    const response = await fetch(''/analyze'', {   method: ''POST'',   body: formData
    }); [PRE38] if (!response.ok) throw new Error(''Upload failed'');   const { jobId
    } = await response.json();   // 轮询结果 while (true) {   const statusResponse = await
    fetch(`/status/${jobId}`);   if (!statusResponse.ok) throw new Error(''Status
    check failed'');     const status = await statusResponse.json();     if (status.status
    === ''completed'') {     result.textContent = status.result;     result.style.display
    = ''block'';     loading.style.display = ''none'';     break;   } else if (status.status
    === ''error'') {     throw new Error(status.error);   }     // 等待再次轮询   await
    new Promise(resolve => setTimeout(resolve, 1000)); } [PRE39] node app.js [PRE40]`
    [PRE41] ``# 摘要    在本章中，我们探讨了开源Ollama工具如何让您能够通过易于使用的API包装LLM，从而构建应用程序。您看到了如何安装Ollama，然后探讨了使用它的某些场景。您还下载并使用了来自谷歌的简单、轻量级的Gemma模型，以及来自Meta的强大、多模态的Llama3.2-vision模型。您不仅探索了与他们聊天，还探索了附加文件上传的功能。Ollama为您提供了一个HTTP端点，您通过使用`curl`命令模拟HTTP流量来实验它。最后，您编写了一个真实世界LLM应用程序的原型，该应用程序分析书籍内容，最初是一个证明概念的简单Python脚本，然后是一个更复杂的基于`node.js`的Web应用程序，该应用程序使用Ollama后端和Gemma
    LLM进行繁重的工作！    在下一章中，我们将在此基础上构建，并探讨RAG的概念，您将构建使用本地向量数据库来增强LLM知识的应用程序。`` [PRE42][PRE43][PRE44][PRE45]`````'
