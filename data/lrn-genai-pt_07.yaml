- en: '6 CycleGAN: Converting blond hair to black hair'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 CycleGAN：将金发转换为黑发
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The idea behind CycleGAN and cycle consistency loss
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN及其循环一致性损失背后的理念
- en: Building a CycleGAN model to translate images from one domain to another
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建CycleGAN模型以将图像从一个领域转换为另一个领域
- en: Training a CycleGAN by using any dataset with two domains of images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有两个图像领域的任何数据集训练CycleGAN
- en: Converting black hair to blond hair and vice versa
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将黑发转换为金发，反之亦然
- en: The generative adversarial networks (GAN) models we have discussed in the last
    three chapters are all trying to produce images that are indistinguishable from
    those in the training set.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前三章中讨论的生成对抗网络（GAN）模型都在尝试生成与训练集中图像无法区分的图像。
- en: 'You may be wondering: Can we translate images from one domain to another, such
    as transforming horses into zebras, converting black hair to blond hair or blond
    hair to black, adding or removing eyeglasses in images, turning photographs into
    paintings, or converting winter scenes to summer scenes? It turns out you can,
    and you’ll acquire such skills in this chapter through CycleGAN!'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道：我们能否将图像从一个领域转换为另一个领域，例如将马变成斑马，将黑发转换为金发或将金发转换为黑发，在图像中添加或去除眼镜，将照片转换为画作，或将冬季场景转换为夏季场景？实际上，你可以做到，你将在本章通过CycleGAN获得这样的技能！
- en: CycleGAN was introduced in 2017.^([1](#footnote-000)) The key innovation of
    CycleGAN is its ability to learn to translate between domains without paired examples.
    CycleGAN has a variety of interesting and useful applications, such as simulating
    the aging or rejuvenation process on faces to assist digital identity verification
    or visualizing clothing in different colors or patterns without physically creating
    each variant to streamline the design process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN于2017年提出.^([1](#footnote-000)) CycleGAN的关键创新是其能够在没有成对示例的情况下学习在不同领域之间进行转换。CycleGAN有各种有趣和有用的应用，例如模拟面部老化或年轻化过程以协助数字身份验证或在不实际创建每个变体的情况下可视化不同颜色或图案的服装，以简化设计过程。
- en: 'CycleGAN uses a cycle consistency loss function to ensure the original image
    can be reconstructed from the transformed image, encouraging the preservation
    of key features. The idea behind cycle consistency loss is truly ingenious and
    deserves to be highlighted here. The CycleGAN in this chapter has two generators:
    let’s call them the black hair generator and the blond hair generator, respectively.
    The black hair generator takes in an image with blond hair (instead of a random
    noise vector as you have seen before) and converts it to one with black hair,
    while the blond hair generator takes in an image with black hair and converts
    it to one with blond hair.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN使用循环一致性损失函数来确保原始图像可以从转换后的图像中重建，从而鼓励保留关键特征。循环一致性损失背后的理念真正巧妙，值得在此强调。本章中的CycleGAN有两个生成器：让我们分别称它们为黑发生成器和金发生成器。黑发生成器接收一张带有金发的图像（而不是像之前看到的随机噪声向量），并将其转换为一张带有黑发的图像，而金发生成器接收一张带有黑发的图像并将其转换为一张带有金发的图像。
- en: To train the model, we’ll give a real image with black hair to the blond hair
    generator to produce a fake image with blond hair. We’ll then give the fake blond
    hair image to the black hair generator to convert it back to an image with black
    hair. If both generators work well, there is little difference between the original
    image with black hair and the fake one after a round-trip conversion. To train
    the CycleGAN, we adjust the model parameters to minimize the sum of adversarial
    losses and cycle consistency losses. As in chapters 3 and 4, adversarial losses
    are used to quantify how well the generator can fool the discriminator and how
    well the discriminator can differentiate between real and fake samples. Cycle
    consistency loss, a unique concept in CycleGANs, measures the difference between
    the original image and the fake image after a round-trip conversion. The inclusion
    of the cycle consistency loss in the total loss function is the key innovation
    in CycleGANs.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们将一张带有黑发的真实图像给金发生成器，以生成一张带有金发的假图像。然后，我们将假金发图像给黑发生成器，将其转换回一张带有黑发的图像。如果两个生成器都工作得很好，经过一次往返转换后，原始黑发图像与假图像之间的差异很小。为了训练CycleGAN，我们调整模型参数以最小化对抗损失和循环一致性损失的加权和。与第3章和第4章一样，对抗损失用于量化生成器欺骗判别器的能力以及判别器区分真实和假样本的能力。循环一致性损失，CycleGAN的独特概念，衡量了原始图像和经过往返转换的假图像之间的差异。将循环一致性损失包含在总损失函数中是CycleGAN的关键创新。
- en: We’ll use black and blond hair images as examples of two domains when training
    CycleGAN. However, the model can be applied to any two domains of images. To drive
    home the message, I’ll ask you to train the same CycleGAN model by using images
    with and without eyeglasses that you used in chapter 5\. The solution is provided
    in the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)),
    and you’ll see that the trained model can indeed add or remove eyeglasses from
    human face images.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练CycleGAN时，我们将黑发和金发图像作为两个域的例子。然而，该模型可以应用于任何两个图像域。为了强调这一点，我将要求你使用第5章中使用的带眼镜和不带眼镜的图像来训练相同的CycleGAN模型。解决方案在本书的GitHub仓库中提供（[https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI)），你将看到训练好的模型确实可以从人脸图像中添加或移除眼镜。
- en: 6.1 CycleGAN and cycle consistency loss
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 CycleGAN与循环一致性损失
- en: CycleGAN extends the basic GAN architecture to include two generators and two
    discriminators. Each generator-discriminator pair is responsible for learning
    the mapping between two distinct domains. It aims to translate images from one
    domain to another (e.g., horses to zebras, summer to winter scenes, and so on)
    while retaining the key characteristics of the original images. It uses a cycle
    consistency loss that ensures the original image can be reconstructed from the
    transformed image, encouraging the preservation of key features.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN扩展了基本的GAN架构，包括两个生成器和两个判别器。每个生成器-判别器对负责学习两个不同域之间的映射。它旨在将图像从一个域转换到另一个域（例如，马到斑马，夏景到冬景等），同时保留原始图像的关键特征。它使用循环一致性损失来确保可以从转换后的图像重建原始图像，从而鼓励保留关键特征。
- en: 'In this section, we’ll first discuss the architecture of CycleGAN. We’ll emphasize
    the key innovation of CycleGANs: cycle consistency loss.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先讨论CycleGAN的架构。我们将强调CycleGAN的关键创新：循环一致性损失。
- en: 6.1.1 What is CycleGAN?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 什么是CycleGAN？
- en: CycleGAN consists of two generators and two discriminators. The generators translate
    images from one domain to another, while the discriminators determine the authenticity
    of the images in their respective domains. These networks are capable of transforming
    photographs into artworks mimicking the style of famous painters or specific art
    movements, thereby bridging the gap between art and technology. They can also
    be used in healthcare for tasks like converting MRI images to CT scans or vice
    versa, which can be helpful in situations where one type of imaging is unavailable
    or too costly.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN由两个生成器和两个判别器组成。生成器将图像从一域转换到另一域，而判别器确定各自域中图像的真实性。这些网络能够将照片转换为模仿著名画家或特定艺术运动风格的美术作品，从而弥合艺术与技术之间的差距。它们还可以用于医疗保健领域，如将MRI图像转换为CT扫描或反之亦然，这在一种成像类型不可用或过于昂贵的情况下可能很有帮助。
- en: For our project in this chapter, we’ll convert between images with black hair
    and blond hair. We therefore use them as an example when explaining how CycleGAN
    works. Figure 6.1 is a diagram of the CycleGAN architecture.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的项目中，我们将转换黑白头发和金发图像。因此，我们使用它们作为例子来解释CycleGAN的工作原理。图6.1是CycleGAN架构的示意图。
- en: '![](../../OEBPS/Images/CH06_F01_Liu.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F01_Liu.png)'
- en: Figure 6.1 The architecture of a CycleGAN to convert images with black hair
    to ones with blond hair and to convert images with blond hair to ones with black
    hair. The diagram also outlines the training steps to minimize adversarial losses.
    How the model minimizes cycle consistency losses is explained in figure 6.2.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 CycleGAN的架构，用于将黑发图像转换为金发图像，以及将金发图像转换为黑发图像。该图还概述了用于最小化对抗损失的训练步骤。模型如何最小化循环一致性损失将在图6.2中解释。
- en: To train CycleGAN, we use unpaired datasets from the two domains we wish to
    translate between. We’ll use 48,472 celebrity face images with black hair and
    29,980 images with blond hair. We adjust the model parameters to minimize the
    sum of adversarial losses and cycle consistency losses. For ease of explanation,
    we’ll explain only adversarial losses in figure 6.1\. I’ll explain how the model
    minimizes cycle consistency losses in the next subsection.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练CycleGAN，我们使用来自我们希望转换的两个域的无配对数据集。我们将使用48,472张黑发名人脸图像和29,980张金发图像。我们调整模型参数以最小化对抗损失和循环一致性损失的总和。为了便于解释，我们将在图6.1中仅解释对抗损失。我将在下一小节中解释模型如何最小化循环一致性损失。
- en: In each iteration of training, we feed real black hair images (top left in figure
    6.1) to the blond hair generator to obtain fake blond hair images. We then feed
    the fake blond hair images, along with real blond hair images, to the blond hair
    discriminator (top middle). The blond hair discriminator produces a probability
    that each one is a real blond hair image. We then compare the predictions with
    the ground truth (whether an image is a true image with blond hair) and calculate
    the loss to the discriminator (`Loss_D_Blond`) as well as the loss to the generator
    (`Loss_G_Blond`).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次训练迭代中，我们将真实的黑发图像（图6.1顶部左侧）输入到金发生成器中，以获得假的金发图像。然后，我们将假的金发图像与真实的金发图像一起输入到金发判别器（顶部中间）。金发判别器产生一个概率，表明每个图像是真实的金发图像。然后，我们将预测与真实值（图像是否为真实金发图像）进行比较，并计算判别器损失（`Loss_D_Blond`）以及生成器损失（`Loss_G_Blond`）。
- en: At the same time, in each iteration of training, we feed real blond hair images
    (middle left) to the black hair generator (bottom left) to create fake black hair
    images. We present the fake black hair images, along with real ones, to the black
    hair discriminator (middle bottom) to obtain predictions that they are real. We
    compare the predictions from the black hair discriminator with the ground truth
    and calculate the loss to the discriminator (`Loss_D_Black`) and the loss to the
    generator (`Loss_G_Black`). We train the generators and discriminators simultaneously.
    To train the two discriminators, we adjust the model parameters to minimize the
    discriminator loss, which is the sum of `Loss_D_Black` and `Loss_D_Blond`.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在每次训练迭代中，我们将真实的金发图像（中间左侧）输入到黑发生成器（底部左侧）以创建假的黑发图像。我们将假的黑发图像与真实图像一起展示给黑发判别器（中间底部），以获取它们是真实的预测。我们比较黑发判别器的预测与真实值，并计算判别器损失（`Loss_D_Black`）和生成器损失（`Loss_G_Black`）。我们同时训练生成器和判别器。为了训练两个判别器，我们调整模型参数以最小化判别器损失，即`Loss_D_Black`和`Loss_D_Blond`的总和。
- en: 6.1.2 Cycle consistency loss
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 循环一致性损失
- en: To train the two generators, we adjust the model parameters to minimize the
    sum of the adversarial loss and cycle consistency loss. The adversarial loss is
    the sum of `Loss_G_Black` and `Loss_G_Blond` that we discussed in the previous
    subsection. To explain cycle consistency loss, let’s look at figure 6.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练两个生成器，我们调整模型参数以最小化对抗损失和循环一致性损失的总和。对抗损失是我们之前小节中讨论的`Loss_G_Black`和`Loss_G_Blond`的总和。为了解释循环一致性损失，让我们看看图6.2。
- en: '![](../../OEBPS/Images/CH06_F02_Liu.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2](../../OEBPS/Images/CH06_F02_Liu.png)'
- en: Figure 6.2 How CycleGAN minimizes cycle consistency losses between original
    black hair images and fake ones after round trips and cycle consistency losses
    between original blond hair images and fake ones after round trips
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 CycleGAN如何最小化原始黑发图像和经过往返后的假黑发图像之间的循环一致性损失，以及原始金发图像和经过往返后的假金发图像之间的循环一致性损失
- en: The loss function for the generators in CycleGAN consists of two parts. The
    first part, the adversarial loss, ensures that generated images are indistinguishable
    from real images in the target domain. For example, `Loss_G_Blond` (defined in
    the previous subsection) ensures that fake blond images produced by the blond
    hair generator resemble real images with blond hair in the training set. The second
    part, the cycle consistency loss, ensures that an image translated from one domain
    to another can be translated back to the original domain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN中生成器的损失函数由两部分组成。第一部分，对抗损失，确保生成的图像在目标域中与真实图像不可区分。例如，`Loss_G_Blond`（在之前的小节中定义）确保由金发生成器产生的假金发图像与训练集中真实金发图像相似。第二部分，循环一致性损失，确保从一个域转换到另一个域的图像可以转换回原始域。
- en: The cycle consistency loss is a crucial component of CycleGANs, ensuring that
    the original input image can be recovered after a round-trip translation. The
    idea is that if you translate a real black hair image (top left in figure 6.2)
    to a fake blond hair image and convert it back to a fake black hair image (top
    right), you should end up with an image close to the original black hair image.
    The cycle consistency loss for black hair images is the mean absolute error, at
    the pixel level, between the fake image and the original real one. Let’s call
    this loss `Loss_Cycle_Black`. The same applies to translating blond hair to black
    hair and then back to blond hair, and we call this loss `Loss_Cycle_Blond`. The
    total cycle consistency loss is the sum of `Loss_Cycle_Black` and `Loss_Cycle_Blond`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失是 CycleGANs 的关键组成部分，确保在经过一次往返翻译后可以恢复原始输入图像。其理念是，如果你将真实的黑色头发图像（图 6.2 的左上角）翻译成假的金色头发图像，并将其转换回假的黑头发图像（右上角），你应该得到一个接近原始黑色头发图像的图像。黑色头发图像的循环一致性损失是假图像与原始真实图像在像素级别的平均绝对误差。让我们称这个损失为
    `Loss_Cycle_Black`。同样的，将金色头发翻译成黑色头发然后再翻译回金色头发，我们称这个损失为 `Loss_Cycle_Blond`。总的循环一致性损失是
    `Loss_Cycle_Black` 和 `Loss_Cycle_Blond` 的总和。
- en: 6.2 The celebrity faces dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 明星面部数据集
- en: We’ll use celebrity face images with black hair and blond hair as the two domains.
    You’ll first download the data in this section. You’ll then process the images
    to get them ready for training later in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用黑色头发和金色头发的明星面部图像作为两个域。你将首先在本节中下载数据。然后，你将处理图片，以便在本章后面的训练中准备它们。
- en: 'You’ll use two new Python libraries in this chapter: `pandas` and `albumentations`.
    To install these libraries, execute the following line of code in a new cell in
    your Jupyter Notebook application on your computer:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用两个新的 Python 库：`pandas` 和 `albumentations`。为了安装这些库，请在你的计算机上的 Jupyter
    Notebook 应用程序的新单元格中执行以下代码行：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Follow the on-screen instructions to finish the installation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 按照屏幕上的说明完成安装。
- en: 6.2.1 Downloading the celebrity faces dataset
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 下载明星面部数据集
- en: To download the celebrity faces dataset, log into Kaggle and go to the link
    [https://mng.bz/Ompo](https://mng.bz/Ompo). Unzip the dataset after downloading
    and place all image files inside the folder /files/img_align_celeba/img_align_celeba/
    on your computer (note there is a subfolder with the same name in the folder itself).
    There are about 200,000 images in the folder. Also download the file `list_attr_celeba.csv`
    from Kaggle and place it in the /files/ folder on your computer. The CSV file
    specifies various attributes of each image.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载明星面部数据集，请登录 Kaggle 并访问链接 [https://mng.bz/Ompo](https://mng.bz/Ompo)。下载后解压数据集，并将所有图片文件放置在你的计算机上
    /files/img_align_celeba/img_align_celeba/ 文件夹内（注意文件夹内有一个同名子文件夹）。该文件夹中大约有 200,000
    张图片。同时从 Kaggle 下载文件 `list_attr_celeba.csv` 并将其放置在你的计算机上的 /files/ 文件夹中。CSV 文件指定了每张图片的各种属性。
- en: 'The celebrity faces dataset contains images with many different hair colors:
    brown, gray, black, blond, and so on. We’ll select images with black or blond
    hair as our training set because these two types are the most abundant in the
    celebrity faces dataset. Run the code in the following listing to select all images
    with black or blond hair.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 明星面部数据集包含许多不同颜色的头发图片：棕色、灰色、黑色、金色等等。我们将选择黑色或金色头发的图片作为我们的训练集，因为这两种类型在明星面部数据集中最为丰富。运行以下列表中的代码以选择所有黑色或金色头发的图片。
- en: Listing 6.1 Selecting images with black or blond hair
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.1 选择黑色或金色头发的图片
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Loads the CSV file that contains image attributes
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载包含图像属性的 CSV 文件
- en: ② Creates two folders to store images with black and blond hair
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建两个文件夹以存储黑色和金色头发的图片
- en: ③ If the attribute Black_Hair is 1, moves the image to the black folder.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果属性 Black_Hair 为 1，则将图片移动到黑色文件夹。
- en: ④ If the attribute Blond_Hair is 1, moves the image to the blond folder.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如果属性 Blond_Hair 为 1，则将图片移动到金色文件夹。
- en: We first use the `pandas` library to load the file `list_attr_celeba.csv` so
    that we know whether each image has black or blond hair in it. We then create
    two folders locally, /files/black/ and /files/blond/, to store images with black
    and blond hair, respectively. Listing 6.1 then iterates through all images in
    the dataset. If an image’s attribute `Black_Hair` is 1, we move it to the folder
    /files/black/; if an image’s attribute `Blond_Hair` is 1, we move it to the folder
    /files/blond/. You’ll see 48,472 images with black hair and 29,980 images with
    blond hair. Figure 6.3 shows some examples of the images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 `pandas` 库加载文件 `list_attr_celeba.csv`，以便我们知道每张图像中是否包含黑色或金色头发。然后我们在本地创建两个文件夹，/files/black/
    和 /files/blond/，分别存储黑色和金色头发的图像。列表 6.1 然后遍历数据集中的所有图像。如果图像的属性 `Black_Hair` 为 1，则将其移动到文件夹
    /files/black/；如果图像的属性 `Blond_Hair` 为 1，则将其移动到文件夹 /files/blond/。您将看到 48,472 张黑色头发图像和
    29,980 张金色头发图像。图 6.3 显示了一些图像示例。
- en: '![](../../OEBPS/Images/CH06_F03_Liu.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/CH06_F03_Liu.png)'
- en: Figure 6.3 Sample images of celebrity faces with black or blond hair
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 黑色或金色头发名人面部样本图像
- en: 'Images in the top row of figure 6.3 have black hair while images in the bottom
    row have blond hair. Further, the image quality is high: all faces are front and
    center, and hair colors are easy to identify. The quantity and quality of the
    training data will help the training of the CycleGAN model.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 顶部行的图像为黑色头发，而底部行的图像为金色头发。此外，图像质量很高：所有面部都位于前方中央，头发颜色易于识别。训练数据的数量和质量将有助于
    CycleGAN 模型的训练。
- en: 6.2.2 Process the black and blond hair image data
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 处理黑白头发图像数据
- en: We’ll generalize the CycleGAN model so that it can be trained on any dataset
    with two domains of images. We’ll also define a `LoadData()` class to process
    the training dataset for the CycleGAN model. The function can be applied to any
    dataset with two domains, whether human face images with different hair colors,
    images with or without eyeglasses, or images with summer and winter scenes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将泛化 CycleGAN 模型，使其能够训练任何具有两个图像域的数据集。我们还将定义一个 `LoadData()` 类来处理 CycleGAN 模型的训练数据集。该函数可以应用于任何具有两个域的数据集，无论是不同头发颜色的面部图像，还是带有或没有眼镜的图像，或者是夏季和冬季场景的图像。
- en: To that end, we have created a local module `ch06util`. Download the files `ch06util.py`
    and `__init__.py` from the book’s GitHub repository ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    and place them in the folder /utils/ on your computer. In the local module, we
    have defined the following `LoadData()` class.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们创建了一个本地模块 `ch06util`。从本书的 GitHub 仓库 ([https://github.com/markhliu/DGAI](https://github.com/markhliu/DGAI))
    下载文件 `ch06util.py` 和 `__init__.py`，并将它们放置在您的计算机上的 /utils/ 文件夹中。在本地模块中，我们定义了以下
    `LoadData()` 类。
- en: Listing 6.2 The `LoadData()` class to process the training data in CycleGAN
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.2 CycleGAN 中处理训练数据的 `LoadData()` 类
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① The two folders root_A and root_B are where the images in the two domains
    are stored
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ① 两个文件夹 root_A 和 root_B 是存储两个域中图像的位置
- en: ② Loads all images in each domain
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ② 加载每个域中的所有图像
- en: ③ Defines a method to count the length of the dataset
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义了一种计算数据集长度的方法
- en: ④ Defines a method to access individual elements in each domain
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 定义了一种访问每个域中各个元素的方法
- en: The `LoadData()` class is inherited from the `Dataset` class in PyTorch. The
    two lists `root_A` and `root_B` contain folders of images in domains A and B,
    respectively. The class loads up images in the two domains and produces a pair
    of images, one from domain A and one from domain B so that we can use the pair
    to train the CycleGAN model later.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadData()` 类继承自 PyTorch 中的 `Dataset` 类。两个列表 `root_A` 和 `root_B` 分别包含域 A 和
    B 中的图像文件夹。该类加载两个域中的图像，并生成一对图像，一个来自域 A，一个来自域 B，以便我们稍后可以使用这对图像来训练 CycleGAN 模型。'
- en: As we did in previous chapters, we create a data iterator with batches to improve
    computational efficiency, memory usage, and optimization dynamics in the training
    process.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，我们创建了一个具有批次的迭代器以改进计算效率、内存使用和训练过程中的优化动态。
- en: Listing 6.3 Processing the black and blond hair images for training
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.3 处理用于训练的黑白头发图像
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Resizes the images to 256 by 256 pixels
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将图像调整大小为 256x256 像素
- en: ② Normalizes the images to the range of -1 to 1
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将图像归一化到 -1 到 1 的范围
- en: ③ Applies the LoadData() class on the images
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在图像上应用 LoadData() 类
- en: ④ Creates a data iterator for training
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 创建用于训练的数据迭代器
- en: 'We first define an instance of the `Compose()` class in the `albumentations`
    library (which is famous for fast and flexible image augmentations) and call it
    `transforms`. The class transforms the images in several ways: it resizes images
    to 256 by 256 pixels and normalizes the values to the range –1 to 1\. The `HorizontalFlip()`
    argument in listing 6.3 creates a mirror image of the original image in the training
    set. Horizontal flipping is a simple yet powerful augmentation technique that
    enhances the diversity of training data, helping models generalize better and
    become more robust. The augmentations and increase in size boost the performance
    of the CycleGAN model and make the generated images realistic.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在 `albumentations` 库（以其快速和灵活的图像增强而闻名）中定义了一个 `Compose()` 类的实例，并将其命名为 `transforms`。该类以多种方式转换图像：它将图像调整大小到
    256x256 像素，并将值归一化到 -1 到 1 的范围内。列表 6.3 中的 `HorizontalFlip()` 参数在训练集中创建原始图像的镜像。水平翻转是一种简单而强大的增强技术，可以增强训练数据的多样性，帮助模型更好地泛化并变得更加鲁棒。增强和尺寸增加提高了
    CycleGAN 模型的性能，并使生成的图像更加逼真。
- en: We then apply the `LoadData()` class to the black and blond hair images. We
    set the batch size to 1 since the images have a large file size, and we use a
    pair of images to train the model in each iteration. Setting the batch size to
    more than 1 may result in your machine running out of memory.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将 `LoadData()` 类应用于黑发和金发图像。由于图像文件大小很大，我们将批大小设置为 1，并在每次迭代中使用一对图像来训练模型。将批大小设置为超过
    1 可能会导致您的机器内存不足。
- en: 6.3 Building a CycleGAN model
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 构建 CycleGAN 模型
- en: We’ll build a CycleGAN model from scratch in this section. We’ll take great
    care to make our CycleGAN model general so that it can be trained using any dataset
    with two domains of images. As a result, we’ll use A and B to denote the two domains
    instead of, for example, black and blond hair images. As an exercise, you’ll train
    the same CycleGAN model by using the eyeglasses dataset that you used in chapter
    5\. This helps you apply the skills you learned in this chapter to other real-world
    applications by using a different dataset.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从头开始构建 CycleGAN 模型。我们将非常小心地使我们的 CycleGAN 模型通用，以便可以使用具有两个图像域的任何数据集进行训练。因此，我们将使用
    A 和 B 来表示两个域，而不是例如黑发和金发图像。作为一个练习，你将使用第 5 章中使用的眼镜数据集来训练相同的 CycleGAN 模型。这有助于你通过使用不同的数据集将本章学到的技能应用于其他实际应用。
- en: 6.3.1 Creating two discriminators
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 创建两个判别器
- en: 'Even though CycleGAN has two discriminators, they are identical ex ante. Therefore,
    we’ll create one single `Discriminator()` class and then instantiate the class
    twice: one instance is discriminator A and the other discriminator B. The two
    domains in CycleGAN are symmetric, and it doesn’t matter which domain we call
    domain A: images with black hair or images with blond hair.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 CycleGAN 有两个判别器，它们在事前是相同的。因此，我们将创建一个单一的 `Discriminator()` 类，然后实例化该类两次：一个实例是判别器
    A，另一个是判别器 B。CycleGAN 中的两个域是对称的，我们称之为域 A：黑发图像或金发图像都无关紧要。
- en: Open the file `ch06util.py` you just downloaded. In it, I have defined the `Discriminator()`
    class.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你刚刚下载的文件 `ch06util.py`。在其中，我定义了 `Discriminator()` 类。
- en: Listing 6.4 Defining the `Discriminator()` class in CycleGAN
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.4 在 CycleGAN 中定义 `Discriminator()` 类
- en: '[PRE4]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The first Conv2d layer has 3 input channels and 64 output channels.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ① 第一个 Conv2d 层有 3 个输入通道和 64 个输出通道。
- en: ② Three more Conv2d layers with 126, 256, and 512 output channels, respectively
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ② 另外三个 Conv2d 层，分别有 126、256 和 512 个输出通道
- en: ③ The last Conv2d layer has 512 input channels and 1 output channel.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 最后一个 Conv2d 层有 512 个输入通道和 1 个输出通道。
- en: ④ Applies the sigmoid activation function on the output so it can be interpreted
    as a probability
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在输出上应用 sigmoid 激活函数，以便它可以被解释为概率
- en: The previous code listing defines the discriminator network. The architecture
    is similar to the discriminator network in chapter 4 and the critic network in
    chapter 5\. The main components are five `Conv2d` layers. We apply the sigmoid
    activation function on the last layer because the discriminator performs a binary
    classification problem. The discriminator takes a three-channel color image as
    input and produces a single number between 0 and 1, which can be interpreted as
    the probability that the input image is a real image in the domain.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码列表定义了判别器网络。其架构与第4章中的判别器网络和第5章中的评论网络相似。主要组件是五个`Conv2d`层。我们在最后一层应用sigmoid激活函数，因为判别器执行的是二元分类问题。判别器以三通道彩色图像为输入，并产生一个介于0到1之间的单个数字，这可以解释为输入图像是域中真实图像的概率。
- en: The `padding_mode="reflect"` argument we used in listing 6.4 means the padding
    added to the input tensor is a reflection of the input tensor itself. Reflect
    padding helps in preserving the edge information by not introducing artificial
    zero values at the borders. It creates smoother transitions at the boundaries
    of the input tensor, which is beneficial for differentiating images in different
    domains in our setting.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在列表6.4中使用的`padding_mode="reflect"`参数意味着添加到输入张量中的填充是输入张量本身的反射。反射填充通过不在边界引入人工零值来帮助保留边缘信息。它在输入张量的边界处创建更平滑的过渡，这对我们在设置中区分不同域中的图像是有益的。
- en: 'We then create two instances of the class and call them `disc_A` and `disc_B`,
    respectively:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建了两个类的实例，分别命名为`disc_A`和`disc_B`：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Imports the Discriminator class from the local module
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从本地模块导入判别器类
- en: ② Creates two instances of the Discriminator class
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建判别器类的两个实例
- en: ③ Initializes weights
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化权重
- en: In the local module `ch06util`, we also defined a `weights_init()` function
    to initialize model weights. The function is defined similarly to the one in chapter
    5\. We then initialize weights in the two newly created discriminators, `disc_A`
    and `disc_B`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地模块`ch06util`中，我们还定义了一个`weights_init()`函数来初始化模型权重。该函数的定义方式与第5章中的类似。然后我们在两个新创建的判别器`disc_A`和`disc_B`中初始化权重。
- en: Now that we have two discriminators, we’ll create two generators next.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个判别器，接下来我们将创建两个生成器。
- en: 6.3.2 Creating two generators
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 创建两个生成器
- en: 'Similarly, we define a single `Generator()` class in the local module and instantiate
    the class twice: one instance is generator A, and the other is generator B. In
    the file `ch06util.py` you just downloaded, we have defined the `Generator()`
    class.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们在本地模块中定义了一个单独的`Generator()`类，并实例化了该类两次：一个实例是生成器A，另一个是生成器B。在您刚刚下载的`ch06util.py`文件中，我们定义了`Generator()`类。
- en: Listing 6.5 The `Generator()` class in CycleGAN
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.5 CycleGAN中的`Generator()`类
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Three Conv2d layers
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ① 三个`Conv2d`层
- en: ② Nine residual blocks
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ② 九个残差块
- en: ③ Two upsampling blocks
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 两个上采样块
- en: ④ Applies tanh activation on the output
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在输出上应用tanh激活
- en: The generator network consists of several `Conv2d` layers, followed by nine
    residual blocks (which I’ll explain in detail later). After that, the network
    has two upsampling blocks that consist of a `ConvTranspose2d` layer, an `InstanceNorm2d`
    layer, and a `ReLU` activation. As we have done in previous chapters, we use the
    tanh activation function at the output layer, so the output pixels are all in
    the range of –1 to 1, the same as the images in the training set.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器网络由几个`Conv2d`层组成，后面跟着九个残差块（我将在后面详细解释）。然后，网络有两个上采样块，包括一个`ConvTranspose2d`层、一个`InstanceNorm2d`层和一个`ReLU`激活。正如我们在前面的章节中所做的那样，我们在输出层使用tanh激活函数，因此输出像素都在-1到1的范围内，与训练集中的图像相同。
- en: 'The residual block in the generator is defined in the local module as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器中的残差块在本地模块中的定义如下：
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A residual connection is a concept in deep learning, particularly in the design
    of deep neural networks. You’ll see it quite often later in this book. It’s a
    technique used to address the problem of vanishing gradients, which often occurs
    in very deep networks. In a residual block, which is the basic unit of a network
    with residual connections, the input is passed through a series of transformations
    (like convolution, activation, and batch or instance normalization) and then added
    back to the output of these transformations. Figure 6.4 provides a diagram of
    the architecture of the residual block defined previously.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接是深度学习中的一个概念，尤其是在深度神经网络的设计中。你会在本书后面的内容中经常看到它。这是一种用于解决深度网络中经常出现的梯度消失问题的技术。在残差块中，它是具有残差连接的网络的基本单元，输入通过一系列变换（如卷积、激活和批量或实例归一化）传递，然后将其加回到这些变换的输出。图
    6.4 展示了之前定义的残差块的架构。
- en: '![](../../OEBPS/Images/CH06_F04_Liu.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F04_Liu.png)'
- en: Figure 6.4 The architecture of a residual block. The input x is passed through
    a series of transformations (two sets of Conv2d layer and InstanceNorm2d layer
    and a ReLU activation). The input x is then added back to the output of these
    transformations, f(x). The output of the residual block is therefore x + f(x).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 残差块的架构。输入 x 通过一系列变换（两套 Conv2d 层和 InstanceNorm2d 层以及中间的 ReLU 激活）。然后输入 x
    被加回到这些变换的输出 f(x) 上。因此，残差块的输出是 x + f(x)。
- en: The transformations in each residual block are different. In this example, the
    input x is passed through two sets of `Conv2d` layer and `InstanceNorm2d` layer
    and a ReLU activation in between. The input x is then added back to the output
    of these transformations, f(x), to form the final output, x+f(x)—hence the name
    residual connection.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 每个残差块中的变换是不同的。在这个例子中，输入 x 通过两套 `Conv2d` 层和 `InstanceNorm2d` 层以及中间的 ReLU 激活。然后输入
    x 被加回到这些变换的输出 f(x) 上，形成最终的输出 x+f(x)，因此得名残差连接。
- en: 'Next, we create two instances of the `Generator()` class and call one of them
    `gen_A` and the other `gen_B`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建两个 `Generator()` 类的实例，并将其中一个命名为 `gen_A`，另一个命名为 `gen_B`：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When training the model, we’ll use the mean absolute error (i.e., L1 loss)
    to measure the cycle consistency loss. We’ll use the mean squared error (i.e.,
    L2 loss) to gauge the adversarial loss. L1 loss is often used if the data are
    noisy and have many outliers since it punishes extreme values less than the L2
    loss. Therefore, we import the following loss functions:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们将使用平均绝对误差（即 L1 损失）来衡量循环一致性损失。我们将使用平均平方误差（即 L2 损失）来衡量对抗损失。当数据有噪声且有许多异常值时，通常使用
    L1 损失，因为它对极端值的惩罚小于 L2 损失。因此，我们导入以下损失函数：
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Both L1 and L2 losses are calculated at the pixel level. The original image
    has a shape of (3, 256, 256) and so is the fake image. To calculate the losses,
    we first calculate the difference (absolute value of this difference for L1 loss
    and the squared value of this difference for L2 loss) between the corresponding
    pixel values between two images at each of the 3 × 256 × 256 = 196608 positions
    and average them over the positions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: L1 和 L2 损失都是在像素级别计算的。原始图像和伪造图像的形状都是 (3, 256, 256)。为了计算损失，我们首先计算两个图像在每个 3 × 256
    × 256 = 196608 个位置上对应像素值之间的差异（对于 L1 损失是差异的绝对值，对于 L2 损失是差异的平方值），然后对这些位置的差异进行平均。
- en: We’ll use PyTorch’s automatic mixed precision package `torch.cuda.amp` to speed
    up training. The default data type in PyTorch tensors is `float32`, a 32-bit floating-point
    number, which takes up twice as much memory as a 16-bit floating number, `float16`.
    Operations on the former are slower than those on the latter. There is a trade-off
    between precision and computational costs. Which data type to use depends on the
    task at hand. `torch.cuda.amp` provides an automatic mixed precision, where some
    operations use `float32` and others `float16`. Mixed precision tries to match
    each operation to its appropriate data type to speed up training.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 PyTorch 的自动混合精度包 `torch.cuda.amp` 来加速训练。PyTorch 张量的默认数据类型是 `float32`，这是一个
    32 位的浮点数，它占用的内存是 16 位浮点数 `float16` 的两倍。对前者的操作比后者慢。精度和计算成本之间存在权衡。使用哪种数据类型取决于手头的任务。`torch.cuda.amp`
    提供了自动混合精度，其中一些操作使用 `float32`，而其他操作使用 `float16`。混合精度试图将每个操作匹配到适当的数据类型以加速训练。
- en: 'As we have done in chapter 4, we’ll use the Adam optimizer for both the discriminators
    and the generators:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第 4 章中所做的那样，我们将使用 Adam 优化器对判别器和生成器进行优化：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Next, we’ll train the CycleGAN model by using images with black or blond hair.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用带有黑发或金发的图像来训练CycleGAN模型。
- en: 6.4 Using CycleGAN to translate between black and blond hair
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 使用CycleGAN在黑发和金发之间进行转换
- en: Now that we have the training data and the CycleGAN model, we’ll train the model
    by using images with black or blond hair. As with all GAN models, we’ll discard
    the discriminators after training. We’ll use the two trained generators to convert
    black hair images to blond hair ones and convert blond hair images to black hair
    ones.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了训练数据和CycleGAN模型，我们将使用带有黑发或金发的图像来训练模型。与所有GAN模型一样，训练完成后我们将丢弃判别器。我们将使用两个训练好的生成器将黑发图像转换为金发图像，并将金发图像转换为黑发图像。
- en: 6.4.1 Training a CycleGAN to translate between black and blond hair
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 训练CycleGAN在黑发和金发之间进行转换
- en: 'As we explained in chapter 4, we’ll use visual inspections to determine when
    to stop training. To that end, we create a function to test what the real images
    look like and what the corresponding generated images look like so that we can
    compare the two to visually inspect the effectiveness of the model. In the local
    module `ch06util`, we define a `test()` function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第4章中解释的，我们将使用视觉检查来确定何时停止训练。为此，我们创建了一个函数来测试真实图像和相应生成的图像看起来像什么，这样我们就可以比较两者以视觉检查模型的有效性。在本地模块`ch06util`中，我们定义了一个`test()`函数：
- en: '[PRE11]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Real images in domains A and B, saved in a local folder
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在本地文件夹中保存的域A和B中的真实图像
- en: ② The corresponding fake images in domains A and B, created by the generators
    in batch i
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ② 由第i批生成器在域A和B中创建的对应伪造图像
- en: We save four images after every 100 batches of training. We save real images
    and the corresponding fake images in the two domains in the local folder so we
    can periodically check the generated images and compare them with the real ones
    to assess the progress of training. We made the function general so that it can
    be applied to images from any two domains.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每100个批次训练后保存四张图像。我们在本地文件夹中保存两个域中的真实图像和相应伪造图像，这样我们可以定期检查生成的图像，并将它们与真实图像进行比较，以评估训练进度。我们使该函数通用，以便它可以应用于任何两个域的图像。
- en: Further, we define a `train_epoch()` function in the local module `ch06util`
    to train the discriminators and the generators for an epoch. The following listing
    highlights the code we use to train the two discriminators.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在本地模块`ch06util`中定义了一个`train_epoch()`函数，用于在一个epoch中训练判别器和生成器。以下列表突出了我们用来训练两个判别器的代码。
- en: Listing 6.6 Training the two discriminators in CycleGAN
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.6 在CycleGAN中训练两个判别器
- en: '[PRE12]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Iterates through all pairs of images in the two domains
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 遍历两个域中所有图像的对
- en: ② Uses PyTorch automatic mixed precision package to speed up training
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用PyTorch自动混合精度包加速训练
- en: ③ The total loss for the two discriminators is the simple average of the adversarial
    losses to the two discriminators.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 两个判别器的总损失是两个判别器对抗损失的简单平均值。
- en: 'We use the `detach()` method here to remove gradients in tensors `fake_A` and
    `fake_B` to reduce memory and speed up computations. The training for the two
    discriminators is similar to what we have done in chapter 4, with a couple of
    differences. First, instead of having just one discriminator, we have two discriminators
    here: one for images in domain A and one for images in domain B. The total loss
    for the two discriminators is the simple average of the adversarial losses of
    the two discriminators. Second, we use the PyTorch automatic mixed precision package
    to speed up training, reducing the training time by more than 50%.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用`detach()`方法来移除张量`fake_A`和`fake_B`中的梯度，以减少内存并加快计算速度。两个判别器的训练与我们在第4章中做的是类似的，有一些不同之处。首先，我们这里有两个判别器，而不是一个：一个用于域A中的图像，另一个用于域B中的图像。两个判别器的总损失是两个判别器对抗损失的简单平均值。其次，我们使用PyTorch自动混合精度包来加速训练，将训练时间减少了50%以上。
- en: We simultaneously train the two generators in the same iteration. The following
    listing highlights the code we use to train the two generators.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在同一迭代中同时训练两个生成器。以下列表突出了我们用来训练两个生成器的代码。
- en: Listing 6.7 Training the two generators in CycleGAN
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.7 在CycleGAN中训练两个生成器
- en: '[PRE13]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Adversarial losses to the two generators
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对两个生成器的对抗损失
- en: ② Cycle consistency losses for the two generators
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ② 两个生成器的循环一致性损失
- en: ③ The total loss for the two generators is the weighted sum of adversarial losses
    and cycle consistency losses.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 两个生成器的总损失是对抗损失和循环一致性损失的加权总和。
- en: ④ Generates images for visual inspection after every 100 batches of training
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 每训练100个批次后生成用于视觉检查的图像
- en: The training for the two generators is different from what we have done in chapter
    4 in two important ways. First, instead of having just one generator, we train
    two generators simultaneously here. Second, the total loss for the two generators
    is the weighted sum of adversarial losses and cycle consistency losses, and we
    weigh the latter 10 times more than the former loss. However, if you change the
    value 10 to other numbers such as 9 or 12, you’ll get similar results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于两个生成器的训练，与我们在第4章所做的方法有两个重要的不同之处。首先，我们在这里同时训练两个生成器，而不是只有一个生成器。其次，两个生成器的总损失是对抗性损失和循环一致性损失的加权总和，我们将后者损失权重提高10倍。然而，如果您将10的值更改为其他数字，如9或12，您将得到类似的结果。
- en: The cycle consistency loss is the mean absolute error between the original image
    and the fake image that’s translated back to the original domain.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 循环一致性损失是原始图像与转换回原始域的伪造图像之间的平均绝对误差。
- en: 'Now that we have everything ready, we’ll start the training loop:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有东西，我们将开始训练循环：
- en: '[PRE14]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Trains the CycleGAN for one epoch using the black and blond hair images
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用黑发和金发图像训练CycleGAN一个epoch
- en: ② Saves the trained model weights
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ② 保存训练好的模型权重
- en: 'The preceding training takes a couple of hours if you use GPU training. It
    may take a whole day otherwise. If you don’t have the computing resources to train
    the model, download the pretrained generators from my website: [https://gattonweb.uky.edu/faculty/lium/ml/hair.zip](https://gattonweb.uky.edu/faculty/lium/ml/hair.zip).
    Unzip the file and place the files `gen_black.pth` and `gen_blond.pth` in the
    folder /files/ on your computer. You’ll be able to convert between black hair
    images and blond hair ones in the next subsection.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用GPU进行训练，之前的训练可能需要几个小时。否则，可能需要整整一天。如果您没有训练模型的计算资源，请从我的网站下载预训练的生成器：[https://gattonweb.uky.edu/faculty/lium/ml/hair.zip](https://gattonweb.uky.edu/faculty/lium/ml/hair.zip)。解压文件，并将文件`gen_black.pth`和`gen_blond.pth`放置在您计算机上的文件夹/files/中。您将在下一小节中能够将黑发图像转换为金发图像。
- en: Exercise 6.1
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.1
- en: When training the CycleGAN model, we assume that domain A contains images with
    black hair and domain B contains images with blond hair. Modify the code in listing
    6.2 so that domain A contains images with blond hair and domain B contains images
    with black hair.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练CycleGAN模型时，我们假设域A包含黑发图像，域B包含金发图像。修改列表6.2中的代码，使域A包含金发图像，域B包含黑发图像。
- en: 6.4.2 Round-trip conversions of black hair images and blond hair images
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 黑发图像和金发图像的往返转换
- en: Due to the high quality and the abundant quantity of the training dataset, we
    have trained the CycleGAN with great success. We’ll not only convert between images
    with black hair and images with blond hair, but we’ll also conduct round-trip
    conversions. For example, we’ll convert images with black hair to images with
    blond hair and then convert them back to images with black hair. That way, we
    can compare the original images with the generated images in the same domain after
    a round trip and see the difference.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据集的高质量和数量丰富，我们成功地训练了CycleGAN。我们不仅将在黑发图像和金发图像之间进行转换，还将进行往返转换。例如，我们将黑发图像转换为金发图像，然后再将其转换回黑发图像。这样，我们可以在往返后比较同一域内的原始图像和生成的图像，并查看差异。
- en: The following listing performs conversions of images between the two domains
    as well as round-trip conversions of images in each domain.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表执行了图像在两个域之间的转换，以及每个域内图像的往返转换。
- en: Listing 6.8 Round-trip conversions of images with black or blond hair
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.8 黑色或金发图像的往返转换
- en: '[PRE15]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Original image with black hair
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ① 原始黑发图像
- en: ② A fake image with black hair after a round trip
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ② 经过一次往返后生成的黑发伪造图像
- en: ③ Original image with blond hair
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 原始金发图像
- en: ④ A fake image with blond hair after a round trip
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 经过一次往返后生成的金发伪造图像
- en: 'We have saved six sets of images in your local folder /files/. The first set
    is the original images with black hair. The second set is the fake blond images
    produced by the trained blond hair generator: the images are saved as `fakeblond0.png`,
    `fakeblond1.png`, and so on. The third set is the fake images with black hair
    after a round trip: we feed the fake images we just created to the trained black
    hair generator to obtain fake images with black hair. They are saved as `fake2black0.png`,
    `fake2black1.png`, and so on. Figure 6.5 shows the three sets of images.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在您的本地文件夹/files/中保存了六组图像。第一组是带有黑色头发的原始图像。第二组是由训练好的金色头发生成器生成的假金色图像：图像保存为`fakeblond0.png`、`fakeblond1.png`等。第三组是往返转换后的假黑色头发图像：我们将刚刚创建的假图像输入到训练好的黑色头发生成器中，以获得假黑色头发图像。它们保存为`fake2black0.png`、`fake2black1.png`等。图6.5显示了这三组图像。
- en: '![](../../OEBPS/Images/CH06_F05_Liu.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F05_Liu.png)'
- en: 'Figure 6.5 A round-trip conversion of images with black hair. Images in the
    top row are the original images with black hair from the training set. Images
    in the middle row are the corresponding fake images with blond hair, produced
    by the trained blond hair generator. Images in the bottom row are fake images
    with black hair after a round trip: we feed the images in the middle row to the
    trained black hair generator to create fake images with black hair.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5展示了带有黑色头发的图像往返转换。顶部行的图像是从训练集中提取的带有黑色头发的原始图像。中间行的图像是由训练好的金色头发生成器生成的相应假发金色头发图像。底部行的图像是在往返转换后生成的假发黑色头发图像：我们将中间行的图像输入到训练好的黑色头发生成器中，以创建假发黑色头发图像。
- en: 'There are three rows of images in figure 6.5\. The top row displays original
    images with black hair from the training set. The middle row displays fake blond
    hair images produced by the trained blond hair. The bottom row contains fake black
    hair images after a round-trip conversion: the images look almost identical to
    the ones in the top row! Our trained CycleGAN model works extremely well.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5中有三行图像。顶部行显示了从训练集中提取的带有黑色头发的原始图像。中间行显示了由训练好的金色头发生成的假发金色头发图像。底部行包含往返转换后的假发黑色头发图像：图像几乎与顶部行中的图像相同！我们训练好的CycleGAN模型工作得非常好。
- en: The fourth set of images in the local folder /files/ are the original images
    with blond hair. The fifth set is the fake image produced by the trained black
    hair generator. Finally, the sixth set contains fake images with blond hair after
    a round trip. Figure 6.6 compares these three sets of images.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 本地文件夹/files/中的第四组图像是带有金色头发的原始图像。第五组是由训练好的黑色头发生成器生成的假图像。最后，第六组包含往返转换后的假发金色头发图像。图6.6比较了这三组图像。
- en: '![](../../OEBPS/Images/CH06_F06_Liu.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH06_F06_Liu.png)'
- en: 'Figure 6.6 A round-trip conversion of images with blond hair. Images in the
    top row are the original images with blond hair from the training set. Images
    in the middle row are the corresponding fake images with black hair, produced
    by the trained black hair generator. Images in the bottom row are fake images
    with blond hair after a round-trip conversion: we feed the images in the middle
    row to the trained blond hair generator to create fake images with blond hair.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6展示了带有金色头发的图像往返转换。顶部行的图像是从训练集中提取的带有金色头发的原始图像。中间行的图像是由训练好的黑色头发生成器生成的相应假发黑色头发图像。底部行的图像是在往返转换后生成的假发金色头发图像：我们将中间行的图像输入到训练好的金色头发生成器中，以创建假发金色头发图像。
- en: 'In figure 6.6, fake black hair images produced by the trained black hair generator
    are shown in the middle row: they have black hair on the same human faces as the
    top row. Fake blond hair images after a round trip are shown in the bottom row:
    they look almost identical to the original blond hair images in the top row.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.6中，中间行显示了由训练好的黑色头发生成器生成的假发黑色头发图像：它们与顶部行中的人类面部上的黑色头发相同。底部行显示了往返转换后的假发金色头发图像：它们几乎与顶部行中的原始金色头发图像完全相同。
- en: Exercise 6.2
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.2
- en: The CycleGAN model is general and can be applied to any training dataset with
    two domains of images. Train the CycleGAN model using the eyeglasses images that
    you downloaded in chapter 5\. Use images with glasses as domain A and images without
    glasses as domain B. Then use the trained CycleGAN to add and remove eyeglasses
    from images (i.e., translating images between the two domains). An example implementation
    and results are in the book’s GitHub repository.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN模型是通用的，可以应用于任何包含两个图像域的训练数据集。使用你在第5章下载的眼镜图像来训练CycleGAN模型。将戴眼镜的图像作为域A，不带眼镜的图像作为域B。然后使用训练好的CycleGAN在图像中添加和去除眼镜（即，在两个域之间转换图像）。示例实现和结果可以在本书的GitHub仓库中找到。
- en: So far, we have focused on one type of generative model, GANs. In the next chapter,
    you’ll learn to use another type of generative model, variational autoencoders
    (VAEs), to generate high-resolution images. You’ll learn the advantages and disadvantages
    of VAEs compared to GANs. More importantly, you’ll learn the encoder-decoder architecture
    in VAEs. The architecture is widely used in generative models, including Transformers,
    which we’ll study later in the book.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们关注了一种生成模型类型，即GANs。在下一章中，你将学习如何使用另一种类型的生成模型，变分自编码器（VAEs），来生成高分辨率图像。你将了解VAEs相对于GANs的优点和缺点。更重要的是，你将学习VAE中的编码器-解码器架构。该架构在生成模型中广泛使用，包括我们将在本书后面学习的Transformers。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: CycleGAN can translate images between two domains without paired examples. It
    consists of two discriminators and two generators. One generator converts images
    in domain A to domain B while the other generator converts images in domain B
    to domain A. The two discriminators classify if a given image is from a specific
    domain.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN可以在没有配对示例的情况下在两个域之间翻译图像。它由两个判别器和两个生成器组成。一个生成器将域A中的图像转换为域B，而另一个生成器将域B中的图像转换为域A。两个判别器用于分类给定图像是否来自特定域。
- en: CycleGAN uses a cycle consistency loss function to ensure the original image
    can be reconstructed from the transformed image, encouraging the preservation
    of key features.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN使用循环一致性损失函数来确保可以从转换后的图像重建原始图像，从而鼓励保留关键特征。
- en: A properly constructed CycleGAN model can be applied to any dataset with images
    from two domains. The same model can be trained with different datasets and be
    used to translate images in different domains.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个正确构建的CycleGAN模型可以应用于任何包含两个域图像的数据集。相同的模型可以用不同的数据集进行训练，并用于在不同域之间翻译图像。
- en: When we have abundant high-quality training data, the trained CycleGAN can convert
    images in one domain to another and convert them back to the original domain.
    The images after a round-trip conversion can potentially look almost identical
    to the original images.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有大量高质量的训练数据时，训练好的CycleGAN可以将一个域的图像转换为另一个域，并将其转换回原始域。经过一次往返转换后的图像可能几乎与原始图像相同。
- en: '* * *'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^([1](#footnote-000-backlink))  Jun-Yan Zhu, Taesung Park, Phillip Isola, and
    Alexie Efros, 2017, “Unpaired Image-to-Image Translation Using Cycle Consistent
    Adversarial Networks.” [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](#footnote-000-backlink))  朱俊彦，朴泰勋，菲利普·伊索拉，亚历克斯·埃弗罗斯，2017，“使用循环一致性对抗网络进行未配对图像到图像的翻译。”
    [https://arxiv.org/abs/1703.10593](https://arxiv.org/abs/1703.10593)。
