- en: 4 Graph attention networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 图注意力网络
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding attention and how it’s applied to graph attention networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解注意力及其在图注意力网络中的应用
- en: Knowing when to use GAT and GATv2 layers in PyTorch Geometric
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解何时在PyTorch Geometric中使用GAT和GATv2层
- en: Using mini-batching via the `NeighborLoader` class
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过`NeighborLoader`类使用小批量处理
- en: Implementing and applying graph attention networks layers in a spam detection
    problem
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在垃圾邮件检测问题中实现和应用图注意力网络层
- en: In this chapter, we extend our discussion of convolutional graph neural network
    (convolutional GNN) architectures by looking at a special variant of such models,
    the graph attention network (GAT). While these GNNs use convolution as introduced
    in the previous chapter, they extend this idea with an *attention mechanism* to
    highlight important nodes in the learning process [1, 2]. In contrast to the conventional
    convolutional GNN, which weights all nodes equally, the attention mechanism allows
    the GAT to learn what aspects in its training to put extra emphasis on.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过查看这种模型的特殊变体，即图注意力网络（GAT），扩展了我们关于卷积图神经网络（卷积GNN）架构的讨论。虽然这些GNNs使用的是前一章中介绍的卷积，但它们通过引入*注意力机制*来扩展这一想法，以突出学习过程中的重要节点[1,
    2]。与传统的卷积GNN不同，它对所有节点给予相同的权重，注意力机制允许GAT学习在训练中额外强调哪些方面。
- en: As with convolution, *attention* is a widely used mechanism in deep learning
    outside of GNNs. Architectures that rely on attention (particularly *transformers*)
    have seen such success in addressing natural language problems that they now dominate
    the field. It remains to be seen if attention will have a similar effect in the
    graph world.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积一样，*注意力*是深度学习（除了GNNs之外）中广泛使用的机制。依赖于注意力的架构（尤其是*变换器*）在解决自然语言问题方面取得了如此大的成功，以至于它们现在主导了该领域。在图世界中，注意力是否会产生类似的影响还有待观察。
- en: GATs shine when dealing with domains where some nodes have more importance than
    the graph structure suggests. Sometimes in a graph, there can be a single high-degree
    node that has an outsized importance on the rest of the graph, and the vanilla
    message passing (covered in the previous chapter) will likely capture its significance
    thanks to the node’s many neighbors. However, sometimes a node can have a large
    effect despite having a similar degree to other nodes. Some examples include social
    networks, where some members of a network have more influence on generating or
    spreading information and news; fraud detection, where a small set of actors and
    transactions drive deception; and anomaly detection, where a small subset of people,
    behaviors, or events will fall outside the norm [3–5]. GATs are especially well
    suited to these types of problems.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理某些节点比图结构显示的重要性更大时，GATs表现出色。有时在图中，可能存在一个高度节点，其对整个图的重要性超出了其度数，而传统的消息传递（在前一章中介绍）可能会因为节点的许多邻居而捕捉到其重要性。然而，有时一个节点即使与其他节点的度数相似，也可能产生很大的影响。一些例子包括社交网络，其中网络的一些成员对生成或传播信息和新闻有更大的影响力；欺诈检测，其中一小部分行为者和交易推动了欺骗；以及异常检测，其中一小部分人、行为或事件将超出常规[3–5]。GATs特别适合这类问题。
- en: In this chapter, we’ll apply GATs to the domain of fraud detection. In our problem,
    we detect fake customer reviews from the Yelp website. For this, we use a network
    of user reviews derived from a dataset that contains Yelp reviews for hotels and
    restaurants in the Chicago area [6, 7].
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将应用GATs到欺诈检测领域。在我们的问题中，我们检测Yelp网站上的虚假客户评论。为此，我们使用一个由包含芝加哥地区酒店和餐厅Yelp评论的数据集派生出的用户评论网络[6,
    7]。
- en: After an introduction to the problem and the dataset, we first train a baseline
    model without the graph structure before applying two versions of the GAT model
    to the problem. At the end, we discuss class imbalance and some ways to address
    this.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍问题和数据集之后，我们首先在没有图结构的情况下训练一个基线模型，然后再应用两种版本的GAT模型来解决这个问题。最后，我们讨论类别不平衡以及一些解决方法。
- en: Code snippets will be used to explain the process, but the majority of code
    and annotation can be found in the repository. As with previous chapters, we provide
    a deeper dive into the theory in section 4.5 at the end of the chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用代码片段来解释过程，但大部分代码和注释可以在存储库中找到。与前面的章节一样，我们在章节末尾的4.5节提供了对理论的深入探讨。
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/JYoP](https://mng.bz/JYoP)). Colab links and data from this chapter
    can be accessed in the same location.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章的代码以笔记本形式存储在GitHub仓库中（[https://mng.bz/JYoP](https://mng.bz/JYoP)）。本章的Colab链接和数据可以在同一位置访问。
- en: 4.1 Detecting spam and fraudulent reviews
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 检测垃圾邮件和欺诈性评论
- en: On consumer-oriented websites and e-commerce platforms such as Yelp, Amazon,
    and Google Business Reviews, it’s common for user-generated reviews and ratings
    to accompany the presentation and description of a product or a service. In the
    United States, more than 90% of adults trust and rely on these reviews and ratings
    when making a purchase decision [3]. At the same time, many of these reviews are
    fake. Capital One estimated that 30% of online reviews weren’t real in 2024 [5].
    In this chapter, we’re going to be training our model to detect fake reviews.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在以消费者为导向的网站和电子商务平台，如Yelp、Amazon和Google商业评论中，用户生成的评论和评分通常伴随着产品或服务的展示和描述。在美国，超过90%的成年人信任并依赖这些评论和评分来做出购买决策[3]。同时，许多这些评论都是虚假的。Capital
    One估计，到2024年，30%的在线评论不是真实的[5]。在本章中，我们将训练我们的模型来检测虚假评论。
- en: Spam or fraudulent review detection has been a well-trodden area in machine
    learning and natural language processing (NLP). As such, several datasets from
    primary consumer sites and platforms are available. In this chapter, we’re going
    to use review data from Yelp.com, a platform of user reviews and ratings that
    focuses on consumer services. On Yelp.com, users can look up local businesses
    in their proximity and browse basic information about the business and written
    feedback from users. Yelp uses internally developed tools and models to filter
    reviews based on their trustworthiness. The process we’ll use to approach the
    problem is shown in figure 4.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾邮件或欺诈性评论检测一直是机器学习和自然语言处理（NLP）中的一个热门领域。因此，从主要消费者网站和平台中可以找到几个数据集。在本章中，我们将使用Yelp.com的评论数据，这是一个专注于消费者服务的用户评论和评分平台。在Yelp.com上，用户可以查找他们附近的本地企业，并浏览有关企业及其用户书面反馈的基本信息。Yelp使用内部开发的工具和模型根据其可信度过滤评论。我们将使用图4.1所示的过程来处理这个问题。
- en: '![figure](../Images/4-1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-1.png)'
- en: Figure 4.1 We’ll tackle the fraudulent user review classification problem using
    both non-graph and graph data.
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1 我们将使用非图和图数据来解决欺诈性用户评论分类问题。
- en: 'First, we’ll establish baselines using non-GNN models and tabular data: logistic
    regression, XGBoost, and scikit-learn’s multilayer perceptron (MLP). Then, we’ll
    apply graph convolutional network (GCN) and GAT to the problem, introducing graph
    structural data.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用非GNN模型和表格数据建立基线：逻辑回归、XGBoost和scikit-learn的多层感知器（MLP）。然后，我们将应用图卷积网络（GCN）和GAT来解决这个问题，引入图结构数据。
- en: 'This fraudulent review problem can be tackled as a node classification problem.
    We’ll use GAT to perform node classification of the Yelp reviews, sifting the
    fraudulent from the legitimate reviews. This classification is binary: “spam”
    or “not spam.”'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个欺诈性评论问题可以作为一个节点分类问题来解决。我们将使用GAT对Yelp评论进行节点分类，从合法评论中筛选出欺诈性评论。这种分类是二元的：“垃圾邮件”或“非垃圾邮件”。
- en: 'We expect that the graph structural data and attention mechanism will give
    an edge to the attention-based GNN models. We’ll follow this process in this chapter:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预期图结构数据和注意力机制将使基于注意力的GNN模型更具优势。在本章中，我们将遵循以下过程：
- en: Load and preprocess the dataset
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集并进行预处理
- en: Define baseline models and results
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义基线模型和结果
- en: Implement the GAT solution and compare it to baseline results
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现GAT解决方案并与基线结果进行比较
- en: 4.2 Exploring the review spam dataset
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 探索评论垃圾邮件数据集
- en: 'Derived from a broader Yelp review dataset, our data focuses on reviews from
    Chicago’s hotels and restaurants. It has also been preprocessed so that the data
    has a graph structure. This means we’re going to be using a specialized version
    of the Yelp Multirelational dataset, characterized by its graph structure and
    its focus on consumer reviews from many Chicago-based hotels and restaurants.
    The Yelp Multirelational dataset is derived from the Yelp Review dataset and processed
    into a graph. This dataset contains the following (final version of the dataset
    is summarized in table 4.1):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从更广泛的电影评论数据集中提取，我们的数据专注于芝加哥的酒店和餐厅评论。它也已经过预处理，以便数据具有图形结构。这意味着我们将使用Yelp Multirelational数据集的专用版本，该版本以其图形结构和其专注于许多芝加哥酒店和餐厅的消费者评论而著称。Yelp
    Multirelational数据集是从Yelp Review数据集派生出来的，并处理成图形。此数据集包含以下内容（数据集的最终版本总结在表4.1中）：
- en: '*45,954 nodes* —Each node represents an individual review, with 14.5% of them
    flagged as likely fraudulent and created by a bot to skew the reviews.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*45,954个节点* —每个节点代表一条单独的评论，其中14.5%被标记为可能欺诈，并由机器人创建以歪曲评论。'
- en: '*Preprocessed node features* —Our nodes come with 32 features that have been
    normalized to facilitate machine learning algorithms.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理节点特征* —我们的节点带有32个特征，这些特征已经被归一化，以方便机器学习算法。'
- en: '*3,892,933 edges* —Edges connect reviews that have a common author or review
    a common business. While the original dataset had multiple types of relational
    edges, we use one with homogenous edges for easier analysis.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*3,892,933条边* —边连接具有共同作者或评论共同业务的评论。虽然原始数据集有多种类型的关联边，但我们使用具有同质边的边以简化分析。'
- en: '*No user or business IDs* —Distinguishing IDs have been removed.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无用户或业务ID* —区分ID已被删除。'
- en: Table 4.1 Overview of the Yelp Multirelational dataset
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1 Yelp Multirelational数据集概述
- en: '| Yelp Review dataset for the city of Chicago processed into a graph, with
    node features based on review text and user data |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 将芝加哥的Yelp Review数据集处理成图形，节点特征基于评论文本和用户数据 |  |'
- en: '| --- | --- |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Number of nodes (reviews)  | 45,954  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 节点数（评论）  | 45,954  |'
- en: '| Filtered (fraudulent) nodes  | 14.5%  |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 过滤（欺诈）节点  | 14.5%  |'
- en: '| Node features  | 32  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 节点特征  | 32  |'
- en: '| Total number of edges (edges are assumed to be homogenous in our analysis)  |
    3,846,979  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 总边数（在我们的分析中假设边是同质的）  | 3,846,979  |'
- en: '| Reviews with a common writer  | 49,315  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 具有共同作者的评论  | 49,315  |'
- en: '| Reviews of a common business and written in the same month  | 73,616  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 同一月份在同一业务中撰写的评论  | 73,616  |'
- en: '| Reviews of a common business that share the same rating  | 3,402,743  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 具有相同评分的同一业务的评论  | 3,402,743  |'
- en: Next, table 4.2 shows examples of text reviews from this dataset, ordered by
    the star rating system.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，表4.2显示了此数据集中的文本评论示例，按星级评分系统排序。
- en: Table 4.2 Sampling of reviews from the YelpChi dataset for one restaurant, in
    descending order by rating (5 being the highest)
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.2 YelpChi数据集中一家餐厅的评论抽样，按评分降序排列（5分是最高分）
- en: '| Rating (1–5) | Date | Review* |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 评分（1-5） | 日期 | 评论* |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 5  | 7/7/08  | Perfection. Snack has become my favorite late lunch/early
    dinner spot. Make sure to try the butter beans!!!  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 5  | 7/7/08  | 完美。Snack已经成为我最喜欢的晚午餐/早晚餐地点。一定要尝试黄油豆！！！  |'
- en: '| 4  | 7/1/13  | Ordered lunch for 15 from Snack last Friday. On time, nothing
    missing and the food was great. I have added it to the regular company lunch list,
    as everyone enjoyed their meal.  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 7/1/13  | 上周五从Snack订购了15份午餐。准时送达，没有遗漏，食物很棒。我已经将它添加到常规公司午餐名单中，因为每个人都喜欢他们的餐点。  |'
- en: '| 3  | 12/8/14  | The food at snack is a selection of popular Greek dishes.
    The appetizer tray is good as is the Greek salad. We were underwhelmed with the
    main courses. There are 4-5 tables here so it’s sometimes hard to get seated.  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 12/8/14  | Snack的食物是希腊流行菜肴的选择。开胃小吃盘和希腊沙拉都很好。我们对主菜不太满意。这里有4-5张桌子，所以有时很难找到座位。  |'
- en: '| 2  | 9/10/13  | Been meaning to try this place for a while-highly recommended
    by a friend. Had the tuna sandwic… . good but got TERRIBLY SICK afterword. Also,
    sage tea was nice.  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 9/10/13  | 一直想尝试这个地方，一个朋友强烈推荐。点了金枪鱼三明治……很好，但之后感觉非常不舒服。还有，迷迭香茶也很不错。  |'
- en: '| 1  | 8/12/12  | Lackluster service, soggy lukewarm spinach pie and two-day-old
    cucumber salad. Go to Local instead!  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 1  | 8/12/12  | 服务平淡，菠菜派湿漉漉的，不热，黄瓜沙拉已经两天了。还是去Local吧！  |'
- en: '| *Spelling, grammar, and punctuation are uncorrected in these reviews.  |  |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| *这些评论中的拼写、语法和标点符号未经校正。  |  |  |'
- en: 4.2.1 Explaining the node features
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 解释节点特征
- en: 'Highlights of this dataset are its node features. These were extracted from
    available metadata such as ratings, timestamps, and review text. They are divided
    into the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本数据集的亮点是其节点特征。这些特征是从可用的元数据中提取的，例如评分、时间戳和评论文本。它们分为以下几类：
- en: Characteristics of the text review
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本评论的特征
- en: Characteristics of the reviewer
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论者的特征
- en: Characteristics of the reviewed business
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 被评论商业的特征
- en: 'These features are then further divided into behavioral and textual features:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征随后进一步分为行为和文本特征：
- en: Behavioral features highlight patterns of behavior and actions of the reviewers.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为特征突出显示评论者的行为和行动模式。
- en: Textual features are based on the text found in the reviews.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本特征基于评论中找到的文本。
- en: 'The process for calculating these features was developed by Rayana and Akoglu
    [7] and Dou [9]. Taking the original formulas from Rayana and Akoglu, Dou preprocessed
    and normalized the feature data that we use in this example. A summary of the
    features is shown in figure 4.2\. (For more details on definitions and how they
    were calculated, refer to the original paper [8].) These node features are summarized
    here:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这些特征的过程是由 Rayana 和 Akoglu [7] 以及 Dou [9] 开发的。Dou 在此例中预处理并标准化了我们从 Rayana 和
    Akoglu 那里得到的特征数据。特征摘要如图 4.2 所示。（有关定义及其计算方法的更多详细信息，请参阅原始论文 [8]。）以下是对节点特征的总结：
- en: 'Reviewer and business features:'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论者和商业特征：
- en: '*Behavioral:*'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*行为：*'
- en: '*Max. number of reviews written in a day (MNR)* —High value suggests spam.'
  id: totrans-61
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每天撰写的最大评论数 (MNR)* — 高值表示垃圾邮件。'
- en: '*Ratio of positive reviews (4-5 star) (PR)* —High value suggests spam.'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*正面评论比率 (4-5 星) (PR)* — 高值表示垃圾邮件。'
- en: '*Ratio of negative reviews (1-2 star) (NR)* —High value suggests spam.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*负面评论比率 (1-2 星) (NR)* — 高值表示垃圾邮件。'
- en: '*Avg. rating deviation (avgRD)* —High value suggests spam.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平均评分偏差 (avgRD)* — 高值表示垃圾邮件。'
- en: '*Weighted rating deviation (WRD)* —High value suggests spam.'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加权评分偏差 (WRD)* — 高值表示垃圾邮件。'
- en: '*Burstiness (BST)* —Specifically, the time frame between the user’s first and
    last review. High value suggests spam.'
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*爆发性 (BST)* — 特指用户首次和最后一次评论之间的时间段。高值表示垃圾邮件。'
- en: '*Entropy of rating distribution (ERD)* —Low value suggests spam.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*熵值分布熵 (ERD)* — 低值表示垃圾邮件。'
- en: '*Entropy of temporal gaps* *D**t’s (ETG)* —Low value is spam indicative.'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间间隔熵 (ETG)* — 低值表示垃圾邮件。'
- en: '*Text-based:*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本：*'
- en: '*Avg. review length in words (RL)* —Low value suggests spam.'
  id: totrans-70
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平均评论长度（单词数）(RL)* — 低值表示垃圾邮件。'
- en: '*Avg./Max. content similarity* *measured with cosine similarity using a bag-of-bigrams
    approach (ACS, MCS)* —High value suggests spam.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*平均/最大内容相似度* — *使用余弦相似度和双词袋方法测量 (ACS, MCS)* — 高值表示垃圾邮件。'
- en: 'Review features:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评论特征：
- en: '*Behavioral:*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*行为：*'
- en: '*Rank order among all the reviews of a product* —Low value suggests spam.'
  id: totrans-74
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产品所有评论中的排名顺序* — 低值表示垃圾邮件。'
- en: '*Absolute rating deviation from product’s average rating (RD)* —High value
    is suspicious.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产品平均评分的绝对评分偏差 (RD)* — 高值可疑。'
- en: '*Extremity of rating (EXT)* —High values (4-5 stars) are considered spammy.'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评分极端性 (EXT)* — 高值 (4-5 星) 被视为垃圾邮件。'
- en: '*Thresholded rating deviation of review (DEV)* —High deviation is suspicious.'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论评分偏差的阈值 (DEV)* — 高偏差可疑。'
- en: '*Early time frame (ETF)* —Reviews that appear too early are suspicious.'
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*早期时间段 (ETF)* — 出现过早的评论可疑。'
- en: '*Singleton Reviewer Detection (ISR)* —If the review is a user’s sole review,
    it’s marked as suspicious.'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*单一评论者检测 (ISR)* — 如果评论是用户的唯一评论，则标记为可疑。'
- en: '*Text-based:*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*文本：*'
- en: '*Percentage of ALL-capital words (PCW)* —High values are suspicious.'
  id: totrans-81
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*全大写单词百分比 (PCW)* — 高值可疑。'
- en: '*Percentage of capital letters (PC)* —High values are suspicious.'
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大写字母百分比 (PC)* — 高值可疑。'
- en: '*Review length in words* —Low values are suspicious.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论长度（单词数）* — 低值可疑。'
- en: '*Ratio of 1st person pronouns like “I”, “my” (PP1)* —Low values are suspicious.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*第一人称代词如“我”、“我的” (PP1)* — 低值可疑。'
- en: '*Ratio of exclamation sentences (RES)* —High values are suspicious.'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*感叹句比率 (RES)* — 高值可疑。'
- en: '*Ratio of subjective words*—*Detected by sentiWordNet (SW)* —High values are
    suspicious.'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*主观词比率* — *由 sentiWordNet 检测 (SW)* — 高值可疑。'
- en: '*Ratio of objective words*—*Detected by sentiWordNet (OW)* —Low values are
    suspicious.'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客观词比率* — *由 sentiWordNet 检测 (OW)* — 低值可疑。'
- en: '*Frequency of review*—*Approximated using locality-sensitive hashing (F)* —High
    values are suspicious.'
  id: totrans-88
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*评论频率* — *使用局部敏感哈希 (F) 近似* — 高值可疑。'
- en: '*Description length based on unigrams and bigrams (DLu, DLb)* —Low values are
    suspicious.'
  id: totrans-89
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于单词和双词的描述长度（DLu, DLb）* — 低值可疑。'
- en: Figure 4.2 gives a summary of the set of features.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2给出了特征集的总结。
- en: '![figure](../Images/4-2.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-2.png)'
- en: Figure 4.2 Summary definitions of node features used in the example. A label
    of high means that a high value of the data indicates a tendency toward spamminess.
    Likewise, a label of low means that a low value of the data indicates a tendency
    toward spamminess. (For more details on the derivation of these features, refer
    to [7].)
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2展示了示例中使用的节点特征的总结定义。高标签表示数据的高值表明了垃圾邮件的倾向性。同样，低标签表示数据低值表明了垃圾邮件的倾向性。（关于这些特征的推导细节，请参阅[7]）
- en: This diverse mix of features requires varying degrees of intuition to interpret.
    These features not only help in understanding the behavior of reviewers but also
    in deducing the context and essence of the reviews. It’s clear that certain features,
    such as Singleton Reviewer Detection or Review Length in Words can provide immediate
    insights, while others, such as Entropy of Temporal Gaps Dt’s, require a more
    considered understanding. Let’s next examine the distributions of these features
    present in the data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特征组合需要不同程度的直觉来解释。这些特征不仅有助于理解评论者的行为，还有助于推断评论的上下文和本质。很明显，某些特征，如单评论者检测或评论长度（以单词计），可以提供直接的洞察，而其他特征，如时间间隔的熵Dt，则需要更深入的理解。接下来，让我们检查数据中这些特征的分部。
- en: 4.2.2 Exploratory data analysis
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 探索性数据分析
- en: In this section, we download and explore the dataset with a focus on node features.
    Node features will serve as the main tabular features in our non-graph baseline
    models.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们下载并探索数据集，重点关注节点特征。节点特征将作为我们非图基线模型中的主要表格特征。
- en: 'The dataset can be downloaded from Yingtong Dou’s GitHub repository ([https://mng.bz/Pdyg](https://mng.bz/Pdyg)),
    compressed in a zip file. The unzipped file will be in MATLAB format. Using the
    `loadmat` function from the `scipy` library and a utility function from Dou’s
    repository, we can produce the objects we need to start (see listing 4.1):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以从Yingtong Dou的GitHub仓库下载（[https://mng.bz/Pdyg](https://mng.bz/Pdyg)），压缩在一个zip文件中。解压后的文件将是MATLAB格式。使用`scipy`库中的`loadmat`函数和Dou仓库中的实用函数，我们可以生成开始所需的对象（参见列表4.1）：
- en: A `features` object containing the node features
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含节点特征的`features`对象
- en: A `labels` object containing the node labels
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个包含节点标签的`labels`对象
- en: An adjacency list object
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个邻接表对象
- en: Listing 4.1 Load data
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1 加载数据
- en: '[PRE0]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 loadmat is a scipy function that loads MATLAB files.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 `loadmat`是scipy库中的一个函数，用于加载MATLAB文件。'
- en: '#2 Retrieves the node labels and features, respectively'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 分别获取节点标签和特征'
- en: '#3 Retrieves and pickles an adjacency list. “Homo” means that this adjacency
    list will be based on a homogenous set of edges; that is, we get rid of the multirelational
    nature of the edges.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 获取并序列化邻接表。“Homo”表示这个邻接表将基于同质边集；也就是说，我们消除了边的多关系性质。'
- en: Once the adjacency list is extracted and pickled, it can then be called in the
    future using
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提取并序列化邻接表，就可以在未来通过以下方式调用它
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With the data loaded, we can now perform some exploratory data analysis (EDA)
    to analyze the graph structure and node features.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据加载完成后，我们现在可以执行一些探索性数据分析（EDA），以分析图结构和节点特征。
- en: 4.2.3 Exploring the graph structure
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 探索图结构
- en: To better understand fraud within our dataset, we explore the underlying graph
    structure. By analyzing the connected components and various graph metrics, we
    can get an overview of the network’s topology. This understanding will reveal
    the data’s inherent characteristics and make sure there are no potential blockers
    to effective GNN training. We present a detailed analysis of the connected components,
    density, clustering coefficients, and other key metrics.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解数据集中的欺诈行为，我们探索了底层图结构。通过分析连通分量和各种图度量，我们可以了解网络的拓扑概览。这种理解将揭示数据固有的特性，并确保没有潜在的阻碍有效GNN训练的因素。我们详细分析了连通分量、密度、聚类系数和其他关键指标。
- en: 'To perform this structural EDA, we use our adjacency list to examine the structural
    nature of our graph using the `NetworkX` library. In the following snippet of
    code, we load the adjacency list object, convert it into a `NetworkX` graph object,
    and then interrogate this graph object for three basic properties. The longer
    code can be found in the repository:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行这种结构化 EDA，我们使用我们的邻接表，通过 `NetworkX` 库来检查我们图的结构的性质。在下面的代码片段中，我们加载邻接表对象，将其转换为
    `NetworkX` 图对象，然后查询此图对象的基本属性。更长的代码可以在存储库中找到：
- en: '[PRE2]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: From the EDA, we obtain the properties listed in table 4.3.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 EDA，我们获得了表 4.3 中列出的属性。
- en: Table 4.3 Graph properties
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.3 图属性
- en: '| Property | Value/details |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 值/详情 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Number of nodes  | 45,954  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 节点数量 | 45,954 |'
- en: '| Number of edges  | 3,892,933  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 边数量 | 3,892,933 |'
- en: '| Average node degree  | 84.71  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 平均节点度数 | 84.71 |'
- en: '| Density  | ~0.00  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 密度 | ~0.00 |'
- en: '| Connectivity  | The graph isn’t connected.  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 连通性 | 图不连通 |'
- en: '| Average clustering coefficient  | 0.77  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 平均聚类系数 | 0.77 |'
- en: '| Number of connected components  | 26  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 连接组件数量 | 26 |'
- en: '| Degree distribution (first 10 nodes)  | [4, 4, 4, 3, 4, 5, 5, 6, 5, 19]  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 度分布（前 10 个节点） | [4, 4, 4, 3, 4, 5, 5, 6, 5, 19] |'
- en: Let’s dig into these properties. The graph is relatively large with 45,954 nodes
    and 3,892,933 edges. This means the graph has a considerable level of complexity
    and will likely contain intricate relationships. The average node degree is 84.71,
    suggesting that, on average, nodes in the graph are connected to around 85 other
    nodes. This indicates that the nodes in the graph are reasonably well connected,
    and there’s a possibility of rich information flow between them. The graph’s density
    is close to 0.00, which indicates it’s quite sparse. In other words, the number
    of actual connections (edges) is much lower than the number of possible connections.
    The density of a graph is its number of edges divided by its total possible edges.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨这些属性。该图相对较大，有 45,954 个节点和 3,892,933 条边。这意味着图具有相当复杂度，可能包含复杂的关系。平均节点度数为
    84.71，表明在图中，节点平均连接到大约 85 个其他节点。这表明图中的节点连接得相当好，它们之间可能存在丰富的信息流。图的密度接近 0.00，这表明它相当稀疏。换句话说，实际连接（边）的数量远低于可能连接的数量。图的密度是其边数除以总可能边数。
- en: The graph isn’t fully connected and consists of 26 separate connected components.
    The presence of multiple connected components may require special consideration
    in modeling, especially if different components represent distinct data clusters
    or phenomena. The average clustering coefficient of 0.77 is relatively high. This
    metric gives an idea of the graph’s “cliquishness.” A high value means that nodes
    tend to cluster together, forming tightly knit groups. This could be indicative
    of local communities or clusters within the data, which can be crucial in understanding
    patterns or anomalies, especially in fraud detection.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图不完全连通，由 26 个独立的连接组件组成。多个连接组件的存在可能在建模时需要特别考虑，尤其是如果不同的组件代表不同的数据集群或现象。平均聚类系数为
    0.77，相对较高。这个指标给出了图“紧密性”的概念。高值意味着节点倾向于聚集在一起，形成紧密的群体。这可能表明数据中的局部社区或集群，这在理解模式或异常，尤其是在欺诈检测中至关重要。
- en: Given that we have 26 distinct components, it’s important to examine them to
    plan for model training. We want to know whether the components are roughly the
    same size, are a mix of sizes, or have one or two components dominating. Do the
    properties of these separate graphs differ significantly? We run a similar analysis
    on the 26 components and summarize the properties in table 4.4, with the components
    displayed in descending order in terms of the number of nodes. The first column
    contains the identifier of the component. From this table, we observe that one
    large component dominates the dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有 26 个不同的组件，检查它们对于模型训练计划很重要。我们想知道这些组件的大小是否大致相同，是否是大小混合，或者有一个或两个组件占主导地位。这些单独图的属性有显著差异吗？我们对
    26 个组件进行类似的分析，并在表 4.4 中总结属性，组件按节点数量降序排列。第一列包含组件的标识符。从这张表中，我们可以观察到有一个大型组件主导了数据集。
- en: Table 4.4 Properties of the 26 graph components, sorted in descending order
    by number of nodes
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.4 26 个图组件的属性，按节点数量降序排列
- en: '| Component ID | Number of nodes | Number of edges | Average node degree |
    Density | Average clustering coeff |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 组件 ID | 节点数量 | 边数量 | 平均节点度数 | 密度 | 平均聚类系数 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 3  | 45,900  | 38,92810  | 169.62  | 0  | 0.77  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 3  | 45,900  | 38,92810  | 169.62  | 0  | 0.77  |'
- en: '| 4  | 13  | 60  | 9.23  | 0.77  | 0.77  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 4  | 13  | 60  | 9.23  | 0.77  | 0.77  |'
- en: '| 2  | 6  | 14  | 4.67  | 0.93  | 0.58  |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 2  | 6  | 14  | 4.67  | 0.93  | 0.58  |'
- en: '| 1, 22  | 3  | 6  | 4  | 2  | 1  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 1, 22  | 3  | 6  | 4  | 2  | 1  |'
- en: '| 5–9, 14, 17, 24, 26  | 2  | 3  | 3  | 3  | 0  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 5–9, 14, 17, 24, 26  | 2  | 3  | 3  | 3  | 0  |'
- en: '| 7–21, 23, 25  | 1  | 1  | 2  | 0  | 0  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 7–21, 23, 25  | 1  | 1  | 2  | 0  | 0  |'
- en: '| In the bottom three rows, several components have identical properties and
    are put in the same row to save space.  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: 在底部三行中，几个组件具有相同的属性，因此被放在同一行以节省空间。
- en: We see here that component 3 is the dominant component, followed by 25 components
    that are tiny in comparison. These tiny components probably won’t have a strong
    influence over our model, so we’ll focus on component 3\. Let’s contrast this
    component with the overall graph, found in table 4.5\. Most of the properties
    are very similar or the same, with the exception of the average node degree, for
    which component 3 is twice as large.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到，组件3是主导组件，其次是25个相对较小的组件。这些较小的组件可能对我们的模型影响不大，因此我们将重点关注组件3。让我们将这个组件与表4.5中找到的整体图进行对比，这个组件的许多属性都非常相似或相同，唯一的例外是平均节点度，组件3是这个数值的两倍。
- en: Table 4.5 Comparing the largest component of the graph, component 3, to the
    overall graph
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.5 比较图的最大组件组件3与整体图
- en: '| Attribute | Component No. 3 | Overall Graph | Insight/Contrast |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 属性 | 组件编号3 | 整体图 | 启示/对比 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Number of nodes  | 45,900  | 45,954  | Component 3 contains almost all nodes
    from the entire graph.  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 节点数量  | 45,900  | 45,954  | 组件3几乎包含了整个图的所有节点。  |'
- en: '| Number of edges  | 3,892,810  | 3,892,933  | Component 3 contributes almost
    all edges of the entire graph.  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 边的数量  | 3,892,810  | 3,892,933  | 组件3几乎贡献了整个图的所有边。  |'
- en: '| Average node degree  | 169.62  | 84.71  | Nodes in component 3 are more densely
    connected than in the overall graph.  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 平均节点度  | 169.62  | 84.71  | 组件3中的节点比整体图中的节点连接得更密集。  |'
- en: '| Density  | 0.00  | 0.00  | Both the component and the entire graph are sparse;
    this property is mainly driven by component 3\.  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 密度  | 0.00  | 0.00  | 组件和整个图都是稀疏的；这种属性主要是由组件3驱动的。  |'
- en: '| Average clustering coefficient  | 0.77  | 0.77  | Component 3 matches the
    overall graph in terms of clustering, indicating its dominance in defining the
    graph’s structure.  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 平均聚类系数  | 0.77  | 0.77  | 组件3在聚类方面与整体图相匹配，表明它在定义图结构方面的主导地位。  |'
- en: 'For our GNN modeling purposes, what should we take away from this structural
    analysis? Primarily, the overwhelming dominance of component 3 in both nodes and
    edges underscores its significance in our dataset; almost the entirety of the
    graph’s structure is encapsulated within this single component. This suggests
    that the patterns, relationships, and anomalies within component 3 will heavily
    influence the model’s training and outcomes. The higher average node degree in
    component 3, compared to the overall graph, indicates a richer interconnectedness,
    emphasizing the importance of capturing these dense connections effectively. Furthermore,
    the identical density and clustering coefficient values between component 3 and
    the entire graph highlight that this component is highly representative of the
    dataset’s overall structural properties. We have two options:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的GNN建模目的，我们应该从这次结构分析中得到什么？主要的是，组件3在节点和边方面的压倒性主导地位强调了它在我们的数据集中的重要性；整个图的结构几乎都包含在这个单一组件中。这表明，组件3中的模式、关系和异常将严重影响模型的训练和结果。与整体图相比，组件3的平均节点度更高，这表明更丰富的相互连接，强调了有效捕捉这些密集连接的重要性。此外，组件3与整个图在密度和聚类系数值上的相同性突显出，这个组件高度代表了数据集的整体结构属性。我们有两种选择：
- en: Assume that the other components will have a minor effect on the model and train
    without making any adjustments.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设其他组件将对模型产生轻微的影响，并且在不做任何调整的情况下进行训练。
- en: Model only component 3 itself, completely leaving the data of the smaller components
    out of the training and test data.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅对组件3本身进行建模，完全将较小组件的数据排除在训练和测试数据之外。
- en: We looked at the structural properties of the graph data to get a glimpse into
    the characteristics of the graph and got some valuable insights to guide GNN model
    design and training for understanding potential fraud patterns. Next, we deep
    dive into the node features.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了图数据的结构特性，以了解图的特征，并获得了指导GNN模型设计和训练以理解潜在欺诈模式的宝贵见解。接下来，我们将深入探讨节点特征。
- en: 4.2.4 Exploring the node features
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 探索节点特征
- en: 'Having explored the structural nature of our graph, we turn to the node features.
    In the code at the beginning of this section, we pulled out the node features
    from the data file:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索了我们的图的结构的性质之后，我们转向节点特征。在本节开头代码中，我们从数据文件中提取了节点特征：
- en: '[PRE3]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note  As discussed previously, these feature definitions were handcrafted by
    Rayana and others [7, 8]. Guided by the feature generation process, Dou et al.
    [8] did the nontrivial work of further processing the Yelp review dataset to create
    a set of normalized node features.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如前所述，这些特征定义是由Rayana和其他人手工制作的[7, 8]。在特征生成过程的指导下，Dou等人[8]进行了非平凡的进一步处理Yelp评论数据集的工作，以创建一组归一化的节点特征。
- en: With some additional work, shown in the code repository, we also add some tags
    and descriptions to the features before we create a chart distribution for each
    feature (example plots are shown in figures 4.3 to 4.5). Each set of plots corresponds
    to features describing the review text, the reviewer, and the business. We want
    to use these plots to check that the node features can be useful in distinguishing
    fraud. Figure 4.3 shows the distributions of two of the features derived from
    the characteristics of the reviews.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些额外的工作，如代码库中所示，我们在为每个特征创建图表分布之前，也为特征添加了一些标签和描述（示例图见4.3至4.5图）。每一组图对应于描述评论文本、评论者和企业的特征。我们希望使用这些图来检查节点特征是否可以在区分欺诈中发挥作用。图4.3显示了从评论的特征中导出的两个特征的分布。
- en: '![figure](../Images/4-3.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-3.png)'
- en: Figure 4.3 Distribution plots of 2 of the 15 normalized node features based
    on the review (see section 4.2.1 for feature definitions)
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3显示了基于评论的15个归一化节点特征中的2个特征的分布图（参见4.2.1节中的特征定义）
- en: Figure 4.4 shows the distributions of two of the features derived from the characteristics
    of the reviewers.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4显示了从评论者的特征中导出的两个特征分布。
- en: '![figure](../Images/4-4.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-4.png)'
- en: Figure 4.4 Distribution plots of two of the nine normalized node features based
    on the reviewer (see section 4.2.1 for feature definitions)
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4显示了基于评论者的9个归一化节点特征中的两个特征的分布图（参见4.2.1节中的特征定义）
- en: Finally, figure 4.5 shows two of the distributions of the features derived from
    the characteristics of the restaurant or hotel being reviewed.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，图4.5显示了从被评论的餐厅或酒店的特征中导出的两个特征分布。
- en: '![figure](../Images/4-5.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-5.png)'
- en: Figure 4.5 Distribution plots of two of the eight normalized node features based
    on the business being reviewed (see section 4.2.1 for feature definitions)
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5显示了基于被评论的企业的8个归一化节点特征中的两个特征的分布图（参见4.2.1节中的特征定义）
- en: By examining the histograms for the 32 node features, we can make several observations.
    First, there’s a pronounced skewness in many of the features. Specifically, features
    such as Rank, RD, and EXT lean toward a right-skewed distribution. This indicates
    that the majority of data points fall on the histogram’s left side, but a few
    higher-value points stretch the histogram toward the right. Conversely, features
    such as MNR_user, PR_user, and NR_user, among others, display a left-skewed distribution.
    In these cases, most of the data points concentrate on the histogram’s right side,
    with a few lower-value points stretching the histogram to the left.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查32个节点特征的直方图，我们可以得出几个观察结果。首先，许多特征都存在明显的偏态。具体来说，如排名（Rank）、RD和EXT等特征倾向于右偏态分布。这表明大多数数据点落在直方图的左侧，但少数高值点将直方图拉伸到右侧。相反，如MNR_user、PR_user和NR_user等其他特征显示左偏态分布。在这些情况下，大多数数据点集中在直方图的右侧，少数低值点将直方图拉伸到左侧。
- en: Some features also exhibit a bimodal distribution, meaning that there are two
    distinct peaks or groups within the data. This suggests that segmenting the data
    and creating separate models for each group could be a useful strategy.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一些特征也表现出双峰分布，这意味着数据中存在两个不同的峰值或组。这表明对数据进行分段并为每个组创建单独的模型可能是一种有用的策略。
- en: Lastly, the long tails in several histograms suggest there are some outliers.
    Given that certain models, such as linear regression, are highly sensitive to
    extreme values, addressing these outliers could be crucial in refining and improving
    our model. This could mean opting for outlier-resistant models, developing strategies
    to mitigate their effect, or even removing them altogether.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，几个直方图中的长尾表明存在一些异常值。鉴于某些模型，如线性回归，对极端值非常敏感，解决这些异常值可能对我们的模型优化和改进至关重要。这可能意味着选择抗异常值模型，制定减轻其影响的策略，甚至完全删除它们。
- en: Given those general insights, let’s examine one of the feature plots more closely.
    PP1 is the ratio of first-person pronouns (i.e., I, me, us, our, etc.) to second
    person pronouns (you, your, etc.) in the review. This feature was developed due
    to an observation that spam reviews typically contain more second person pronouns.
    From the distribution plot for PP1, we observe that the distribution is skewed
    left, with a tail that peaks at low values. Thus, if a low ratio is an indicator
    of a spammy review, this feature would be good at distinguishing spam reviews.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些一般见解的基础上，让我们更仔细地检查一个特征图。PP1是评论中第一人称代词（即我、我们、我们的等）与第二人称代词（你、你的等）的比例。这个特征是由于观察到垃圾邮件通常包含更多的第二人称代词而开发的。从PP1的分布图中，我们观察到分布是偏左的，尾部在低值处达到峰值。因此，如果低比例是垃圾邮件的指标，这个特征将很好地区分垃圾邮件。
- en: To conclude our exploration of the node features, this data exhibits diverse
    characteristics, with many opportunities for model training. Further preprocessing,
    which could involve outlier handling, skewed feature transformation, data segmentation,
    and feature scaling, may be crucial in optimizing the model’s predictive performance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们对节点特征的探索，这些数据表现出多种特性，为模型训练提供了许多机会。进一步的前处理，可能包括异常值处理、偏斜特征转换、数据分段和特征缩放，可能对优化模型的预测性能至关重要。
- en: Our exploration of the review spam dataset revealed some patterns, anomalies,
    and insights. From the intricate structural characteristics of the dataset, represented
    largely by dominant component 3, to the node features that provide promising indications
    for distinguishing between genuine and fraudulent reviews, we’ve laid the groundwork
    for our model training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对评论垃圾邮件数据集的探索揭示了某些模式、异常和见解。从数据集的复杂结构特征来看，主要由主要成分3表示，到提供区分真实和欺诈评论的潜在指示的节点特征，我们已经为我们的模型训练奠定了基础。
- en: In section 4.3, we’ll embark on training our baseline models. These initial
    models serve as a foundation, helping us gauge the effectiveness of basic model
    performance. Through these models, we’ll harness the potential of the data’s graph
    structure and node features to separate fraud and spam from genuine reviews.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4.3节中，我们将开始训练我们的基线模型。这些初始模型作为基础，帮助我们评估基本模型性能的有效性。通过这些模型，我们将利用数据的图结构和节点特征来区分欺诈和垃圾邮件与真实评论。
- en: 4.3 Training baseline models
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 训练基线模型
- en: 'Given our dataset, we’ll begin the training phase by first developing three
    baseline models: logistic regression, XGBoost, and an MLP. Note that for these
    models, the data will have a tabular format, with the node features serving as
    our columnar features. There will be one row or observation for every node of
    our graph dataset. Next, we’ll develop an additional GNN baseline by training
    a GCN to evaluate the effect of introducing graph structured data to our problem.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的数据集，我们将首先开发三个基线模型：逻辑回归、XGBoost 和一个MLP。请注意，对于这些模型，数据将以表格格式呈现，节点特征作为我们的列特征。我们的图数据集的每个节点将有一个行或观察值。接下来，我们将通过训练一个GCN来开发一个额外的GNN基线，以评估引入图结构数据到我们问题中的影响。
- en: 'We now split our tabular data into test and train sets, and apply the three
    baseline models. First, the test/train splitting:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将我们的表格数据分为测试集和训练集，并应用三个基线模型。首先，进行测试/训练分割：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Splits data into test and train sets with an 80/20 split'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将数据分为测试集和训练集，比例为80/20'
- en: '#2 Double-checks the object shapes'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 两次检查对象形状'
- en: We can use this split data for each of the three models. For this training,
    we’re only using the node features and labels. There is no use of the graph data
    structure or geometry. For the baseline models and for the GNNs, we’ll mainly
    rely on Receiver Operating Characteristic (ROC) and Area Under the Curve (AUC)
    to gauge performance and to compare the performance of our GAT models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些分割数据为三个模型中的每一个。对于这次训练，我们只使用节点特征和标签。没有使用图数据结构或几何形状。对于基线模型和 GNN，我们将主要依靠受试者工作特征（ROC）和曲线下面积（AUC）来衡量性能，并比较我们的
    GAT 模型的性能。
- en: 4.3.1 Non-GNN baselines
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 非图神经网络（GNN）基线
- en: 'We start by using a logistic regression model with the scikit-learn implementation
    and the default hyperparameters:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用 scikit-learn 的实现和默认超参数的逻辑回归模型：
- en: '[PRE5]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Logistic regression model instantiation and training'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 实例化和训练逻辑回归模型'
- en: '#2 Accuracy score'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 准确度得分'
- en: 'This model yields an AUC of 76.12%. For the ROC performance, we’ll also use
    a function from scikit-learn. We’ll also recycle the true positive rate (`tpr`)
    and false positive rate (`fpr`) to compare with our other baseline models:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型产生了 76.12% 的 AUC。对于 ROC 性能，我们还将使用 scikit-learn 中的一个函数。我们还将回收真实阳性率（`tpr`）和假阳性率（`fpr`）来与我们的其他基线模型进行比较：
- en: '[PRE6]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 ROC curve calculation, yielding false positive rate (fpr) and true positive
    rate (tpr)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 计算 ROC 曲线，得到假阳性率（fpr）和真阳性率（tpr）'
- en: In figure 4.6, we see the ROC curve. We find that the curve is relatively balanced
    between false positives and false negatives but that the overall specificity is
    quite poor, given how near it is to the diagonal.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 4.6 中，我们看到 ROC 曲线。我们发现曲线在假阳性和假阴性之间相对平衡，但考虑到它接近对角线，整体特异性相当差。
- en: '![figure](../Images/4-6.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-6.png)'
- en: Figure 4.6 ROC curve for logistic regression baseline model (orange line) and
    chance line (blue diagonal line). An AUC of 76% indicates a model that can be
    improved.
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6 显示了逻辑回归基线模型（橙色线）和机会线（蓝色对角线）。76% 的 AUC 表明模型还有改进的空间。
- en: XGBoost
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: XGBoost
- en: The XGBoost baseline follows the logistic regression, as shown in listing 4.2\.
    We use a barebones model with the same training and test sets. For comparison,
    we differentiate the names of the generated predictions (named `pred2`), the true
    positive rate (`tpr2`), and the false positive rate (`fpr2`).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 基线遵循逻辑回归，如列表 4.2 所示。我们使用了一个裸机模型，具有相同的训练和测试集。为了比较，我们区分了生成的预测名称（命名为 `pred2`）、真实阳性率（`tpr2`）和假阳性率（`fpr2`）。
- en: Listing 4.2 XGBoost baseline and plot
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2 XGBoost 基线和绘图
- en: '[PRE7]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 For comparison, we name the XGBoost predictions “ypred2”.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 为了比较，我们将 XGBoost 预测命名为“ypred2”。'
- en: '#2 For comparison, we distinguish the tpr and fpr of XGBoost and plot them
    alongside the logistic regression result.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 为了比较，我们区分了 XGBoost 的 tpr 和 fpr，并将它们与逻辑回归结果并排绘制。'
- en: Figure 4.7 shows the ROC curves for XGBoost and logistic regression. It’s clear
    that XGBoost has superior performance for this metric.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 显示了 XGBoost 和逻辑回归的 ROC 曲线。很明显，XGBoost 在这个指标上具有优越的性能。
- en: '![figure](../Images/4-7.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-7.png)'
- en: Figure 4.7 ROC curve for the XGBoost (dotted line), shown with the logistic
    regression curve (solid line). We see that the XGBoost curve shows a better performance
    than the logistic regression. The diagonal line is the chance line.
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7 显示了 XGBoost（虚线）和逻辑回归曲线（实线）。我们看到 XGBoost 曲线比逻辑回归曲线表现更好。对角线是机会线。
- en: XGBoost fares better than logistic regression with this data, yielding an AUC
    of 94%, and with a superior ROC curve. This highlights that even a simple model
    can be suitable for some problems, and it’s always a good idea to check performance.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 在此数据上比逻辑回归表现更好，产生了 94% 的 AUC，并具有更优越的 ROC 曲线。这表明即使是简单的模型也可能适用于某些问题，检查性能始终是一个好主意。
- en: Multilayer perceptron
  id: totrans-198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多层感知器
- en: For the MLP baseline, we use PyTorch to build a simple, three-layer model, as
    shown in listing 4.3\. As with PyTorch, we establish the model using a class,
    defining the layers and the forward pass. In the MLP, we use binary cross-entropy
    (BCE) as the loss function, which is commonly used in binary classification problems.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 MLP 基线，我们使用 PyTorch 构建了一个简单、三层模型，如列表 4.3 所示。与 PyTorch 类似，我们通过一个类来建立模型，定义层和前向传递。在
    MLP 中，我们使用二元交叉熵（BCE）作为损失函数，这在二元分类问题中是常用的。
- en: Listing 4.3 MLP baseline and plot
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3 MLP 基线模型和绘图
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Imports needed packages for this section'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 导入本节所需的包'
- en: '#2 Defines the MLP architecture using a class'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用类定义 MLP 架构'
- en: '#3 Instantiates the defined model'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 实例化定义的模型'
- en: '#4 Sets key hyperparameters'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 设置关键超参数'
- en: '#5 Added to the account for class imbalance'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 考虑到类别不平衡'
- en: '#6 Defines the optimizer and the training criterion'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 定义了优化器和训练标准'
- en: '#7 Uses BCE loss as the loss function'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 使用BCE损失作为损失函数'
- en: '#8 Converts training data to torch data types: torch tensors'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 将训练数据转换为torch数据类型：torch张量'
- en: '#9 The training loop. In this example, we’ve specified 100 epochs.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 训练循环。在这个例子中，我们指定了100个epoch。'
- en: '#10 Differentiates the tpr and fpr for comparison'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 区分tpr和fpr进行比较'
- en: '#11 Plots all three ROC curves together'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 绘制所有三个ROC曲线'
- en: Figure 4.8 shows the ROC results for logistic regression, XGBoost, and an MLP.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8显示了逻辑回归、XGBoost和MLP的ROC结果。
- en: '![figure](../Images/4-8.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4-8.png)'
- en: Figure 4.8 ROC curves for all three baseline models. The curves for logistic
    regression and MLP overlap. The XGBoost model shows the best performance for this
    metric. The diagonal line is the chance line.
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.8展示了所有三个基线模型的ROC曲线。逻辑回归和MLP的曲线重叠。XGBoost模型在此指标上表现出最佳性能。对角线是机会线。
- en: The MLP run for 100 epochs yields an accuracy of 85.9% in the middle of our
    baselines. Its ROC curve is only slightly better than the logistic regression
    models. These results are summarized in table 4.6.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: MLP运行100个epoch，在基线中达到85.9%的准确率。其ROC曲线仅略优于逻辑回归模型。这些结果总结在表4.6中。
- en: Table 4.6 Log loss and ROC AUC for the three baseline models
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.6 三种基线模型的日志损失和ROC AUC
- en: '| Model | Log Loss | ROC AUC |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 日志损失 | ROC AUC |'
- en: '| --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| logistic regression  | 0.357  | 75.90%  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 0.357 | 75.90% |'
- en: '| XGBoost  | 0.178  | 94.17%  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 0.178 | 94.17% |'
- en: '| Multilayer perceptron  | 0.295  | 85.93%  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 | 0.295 | 85.93% |'
- en: To summarize this section, we’ve run three baseline models to use as benchmarks
    against our GNN models. These baselines used no structural graph data, only a
    set of tabular features derived from the node features. We didn’t attempt to optimize
    these models, and XGBoost ended up performing the best with an accuracy of 89.25%.
    Next, we’ll train one more baseline using GCN and then apply the GATs.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节，我们运行了三个基线模型作为我们的GNN模型的基准。这些基线没有使用结构化图数据，只使用了一组从节点特征派生出的表格特征。我们没有尝试优化这些模型，XGBoost最终以89.25%的准确率表现最佳。接下来，我们将使用GCN训练另一个基线，然后应用GAT。
- en: 4.3.2 GCN baseline
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 GCN基线
- en: In this section, we’ll apply GNNs to our problem, starting with the GCN from
    chapter 3 before moving on to a GAT model. We anticipate that our GNN models will
    outperform other baselines thanks to the graph structural data, and the models
    with an attention mechanism will be best. For the GNN models, we need to make
    some changes to our pipeline. A lot of this has to do with the data preprocessing
    and data loading.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将应用GNN到我们的问题上，从第3章中的GCN开始，然后转向GAT模型。我们预计我们的GNN模型将由于图结构数据而优于其他基线，并且具有注意力机制的模型将表现最佳。对于GNN模型，我们需要对我们的管道进行一些修改。这其中的很多都与数据预处理和数据加载有关。
- en: Data preprocessing
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'One critical first step is to prepare the data for use by our GNNs. This follows
    some of what has already been covered in chapters 2 and 3\. The code for this
    is provided in listing 4.4, where we take the following steps:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个关键步骤是为我们的GNN准备数据。这遵循了第2章和第3章中已经介绍的一些内容。这段代码在列表4.4中提供，我们采取以下步骤：
- en: '*Establish the train/test split.* We use the same `test_train_split` function
    from before, slightly tweaked to produce indices, and we only keep the resulting
    indices.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*建立训练/测试分割。* 我们使用之前相同的`test_train_split`函数，稍作调整以生成索引，并且我们只保留生成的索引。'
- en: '*Transform our dataset into PyG tensors.* For this, we start with the homogenous
    adjacency list generated in an earlier section. Using NetworkX, we convert this
    to a NetworkX `graph` object. From there, we use the PyG `from_networkx` function
    to convert this to a PyG `data` object.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将我们的数据集转换为PyG张量。* 为此，我们从一个早期部分生成的同构邻接列表开始。使用NetworkX，我们将其转换为NetworkX `graph`对象。从那里，我们使用PyG的`from_networkx`函数将其转换为PyG
    `data`对象。'
- en: '*Apply the train/test split to the converted data objects.* For this, we use
    the indices from the first step.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*将训练/测试分割应用于转换后的数据对象。* 为此，我们使用第一步中的索引。'
- en: We want to show a variety of ways to arrange the training data for ingestion.
    So, for the GCN, we’ll run the entire dataset through the model, while in the
    GAT example, we’ll batch the training data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望展示多种安排训练数据以供摄入的方式。因此，对于GCN，我们将整个数据集通过模型运行，而在GAT示例中，我们将训练数据进行分批。
- en: Listing 4.4 Converting the datatypes of our training data
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4 转换训练数据的数据类型
- en: '[PRE9]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Establishes the train/test split. We’ll only use the index variables.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 建立训练/测试分割。我们将只使用索引变量。'
- en: '#2 Establishes the train/test split. We’ll only use the index variables.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 建立训练/测试分割。我们将只使用索引变量。'
- en: '#3 Takes the adjacency list and transforms it into PyG data objects'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 将邻接表转换为PyG数据对象'
- en: '#4 Establishes the train/test split in the data objects'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 在数据对象中建立训练/测试分割'
- en: With the preprocessing done, we’re ready to apply the GCN and GAT solutions.
    We detailed the GCN architecture in chapter 3\. In listing 4.5, we establish a
    two-layer GCN, trained over 1,000 epochs. We choose two layers due to the insight
    from chapter 3 that, in general, a low model depth improves performance and prevents
    over-smoothing.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理完成后，我们准备应用GCN和GAT解决方案。我们在第3章详细介绍了GCN架构。在列表4.5中，我们建立了一个两层GCN，在1,000个epoch上训练。我们选择两层是因为第3章的见解，一般来说，低模型深度可以提高性能并防止过平滑。
- en: Listing 4.5 GCN definition and training
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5 GCN定义和训练
- en: '[PRE10]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Defines a two-layer GCN architecture'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义了两层GCN架构'
- en: '#2 Instantiates the model and puts the model and data on the GPU'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 实例化模型并将模型和数据放在GPU上'
- en: '#3 Training loop'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练循环'
- en: '#4 For each epoch, we feed the entire data object through the model and then
    use the training mask to calculate the loss.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 对于每个epoch，我们将整个数据对象通过模型，然后使用训练掩码来计算损失。'
- en: '#5 Calculates false positive rate (fpr) and true positive rate (tpr)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 计算假阳性率（fpr）和真阳性率（tpr）'
- en: Applying the solutions
  id: totrans-246
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 应用解决方案
- en: One item of note is the use of the masks in our training. While we establish
    loss using the nodes in the training mask, for forward propagation, we must pass
    the entire graph through the model. Why is this so? Unlike traditional machine
    learning models that work on independent data points (e.g., rows in a tabular
    dataset), GNNs operate on graph-structured data where the relationships between
    nodes are critical. When training a GCN, each node’s embedding is updated based
    on its neighbors’ information. Because this message-passing process involves aggregating
    information from a node’s local neighborhood, the model needs access to the entire
    graph structure so that it can compute these aggregations correctly and accurately
    perform this process.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是我们训练中使用了掩码。虽然我们使用训练掩码中的节点来建立损失，但在前向传播时，我们必须将整个图通过模型传递。为什么是这样？与在独立数据点（例如表格数据集的行）上工作的传统机器学习模型不同，GNNs在图结构数据上操作，其中节点之间的关系至关重要。在训练GCN时，每个节点的嵌入是基于其邻居的信息更新的。因为这个消息传递过程涉及到从节点的局部邻域中聚合信息，所以模型需要访问整个图结构，以便它可以正确准确地计算这些聚合并有效地执行此过程。
- en: So, during training, even though we’re only interested in the prediction for
    certain nodes (those in the training set), passing the entire graph through the
    model ensures that all necessary context is considered. If only part of the graph
    were passed through the model, the network would lack the complete information
    needed to propagate messages correctly and update node representations effectively.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在训练过程中，尽管我们只对某些节点的预测感兴趣（那些在训练集中的节点），但通过将整个图通过模型传递确保了考虑了所有必要上下文。如果只将图的一部分通过模型传递，网络将缺乏传播消息和有效更新节点表示所需的所有完整信息。
- en: A training session of 100 epochs for the GCN yields an accuracy of 94.37%. By
    introducing the graph data, we see incremental improvement against the XGBoost
    model. Table 4.7 compares the model performance levels.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: GCN的100个epoch的训练会得到94.37%的准确率。通过引入图数据，我们看到了与XGBoost模型相比的渐进改进。表4.7比较了模型性能水平。
- en: Table 4.7 AUC for the four baseline models
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.7 四个基线模型的AUC
- en: '| Model | AUC |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | AUC |'
- en: '| --- | --- |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logistic regression  | 75.90%  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 75.90% |'
- en: '| XGBoost  | 94.17%  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost | 94.17% |'
- en: '| Multilayer perceptron  | 85.93%  |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 | 85.93% |'
- en: '| GCN  | 94.37%  |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 94.37% |'
- en: To summarize, we’ve seen that including graph structural information using a
    GNN model slightly improves performance compared to a purely feature-based or
    tabular model. It’s clear that the XGBoost model has shown impressive results
    even without the use of graph structures. However, the GCN model’s marginally
    better performance underlines the potential of GNNs in using relational information
    embedded in graph data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们已看到，使用 GNN 模型包含图结构信息与仅基于特征或表格模型相比，略微提高了性能。很明显，XGBoost 模型即使没有使用图结构也展示了令人印象深刻的成果。然而，GCN
    模型略微更好的性能凸显了 GNN 在利用图数据中嵌入的关系信息方面的潜力。
- en: In the next phase of our study, our attention will turn to graph attention networks
    (GATs). GATs have an attention mechanism that is especially tailored to learning
    how to weigh the significance of neighbors during a message-passing step. This
    can potentially offer even better model performance. In the next section, we’ll
    delve into the details of training GAT models and comparing their outcomes with
    the baselines we’ve established. Let’s proceed with GAT model training.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究的下一阶段，我们的注意力将转向图注意力网络（GATs）。GATs 具有专门针对在消息传递步骤中如何权衡邻居重要性进行学习的注意力机制。这可能会提供更好的模型性能。在下一节中，我们将深入了解训练
    GAT 模型的细节，并将它们的成果与我们已经建立的基线进行比较。让我们继续进行 GAT 模型的训练。
- en: 4.4 Training GAT models
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 训练 GAT 模型
- en: To train our GAT models, we’ll apply two PyG implementations (GAT and GATv2)
    [2]. In this section, we’ll dive straight into training the models without discussing
    what attention means for machine learning models and why it’s helpful. However,
    for a short overview on attention and why attention might be all you need, see
    section 4.5.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的 GAT 模型，我们将应用两种 PyG 实现（GAT 和 GATv2）[2]。在本节中，我们将直接进入模型的训练过程，而不讨论对于机器学习模型来说注意力意味着什么以及为什么它是有帮助的。然而，关于注意力及其为何可能就是你所需要的简要概述，请参阅第
    4.5 节。
- en: We’ll be training two different GAT models. These both follow the same fundamental
    idea—that we’re replacing the aggregation operator in our GCN with an attention
    mechanism to learn what messages (node features) the model should pay the most
    attention to. The first—GATConv—is a simple extension to the GCN in chapter 3
    with the attention mechanism. The second is a slight variation to this model known
    as GATv2Conv. This model is the same as GATConv except that it addresses a limitation
    in the original implementation, namely that the attention mechanism is *static*
    over individual GNN layers. Instead, for GATv2Conv, the attention mechanism is
    dynamic across layers.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练两种不同的 GAT 模型。这两个模型都遵循相同的基本思想——我们将用注意力机制替换我们的 GCN 中的聚合操作，以学习模型应该最关注哪些消息（节点特征）。第一个——GATConv——是对第
    3 章中 GCN 的简单扩展，加入了注意力机制。第二个是对此模型稍作修改的 GATv2Conv。这个模型与 GATConv 相同，除了它解决了原始实现中的一个限制，即注意力机制在单个
    GNN 层上是静态的。相反，对于 GATv2Conv，注意力机制在层之间是动态的。
- en: To reiterate this, the original GAT model only computes the attention weights
    once per training loop by using individual node and neighborhood features, and
    these weights are static across all layers. In GATv2, the attention weights are
    calculated on the node features as they are transformed through the layers. This
    allows GATv2 to be more expressive, learning to emphasize the influence of node
    neighborhoods throughout the trained model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，原始的 GAT 模型仅在每次训练循环中通过使用单个节点和邻域特征来计算注意力权重一次，并且这些权重在所有层中都是静态的。在 GATv2 中，注意力权重是在节点特征通过层变换时计算的。这允许
    GATv2 更具表现力，学习在整个训练模型中强调节点邻域的影响。
- en: Both models introduce a significant computational overhead due to the introduction
    of the attention mechanism. To address this, we introduce mini-batching to our
    training loop.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 由于引入了注意力机制，这两个模型引入了显著的计算开销。为了解决这个问题，我们在训练循环中引入了小批量处理。
- en: 4.4.1 Neighborhood loader and GAT models
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 邻域加载器和 GAT 模型
- en: From an implementation point of view, one key difference between the previously
    studied convolutional models and our GAT models is the much larger memory requirements
    of GAT models [9]. The reason for this is that GAT requires a calculation of attention
    scores for every attention head and for every edge. This in turn requires the
    PyTorch `autograd` method to hold tensors in memory that can scale up considerably,
    depending on the number of edges, heads, and (twice) the number of node features.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从实现的角度来看，先前研究的卷积模型和我们的 GAT 模型之间有一个关键的区别，那就是 GAT 模型的内存需求要大得多 [9]。这是因为 GAT 需要对每个注意力头和每条边进行注意力分数的计算。这反过来又需要
    PyTorch 的 `autograd` 方法在内存中保留可以显著扩展的张量，这取决于边的数量、头的数量和（两倍）节点特征的数量。
- en: To get around this problem, we can divide our graph into batches and load these
    batches into the training loop. This is in contrast to what we did with our GCN
    model where we trained on one single batch (the entire graph). PyG’s `NeighborLoader`
    (in its `dataloader` module) allows such mini-batch training, where we provide
    implementation code for this in listing 4.6\. (PyG function `NeighborLoader` is
    based on the “Inductive Representation Learning on Large Graphs” paper [10].)
    The key input parameters for `NeighborLoader` are
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们可以将我们的图分成批次，并将这些批次加载到训练循环中。这与我们之前对 GCN 模型的做法形成对比，我们当时在一个单独的批次（整个图）上训练。PyG
    的 `NeighborLoader`（在其 `dataloader` 模块中）允许这种小批量训练，我们在这个列表 4.6 中提供了相应的实现代码。（PyG
    函数 `NeighborLoader` 基于“在大型图上的归纳表示学习”论文 [10]。）`NeighborLoader` 的关键输入参数是
- en: '`num_neighbors`—How many neighbor nodes will be sampled, multiplied by the
    number of iterations (i.e., GNN layers). In our example, we specify 1,000 nodes
    over two iterations.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_neighbors`——将被采样的邻居节点数量，乘以迭代次数（即 GNN 层）。在我们的例子中，我们指定在两次迭代中采样 1,000 个节点。'
- en: '`batch_size`—The number of nodes selected for each batch. In our example, we
    set the batch size to be `128`.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`batch_size`——每个批次选择的节点数量。在我们的例子中，我们将批量大小设置为 `128`。'
- en: Listing 4.6 Setting up `NeighborLoader` for GAT
  id: totrans-269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6 为 GAT 设置 `NeighborLoader`
- en: '[PRE11]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Samples 1,000 neighbors for each node in two iterations'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在两次迭代中为每个节点采样 1,000 个邻居'
- en: '#2 Uses a batch size for sampling training nodes'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 使用批量大小来采样训练节点'
- en: In creating our GAT model, there are two key changes to make relative to our
    GCN class. First, because we’re training in batches, we want to apply a batch-norm
    layer. Batch normalization is a technique used to normalize the inputs of each
    layer in a neural network to have a mean of 0 and a standard deviation of 1\.
    This helps stabilize and accelerate the training process by reducing internal
    covariate shift, allowing the use of higher learning rates, and improving the
    overall performance of the model.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的 GAT 模型时，相对于我们的 GCN 类别，有两个关键的改变。首先，因为我们是在批量训练，所以我们想应用一个批量归一化层。批量归一化是一种用于将神经网络中每一层的输入归一化到均值为
    0 和标准差为 1 的技术。这有助于通过减少内部协变量偏移来稳定和加速训练过程，允许使用更高的学习率，并提高模型的总体性能。
- en: Second, we note that our GAT layers have an additional input parameter—`heads`—which
    is the number of multihead attentions. In our example, our first `GATConv` layer
    has two heads, as specified in listing 4.7\.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们注意到我们的 GAT 层有一个额外的输入参数——`heads`——它表示多头注意力的数量。在我们的例子中，我们的第一个 `GATConv` 层有两个头，如列表
    4.7 所指定。
- en: The second `GATConv` layer, which is the output layer, has one head. In this
    GAT model, because we want the final layer to have a single representation for
    each node for our task, we use one head. Multiple heads would result in a confusing
    output with multiple node representations.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个 `GATConv` 层，即输出层，有一个头。在这个 GAT 模型中，因为我们希望最终层对每个节点都有一个单一的表现形式来完成我们的任务，所以我们使用一个头。多个头会导致输出混乱，出现多个节点表现形式。
- en: Listing 4.7 GAT-based architecture
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7 基于 GAT 的架构
- en: '[PRE12]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 GAT layers have a heads parameter, which determines the number of attention
    mechanisms in each layer. In this implementation, the first layer (conv1) uses
    multiple heads for richer feature extraction, while the final output layer (conv2)
    uses a single head to aggregate the learned information into a # single output
    for each node.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 GAT 层有一个 `heads` 参数，它决定了每一层中的注意力机制数量。在这个实现中，第一层（conv1）使用多个头进行更丰富的特征提取，而最终的输出层（conv2）使用一个头将学习到的信息聚合为每个节点的单一输出。'
- en: '#2 Because mini-batch training is being performed, a batch-norm layer is added.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 因为正在执行小批量训练，所以添加了一个批量归一化层。'
- en: Our training routine for GAT is similar to the single-batch GCN, which we provide
    in the following listing, except that we now need a nested loop for each batch.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对GAT的训练程序与单批次的GCN相似，我们将在以下列表中提供，但现在我们需要为每个批次进行嵌套循环。
- en: Listing 4.8 Training loop for GAT
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.8 GAT的训练循环
- en: '[PRE13]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Nested loop for mini-batch training. Each iteration here is a batch of nodes
    loaded by NeighborLoader.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于迷你批次的嵌套循环。这里的每次迭代都是通过NeighborLoader加载的节点批次。'
- en: The steps outlined previously are the same for GATv2Conv, which can be found
    in our repository. Training GATConv and GATv2Conv yields accuracies of 95.65%
    and 95.10%, respectively. As shown in table 4.8, our GAT models outperform the
    baseline models and GCN. Figure 4.9 shows the ROC results of the GCN and GAT models.
    Figure 4.10 shows the ROC results of the GCN, GAT, and GATv2 models.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 之前概述的步骤对GATv2Conv相同，可以在我们的存储库中找到。训练GATConv和GATv2Conv分别产生95.65%和95.10%的准确率。如表4.8所示，我们的GAT模型优于基线模型和GCN。图4.9显示了GCN和GAT模型的ROC结果。图4.10显示了GCN、GAT和GATv2模型的ROC结果。
- en: Table 4.8 ROC AUC of the models
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.8 模型的ROC AUC
- en: '| Model | ROC AUC (%) |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ROC AUC (%) |'
- en: '| --- | --- |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Logistic regression  | 75.90  |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 75.90  |'
- en: '| XGBoost  | 94.17  |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| XGBoost  | 94.17  |'
- en: '| Multilayer perceptron  | 85.93  |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 多层感知器 | 85.93  |'
- en: '| GCN  | 94.37  |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| GCN  | 94.37  |'
- en: '| GAT  | 95.65  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| GAT  | 95.65  |'
- en: '| GATv2  | 95.10  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| GATv2  | 95.10  |'
- en: '![figure](../Images/4-9.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-9.png)'
- en: Figure 4.9 ROC curves for GCN and GATConv. The GATConv model shows the best
    performance for this metric because it has a higher AUC and because its false
    positive rate is markedly lower. The diagonal line is the chance line.
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9 GCN和GATConv的ROC曲线。GATConv模型在此指标上表现出最佳性能，因为它具有更高的AUC，并且其假阳性率明显较低。对角线是机会线。
- en: '![figure](../Images/4-10.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-10.png)'
- en: Figure 4.10 ROC curves for GCN, GATConv, and GATv2\. Both GAT models outperform
    GCN. GATv2 has the same higher false positive profile than GAT but has a similar
    true positive rate.
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.10 GCN、GATConv和GATv2的ROC曲线。GAT模型都优于GCN。GATv2具有与GAT相同的更高的假阳性特征，但具有相似的真实阳性率。
- en: 'When observing the ROC curves, we see that both GAT models outperform the GCN.
    We also see that both have better false positive rates. This is crucial for fraud/spam
    detection as false positives can lead to genuine transactions/users being incorrectly
    flagged, causing inconvenience and loss of trust. For GATv2, we notice that for
    true positive rates, its performance is the same as for GCN and GAT. This indicates
    that while it’s conservative in not mislabeling genuine transactions as fraudulent,
    it might miss some actual frauds. These insights can lead to paths to refine the
    models or to decision-making about which to use. Despite the favorable AUC curves
    and scores, we must address one final problem that affects the usability of our
    GAT models: class imbalance.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察ROC曲线时，我们看到GAT模型都优于GCN。我们还看到两者都有更好的假阳性率。这对于欺诈/垃圾邮件检测至关重要，因为假阳性可能导致真正的交易/用户被错误标记，造成不便和信任损失。对于GATv2，我们注意到在真正阳性率方面，其性能与GCN和GAT相同。这表明，虽然它在避免将真实交易错误标记为欺诈方面比较保守，但它可能错过一些实际的欺诈行为。这些见解可以导致改进模型或做出决策的路径。尽管AUC曲线和得分很有利，但我们必须解决影响我们GAT模型可用性的一个最终问题：类别不平衡。
- en: 4.4.2 Addressing class imbalance in model performance
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 解决模型性能中的类别不平衡问题
- en: 'Class imbalance is a critical challenge in GNN problems, where the minority
    class, often representing rare but important instances (e.g., fraudulent activities),
    is significantly underrepresented compared to the majority class. In our dataset,
    only 14.5% of the nodes are labeled as fraudulent, making it challenging for the
    model to effectively learn from this sparse data. While high AUC scores may suggest
    good overall performance, they can be misleading, masking poor performance on
    the minority class that is crucial for a balanced evaluation. A deeper analysis
    reveals a critical oversight: class imbalance significantly undermines our precision
    and F1 scores.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 类别不平衡是GNN问题中的一个关键挑战，其中少数类（通常代表罕见但重要的实例，例如欺诈活动）与多数类相比代表性显著不足。在我们的数据集中，只有14.5%的节点被标记为欺诈，这使得模型难以从这些稀疏数据中有效学习。虽然高AUC分数可能表明整体性能良好，但它们可能是误导性的，掩盖了对少数类的性能不足，而少数类对于平衡评估至关重要。更深入的分析揭示了一个关键疏忽：类别不平衡严重影响了我们的精确度和F1分数。
- en: In response to this challenge, several methods have been developed specifically
    for GNNs to address class imbalance. Traditional techniques such as the Synthetic
    Minority Over-sampling Technique (SMOTE) have been adapted to create graph-specific
    methods such as GraphSMOTE, which generates synthetic nodes and edges to balance
    the class distribution without disrupting the graph structure. Other approaches
    include resampling techniques (both over-sampling and under-sampling), cost-sensitive
    learning, architectural modifications, and attention mechanisms that focus on
    minority class features [11, 12].
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，已经开发出针对 GNN 的几种专门方法来解决类别不平衡问题。传统的技术，如合成少数类过采样技术（SMOTE），已被改编为创建特定于图的图
    SMOTE 方法，该方法生成合成节点和边以平衡类别分布，同时不破坏图结构。其他方法包括重采样技术（包括过采样和欠采样）、成本敏感学习、架构修改以及关注少数类特征的注意力机制
    [11, 12]。
- en: While these methods help improve model performance, they come with unique challenges,
    such as preserving the graph’s topology, maintaining node dependencies, and ensuring
    scalability. Recent advancements, such as Graph-of-Graph Neural Networks (G2GNN),
    have been developed to handle these problems more effectively. By understanding
    and applying these strategies, we can enhance the robustness and fairness of GNN
    models in real-world applications where class imbalance is a common problem. Taking
    the GATv2 model from the previous section as the illustration, we compare its
    F1, recall, and precision with that of XGBoost in table 4.9\. XGBoost has superior
    performance, while GATv2 struggles to handle the imbalanced data.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些方法有助于提高模型性能，但它们也带来了独特的挑战，例如保留图的拓扑结构、维护节点依赖性以及确保可扩展性。最近的发展，如图图神经网络（G2GNN），已被开发出来更有效地处理这些问题。通过理解和应用这些策略，我们可以增强
    GNN 模型在实际应用中的鲁棒性和公平性，在这些应用中，类别不平衡是一个常见问题。以上一节中的 GATv2 模型为例，我们在表 4.9 中比较了其 F1、召回率和精度与
    XGBoost 的比较。XGBoost 具有优越的性能，而 GATv2 在处理不平衡数据方面存在困难。
- en: Table 4.9 Comparing F1, recall, and precision between the GATv2 and XGBoost
    models trained in this chapter
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.9 比较本章训练的 GATv2 和 XGBoost 模型的 F1、召回率和精度
- en: '| Metric | GATv2 | XGBoost |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | GATv2 | XGBoost |'
- en: '| --- | --- | --- |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| F1 score  | 0.254  | 0.734  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| F1 分数 | 0.254  | 0.734  |'
- en: '| Precision  | 0.145  | 0.855  |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 精确度 | 0.145  | 0.855  |'
- en: '| Recall  | 1  | 0.643  |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 1  | 0.643  |'
- en: The GATv2 model’s performance reflects a common challenge faced in scenarios
    with significant class imbalances. With the minority class constituting only 14.5%
    of the data, the model emphasizes maximizing recall, achieving a perfect recall
    score of 1.000\. This suggests that the model correctly identifies every instance
    of the minority class, avoiding any missed detections of potentially crucial cases.
    However, this comes at a significant cost to precision, which is notably low at
    0.145\. This indicates that while the GAT is effective in detecting all true positives,
    it also misclassifies many negative cases as positive, leading to a high number
    of false positives. As a result, the F1 score, which reflects both precision and
    recall, is low at 0.254, highlighting the inefficiency of the model in balancing
    detection with accuracy.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: GATv2 模型的性能反映了在具有显著类别不平衡的场景中面临的常见挑战。由于少数类仅占数据的 14.5%，该模型强调最大化召回率，实现了完美的召回率分数
    1.000。这表明模型正确识别了所有少数类的实例，避免了任何可能至关重要的案例的遗漏检测。然而，这以牺牲精度为代价，精度显著低至 0.145。这表明虽然 GAT
    在检测所有真阳性方面有效，但它也将许多负例错误地分类为阳性，导致大量假阳性。因此，反映精确度和召回率的 F1 分数低至 0.254，突显了模型在平衡检测与准确性方面的低效。
- en: 'To alleviate this, we implemented two strategies aimed at mitigating class
    imbalance: SMOTE illustrated in figure 4.11 and a custom reshuffling approach
    shown in figure 4.12.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解这一问题，我们实施了两种旨在减轻类别不平衡的策略：图 4.11 中所示的 SMOTE 和图 4.12 中所示的定制洗牌方法。
- en: '![figure](../Images/4-11.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-11.png)'
- en: Figure 4.11 An illustration of SMOTE, which seeks to provide a more balanced
    dataset by upsampling the minority class. On the left, we begin with the original
    dataset. In the middle, SMOTE creates synthetic data in the minority class. On
    the right, with the synthetic data added to the minority class, the dataset is
    more balanced.
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.11 SMOTE 的示意图，它通过上采样少数类来寻求提供更平衡的数据集。在左侧，我们从原始数据集开始。在中间，SMOTE 在少数类中创建合成数据。在右侧，将合成数据添加到少数类后，数据集更加平衡。
- en: SMOTE was used to generate synthetic nodes, reflecting the average degree characteristics
    of the original dataset, and to artificially enhance the representation of minority
    classes. The reshuffling method took a different approach by avoiding the generation
    of synthetic data. Instead, it ensures a balanced class representation in each
    training batch by redistributing the majority class data across the batches. This
    is achieved using the `BalancedNodeSampler` class, which guarantees that each
    batch has an equal number of nodes from both the majority and minority classes.
    For each batch, the sampler randomly selects a balanced set of nodes, extracts
    the corresponding subgraph, and re-indexes the nodes to maintain consistency.
    A typical batch redistribution from this process is illustrated in figure 4.12\.
    This class is shown in listing 4.9.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SMOTE 生成合成节点，反映原始数据集的平均度特征，并人为增强少数类的表示。重排方法采取了不同的方法，通过避免生成合成数据。相反，它通过在批次之间重新分配多数类数据来确保每个训练批次中类别的平衡表示。这是通过使用
    `BalancedNodeSampler` 类来实现的，该类保证每个批次都有来自多数类和少数类的节点数量相等。对于每个批次，采样器随机选择一个平衡的节点集，提取相应的子图，并重新索引节点以保持一致性。图
    4.12 中展示了这个过程的一个典型批次重新分配。该类在列表 4.9 中显示。
- en: '![figure](../Images/4-12.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/4-12.png)'
- en: Figure 4.12 Illustration of the reshuffling method using an example of 100 data
    points, with 76 in the majority class and 24 in the minority class. When creating
    batches for training, each batch is made to contain equal portions of the majority
    and minority classes.
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.12 使用 100 个数据点的示例说明重排方法，其中多数类有 76 个，少数类有 24 个。在创建训练批次时，每个批次都包含多数类和少数类相等的部分。
- en: Listing 4.9 `BalancedNodeSampler` class
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.9 `BalancedNodeSampler` 类
- en: '[PRE14]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Optional: defines fixed sampling size per class'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 可选：为每个类别定义固定的采样大小'
- en: '#2 Indices for the majority class'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 多数类的索引'
- en: '#3 Indices for the minority class'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 少数类的索引'
- en: '#4 Determines balanced batch size'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 确定平衡的批量大小'
- en: '#5 Randomly selects nodes for both classes'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 随机选择两个类别的节点'
- en: '#6 Combines samples from both classes into a single batch'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 将两个类别的样本合并到一个批次中'
- en: '#7 Creates a mask for sampled nodes'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 为采样节点创建掩码'
- en: '#8 Filters edges between sampled nodes'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 过滤采样节点之间的边'
- en: '#9 Re-indexes sampled nodes'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 重新索引采样节点'
- en: In this case, SMOTE didn’t yield performance improvement. Therefore, we’ll focus
    on the results of applying the reshuffling method. The metrics in table 4.10 demonstrate
    that our interventions have not only improved the fairness of the models but also
    enhanced their robustness by better capturing the minority class without sacrificing
    overall accuracy. While the reshuffling method’s AUC doesn’t exceed XGBoost (94.17%),
    it handles the class imbalance well with superior F1, precision, and recall.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，SMOTE 没有带来性能提升。因此，我们将关注应用重排方法的结果。表 4.10 中的指标表明，我们的干预不仅提高了模型的公平性，而且通过更好地捕捉少数类而不会牺牲整体精度，增强了模型的鲁棒性。虽然重排方法的
    AUC 不超过 XGBoost（94.17%），但它以优越的 F1、精确率和召回率很好地处理了类别不平衡。
- en: Table 4.10 Comparing F1, precision, recall, and AUC of the GATv2 model trained
    with a class reshuffling method
  id: totrans-328
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.10 使用类重排方法训练的 GATv2 模型的 F1、精确率、召回率和 AUC 对比
- en: '| Metric | Value |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 值 |'
- en: '| --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Mean validation F1 score  | 0.809  |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 平均验证 F1 分数 | 0.809  |'
- en: '| Mean validation precision  | 0.878  |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 平均验证精确率 | 0.878  |'
- en: '| Mean validation recall  | 0.781  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 平均验证召回率 | 0.781  |'
- en: '| Mean validation AUC  | 0.914  |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 平均验证 AUC | 0.914  |'
- en: 4.4.3 *Deciding between GAT and XGBoost*
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 *决定使用 GAT 还是 XGBoost*
- en: The choice between using XGBoost and GATs should be informed by specific use-case
    requirements and constraints. XGBoost offers efficiency and speed, which are advantageous
    for projects with limited computational resources or when quick model training
    is required. However, GATs provide the added benefit of deeply integrating node
    relational data, which is essential for projects where internode relationships
    are pivotal to understanding complex data patterns.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 XGBoost 和 GATs 之间的选择应该根据具体的用例需求和约束来决定。XGBoost 提供了效率和速度，这对于计算资源有限的项目或需要快速模型训练的项目来说是有利的。然而，GATs
    提供了深度集成节点关系数据的额外好处，这对于节点关系对于理解复杂数据模式至关重要的项目是必不可少的。
- en: GATs are particularly valuable for their ability to be integrated into broader
    deep learning frameworks, offering enhanced node embeddings that encapsulate rich
    contextual information, thus making them suitable for complex relational datasets.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: GATs特别有价值，因为它们能够集成到更广泛的深度学习框架中，提供封装了丰富上下文信息的节点嵌入，因此适合复杂的关系数据集。
- en: Our exploration into methods for addressing class imbalance has significantly
    informed our understanding of model performance in real-world scenarios. These
    insights are crucial for the effective development of robust and effective models,
    especially in fields where precision and recall are critically balanced. In the
    next, optional section, we go deeper into the concepts underlying GATs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对解决类别不平衡的方法的探索，极大地丰富了我们对模型在实际场景中性能的理解。这些见解对于开发稳健且有效的模型至关重要，特别是在精度和召回率需要平衡的关键领域。在下一节（可选）中，我们将更深入地探讨GATs背后的概念。
- en: 4.5 Under the hood
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 内部机制
- en: In this section, we discuss some of the additional details about attention and
    GATs. This is provided for those who want to know what’s going on under the hood,
    but you can safely skip this section if you’re more interested in learning how
    to apply the models. We dive into the equations from the GAT paper [8] and explain
    attention from a more intuitive perspective.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了关于注意力和GATs的一些附加细节。这是为那些想要了解内部机制的人提供的，但如果你更感兴趣于学习如何应用模型，你可以安全地跳过这一节。我们深入到GAT论文[8]中的方程，并从更直观的角度解释注意力。
- en: 4.5.1 Explaining attention and GAT models
  id: totrans-341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 解释注意力和GAT模型
- en: In this section, we provide a foundational overview of attention mechanisms.
    Attention, self-attention, and multihead attention are explained conceptually.
    Then, GATs are positioned as an extension of convolutional GNNs.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个关于注意力机制的概述。解释了注意力、自注意力和多头注意力等概念。然后，将GATs定位为卷积GNNs的扩展。
- en: 'Concept 1: The various attention mechanism types'
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念1：各种注意力机制类型
- en: Attention is one of the most important concepts introduced into deep learning
    in the past decade. It’s the basis for the, now famous, transformer model that
    powers many of the breakthroughs in generative models such as large language models
    (LLMs). Attention is the mechanism by which a model can learn what aspects in
    its training to put extra emphasis on [13, 14]. What are the various types of
    attention in a model?
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力是过去十年中引入深度学习中最重要概念之一。它是现在著名的、由注意力机制驱动的转换器模型的基础，该模型推动了诸如大型语言模型（LLMs）等生成模型中的许多突破。注意力是模型学习在其训练中哪些方面需要额外重视的机制[13,
    14]。模型中有哪些不同类型的注意力？
- en: Attention
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力
- en: Imagine you’re reading a novel where the storyline isn’t linear but rather jumps
    around, connecting various characters, events, or even parallel storylines. While
    reading a chapter about a specific character, you remember and consider other
    parts of the book where this character has appeared or been mentioned. Your understanding
    of this character at any given moment is influenced by these different parts of
    the book.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 想象你正在阅读一部小说，其情节不是线性的，而是跳跃式的，连接着各种人物、事件，甚至平行故事线。在阅读关于特定角色的章节时，你会回忆并考虑书中其他部分，该角色曾出现或被提及的地方。在任何给定时刻，你对这个角色的理解都会受到这些不同部分的影响。
- en: In deep learning and GNNs, attention serves a similar purpose. When processing
    a sentence in an NLP problem, attention means the model can learn the importance
    of neighboring words. For a GNN considering a specific node in a graph, the model
    uses attention to weigh the importance of neighboring nodes. This helps the model
    decide which neighboring nodes are most relevant when trying to understand the
    current node, similar to how you remember relevant parts of the book to better
    understand a character.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和图神经网络（GNNs）中，注意力机制发挥着类似的作用。在处理自然语言处理（NLP）问题中的句子时，注意力意味着模型可以学习邻近词语的重要性。对于一个考虑图中特定节点的GNN，模型使用注意力来权衡邻近节点的重要性。这有助于模型在尝试理解当前节点时决定哪些邻近节点最为相关，类似于你如何通过记住书中相关的部分来更好地理解一个角色。
- en: Self-attention
  id: totrans-348
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自注意力
- en: Imagine reading a sentence in the novel that refers to multiple characters and
    events, some of which are related in complex ways. To understand this sentence
    fully, you have to recall how each character and event relate to each other, all
    within the scope of that sentence. You might find yourself focusing more on certain
    characters or events that are crucial to understanding the context of the sentence
    you’re currently reading.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下在小说中阅读一句话，这句话提到了多个角色和事件，其中一些以复杂的方式相互关联。要完全理解这句话，你必须回忆起每个角色和事件是如何相互关联的，所有这些都在这句话的范围内。你可能会发现自己更多地关注那些对理解你正在阅读的句子的上下文至关重要的角色或事件。
- en: For a GNN using self-attention, each node in a graph not only considers its
    immediate neighbors but also takes into account its own features and position
    in the graph. By doing this, each node receives a new representation influenced
    by a weighted context of itself and other nodes, which helps in tasks that require
    understanding the relationships between nodes in a complex graph.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用自注意力的GNN，图中的每个节点不仅考虑其直接邻居，还考虑其自身的特征和在图中的位置。通过这样做，每个节点都会接收到一个新的表示，该表示受其自身和其他节点的加权上下文的影响，这有助于需要理解复杂图中节点之间关系的任务。
- en: Multihead attention
  id: totrans-351
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多头注意力
- en: Suppose you’re a member of a book club that is reading the novel, and each member
    of your club is asked to focus on different aspects of the novel—one on character
    development, another on plot twists, and yet another on thematic elements. When
    you all come together to discuss, you get a multifaceted understanding of the
    book.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是读书俱乐部的一员，俱乐部正在阅读小说，每个成员都被要求关注小说的不同方面——一个关注人物发展，另一个关注情节转折，还有一个关注主题元素。当你们聚在一起讨论时，你们对这本书的理解就变得多方面了。
- en: Similarly, in GNNs, multihead attention allows the model to have multiple “heads,”
    or attention mechanisms, focusing on various aspects or features of the neighboring
    nodes. These different heads can learn different patterns or relationships within
    the graph, and their outputs are usually aggregated to form a more complete understanding
    of each node’s role within the larger graph.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在GNN中，多头注意力允许模型具有多个“头”或注意力机制，关注邻居节点的各个方面或特征。这些不同的头可以学习图中的不同模式或关系，它们的输出通常被聚合，以形成一个更完整的理解，了解每个节点在更大图中的作用。
- en: 'Concept 2: GATs as variants of convolutional GNNs'
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概念2：GATs作为卷积GNN的变体
- en: GATs extend convolutional GNNs by incorporating attention mechanisms. In traditional
    convolutional GNNs such as GCNs, the contributions from all neighbors during the
    message-passing step are equally weighted when aggregated. GATs, however, add
    in attention scores to the aggregation function to weigh these contributions.
    This is still permutation invariant (by design) but more descriptive than the
    summation operation in GCNs.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: GATs通过结合注意力机制扩展了卷积GNN。在传统的卷积GNN，如GCNs中，在消息传递步骤中，所有邻居的贡献在聚合时是等权重的。然而，GATs在聚合函数中添加了注意力分数来权衡这些贡献。这仍然是排列不变的（按设计），但比GCNs中的求和操作更具描述性。
- en: PyG implementations
  id: totrans-356
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyG实现
- en: 'PyG offers two versions of GAT layers. The two are distinguished by the types
    of attention used and the calculation of attention scores:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: PyG提供了两种GAT层的版本。这两种版本的区别在于使用的注意力和注意力分数的计算：
- en: '`GATConv`—Based on Veličković’s paper [1], this layer uses self-attention to
    calculate attention scores across the entire graph. It can also be configured
    to use multihead attention, thereby employing multiple “heads” to focus on various
    aspects of the input nodes.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GATConv`—基于Veličković的论文[1]，这一层在整个图上使用自注意力来计算注意力分数。它还可以配置为使用多头注意力，从而使用多个“头”来关注输入节点的各个方面。'
- en: '`GATv2Conv`—This layer improves upon GATConv by introducing *dynamic atten**tion*.
    Here, self-attention scores are recalculated in a node-specific context across
    layers, making the model more expressive in how it learns to weigh node representations
    constructed during the message-passing step within each layer of a GNN. As with
    `GATConv`, it supports multihead attention to capture various features or aspects
    more effectively.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GATv2Conv`—这一层通过引入*动态注意力*对GATConv进行了改进。在这里，节点特定的上下文中，在层之间重新计算自注意力分数，使得模型在如何学习在每个GNN层的消息传递步骤中构建节点表示的权重方面更具表现力。与`GATConv`一样，它支持多头注意力，以更有效地捕捉各种特征或方面。'
- en: Tradeoffs vs. other convolutional GNNs
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与其他卷积GNN的权衡
- en: 'As implemented in PyG, GAT layers have advantages due to the use of attention.
    There are performance tradeoffs to consider, however. Key factors to consider
    are:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyG 中实现时，由于使用了注意力，GAT 层具有优势。然而，需要考虑性能权衡。需要考虑的关键因素是：
- en: '*Performance*—GATs generally have higher performance than standard convolutional
    GNNs as they can focus on the most relevant features.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能*——GATs 通常比标准的卷积 GNNs 有更高的性能，因为它们可以关注最相关的特征。'
- en: '*Training time*—Increased performance comes at the cost of more time required
    to train the models due to the added complexity of computing the attention mechanisms.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练时间*——由于计算注意力机制的增加复杂性，提高性能需要更多的时间来训练模型。'
- en: '*Scalability*—The computational cost also affects the scalability, making GATs
    less suitable for very large or dense graphs.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可扩展性*——计算成本也影响了可扩展性，使得 GATs 不太适合非常大的或密集的图。'
- en: 4.5.2 Over-smoothing
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 过度平滑
- en: You’ve learned how to change the aggregation operation used in the message-passing
    step to include more complicated methods, such as attention mechanisms. However,
    there is always a risk of performance degradation when applying multiple rounds
    of message passing. This effect, known as *over-smoothing*, occurs because, after
    multiple rounds of message passing [15], the updated features can converge to
    similar values. An example of this is shown in figure 4.13\.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何更改消息传递步骤中使用的聚合操作，以包括更复杂的方法，例如注意力机制。然而，在应用多轮消息传递时，总是存在性能退化的风险。这种效应被称为
    *过度平滑*，因为它在经过多轮消息传递 [15] 后，更新的特征可能会收敛到相似值。这种效应的示例如图 4.13 所示。
- en: '![figure](../Images/4-13.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/4-13.png)'
- en: Figure 4.13 Example of over-smoothing based on changing node features
  id: totrans-368
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.13 基于改变节点特征的过度平滑示例
- en: As we know, message passing occurs at each layer of a GNN. In fact, a GNN that
    has many layers is more at risk of over-smoothing than one that has fewer layers.
    This is one of the reasons why GNNs are typically more shallow than traditional
    deep learning models.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，消息传递发生在 GNN 的每一层。事实上，具有许多层的 GNN 比具有较少层的 GNN 更容易受到过度平滑的影响。这是 GNN 通常比传统深度学习模型更浅的一个原因。
- en: Another cause of over-smoothing happens when a problem has a significant long-range
    (in terms of number of hops) task that needs solving. For example, a node could
    be influenced by a far-off node. This is also known as having a large “problem
    radius.” Whenever we have a graph where nodes can have a very large effect on
    other nodes despite being multiple hops away, then the problem radius should be
    considered large. For example, social media networks might have a large problem
    radius if certain individuals such as celebrities can influence other individuals
    despite being distantly connected. Usually, this occurs when a graph is sufficiently
    large to have distantly connected nodes.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个导致过度平滑的原因是，当问题有一个需要解决的显著长距离（从跳数的角度来看）任务时。例如，一个节点可能受到一个遥远节点的 影响。这也被称为拥有较大的“问题半径”。每当我们在一个图中，节点可以非常大地影响其他节点，尽管它们相隔多个跳数时，那么问题半径应该被认为是大的。例如，如果某些个体，如名人，尽管与他们的联系遥远，但仍能影响其他个体，那么社交媒体网络可能有一个很大的问题半径。通常，这种情况发生在图足够大，以至于有遥远连接的节点时。
- en: In general, if you think a problem may be at risk of over-smoothing, be careful
    with how many layers you introduce to the GNN, that is, how deep you make it.
    However, note that certain architectures appear less at risk of over-smoothing
    than others. For example, GraphSAGE samples a fixed number of neighbors and aggregates
    their information. This sampling can mitigate over-smoothing. On the other hand,
    GCNs are more at risk because they don’t have this sampling process, and while
    the attention mechanism partially lowers the risk, GATs can also suffer from over-smoothing
    because the aggregation is still local.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，如果你认为一个问题可能存在过度平滑的风险，那么在引入 GNN 的层数时要小心，也就是说，要使其深度适中。然而，请注意，某些架构比其他架构更不容易出现过度平滑。例如，GraphSAGE
    采样固定数量的邻居并聚合它们的信息。这种采样可以减轻过度平滑。另一方面，GCNs 更容易受到过度平滑的影响，因为它们没有这种采样过程，尽管注意力机制部分降低了风险，但
    GATs 也可能受到过度平滑的影响，因为聚合仍然是局部的。
- en: 4.5.3 Overview of key GAT equations
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.3 关键 GAT 方程概述
- en: In this section, we’ll briefly cover the key equations given in the GAT paper
    by Veličković et al. [1] and tie them to the concepts we’ve covered about GATs.
    GATs use attention mechanisms to learn which neighboring nodes are more important
    when updating a node’s features. They do this by computing attention scores (equations
    1–3), which are then used to weigh and combine the features of neighboring nodes
    (equations 4–6). The use of multihead attention enhances the model’s expressiveness
    and robustness, allowing it to learn from multiple perspectives simultaneously.
    This approach can be computationally expensive, but it generally improves the
    performance of GNNs on various tasks such as node classification and link prediction.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要介绍 Veličković 等人在 GAT 论文中给出的关键方程 [1]，并将它们与我们关于 GAT 的概念联系起来。GAT 使用注意力机制来学习在更新节点特征时哪些邻居节点更重要。它们通过计算注意力分数（方程
    1–3）来实现，然后使用这些分数来权衡和组合邻居节点的特征（方程 4–6）。使用多头注意力增强了模型的表达能力和鲁棒性，使其能够从多个角度同时学习。这种方法在计算上可能很昂贵，但通常可以提高
    GNN 在各种任务（如节点分类和链接预测）上的性能。
- en: Attention coefficients calculation (equations 4.1–4.3)
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意力系数计算（方程 4.1–4.3）
- en: The first step in using GATs is to compute the attention scores or coefficients
    for each pair of connected nodes. These coefficients indicate how much “attention”
    or importance a node should give to its neighbor. Raw attention scores [1] are
    calculated as
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GAT 的第一步是计算每对连接节点的注意力分数或系数。这些系数表示节点应给予其邻居多少“注意力”或重要性。原始注意力分数 [1] 计算如下：
- en: (4.1)
  id: totrans-376
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.1)
- en: '![figure](../Images/Equation-4-1.png)'
  id: totrans-377
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-1.png)'
- en: 'Here, *e*[ij] represents the raw attention score from node iii to its neighbor
    *j*:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*e*[ij] 代表从节点 iii 到其邻居 *j* 的原始注意力分数：
- en: '**h**[*i*] and **h**[*j*] are the feature vectors (representations) of nodes
    *i* and *j*.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**h**[*i*] 和 **h**[*j*] 是节点 *i* 和 *j* 的特征向量（表示）。'
- en: '**W** is a learnable weight matrix that linearly transforms the features of
    each node to a higher dimensional space.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W** 是一个可学习的权重矩阵，它将每个节点的特征线性变换到更高维的空间。'
- en: '*α* is an attention mechanism (usually a neural network) that computes the
    importance score for each node pair.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α* 是一个注意力机制（通常是神经网络），它计算每个节点对的重要性分数。'
- en: The idea is to assess how much information node *i* should consider from node
    *j*. Normalized attention coefficients [1] are calculated as
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是评估节点 *i* 应从节点 *j* 考虑多少信息。归一化注意力系数 [1] 计算如下：
- en: (4.2)
  id: totrans-383
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.2)
- en: '![figure](../Images/Equation-4-2.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-2.png)'
- en: 'Once we have raw scores *e*[ij], we normalize them using a softmax function:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了原始分数 *e*[ij]，我们使用 softmax 函数对其进行归一化：
- en: '*α*[*ij*] represents the normalized attention coefficient that quantifies the
    importance of node *j*’s features to node *i*.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*α*[*ij*] 代表归一化注意力系数，它量化了节点 *j* 的特征对节点 *i* 的重要性。'
- en: The softmax ensures that all attention coefficients for a given node iii sum
    up to 1, making them comparable across different nodes.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Softmax 确保给定节点的所有注意力系数之和为 1，使它们在不同节点之间具有可比性。
- en: 'Following is a detailed computation of attention coefficients [1]:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是注意力系数 [1] 的详细计算：
- en: (4.3)
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.3)
- en: '![figure](../Images/Equation-4-3.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-3.png)'
- en: Here, the attention mechanism *α* is implemented using a single-layer feed-forward
    neural network with parameters **a**. The term ![figure](../Images/Equation-4-4.png)
    involves concatenating the transformed feature vectors of nodes *i* and *j*, and
    then applying a linear transformation followed by a nonlinear activation (leaky
    rectified linear unit [leaky ReLU]).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，注意力机制 *α* 使用具有参数 **a** 的单层前馈神经网络实现。项 ![figure](../Images/Equation-4-4.png)
    涉及连接节点 *i* 和 *j* 的转换特征向量的拼接，然后应用线性变换后跟非线性激活（漏斗形修正线性单元 [leaky ReLU]）。
- en: Node representation update (equations 4.4–4.6)
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 节点表示更新（方程 4.4–4.6）
- en: 'After computing the attention coefficients, the next step is to use them to
    aggregate information from the neighbors and update the node representations with
    attention [1]:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 计算注意力系数后，下一步是使用它们从邻居处聚合信息，并使用注意力 [1] 更新节点表示：
- en: (4.4)
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.4)
- en: '![figure](../Images/Equation-4-5.png)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-5.png)'
- en: 'This equation computes the new representation **h**[*i*]*''* for node *i*:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 此方程计算节点 *i* 的新表示 **h**[*i*]*'：
- en: The term ![figure](../Images/Equation-4-6.png) represents a weighted sum of
    the neighboring node features, where each feature vector is weighted by its corresponding
    attention coefficient *α*[*ij*].
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 术语![figure](../Images/Equation-4-6.png)表示相邻节点特征的加权求和，其中每个特征向量由其相应的注意力系数 *α*[*ij*]
    加权。
- en: '*σ* is a nonlinear activation function (like ReLU or sigmoid) that introduces
    nonlinearity into the model, helping it learn complex patterns.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*σ* 是一个非线性激活函数（如ReLU或sigmoid），它将非线性引入模型，帮助其学习复杂模式。'
- en: The multihead attention mechanism [1] is calculated as
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 多头部注意力机制[1]的计算如下：
- en: (4.5)
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.5)
- en: '![figure](../Images/Equation-4-7.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-7.png)'
- en: 'To stabilize the learning process, GATs use multihead attention, as discussed
    earlier:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 为了稳定学习过程，GATs使用多头部注意力，如前所述：
- en: Here, *K* attention heads independently compute different sets of attention
    coefficients and corresponding weighted sums.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这里，*K* 个注意力头部独立计算不同集合的注意力系数和相应的加权求和。
- en: The results from all heads are concatenated to form a richer, more expressive
    node representation.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有头部的结果被拼接起来，形成一个更丰富、更具表现力的节点表示。
- en: 'The following shows averaging for multihead attention in the final layer [1]:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了最终层中多头部注意力的平均值[1]：
- en: (4.6)
  id: totrans-406
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (4.6)
- en: '![figure](../Images/Equation-4-9.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/Equation-4-9.png)'
- en: In the final prediction layer of the network, instead of concatenating the outputs
    from different heads, we take their average. This reduces the dimensionality of
    the final output and simplifies the model’s prediction process.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络的最终预测层中，我们不是将不同头部的输出进行拼接，而是取它们的平均值。这降低了最终输出的维度，并简化了模型的预测过程。
- en: Summary
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A graph attention network (GAT) is a specialized type of graph neural network
    (GNN) that incorporates attention mechanisms to focus on the most relevant nodes
    during the learning process.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图注意力网络（GAT）是一种特殊的图神经网络（GNN），它结合了注意力机制，在学习过程中关注最相关的节点。
- en: GATs excel in domains where certain nodes have disproportionate importance,
    such as social networks, fraud detection, and anomaly detection.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GATs在节点具有不成比例重要性的领域表现出色，例如社交网络、欺诈检测和异常检测。
- en: The chapter uses a dataset derived from Yelp reviews, focusing on detecting
    fake reviews for hotels and restaurants in Chicago. Reviews are represented as
    nodes, with edges representing shared characteristics (e.g., common authors or
    businesses).
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章使用从Yelp评论中提取的数据集，专注于检测芝加哥的酒店和餐厅的虚假评论。评论被表示为节点，边表示共享特征（例如，共同作者或企业）。
- en: GATs were applied to this dataset to classify nodes (reviews) as fraudulent
    or legitimate. The GAT models showed improvements over baseline models such as
    logistic regression, XGBoost, and graph convolutional networks (GCNs).
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GATs被应用于此数据集，将节点（评论）分类为欺诈或合法。GAT模型在基线模型（如逻辑回归、XGBoost和图卷积网络（GCNs））上显示出改进。
- en: GATs are memory-intensive due to their need to compute attention scores for
    all edges. To handle this, mini-batching with the `NeighborLoader` class in PyTorch
    Geometric (PyG) was used.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于需要计算所有边的注意力分数，GATs内存密集。为了处理这个问题，使用了PyTorch Geometric（PyG）中的`NeighborLoader`类的mini-batching。
- en: The GAT layers in PyG, such as `GATConv` and `GATv2Conv`, apply different types
    of attention to graph learning problems.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyG中的GAT层，如`GATConv`和`GATv2Conv`，将不同类型的注意力应用于图学习问题。
- en: Strategies such as SMOTE and class reshuffling can be employed to address class
    imbalance. For our case, class reshuffling significantly improved model performance.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以采用如SMOTE和类别重排等策略来解决类别不平衡问题。在我们的案例中，类别重排显著提高了模型性能。
