- en: Chapter 2\. Understanding LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 理解LLMs
- en: So you want to become the LLM whisperer who unlocks the wealth of their knowledge
    and processing power with clever prompts? Well, to appreciate which kinds of prompts
    *are* clever and tease the right answer from the LLM, you first need to understand
    how LLMs process information―how they *think*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你想要成为LLM的“密语者”，用巧妙的提示解锁他们的知识和处理能力？好吧，为了欣赏哪些类型的提示是巧妙的，并从LLM中引出正确的答案，你首先需要了解LLMs如何处理信息——它们是如何*思考*的。
- en: In this chapter, we’ll approach this problem onion style. You’ll first see LLMs
    from the very outside as trained mimics of text in [“What Are LLMs?”](#ch02_what_are_llms_1728407258904985).
    You’ll learn how they split the text into bite-size chunks called tokens in [“How
    LLMs See the World”](#ch02_how_llms_see_the_world_1728407258905418), and you’ll
    learn about the fallout if they can’t easily accomplish that split.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将以洋葱式的方法来解决这个问题。你首先将在[“什么是LLMs？”](#ch02_what_are_llms_1728407258904985)中看到LLMs作为训练有素的文本模仿者。你将了解它们如何将文本分割成称为标记的小块，在[“LLMs如何看待世界”](#ch02_how_llms_see_the_world_1728407258905418)中，你将了解如果它们不能轻松完成这种分割会有什么后果。
- en: You’ll also find out how the token sequences are generated bit by bit in [“One
    Token at a Time”](#ch02_one_token_at_a_time_1728407258905818), and you’ll learn
    about the different ways to choose the next token in [“Temperature and Probabilities”](#ch02_temperature_and_probabilities_1728407258905997).
    Finally, in [“The Transformer Architecture”](#ch02_the_transformer_architecture_1728407258906064),
    you’ll delve into the very inner workings of an LLM, understand it as a collection
    of minibrains that communicate through a Q&A game called *attention*, and learn
    what that means for prompt order.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将了解到在[“一次一个标记”](#ch02_one_token_at_a_time_1728407258905818)中，标记序列是如何一点一点生成的，你将了解选择下一个标记的不同方法[“温度和概率”](#ch02_temperature_and_probabilities_1728407258905997)。最后，在[“Transformer架构”](#ch02_the_transformer_architecture_1728407258906064)中，你将深入了解LLM的内部运作，将其理解为一组通过名为*注意力*的问答游戏进行通信的微型大脑，并了解这对提示顺序意味着什么。
- en: During all that, please keep in mind that this is a book about *using* LLMs,
    not about LLMs themselves. So, there are a lot of cool technical details that
    we’re *not* mentioning because they’re not relevant for prompt engineering. If
    you want matrix multiplications and activation functions, you’ll need to turn
    elsewhere—the classic reference [The Illustrated Transformer](https://oreil.ly/9hGyN)
    is an excellent starting point for a deep dive. But we promise you won’t need
    that amount of technical background if all you want to do is write great prompts―so
    let’s dive into what you do need to know.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些过程中，请记住，这是一本关于*使用*LLMs的书，而不是关于LLMs本身。因此，有很多酷炫的技术细节我们没有提到，因为它们与提示工程无关。如果你想要矩阵乘法和激活函数，你需要去别处寻找——经典的参考书[《图解Transformer》](https://oreil.ly/9hGyN)是深入研究的一个很好的起点。但我们承诺，如果你只想编写出色的提示，你不需要那么多的技术背景——让我们深入了解你需要知道的内容。
- en: What Are LLMs?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是LLMs？
- en: 'At the most basic level, an *LLM* is a service that takes a string and returns
    a string: text in, text out. The input is called the *prompt*, and the output
    is called the *completion* or sometimes, the *response* (see [Figure 2-1](#ch02_figure_1_1728407258873233)).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在最基本层面上，*LLM*是一种服务，它接受一个字符串并返回一个字符串：输入是*提示*，输出是*完成*或有时，*响应*（参见[图2-1](#ch02_figure_1_1728407258873233)）。
- en: '![A white oval with black text  Description automatically generated](assets/pefl_0201.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![一个白色椭圆形，黑色文字  描述自动生成](assets/pefl_0201.png)'
- en: Figure 2-1\. An LLM taking the prompt “One, Two,” and presenting the completion
    “ Buckle My Shoe”
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 一个LLM接受提示“一，二”，并呈现完成“系我的鞋”
- en: When an untrained LLM first sees the light of day, its completions will look
    like a pretty random jumble of unicode symbols and bear no clear relationship
    to the prompt. It needs to be *trained* before it’s useful. Then, the LLM won’t
    just answer strings with strings but language with language.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个未经训练的LLM第一次见到光明时，它的完成将看起来像是一堆相当随机的unicode符号的混乱组合，与提示没有明显的关联。在它变得有用之前，它需要被*训练*。然后，LLM不会只是用字符串回答字符串，而是用语言回答语言。
- en: Training takes skill, compute, and time far beyond the scope of most project
    groups, so most LLM applications use off-the-shelf generalist models (known as
    *foundation models*) that are already trained (maybe after a bit of fine-tuning;
    see the sidebar). So, we don’t expect you to train an LLM yourself―but if you
    want to use an LLM, especially programmatically, it is essential for you to understand
    what it *has been trained* to do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 训练需要技能、计算和时间，这些远远超出了大多数项目组的范围，因此大多数LLM应用都使用现成的通用模型（称为**基础模型**），这些模型已经过训练（可能经过一些微调；参见侧边栏）。所以，我们并不期望你自己训练一个LLM——但如果你想使用LLM，尤其是以编程方式使用，了解它**已经被训练**做什么是至关重要的。
- en: LLMs are trained using a large set of documents (again, strings) known as the
    *training set*. The kind of documents depends on the purpose of the LLM (see [Figure 2-2](#ch02_figure_2_1728407258873266)
    for an example). The training set is often a mixture of different training inputs
    such as books, articles, conversations on platforms such as Reddit, and code on
    sites such as GitHub. From the training set, the model is supposed to learn how
    to produce output that looks just like the training set. Concretely, when the
    model receives a prompt that is the beginning of a document from its training
    set, the resulting completion should be the text that is most likely to continue
    the original document. In other words, models mimic.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）是通过使用一组称为**训练集**的大量文档（再次，字符串）进行训练的。文档的类型取决于LLM的目的（例如，参见[图2-2](#ch02_figure_2_1728407258873266)）。训练集通常是不同训练输入的混合，如书籍、文章、在Reddit等平台上的对话，以及GitHub等网站上的代码。从训练集中，模型应该学习如何产生看起来与训练集相似的输出。具体来说，当模型接收到一个提示，即其训练集中文档的开始部分时，产生的完成文本应该是最有可能继续原始文档的文本。换句话说，模型是模仿的。
- en: '![Points scored](assets/pefl_0202.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![得分点](assets/pefl_0202.png)'
- en: Figure 2-2\. Composition of [“The Pile”](https://oreil.ly/MbYsy), a popular
    open source training set comprising a mixture of factual prose, fictional prose,
    dialogues, and other internet content
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. [“The Pile”](https://oreil.ly/MbYsy)的组成，这是一个流行的开源训练集，包含事实性散文、虚构散文、对话和其他互联网内容的混合
- en: 'So, how’s an LLM different from, say, a big search engine index full of the
    training data? After all, a search engine would *ace* the task the LLM was trained
    with—given the beginning of a document, it could find a completion for that document
    with 100% accuracy. And yet, having a search engine that just parrots the training
    set isn’t the goal here: the LLM shouldn’t learn to recite the training set by
    heart but to apply the patterns it encounters there (in particular, logical and
    reasoning patterns) to complete any prompt, not just those from the training set.
    Mere rote memorization is considered a defect. Both the inner architecture of
    the LLM (which encourages it to abstract from concrete examples) and the training
    procedure (which tries to feed it diverse, nonrepetitive data and measures success
    on unseen data) are supposed to prevent this defect.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，LLM与一个充满训练数据的大型搜索引擎索引有何不同？毕竟，搜索引擎会**轻松完成**LLM训练的任务——给定文档的开始，它可以以100%的准确率找到该文档的完成文本。然而，仅仅拥有一个重复训练集的搜索引擎并不是目标：LLM不应该学习通过死记硬背来背诵训练集，而应该将它在其中遇到的模式（特别是逻辑和推理模式）应用到任何提示中，而不仅仅是训练集中的提示。单纯的死记硬背被认为是缺陷。LLM的内部架构（鼓励它从具体例子中抽象出来）和训练过程（试图给它提供多样化、非重复的数据，并在未见过的数据上衡量成功）都旨在防止这种缺陷。
- en: That prevention sometimes fails, and instead of learning facts and patterns,
    the model learns chunks of text by rote—which is known as *overfitting*. Large-scale
    overfitting should be rare in off-the-shelf models, but it’s worth being aware
    of the possibility that if an LLM seemingly solves a problem that it’s seen during
    training, it doesn’t necessarily mean that the LLM will do as well when confronted
    with a similar problem it hasn’t seen before.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 预防有时会失败，模型不是学习事实和模式，而是通过死记硬背学习文本块——这被称为**过拟合**。在现成的模型中，大规模过拟合应该是罕见的，但值得注意的可能性是，如果一个LLM似乎解决了它在训练期间看到的问题，这并不一定意味着LLM在面对它之前未见过的问题时表现会好。
- en: Nevertheless, after you work with LLMs for a while, you start to develop an
    intuition for how an LLM will behave based on the task it was trained on. So when
    you want to know how a given prompt might be completed, don’t ask yourself how
    a reasonable person would “reply” to the prompt but rather how a document that
    happens to start with the prompt might continue.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在你使用LLM一段时间后，你开始根据训练任务发展出对LLM行为的直觉。所以，当你想知道一个特定的提示可能如何被完成时，不要问自己一个合理的人会如何“回应”提示，而应该考虑一个恰好以提示开始的文档可能会如何继续。
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Assume you have picked a document from the training set at random. All you know
    about it is, it starts with the prompt. What is the statistically most likely
    continuation? That’s the LLM output you should expect.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你随机从训练集中选择了一份文档。你所知道的是，它以提示开始。最可能的后续是什么？这就是你应该期待的LLM输出。
- en: Completing a Document
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成文档
- en: 'Here’s an example of reasoning about document completions. Consider the following
    text:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于文档结尾推理的例子。考虑以下文本：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For a text that starts like this, what might be the statistically most likely
    completion?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个以这种方式开始的文本，统计上最可能的结尾可能是什么？
- en: '`y2ior3w`'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`y2ior3w`'
- en: '`Thursday.`'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`星期四。`'
- en: '`all.`'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`all.`'
- en: None of these completions are absolutely *impossible*. Sometimes, a cat runs
    over the keyboard and completion 1 is generated, and other times, a sentence gets
    garbled in rewriting and 2 appears. But by far the most likely continuation is
    3, and almost all LLMs will choose this continuation.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结尾都不是绝对**不可能**的。有时，猫会跑到键盘上，生成结尾1，有时，在重写过程中句子会混乱，出现结尾2。但最有可能的后续选项是3，几乎所有LLM都会选择这个选项。
- en: 'Let’s take completion 3 as given and run the LLM a bit further:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把结尾3作为既定事实，进一步运行LLM：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For a text that starts like that, what is the statistically most likely completion?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个以这种方式开始的文本，统计上最可能的结尾是什么？
- en: '`This is why I chose to settle down with a book tonight.`'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`这就是我今晚选择看书的原因。`'
- en: '`Shall we watch the game at your place instead?`'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`我们在你那里看比赛怎么样？`'
- en: '`\n`'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`\n`'
- en: '`\n`'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`\n`'
- en: '`First, try unplugging the TV from the wall and plugging it back in.`'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`首先，尝试将电视从墙上拔下来，然后再插回去。`'
- en: Well, it depends on the training set. Let’s say the LLM was trained on a dataset
    of narrative prose such as short stories, novels, magazines, and newspapers—in
    that case, completion *a*, about reading a book, sounds rather more likely than
    the others. While the sentence about the TV, followed by the question from completion
    *b*, could well appear somewhere in the middle of a story, a story wouldn’t open
    with this question without at least the starting quotation marks (“). So it’s
    unlikely that a model trained on short stories would predict option *b*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这取决于训练集。假设LLM是在一个包含叙事散文的数据集上训练的，比如短篇小说、小说、杂志和报纸——在这种情况下，关于阅读书籍的结尾a听起来比其他选项更有可能。虽然关于电视的句子，后面跟着结尾b中的问题，可能会出现在故事中间的某个地方，但一个故事不会在没有至少起始引号（“”）的情况下以这个问题开头。所以，一个基于短篇小说训练的模型不太可能预测选项b。
- en: 'But throw emails and conversation transcripts into the training set, and suddenly,
    option *b* appears very plausible. I made up both of them, though: it’s the third
    option that was produced by an actual LLM (OpenAI’s text-davinci-003, which is
    a variant of GPT-3), mimicking the advice and customer service conversations that
    abound in its training set.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果你把电子邮件和对话记录加入到训练集中，选项b看起来就非常可信了。尽管我编造了这两个选项：第三个选项是由实际的LLM（OpenAI的text-davinci-003，它是GPT-3的一个变体）生成的，它模仿了其训练集中大量存在的建议和客户服务对话。
- en: 'A theme is emerging here: the better you know the training data, the better
    the intuition you can form about the likely output of an LLM trained on that training
    data. Many commercial LLMs don’t publish their training data—choosing a good training
    set is a big part of the special sauce that makes their models successful. Even
    then, however, it’s usually possible to form some sensible expectations about
    the kind of documents the training set consists of.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现了一个主题：你越了解训练数据，你就能形成关于在那种训练数据上训练的LLM可能输出的更好直觉。许多商业LLM不发布他们的训练数据——选择一个好的训练集是使他们的模型成功的关键部分。即使如此，通常仍然可以形成一些关于训练集包含的文档类型的合理预期。
- en: Human Thought Versus LLM Processing
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人类思维与LLM处理
- en: 'The LLM selects the most likely looking continuation, and this goes against
    some assumptions humans make when reading text. That’s because when humans produce
    text, they do so as part of a process that involves more than producing plausible-looking
    text output. Let’s say you want to write a blog post about a podcast you came
    across at the podcasting site Acast. You might start writing the following: ``In
    their newest installment of `The rest is history`, they talk about the Hundred
    Years’ War (listen on acast at http://.`` Of course you don’t know the URL by
    heart, so this is the point where you stop writing and do a quick internet search.
    Hopefully, you find the correct link: shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords.
    Or maybe you can’t find it, in which case, you might go back and delete the whole
    bracket and replace it with `(episode unfortunately not available anymore).`'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 选择看起来最可能的后续内容，这与人类在阅读文本时做出的某些假设相矛盾。这是因为当人类产生文本时，他们这样做是作为一个涉及产生看似合理的文本输出的过程的一部分。假设你想写一篇关于你在播客网站
    Acast 上遇到的播客的博客文章。你可能开始写以下内容：``在他们最新的《历史的其余部分》中，他们谈论了百年战争（在 acast 上收听，http://.``
    当然，你可能并不记得 URL，所以这就是你停止写作并快速进行网络搜索的点。希望你能找到正确的链接：shows.acast.com/the-rest-is-history-podcast/episodes/321-hundred-years-war-a-storm-of-swords。或者，也许你找不到它，在这种情况下，你可能回去删除整个括号，并用
    `(episode unfortunately not available anymore)` 代替。
- en: The model can’t google or edit, so it just guesses.^([1](ch02.html#id333)) Nor
    will the raw LLM express any doubt,^([2](ch02.html#id334)) add a disclaimer that
    it was just guessing, or show any other trace of evidence that the information
    is merely a guess rather than actual knowledge—because after all, the model *always*
    guesses.^([3](ch02.html#id335)) This guess just happened to be made at a point
    where humans typically switch to a different mode of producing their text (googling
    rather than pressing the first keys that come to mind).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无法进行谷歌搜索或编辑，所以它只是猜测.^([1](ch02.html#id333)) 原始的 LLM 也不会表达任何怀疑，^([2](ch02.html#id334))
    添加一个免责声明说它只是在猜测，或者显示任何其他证据表明信息仅仅是猜测而不是实际知识——因为毕竟，模型 *总是* 在猜测.^([3](ch02.html#id335))
    这个猜测恰好发生在人类通常切换到产生文本的不同模式（谷歌搜索而不是按下第一个想到的键）的时刻。
- en: LLMs are really good at emulating any patterns they find in the items they guess
    about. After all, this is pretty much exactly what they were trained for. So if
    they make up a Social Security number, it’ll be a string of plausible digits,
    and if they make up the URL of a podcast, it’ll look like the URL of a podcast.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 真的是在模仿他们在猜测的项目中发现的任何模式方面非常出色。毕竟，这正是他们被训练的内容。所以，如果他们编造一个社会保障号码，它将是一串看似合理的数字，如果他们编造一个播客的
    URL，它看起来就像播客的 URL。
- en: 'In this case, I tried OpenAI’s text-curie-001, a small variant of GPT3, and
    this LLM completed the URL as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我尝试了 OpenAI 的 text-curie-001，这是 GPT3 的小型变体，这个 LLM 如下完成了 URL：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Is Dr. Martin Kemp a real person here? Maybe one who is involved with history
    podcasts? Maybe even the podcast we’re talking about? There is an art historian
    named Martin Kemp at Oxford, though whether the completion could refer to him
    sounds like a theory of language problem rather than an LLM question (see [Figure 2-3](#ch02_figure_3_1728407258873288)).
    At any rate, he didn’t talk about the Hundred Years’ War on the podcast *The Rest
    Is History*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 马丁·肯普博士在这里是不是一个真实的人？也许是一个与历史播客有关的人？甚至是我们正在谈论的播客？牛津有一个名叫马丁·肯普的艺术史学家，但这个完成是否可以指代他听起来更像是一个语言理论问题而不是
    LLM 问题（见[图 2-3](#ch02_figure_3_1728407258873288)）。无论如何，他在播客《历史的其余部分》中并没有谈论百年战争。
- en: '![A cartoon of a child at a computer  Description automatically generated](assets/pefl_0203.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![一个孩子在电脑前的卡通  自动生成的描述](assets/pefl_0203.png)'
- en: Figure 2-3\. People’s language reflects reality; models’ language reflects people
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 人们的语言反映了现实；模型的语言反映了人们
- en: Hallucinations
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 幻觉
- en: 'The fact that LLMs are trained as “training data mimic machines” has unfortunate
    consequences: *hallucinations,*^([4](ch02.html#id338)) which are factually wrong
    but plausible-looking pieces of information produced confidently by the model.
    They are a common problem when using LLMs, either ad hoc or within applications.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 被训练成“训练数据模仿机器”的事实带来了不幸的后果：*幻觉*，^([4](ch02.html#id338))，这是模型自信地产生的看似合理但实际上错误的信息片段。它们在使用
    LLM 时是一个常见问题，无论是临时使用还是在使用应用程序时。
- en: Since hallucinations don’t differ from other completions *from the perspective
    of the model*, prompt directives like “Don’t make stuff up” are of very limited
    use. Instead, the typical approach is to get the model to provide some background
    that can be checked. That could be an explanation of its reasoning,^([5](ch02.html#id340))
    a calculation that can be performed independently, a source link, or keywords
    and details that can be searched for. For example, it’s much harder to check the
    sentence “There was an English king who married his cousin,” than “There was an
    English king who married his cousin, namely George IV, who married Caroline of
    Brunswick.” The best antidote to hallucinations is “Trust but verify,” just minus
    the trust.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于从模型的角度来看，幻觉与其他完成内容没有区别，因此像“不要编造东西”这样的提示指令非常有限。相反，典型的方法是让模型提供一些可以检查的背景信息。这可能是对其推理的解释，^([5](ch02.html#id340))，可以独立进行的计算，一个来源链接，或者可以搜索的关键词和细节。例如，检查句子“有一个英格兰国王娶了他的表亲”比检查句子“有一个英格兰国王娶了他的表亲，即乔治四世，他娶了布鲁克郡的卡罗琳。”要困难得多。对抗幻觉的最佳方法就是“相信但核实”，只是没有信任。
- en: Hallucinations can also be induced. If your prompt references something that
    doesn’t exist, an LLM will typically continue to assume its existence. Documents
    that start out with wrong claims and then correct themselves halfway through are
    rare. So the model will typically assume its prompt to be true, and this is known
    as *truth bias*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉也可以被诱导。如果你的提示提到了不存在的东西，LLM通常会继续假设它的存在。开始就有错误主张然后中途纠正的文件是罕见的。所以，模型通常会假设其提示是真实的，这被称为*真实偏差*。
- en: You can make truth bias work for you—if you want the model to assess a hypothetical
    or counterfactual situation, there’s no need to say, “Pretend that it’s 2030 and
    Neanderthals have been resurrected.” Just begin with “It’s 2031, a full year since
    the first Neanderthals were resurrected.”
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以让真实偏差为你工作——如果你想让模型评估一个假设或反事实情况，你不需要说，“假装现在是2030年，尼安德特人已经被复活。”只需从“现在是2031年，自从第一个尼安德特人被复活已经整整一年了。”开始。
- en: Tip
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you have access to an LLM producing completions (i.e., the raw LLM, not wrapped
    in a chat interface like ChatGPT), this might be a good occasion to try out entering
    a couple of so-called *make-believe* prompts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够访问一个生成完整回答的LLM（即原始LLM，而不是像ChatGPT这样的聊天界面），这可能是一个尝试输入几个所谓的*虚构*提示的好机会。
- en: Like the example about resurrected Neanderthals in preceding text, make-believe
    prompts elicit answers to hypothetical questions not by asking the question outright
    but by implying that the hypothetical scenario actually came to pass.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前面提到的关于复活尼安德特人的例子一样，虚构提示通过暗示假设情景实际上已经发生，而不是直接提出问题来引发对假设问题的回答。
- en: Compare the suggestion with a chat LLM’s answer. How does it differ?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 将建议与聊天LLM的答案进行比较。它们有何不同？
- en: However, an LLM’s truth bias is also dangerous, particularly to programmatic
    applications. It’s all too easy to mess up in programmatic prompt creation and
    introduce counterfactual or nonsensical elements. A human might read through the
    prompt, put down the paper, raise their eyebrows at you, and go, “Really?” The
    LLM doesn’t have this option. It’ll do its best to pretend the prompt is real,
    and it’s unlikely to correct you. So you are responsible for giving it a prompt
    that doesn’t need correction.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLM的真实偏差也是危险的，尤其是对程序化应用来说。在程序化提示创建中出错并引入反事实或无意义元素真是太容易了。一个人可能会阅读提示，放下纸张，对你挑起眉毛，然后说，“真的吗？”LLM没有这个选项。它会尽力假装提示是真实的，而且不太可能纠正你。所以，你负责给出一个不需要纠正的提示。
- en: How LLMs See the World
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM如何看待世界
- en: 'In [“What Are LLMs?”](#ch02_what_are_llms_1728407258904985) you learned that
    LLMs consume and produce strings. It’s worth getting under the hood on this statement
    a bit: how do LLMs see strings? We’re used to thinking of strings as sequences
    of characters, but that’s not quite what the LLM sees. It can reason about characters,
    but that’s not a native ability, and it requires the equivalent of rather deep
    concentration on the part of the LLM—at the time of writing (autumn 2024), even
    the most advanced models can still be fooled by questions such as [“How many Rs
    in ‘strawberry''?”](https://oreil.ly/Lh3o0).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“什么是LLM？”](#ch02_what_are_llms_1728407258904985)中，你了解到LLM消耗和产生字符串。值得深入探讨这个说法：LLM如何看待字符串？我们习惯于将字符串视为字符序列，但LLM看到的并非如此。LLM可以推理字符，但这不是其原生能力，并且需要LLM进行相当深入的关注——截至写作时（2024年秋季），即使是最高级的模型也可能被诸如[“‘strawberry’中有多少个R？”](https://oreil.ly/Lh3o0)这样的问题所欺骗。
- en: 'Maybe it’s worth pointing out that *we* don’t really read strings in characters
    either. At a very early stage of human processing, they are grouped together into
    words. What we then read are the words, not the letters. That’s why we often read
    over typos without spotting them: they’re already corrected by our brain by the
    time they reach the conscious part of our processing.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 可能值得指出的是，*我们*实际上也不是按字符读取字符串的。在人类处理的最早期阶段，它们被组合成单词。我们阅读的是单词，而不是字母。这就是为什么我们经常在阅读时忽略错误：它们在我们处理过程的意识部分到达之前已经被我们的大脑纠正了。
- en: You can have lots of fun with purposely garbled sentences just at the edge of
    what your inner autocorrect function can cope with (see [Figure 2-4](#ch02_figure_4_1728407258873307),
    left). However, if you garble the text in a way that doesn’t respect word boundaries,
    your readers are going to have a very bad day (see [Figure 2-4](#ch02_figure_4_1728407258873307),
    right).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在你的自动更正功能能够处理的边缘故意打乱句子，这会很有趣（参见[图2-4](#ch02_figure_4_1728407258873307)，左侧）。然而，如果你以不尊重单词边界的方式打乱文本，你的读者可能会度过一个非常糟糕的一天（参见[图2-4](#ch02_figure_4_1728407258873307)，右侧）。
- en: '![A black background with a black square  Description automatically generated
    with medium confidence](assets/pefl_0204.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![黑色背景上的黑色方块  自动生成的中等置信度描述](assets/pefl_0204.png)'
- en: Figure 2-4\. Two ways of scrambling the same text
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。打乱相同文本的两种方式
- en: The left part of the figure leaves the word boundaries intact and scrambles
    the order of the letters within each word, while the right part leaves the order
    of the letters intact but changes the word boundaries. Most people find the left
    variant significantly easier to read.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图表的左侧部分保留了单词边界，同时打乱了每个单词内字母的顺序，而右侧部分保留了字母的顺序，但改变了单词边界。大多数人发现左侧的变体更容易阅读。
- en: Like humans, LLMs don’t read the single letters either. When you send a text
    to the model, it’s first broken down into a series of multiletter chunks called
    *token*s. They’re typically three to four characters long, but there are also
    longer tokens for common words or letter sequences. The set of tokens used by
    a model is called its *vocabulary*.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与人类一样，LLM也不阅读单个字母。当你向模型发送文本时，它首先被分解成一系列多字母块，称为*标记*。它们通常是三到四个字符长，但也有更长的标记用于常见单词或字母序列。模型使用的标记集合称为其*词汇表*。
- en: When reading a text, the model first passes it through a tokenizer that transforms
    it into a sequence of tokens. Only then is it passed to the LLM proper. Then,
    the LLM produces a series of tokens (represented internally as numbers), which
    is translated back to text before you get it back (see [Figure 2-5](#ch02_figure_5_1728407258873328)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当阅读文本时，模型首先将其通过标记器转换成一系列标记。然后，它才被传递给LLM本身。然后，LLM生成一系列标记（内部表示为数字），在将其转换回文本之前，这些标记被转换回文本（参见[图2-5](#ch02_figure_5_1728407258873328)）。
- en: '![A diagram of a computer code  Description automatically generated](assets/pefl_0205.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![计算机代码的图示  自动生成的描述](assets/pefl_0205.png)'
- en: Figure 2-5\. A tokenizer translating text into a sequence of numbers the LLM
    works on—and back
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。将文本转换为LLM工作的数字序列的标记器，并将其转换回文本
- en: Note that not all tokenizers include composite tokens starting with whitespace,
    but many do. Notable examples are [OpenAI’s tokenizers](https://oreil.ly/c1QgI).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，并非所有标记器都包括以空白字符开头的复合标记，但许多标记器确实如此。值得注意的例子是[OpenAI的标记器](https://oreil.ly/c1QgI)。
- en: LLMs see text as consisting of tokens, and humans see it as consisting of words.
    That makes it sound like LLMs and humans see text in a very similar way, but there
    are a few critical differences.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）将文本视为由标记（tokens）组成，而人类则将其视为由单词组成。这听起来好像LLMs和人类以非常相似的方式看待文本，但实际上存在一些关键差异。
- en: 'Difference 1: LLMs Use Deterministic Tokenizers'
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差异1：LLMs使用确定性标记器
- en: As humans, our translation of letters into words is fuzzy. We try to find a
    word that is the most similar to the letter sequence we see. On the other hand,
    LLMs use deterministic tokenizers―which make typos stand out like sore thumbs.
    The word ghost is a single token in OpenAI’s GPT tokenizer (a tokenizer that is
    used widely, not just for OpenAI’s models). However, the typo “gohst” is translated
    into a sequence of three tokens―g-oh-st—that’s obviously different, which makes
    it easy for the LLM to spot the typo. Nevertheless, LLMs are typically rather
    resilient against typos since they are used to them from their training set.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类，我们将字母翻译成单词是模糊的。我们试图找到一个与我们所看到的字母序列最相似的单词。另一方面，LLMs使用确定性标记器——这使得拼写错误像突出的痛处一样明显。在OpenAI的GPT标记器（一个广泛使用的标记器，不仅用于OpenAI的模型）中，单词“ghost”是一个单独的标记。然而，拼写错误“gohst”被翻译成三个标记的序列——g-oh-st——这显然不同，这使得LLM很容易发现这个错误。尽管如此，LLMs通常对拼写错误有很强的抵抗力，因为它们在训练集中已经习惯了这些错误。
- en: 'Difference 2: LLMs Can’t Slow Down and Examine Letters'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 差异2：LLMs无法减慢速度并检查字母
- en: We humans can slow down and consciously examine each letter individually, but
    an LLM can only use its built-in tokenizer (and it can’t slow down either). Many
    LLMs have learned from the training set what letters which token consists of,
    but this makes all syntactic tasks that require the model to break up or reassemble
    tokens much more difficult.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们人类可以减慢速度并自觉地逐个检查每个字母，但LLM只能使用其内置的标记器（而且它也无法减慢速度）。许多LLMs已经从训练集中学习了哪些标记由哪些字母组成，但这使得所有需要模型分解或重新组装标记的句法任务变得更加困难。
- en: There’s a good example of this in [Figure 2-6](#ch02_figure_6_1728407258873344),
    which depicts a ChatGPT conversation about reversing letters in words. Reversing
    the letters is a simple pattern manipulation, and LLMs are normally really good
    at that. But breaking apart and reassembling the tokens proves to be too difficult
    for the LLM, so both reversal and re-reversal are very far off.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-6](#ch02_figure_6_1728407258873344)中有一个很好的例子，它描绘了ChatGPT关于反转单词中字母的对话。反转字母是一种简单的模式操作，LLMs通常在这方面非常擅长。但是，分解和重新组装标记对于LLM来说太难了，因此反转和再次反转都偏离得很远。
- en: In the figure, both the initial reversal and the re-reversal are full of errors.
    The takeaway for you as application builder here is to avoid giving the model
    such tasks involving the subtoken level, if you can.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，初始反转和再次反转都充满了错误。对于你作为应用构建者来说，这里的一个启示是，如果可能的话，避免给模型分配涉及子标记级别的任务。
- en: Tip
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If the task you want the LLM to perform includes a component that requires the
    model to break tokens apart and reassemble them, consider whether you can take
    care of that component in pre- or post-processing.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望LLM执行的任务包括需要模型分解和重新组装标记的组件，考虑一下你是否可以在预处理或后处理中处理该组件。
- en: '![A screenshot of a chat Description automatically generated](assets/pefl_0206.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![聊天截图自动生成的描述](assets/pefl_0206.png)'
- en: Figure 2-6\. [ChatGPT trying and failing to reverse letters](https://oreil.ly/KKso8)
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. [ChatGPT尝试并失败反转字母](https://oreil.ly/KKso8)
- en: As an example of how to use the tip in the box, let’s say your application is
    using an LLM to play a game like Scattergories, in which the aim is to find examples
    with syntactic properties, like “prohibition activist starting with *W*,” “European
    country starting with *Sw*,” or “fruit with 3 occurrences of the letter R in its
    name.” Then, it might make sense for you to use your LLM as an oracle to obtain
    a large list of prohibition activists or European countries and then use syntactic
    logic to filter down that list. If you try to let the LLM shoulder the whole burden,
    you might encounter failings (see [Figure 2-7](#ch02_figure_7_1728407258873377)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 以盒中提示的使用为例，假设你的应用程序正在使用LLM来玩Scattergories这样的游戏，其目的是找到具有句法属性的例子，如“以*W*开头的禁运活动家”、“以*Sw*开头的欧洲国家”或“其名称中包含3个R字母的水果。”那么，使用你的LLM作为先知以获取大量禁运活动家或欧洲国家的名单，然后使用句法逻辑来筛选该名单可能是有意义的。如果你试图让LLM承担全部负担，你可能会遇到失败（见[图2-7](#ch02_figure_7_1728407258873377)）。
- en: Note that the model in the figure is not deterministic, and it fails in two
    different ways (see the [first](https://oreil.ly/yIIkg) and [second attempts](https://oreil.ly/PfywQ)).
    Note also that [ Sweden], [ Switzerland], and [ Somalia] are all individual tokens
    in ChatGPT’s tokenizer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图中的模型不是确定性的，并且以两种不同的方式失败（见[第一次尝试](https://oreil.ly/yIIkg)和[第二次尝试](https://oreil.ly/PfywQ)）。还要注意，[瑞典](https://oreil.ly/yIIkg)、[瑞士](https://oreil.ly/PfywQ)和[索马里](https://oreil.ly/PfywQ)在ChatGPT的标记器中都是独立的标记。
- en: '![A screenshot of a phone  Description automatically generated](assets/pefl_0207.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![手机截图  自动生成的描述](assets/pefl_0207.png)'
- en: Figure 2-7\. ChatGPT having trouble identifying countries starting with Sw
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-7. ChatGPT在识别以Sw开头的国家时遇到困难
- en: 'Difference 3: LLMs See Text Differently'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3个区别：LLMs对文本的感知不同
- en: The final difference we want to highlight is that we humans have an intuitive
    understanding of many aspects of tokens and letters. In particular, we *see* them,
    so we know which letters are round and which are square. We understand ASCII art
    because we see it (although many models will have learned a substantial amount
    of ASCII art by heart). For us, a letter with an accent on it is just a variant
    of the same letter, and we have no great díffícúlty ígnóríng thém whílé réádíng
    á téxt whéré théy ábóúnd. On the other hand, the model, even if it manages, will
    have to use a significant amount of its processing power, leaving less for the
    actual application you have in mind.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要强调的最后一个区别是，我们对许多方面标记和字母的直观理解。特别是，我们*看到*它们，因此我们知道哪些字母是圆形的，哪些是方形的。我们理解ASCII艺术，因为我们看到了它（尽管许多模型已经通过记忆学习到了大量的ASCII艺术）。对我们来说，带有重音的字母只是相同字母的变体，我们在阅读包含它们的文本时没有遇到太大的困难。另一方面，即使模型能够做到，它也必须使用大量的处理能力，这会减少你打算实际应用的部分。
- en: A particular case here is capitalization. Consider [Figure 2-8](#ch02_figure_8_1728407258873396).
    Why has this simple task goed… I mean… *gone* so badly? Keeping the pitfalls of
    tokenization in mind, you might try to hazard a guess yourself before you read
    on.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个特殊情况是首字母大写。考虑[图2-8](#ch02_figure_8_1728407258873396)。为什么这个简单的任务…我的意思是…*失败了*？考虑到标记化的陷阱，在你继续阅读之前，你可能自己尝试猜测一下。
- en: '![A computer screen with a sign  Description automatically generated](assets/pefl_0208.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![带有标志的计算机屏幕  自动生成的描述](assets/pefl_0208.png)'
- en: Figure 2-8\. Asking OpenAI’s text-babbage-001 model to translate a text to all
    caps
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8. 要求OpenAI的text-babbage-001模型将文本翻译为大写
- en: This produces some funny and typical mistakes—note that we are using a very
    small model for demonstration purposes, and larger models are not usually caught
    out quite as easily as this.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了一些有趣且典型的错误——请注意，我们在这里使用一个非常小的模型进行演示，而较大的模型通常不会像这样轻易被抓住。
- en: For humans, the capital letter *A* is just a variant of the lowercase *a* for
    humans, but the tokens that contain the capital letter are very different from
    the tokens that contain the lowercase letter. This is something the models are
    very aware of since they have seen plenty of training data about it. They know
    that the token *For* after a period is very similar to the token *for* in the
    middle of a sentence.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类来说，大写字母*A*只是小写字母*a*的变体，但对于包含大写字母的标记与包含小写字母的标记非常不同。这是模型非常清楚的事情，因为它们已经看到了大量的关于这方面的训练数据。它们知道句号后的标记*For*与句子中间的标记*for*非常相似。
- en: However, most tokenizers do not make it easy for models to learn these connections
    since capitalized tokens don’t always correspond one-to-one to noncapitalized
    ones. For example the GPT tokenizer translates “strange new worlds” as `[str][ange][
    new][ worlds]`, which is four tokens. But in all caps, the tokenization goes `[STR][ANGE][
    NEW][ WOR][L][DS]`, which is six tokens. Similarly, the word *gone* is a single
    token, while `[G][ONE]` are two.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，大多数标记器并没有让模型容易学习这些联系，因为大写标记并不总是与未大写的标记一一对应。例如，GPT标记器将“strange new worlds”翻译为`[str][ange][
    new][ worlds]`，这是四个标记。但在全大写的情况下，标记化变为`[STR][ANGE][ NEW][ WOR][L][DS]`，这是六个标记。同样，单词*gone*是一个标记，而`[G][ONE]`是两个。
- en: Better LLMs are better at dealing with these capitalization matters, but it’s
    still work for them that detracts from the real meat of your problem, which likely
    isn’t capitalization. (You don’t need an LLM to capitalize text after all!) So
    the wise prompt engineer will try to avoid burdening the models overmuch by having
    the LLM translate between capitalizations all the time.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的 LLM 在处理这些大小写问题方面更擅长，但这仍然是对它们工作的干扰，这可能会影响你真正问题的核心，而这可能不是大小写问题。（毕竟，你不需要 LLM
    来在文本后添加大小写！）因此，明智的提示符工程师会尽量避免过度负担模型，避免让 LLM 不断在大小写之间进行翻译。
- en: Counting Tokens
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算标记数
- en: You can’t mix and match tokenizers and models. Every model uses a fixed tokenizer,
    so it’s well worth understanding your model’s tokenizer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能混合使用分词器和模型。每个模型都使用一个固定的分词器，因此了解你模型的分词器是非常有价值的。
- en: When writing an LLM application, you’ll probably want to be able to run the
    tokenizer while prompt engineering, using a library such as [Hugging Face](https://oreil.ly/6Jfhy)
    or [tiktoken](https://oreil.ly/y9N7j). However, the most common application of
    your tokenizer will be more mundane than complex token boundary analysis. You’ll
    most often use the tokenizer just for counting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写 LLM 应用程序时，你可能会希望在提示符工程期间运行分词器，使用像 [Hugging Face](https://oreil.ly/6Jfhy)
    或 [tiktoken](https://oreil.ly/y9N7j) 这样的库。然而，你分词器最常见的应用将比复杂的标记边界分析更为平凡。你通常会使用分词器仅仅是为了计数。
- en: 'That’s because the number of tokens determines *how long* your text is, from
    the perspective of the model. That includes all aspects of length: how much time
    the model will spend reading through the prompt scales roughly linearly with the
    number of tokens in the prompt. Also, how much time it spends creating the solution
    scales linearly with the number of tokens produced. Ditto for the computational
    cost: how much computational power a prediction requires scales with its length.
    That’s why most model-as-a-service offerings charge per token produced or processed.
    At the time of writing, a dollar would normally buy you between 50,000 and 1,000,000
    output tokens, depending on the model.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为标记数决定了从模型的角度来看文本的 *长度*。这包括所有方面的长度：模型在阅读提示符时花费的时间大致与提示符中的标记数成线性关系。同样，它创建解决方案所花费的时间也与其产生的标记数成线性关系。计算成本也是如此：预测所需的计算能力与其长度成正比。这就是为什么大多数模型即服务提供商会按产生的或处理的标记数收费。在撰写本文时，一美元通常可以购买
    50,000 到 1,000,000 个输出标记，具体取决于模型。
- en: Finally, the number of tokens is what counts for the question of the *context
    window*—the amount of text the LLM can handle at any given time. That’s a limitation
    of all modern LLMs that we’re going to revisit again and again throughout this
    book.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标记数是关于 *上下文窗口* 问题的答案——LLM 在任何给定时间可以处理的文本量。这是所有现代 LLM 的一个限制，我们将在整本书中反复回到这个问题。
- en: 'The LLM doesn’t just take any text and produce any text. It takes a text with
    a number of tokens that’s smaller than the *context window size,* and its completion
    is such that the prompt plus the completion cannot have more tokens than the context
    window size either. Context window sizes are typically measured in thousands of
    tokens, and that’s nothing to sneeze at, in theory: it’s several, often dozens,
    and sometimes hundreds of pages of A4 size. But practice tends to sneeze at it
    nevertheless: however long your context window, you’ll be tempted to fill it and
    overfill it, so you need to count tokens to stop that from happening.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 并不是随意接收任何文本并生成任何文本。它接收的文本包含的标记数小于 *上下文窗口大小*，其生成的文本使得提示符加上生成的文本的标记数也不会超过上下文窗口大小。上下文窗口大小通常以千个标记来衡量，这在理论上并不是微不足道的事情：它相当于几页、通常是几十页，有时甚至几百页的A4纸大小。但实践中往往会被忽视：无论你的上下文窗口有多长，你都会倾向于填满它，甚至超负荷填满它，因此你需要计算标记数来防止这种情况发生。
- en: There is no general formula for translating the number of characters to the
    number of tokens. It depends on the text and on the tokenizer. The very common
    GPT tokenizer linked above has about four characters per token when tokenizing
    an English natural language text. That’s pretty typical, although newer tokenizers
    can be slightly more efficient (i.e., they can have more characters per token,
    on average).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将字符数转换为标记数没有通用的公式。这取决于文本和分词器。上面提到的非常常见的 GPT 分词器在分词英文自然语言文本时，每个标记大约有四个字符。这相当典型，尽管新的分词器可能稍微更有效率（即，它们可以有更多的字符每个标记，平均而言）。
- en: Most tokenizers are optimized for English^([6](ch02.html#id367)) and will be
    less efficient for other languages, meaning they’ll have fewer characters per
    token. Random strings of digits are even less efficient, clocking in at a little
    over two characters per token. It’s even worse for random alphanumeric strings
    like cryptographic keys, which usually have less than two characters per token.
    Strings with rare characters will have the least number of characters per token—for
    instance, the unicode smiley, ☺, actually has two tokens.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数分词器针对英语^([6](ch02.html#id367)) 进行优化，对于其他语言将效率较低，这意味着它们每个标记的字符数会更少。随机数字字符串甚至效率更低，每个标记大约有超过两个字符。对于像加密密钥这样的随机字母数字字符串，每个标记的字符会更少，通常每个标记不到两个字符。包含罕见字符的字符串每个标记的字符数最少——例如，unicode笑脸☺实际上有两个标记。
- en: Note
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Most LLMs use vocabularies with at least a couple of special tokens: most commonly,
    at least an end-of-text token, which in training is appended to each training
    document so that the model learns when it’s over. Whenever the model outputs that
    token, the completion is cut off at that point.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数LLM使用至少包含几个特殊标记的词汇表：最常见的是至少一个文本结束标记，在训练中，该标记被附加到每个训练文档上，以便模型学习何时结束。每当模型输出该标记时，完成就会在那个点截断。
- en: One Token at a Time
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逐个标记
- en: Let’s peel another layer off the onion―the last one before we come to the core.
    Under the hood, the LLM isn’t directly text to text, and it’s not really directly
    tokens to tokens either. It’s *multiple* tokens to a single token. The model is
    just constantly repeating the operation to get the next token, accumulating these
    single tokens as long as needed to get a proper text out.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再剥去洋葱的另一层——在我们到达核心之前的那一层。在底层，LLM不是直接从文本到文本，也不是真正直接从标记到标记。它是 *多个* 标记对应一个标记。模型只是不断地重复操作以获取下一个标记，只要需要就积累这些单个标记，以生成适当的文本。
- en: Auto-Regressive Models
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自回归模型
- en: A single pass through the LLM gives you the statistically most likely next token.^([7](ch02.html#id370))
    Then, this token is pasted onto the prompt, and the LLM makes another pass to
    get the statistically most likely next token *given the new prompt,*^([8](ch02.html#id371))
    and so on (see [Figure 2-9](#ch02_figure_9_1728407258873413)). Such a process
    that makes its predictions one token at a time, with the next prediction depending
    on the previous predictions, is called *autoregressive*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过LLM的单次遍历，你可以得到统计上最可能的下一个标记.^([7](ch02.html#id370)) 然后，这个标记被粘贴到提示中，LLM再次遍历以获取基于新提示的统计上最可能的下一个标记
    *，*^([8](ch02.html#id371)) 以此类推（见图2-9）。这种一次预测一个标记的过程，下一个预测依赖于前一个预测，被称为 *自回归*。
- en: You know how when you write text on your phone, you can get three-word suggestions
    above your keyboard? Running an LLM is like repeatedly pressing the middle button.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道当你用手机写文本时，你可以在键盘上方得到三个单词的建议吗？运行LLM就像反复按中间按钮一样。
- en: 'This regular, almost monotonous pattern of one token every step points to a
    big difference between LLMs generating text and humans typing text: while we may
    stop and check, think, or reflect, the model needs to produce one token every
    step. The LLM doesn’t get extra time if it needs to think longer,^([9](ch02.html#id372))
    and it can’t stall.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这种每一步一个标记的规律、几乎单调的模式指向了LLM生成文本和人类打字文本之间的一大差异：虽然我们可能会停下来检查、思考或反思，但模型需要每一步都产生一个标记。如果LLM需要更长时间思考，它不会得到额外的时间，^([9](ch02.html#id372))
    而且它不能停滞。
- en: '![A screenshot of a computer  Description automatically generated](assets/pefl_0209.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述由系统自动生成](assets/pefl_0209.png)'
- en: Figure 2-9\. LLMs generating their response one token at a time
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9\. LLM逐个标记生成响应
- en: And once it’s put out a token, the LLM is committed to that token. The LLM can’t
    backtrack and erase the token. It also won’t issue corrections where it states
    that what it output previously is incorrect, because it’s not been trained on
    documents where mistakes get taken back explicitly in the text—after all, the
    humans who wrote those documents *can* backtrack and correct the mistakes at the
    places where they occur, so explicit takebacks are very rare in finished documents.
    Oh wait, actually, *takebacks* is more commonly spelled as two words, so let me
    write explicit take backs instead.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦它发布了令牌，LLM 就会坚持这个令牌。LLM 不能回溯并删除令牌。它也不会发布更正，声称之前输出的内容是错误的，因为它没有在文本中明确指出错误被撤回的文档上进行过训练——毕竟，撰写这些文档的人类可以在错误发生的地方回溯并更正错误，所以明确的撤回在完成的文档中非常罕见。哦，等等，实际上，“撤回”这个词更常见的是用两个词拼写，所以让我写“明确的撤回”吧。
- en: This trait can make LLMs appear stubborn and somewhat ridiculous, when they
    keep exploring a path that obviously makes no sense. But really, what this means
    is that, when necessary, such mistake recognition and backtracking capability
    needs to be supplied *by the application designer:* you.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特性可能会让LLM看起来固执且有些荒谬，当它们继续探索显然没有意义的路径时。但事实上，这意味着，当需要时，这种错误识别和回溯能力需要由应用程序设计者提供：你。
- en: Patterns and Repetitions
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式和重复
- en: Another issue with autoregressive systems is that they can fall into their own
    patterns. LLMs are good at recognizing patterns, so they sometimes (by chance)
    create a pattern and can’t find a good point to leave it. After all, *given the
    pattern*, at any given token, it’s more likely that it continues than that it
    breaks. This leads to very repetitive solutions (see [Figure 2-10](#ch02_figure_10_1728407258873441)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自动回归系统另一个问题是它们可能会陷入自己的模式。LLM 很擅长识别模式，所以它们有时（偶然）会创建一个模式，但找不到一个好的点来停止。毕竟，*给定模式*，在任何给定的令牌处，它继续的可能性比它中断的可能性更大。这导致非常重复的解决方案（见[图2-10](#ch02_figure_10_1728407258873441)）。
- en: '![A black screen with white text  Description automatically generated](assets/pefl_0210.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![一个黑色屏幕上带有白色文字的图片  描述自动生成](assets/pefl_0210.png)'
- en: Figure 2-10\. A list of reasons produced by OpenAI’s text-curie-001 model (an
    older model chosen for demonstration purposes, since newer models rarely fall
    into the repetition trap quite as awkwardly)
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10\. OpenAI的text-curie-001模型（为了演示目的选择了一个较老的模型，因为较新的模型很少会如此尴尬地陷入重复陷阱）生成的理由列表
- en: 'In the figure, an LLM has produced a list of reasons for liking a TV show.
    How many patterns can you spot? Here are the ones we found:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，一个LLM产生了一个喜欢电视剧的理由列表。你能发现多少个模式？以下是我们找到的模式：
- en: The items are consecutively numbered statements, each of which fits on one line.
    That seems desirable.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条目是连续编号的陈述，每个都适合一行。这似乎是理想的。
- en: They all start with “The,” which seems tolerable.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们都以“The”开头，这似乎是可以容忍的。
- en: They are of the form “X is Y and Z.” That’s annoying because it endangers correctness.
    What if there is no appropriate Z? The model might invent one. However, it stops
    after item 5.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的形式是“X是Y和Z”。这很烦人，因为它会危及正确性。如果没有合适的Z怎么办？模型可能会发明一个。然而，它在第5个条目后停止。
- en: After several items in a row started with “The franchise,” they all did. That’s
    stupid.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在连续几个条目都以“The franchise”开头之后，它们都这样做了。这也太愚蠢了。
- en: Toward the end, *legacy*, *following*, *future*, *foundation*, and *fanbase*
    are repeated ad nauseam. That’s stupid too.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到了最后，*遗产*、*追随*、*未来*、*基础*和*粉丝群*被反复提及。这也太愚蠢了。
- en: The list goes on and on and never stops. That’s because after each item, it’s
    more likely that the list will continue than that this will be the last item.
    And the model doesn’t get bored.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表一个接一个，永无止境。这是因为在每个条目之后，列表继续的可能性比这是最后一个条目的可能性更大。而且模型不会感到无聊。
- en: Toward the end, *legacy*, *following*, *future*, *foundation*, and *fanbase*
    are repeated ad nauseam. That’s stupid too.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到了最后，*遗产*、*追随*、*未来*、*基础*和*粉丝群*被反复提及。这也太愚蠢了。
- en: The list goes on and on and never stops. That’s because after each item, it’s
    more likely that the list will continue than that this will be the last item.
    And the model doesn’t get bored.^([10](ch02.html#id375))
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 列表一个接一个，永无止境。这是因为在每个条目之后，列表继续的可能性比这是最后一个条目的可能性更大。而且模型不会感到无聊。[10](ch02.html#id375)
- en: The way to deal with such repetitive solutions is typically to simply detect
    and filter them out. Another way is to randomize the output a bit. We’ll talk
    about randomization of output in the next section.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 处理此类重复解决方案的方法通常是简单地检测并过滤掉它们。另一种方法是稍微随机化输出。我们将在下一节中讨论输出的随机化。
- en: Temperature and Probabilities
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 温度和概率
- en: In the previous section, you learned that the LLM computes the most likely token.
    But if you peel back one more layer of the onion that is the LLM, it turns out
    that actually, it computes the probability of *all possible tokens* before choosing
    a single one. The process under the hood that chooses the actual token is called
    *sampling* (see [Figure 2-11](#ch02_figure_11_1728407258873458)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你了解到 LLM 计算最可能的标记。但如果你再剥去 LLM 的另一层洋葱，你会发现实际上，它在选择单个标记之前会计算所有可能标记的概率。选择实际标记的过程称为
    *抽样*（见[图 2-11](#ch02_figure_11_1728407258873458)）。
- en: '![A computer code on a black background  Description automatically generated](assets/pefl_0211.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![黑色背景上的计算机代码  自动生成的描述](assets/pefl_0211.png)'
- en: Figure 2-11\. The sampling process in action
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-11\. 抽样过程实际操作
- en: Note that the LLM doesn’t just compute the most likely token; it computes the
    likelihood of all the tokens.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，LLM 不仅计算最可能的标记；它计算所有标记的可能性。
- en: Many models will share these probabilities with you. The model typically returns
    them as *logprobs* (i.e., the natural logarithms of the token’s probability).
    The higher the logprob, the more likely the model considers this token to be.
    Logprobs are never bigger than 0 because a logprob of 0 would mean that the model
    is certain that this is the next token. Expect the most likely token to have a
    logprob between –2 and 0 (see [Figure 2-12](#ch02_figure_12_1728407258873476)).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型会与你共享这些概率。模型通常将它们作为 *logprobs* 返回（即标记概率的自然对数）。logprob 越高，模型认为这个标记的可能性就越大。logprob
    从不大于 0，因为 logprob 为 0 意味着模型确定这是下一个标记。预期最可能的标记的 logprob 在 –2 和 0 之间（见[图 2-12](#ch02_figure_12_1728407258873476)）。
- en: '![A screenshot of a computer  Description automatically generated](assets/pefl_0212.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  自动生成的描述](assets/pefl_0212.png)'
- en: Figure 2-12\. An example API call requesting logprobs and extracting the logprobs
    of the chosen completion
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-12\. 一个请求 logprobs 并提取所选完成项的 logprobs 的 API 调用示例
- en: 'Note that in the figure, setting the request parameter `logprobs` to `3` means
    that the logprobs for the three most likely tokens will be returned. However,
    you may not always want the *most* *likely* token. Especially if you have a way
    of automatically testing your completions, you may want to generate a couple of
    alternatives and throw out the bad ones. The typical way to do this is by using
    a *temperature* greater than 0\. The temperature is a number of at least zero
    that determines how “creative” the model should be. More specifically, if the
    temperature is greater than 0, the model will give a stochastic completion, where
    it selects the most likely token with the highest probability but maybe also returns
    less likely but still not totally absurd tokens. The higher the temperature and
    the closer the logprobs of the best tokens are to each other, the more likely
    it is that the second-best-placed token will be selected, or even the third or
    fourth or fifth. The exact formula is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在图中，将请求参数 `logprobs` 设置为 `3` 表示将返回三个最可能标记的 logprobs。然而，你可能并不总是想要 *最* *可能*
    的标记。特别是如果你有自动测试你的完成项的方法，你可能想要生成几个替代方案并丢弃不好的那些。通常的做法是使用大于 0 的 *温度*。温度是一个至少为零的数字，它决定了模型应该有多“有创意”。更具体地说，如果温度大于
    0，模型将给出一个随机完成，其中它选择概率最高的最可能标记，但也可能返回不太可能但仍非完全荒谬的标记。温度越高，最佳标记的 logprobs 越接近，第二好的标记被选中的可能性就越大，甚至第三、第四或第五。确切公式如下：
- en: $p left-parenthesis normal t normal o normal k normal e normal n Subscript i
    Baseline right-parenthesis equals StartFraction exp left-parenthesis normal l
    normal o normal g normal p normal r normal o normal b Subscript i Baseline slash
    t right-parenthesis Over sigma-summation Underscript j Endscripts exp left-parenthesis
    normal l normal o normal g normal p normal r normal o normal b Subscript j Baseline
    slash t right-parenthesis EndFraction$
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: $p_{left-parenthesis normal t normal o normal k normal e normal n Subscript
    i Baseline right-parenthesis} = StartFraction exp left-parenthesis normal l normal
    o normal g normal p normal r normal o normal b Subscript i Baseline slash t right-parenthesis}
    Over sigma-summation Underscript j Endscripts exp left-parenthesis normal l normal
    o normal g normal p normal r normal o normal b Subscript j Baseline slash t right-parenthesis}
    EndFraction$
- en: 'Let’s look at possible temperatures and when you should choose each one:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看可能的温度以及何时应该选择每个温度：
- en: '0'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '0'
- en: You want the most likely token. No alternatives. This is the recommended setting
    when correctness is paramount. Additionally, running the LLM at temperature 0
    is close to deterministic,^([11](ch02.html#id379)) and in some applications, repeatability
    is an advantage.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望得到最可能的标记，没有其他选择。这是当正确性至关重要时的推荐设置。此外，在温度0下运行LLM几乎接近确定性，^([11](ch02.html#id379))在某些应用中，可重复性是一个优势。
- en: 0.1–0.4
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 0.1–0.4
- en: If there’s an alternative token that’s only slightly less likely than the front-runner,
    you want some small chance for that to be picked. A typical use case is that you
    want to generate a small number of different solutions (for example, because you
    know how to filter out the best one). Or maybe you just want one completion but
    a more colorful, creative solution than what you expect at temperature 0.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个替代标记只比领先者略不可能，你希望有很小机会被选中。一个典型的用例是你想要生成少量不同的解决方案（例如，因为你知道如何过滤掉最好的一个）。或者也许你只是想要一个完成结果，但比在温度0时预期的更加丰富多彩、富有创意。
- en: 0.5–0.7
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 0.5–0.7
- en: You want a greater impact of chance on the solution, and you are fine with getting
    completions that are “inaccurate” in the sense that sometimes, a token will be
    chosen even though the model thinks another alternative is clearly more likely.
    The typical use case is if you want a large number of independent solutions, likely
    10 or more.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望随机性对解决方案的影响更大，并且你接受得到一些“不准确”的完成结果，也就是说，有时即使模型认为另一个选择更可能，也会选择一个标记。典型的用例是你想要大量独立的解决方案，可能是10个或更多。
- en: '1'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '1'
- en: You want the token distribution to mirror the statistical training set distribution.
    Assume, for example, that your prefix is “One, Two,” and in the training set,
    this is followed by the token `[ Buck]` in 51% of cases and by `[ Three]` in 31%
    of cases (and the model has been trained well enough to pick that up). If you
    run the model several times at temperature 1, then 51% of the time, you’ll get
    `[ Buck]`, and 31% of the time, you’ll get `[ Three].`
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望标记分布与统计训练集分布相匹配。例如，假设你的前缀是“一，二”，在训练集中，51%的情况下后面跟着标记 `[ Buck]`，31%的情况下后面跟着
    `[ Three]`（并且模型已经训练得足够好，能够捕捉到这一点）。如果你在温度1的情况下运行模型多次，那么51%的时间你会得到 `[ Buck]`，31%的时间你会得到
    `[ Three]`。
- en: '1'
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '1'
- en: You want a text that’s “more random” than the training set. This means the model
    is less likely to pick the “standard” continuation than the typical document from
    the training set and more likely to pick a “particularly weird” continuation than
    the typical document from the training set.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你希望得到的文本比训练集“更随机”。这意味着模型不太可能选择训练集中典型文档的“标准”后续内容，而更有可能选择比训练集中典型文档“特别奇怪”的后续内容。
- en: High temperatures can make LLMs sound like they’re drunk. Over the course of
    long generations at temperatures greater than 1, the error rate usually gets worse
    over time. The reason is that temperature affects only the very last layer of
    computation when probabilities are turned into output, so it doesn’t affect the
    main part of the LLM’s processing that computes those probabilities in the first
    place. So the model recognizes the errors in the text it just generated as a pattern,
    and it tries to mimic that pattern by generating its own errors. Then the high
    temperature causes even more errors on top of that (see [Figure 2-13](#ch02_figure_13_1728407258873494)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 高温度会使LLM听起来像喝醉了。在温度大于1的长篇生成过程中，错误率通常会随着时间的推移而变差。原因是温度只影响将概率转换为输出的计算的最后几层，因此它不影响LLM处理的主要部分，即首先计算这些概率的部分。因此，模型将刚刚生成的文本中的错误识别为一种模式，并试图通过生成自己的错误来模仿这种模式。然后，高温度会在上面造成更多的错误（参见[图2-13](#ch02_figure_13_1728407258873494)）。
- en: '![A close-up of a text  Description automatically generated](assets/pefl_0213.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![文本的特写  自动生成的描述](assets/pefl_0213.png)'
- en: Figure 2-13\. High temperature affecting LLMs a bit like alcohol affects humans
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-13. 高温对LLM的影响有点像酒精对人类的影响
- en: The figure shows this deterioration at high temperatures, where the generation
    of item 3 starts out error prone but legible and ends in a state where even the
    individual words are unrecognizable. Note that each item in the figure has been
    sampled at an increasingly high temperature from OpenAI’s text-davinci-003.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示了高温下的这种退化，其中第3个项目的生成开始时容易出错但可辨认，最终处于一个甚至单个单词都难以辨认的状态。请注意，图中的每个项目都是从OpenAI的text-davinci-003以越来越高的温度采样的。
- en: Let’s return to the example of the model writing a list. A typical list in text
    stops at a few items, say 3, or 4, or 5\. If it’s a longer list, then 10 is the
    next most obvious stopping point. After each new line, it can either continue
    the list by producing the number that’s next up as the next token, or it can declare
    itself done with the list by producing a second new line (or something else entirely,
    maybe).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到模型编写列表的例子。文本中的典型列表通常在几个项目后停止，比如说3个、4个或5个。如果列表更长，那么10个就是下一个最明显的停止点。在每一行之后，它可以通过产生下一个数字作为下一个标记来继续列表，或者通过产生第二个新行（或其他完全不同的东西）来声明列表结束。
- en: At temperature 0, the LLM will always choose the option it considers more likely
    for this line. Often, that means it will always continue, at least after it’s
    passed the last obvious stopping point. At temperature 1, if the LLM makes the
    judgment that a continuation has probability *x*, then it will only continue with
    probability *x*. So, over the course of many items, it’s likely that the LLM will
    end the list sooner or later, with an expected length similar to the length of
    lists in the training set. In general, it’s a trade-off (see [Table 2-1](#ch02_table_1_1728407258884202)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在温度0时，LLM将始终选择它认为更可能的选项。通常这意味着它将始终继续，至少在它通过最后一个明显的停止点之后。在温度1时，如果LLM判断继续的概率为*x*，那么它将以概率*x*继续。因此，在许多项目的过程中，LLM最终可能会早点或晚点结束列表，预期的长度与训练集中列表的长度相似。总的来说，这是一个权衡（见[表2-1](#ch02_table_1_1728407258884202)）。
- en: Table 2-1\. The advantages of the different temperature regimes
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1. 不同温度制度的好处
- en: '| High temperature | Low temperature |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 高温 | 低温 |'
- en: '| --- | --- |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| + More alternatives. | + More correct solutions. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| + 更多替代方案。 | + 更多正确解决方案。 |'
- en: '| + Many properties of generations (e.g., list length) have the same distribution
    as in the training set. | + More replicable (deterministic). |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| + 生成（例如，列表长度）的许多属性与训练集具有相同的分布。 | + 更可复现（确定性）。 |'
- en: There are other ways of sampling, most notably *beam search*, which tries to
    account for the fact that choosing a particular token that looks likely can make
    the next choice hard because no good follow-on token exists. Beam search accomplishes
    this by looking ahead for the next few tokens and making sure that a likely sequence
    exists. This can lead to more accurate solutions, but it’s less often used in
    applications because of its much higher time and compute cost.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他采样方法，最著名的是*beam search*，它试图解释选择看起来可能性的特定标记可能会使下一个选择变得困难，因为没有好的后续标记存在。beam
    search通过向前查看接下来的几个标记并确保存在一个可能的序列来完成这一点。这可能导致更准确的结果，但由于其更高的时间和计算成本，它较少在应用中使用。
- en: The Transformer Architecture
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformer 架构
- en: It’s time to cut away the final layer of the onion and look at the LLM’s brain
    directly. You peel it back and see….it’s not one brain at all. It’s thousands
    of minibrains. All are identical in structure, and each one is performing a very
    similar task. There’s a minibrain sitting atop each token in the sequence, and
    together, these minibrains make up the *transformer*, which is the architecture
    used by all modern LLMs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候剥去洋葱的最后一层，直接查看LLM的大脑了。你把它剥开，看到……它根本不是一个大脑。它是成千上万的微型大脑。它们在结构上完全相同，每个都在执行一个非常相似的任务。每个标记上方都有一个微型大脑，这些微型大脑共同构成了*transformer*，这是所有现代LLM使用的架构。
- en: 'Each minibrain starts out by being told which token it’s sitting on and its
    position in the document. The minibrain keeps thinking about this for a fixed
    number of steps, known as *layers*. During this time, it can receive information
    from the minibrains to the left. The minibrain’s task is to understand the document
    from the perspective of its location, and it uses this understanding in two ways:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 每个微型大脑首先被告知它所在的标记以及它在文档中的位置。微型大脑会持续思考固定数量的步骤，这被称为*层*。在这段时间里，它可以接收来自左侧微型大脑的信息。微型大脑的任务是从其位置的角度理解文档，并且它使用这种理解以两种方式：
- en: In all steps before the last one, it shares some of its intermediate results
    with the minibrains to its right. (We’ll discuss this in more detail later.)
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一个步骤之前的所有步骤中，它将其一些中间结果与右侧的微型大脑共享。（我们将在稍后更详细地讨论这一点。）
- en: For the last step, it’s asked to make a prediction of what the token immediately
    to its right would be.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于最后一步，它被要求预测其右侧的标记将是什么。
- en: 'Every minibrain goes through the same process of computing and sharing intermediate
    results and then making a guess. In fact, the minibrains are clones of each other:
    their processing logic is the same, and all that differs is the inputs: which
    token they start with and which intermediate results they get told of by the minibrains
    to their left.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 每个微型大脑都会经历相同的计算和共享中间结果的过程，然后做出猜测。事实上，微型大脑是彼此的克隆：它们的处理逻辑是相同的，唯一不同的是输入：它们开始的标记以及左侧微型大脑告诉它们的中间结果。
- en: But the reason they go through these steps is different. The minibrain at the
    very last token, at the very right, runs to predict the next token. What it shares
    from its intermediate result isn’t important because there are no brains to the
    right that listen, but all the other minibrains are the other way around. Their
    purpose is to share their intermediate results with the brains to their right,
    and what predictions they make about the tokens directly to their right doesn’t
    matter because the tokens to *their* immediate right are already known.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 但他们经历这些步骤的原因是不同的。在最后一个标记的非常右侧，微型大脑运行以预测下一个标记。它从中间结果中共享的内容并不重要，因为没有大脑在右侧倾听，但所有其他微型大脑都是相反的。它们的目的是将它们的中间结果与右侧的大脑共享，并且它们对直接右侧的标记所做的预测并不重要，因为*它们*直接右侧的标记已经知道了。
- en: 'When the rightmost token makes its prediction, the autoregression from [“One
    Token at a Time”](#ch02_one_token_at_a_time_1728407258905818) kicks in: it spits
    out the new token, and a brand new minibrain is set on top of it to refine its
    understanding of what’s going on at its position for a fixed number of layers.
    After that, it predicts the next token. Rinse and repeat―or rather, cache and
    repeat because this calculation will be used over and over again for every subsequent
    token in the prompt and the generated completion.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 当最右侧的标记做出预测时，从[“一次一个标记”](#ch02_one_token_at_a_time_1728407258905818)开始的自动回归开始：它吐出新的标记，并在其上方放置一个新的微型大脑，以固定数量的层来细化对其位置上发生的事情的理解。之后，它预测下一个标记。重复这个过程——或者更确切地说，缓存并重复，因为这种计算将被反复用于提示和生成的完成中的每个后续标记。
- en: 'An example of this algorithm is shown in [Figure 2-14](#ch02_figure_14_1728407258873523),
    where each column represents one minibrain and how its state changes over time.
    In the example, you’ve just asked the model to complete “One, Two,” and ultimately,
    you’ll end up with the two tokens `[ Buck]` and `[le].` Let’s follow the transformer
    as it arrives at that response. There’s a minibrain sitting on each of the four
    input tokens: `[One]`, `[,]`, `[Two]`, and `[,]` (the last of which is the second
    appearance of the same token). Each of them thinks for four layers,^([12](ch02.html#id385))
    consecutively refining its understanding of the text the tokens are processing.
    In each step, they are updated from the tokens to the left about what they’ve
    learned so far. Each of them computes a guess for what the token to its right
    might be.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法的一个例子在[图2-14](#ch02_figure_14_1728407258873523)中显示，其中每一列代表一个微型大脑及其状态随时间的变化。在示例中，你刚刚要求模型完成“一，二”，最终你将得到两个标记
    `[ Buck]` 和 `[le]`。让我们跟随变压器，看看它是如何到达那个响应的。每个四个输入标记 `[One]`、`[,]`、`[Two]` 和 `[,]`（最后一个是相同标记的第二次出现）上都有一个微型大脑。每个微型大脑都会连续思考四层，依次细化对正在处理的文本的理解。在每一步中，它们都会根据左侧标记所学的信息进行更新。每个微型大脑都会计算一个猜测，猜测其右侧的标记可能是什么。
- en: 'The first couple of guesses are for tokens that are still part of the prompt:
    `[One]`, `[,]`, `[Two]`, and `[,]`. We already know the prompt, so the guesses
    are just thrown away. But then, the model arrives at the completion, and there,
    the guess is the whole point. So the next guess is turned into a prediction, which
    is the token `[ Buck]`. A new minibrain is commissioned to be placed above that
    token, going through its four steps and arriving at the prediction `[le]`. If
    you continue the completion, a further minibrain will be planted atop `[le]`,
    and so on.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 前几个猜测是为了仍然是提示部分标记的token：`[One]`，`[,]`，`[Two]`，和`[,]`。因为我们已经知道了提示，所以这些猜测就被扔掉了。但是，当模型到达完成阶段时，那里的猜测就是整个重点。所以下一个猜测就变成了预测，即token
    `[Buck]`。一个新的minibrain被委托放置在那个token之上，经过其四个步骤，到达预测 `[le]`。如果你继续完成，还会在 `[le]` 之上放置另一个minibrain，以此类推。
- en: '![A diagram of a cloud computing system   Description automatically generated](assets/pefl_0214.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![云计算系统图   自动生成的描述](assets/pefl_0214.png)'
- en: Figure 2-14\. The inner workings of the model producing one token—later layers
    are drawn on top of previous layers
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14。产生一个token的模型内部工作原理——后续层绘制在先前层之上
- en: 'Now, let’s go back and talk about the “intermediate results” that are shared
    among the minibrains. The way they are shared is known as the attention mechanism—it’s
    the central innovation of the transformer architecture for LLMs (as mentioned
    in [Chapter 1](ch01.html#ch01_1_introduction_to_prompt_engineering_1728408393615260)).
    Attention is a way of passing information among the minibrains. Of course, there
    may be thousands of minibrains, and every one of them might know something of
    interest to every other one. To keep this information exchange from descending
    into chaos, it needs to be very tightly regulated. Here’s how it works:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到minibrain之间共享的“中间结果”上来。它们共享的方式被称为注意力机制——这是LLM的transformer架构的核心创新（如[第1章](ch01.html#ch01_1_introduction_to_prompt_engineering_1728408393615260)中提到的）。注意力是在minibrain之间传递信息的一种方式。当然，可能有成千上万的minibrain，每个都可能知道对其他每个都感兴趣的一些东西。为了防止这种信息交换陷入混乱，它需要非常严格地管理。以下是它是如何工作的：
- en: Each minibrain has some things it wants to know, so it submits a couple of questions,
    in the hope they might get answered by another minibrain. Let’s say that one minibrain
    sits upon the token `[my]`. The minibrain would like to know who that might refer
    to, so a reasonable question would be to ask, “Who is talking?”
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个minibrain都有一些它想知道的事情，所以它会提交几个问题，希望它们可能被另一个minibrain回答。假设有一个minibrain位于标记 `[my]`
    上。这个minibrain想知道这可能指的是谁，所以一个合理的问题可能是询问，“谁在说话？”
- en: Each minibrain has some things it can share, so it submits a couple of items,
    in the hope they might be useful to another minibrain. Let’s say one minibrain
    sits upon the token `[Susan]`, and it’s already learned before that this token
    is the last word of an introduction, like “Hello, I’m Susan.” So in case it might
    help another minibrain down the line, it will submit the information, “The person
    talking right now is Susan.”
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个minibrain都有一些它可以分享的东西，所以它会提交几个项目，希望它们可能对另一个minibrain有用。假设有一个minibrain位于标记
    `[Susan]` 上，并且它之前已经学会了这个token是介绍中的最后一个词，比如“Hello, I’m Susan。”所以，如果这可能帮助到后续的另一个minibrain，它将提交信息，“现在正在说话的人是Susan。”
- en: Now, every question is matched up with its best-fitting answer. “Who is talking?”
    matches up very well with “The person talking right now is Susan.”
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，每个问题都会与其最佳答案匹配。“谁在说话？”与“现在正在说话的人是Susan”匹配得非常好。
- en: The best-fitting answer to each question is revealed to the minibrain that asked
    the question, so the minibrain at the token `[my]` gets told “The person talking
    right now is Susan.” Of course, while the minibrains from this example talk to
    each other in English, in reality, they use a “language” that consists of long
    vectors of numbers^([13](ch02.html#id386)) and that is unique to every LLM, since
    it’s something the LLM “invents” during training.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个问题的最佳答案都会被询问问题的minibrain得知，因此位于标记 `[my]` 的minibrain被告知“现在正在说话的人是Susan”。当然，虽然这个例子中的minibrain用英语交谈，但在现实中，它们使用的是一种由长数字向量组成的“语言”，这种语言对每个LLM都是独特的，因为这是LLM在训练期间“发明”的^([13](ch02.html#id386))。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Information only ever flows from the left to the right.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 信息始终只从左到右流动。
- en: Information only ever flows from the bottom to the top.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 信息始终只从底部流向顶部。
- en: 'In modern LLMs, this Q&A mechanism obeys one more constraint, which is called
    *masking*: not *all* minibrains can answer a question; only the ones to the *left*
    of the minibrain asking the question can answer it. And a minibrain never gets
    told whether its answer was used, so the brains on the right can never influence
    the ones to the left.^([14](ch02.html#id388))'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代 LLM 中，这种问答机制遵循一个额外的约束，称为 *掩码*：不是 *所有* 小脑都可以回答问题；只有那些在询问问题的脑部 *左边* 的小脑可以回答。而且，小脑永远不会被告知其答案是否被使用，所以右边的脑部永远不会影响左边的小脑。[14](ch02.html#id388)
- en: That flow has some practical consequences. For example, to compute the state
    of one minibrain at one layer, the model only needs the states to the left (earlier
    minibrains at this layer) and below (the same minibrain at earlier layers). That
    means some of the computation can go in parallel—and this is one of the reasons
    generative transformers are so efficient to train. At each point in time, the
    already computed stages form a triangle (see [Figure 2-15](#ch02_figure_15_1728407258873541)).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种流动有一些实际的影响。例如，为了计算某一层的一个小脑的状态，模型只需要左边的状态（这一层的较早的小脑）和下面的状态（较早层中的相同小脑）。这意味着一些计算可以并行进行——这也是生成式变换器易于训练的原因之一。在每一个时间点，已经计算的阶段形成一个三角形（参见[图
    2-15](#ch02_figure_15_1728407258873541)）。
- en: '![A diagram of a computer  Description automatically generated with medium
    confidence](assets/pefl_0215.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![计算机图示  描述由 medium confidence 自动生成](assets/pefl_0215.png)'
- en: Figure 2-15\. Calculating the inner state of an LLM
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-15\. 计算 LLM 的内部状态
- en: In the figure, first (at the upper left), only the lowest layer at the first
    token can be computed. Next (at the upper middle), both the second-lowest layer
    at the first token and the lowest layer at the second token can be computed. One
    step later (at the upper right), the third layer can be computed at the first
    token, the second layer at the second token, and the first layer at the third
    token…all the way until all states are computed and a new token can be sampled.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中，首先（在左上角），只能计算第一个标记的第一个最低层。接下来（在上中），第一个标记的第二低层和第二个标记的最低层都可以计算。再下一步（在右上角），第一个标记可以计算第三层，第二个标记可以计算第二层，第三个标记可以计算第一层……一直计算到所有状态都被计算出来，并且可以采样一个新的标记。
- en: Parallelism allows speedup, but that way of computing in a triangle breaks down
    when the model switches from reading the prompt to creating the completion. The
    model has to wait until a token has been processed to the very end before choosing
    the next token and computing the very first state of the new minibrain. This is
    why LLMs are much faster at reading through a long prompt than they are at generating
    a long completion. Speed scales with both the number of tokens processed and the
    number of tokens generated, but prompt tokens are about an order of magnitude
    faster.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 并行性可以加速，但当模型从读取提示转换为创建补全时，这种在三角形中计算的方式就会崩溃。模型必须等待一个标记被处理到最末端，然后才能选择下一个标记并计算新小脑的第一个状态。这就是为什么
    LLM 在阅读长提示时比生成长补全要快得多。速度随着处理的标记数和生成的标记数而增加，但提示标记的速度大约快一个数量级。
- en: 'This triangle structure reflects a general “backward-and-downward” direction
    of vision for the LLM, or maybe a better way to understand it is “backward-and-dumbward”:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这种三角形结构反映了 LLM 的一般“向后向下”的视觉方向，或者也许更好的理解方式是“向后向下”：
- en: Backward
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 向后
- en: The minibrains can only ever look to their left. They can look as far back as
    they want, but never forward. That’s what people refer to when they call GPT or
    other LLMs *unidirectional* transformers. No information ever travels from a minibrain
    on the right to a minibrain on the left. That makes generative transformers easy
    to train and to run, but it has huge ramifications for how they process information.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 小脑只能向左看。它们可以看多远就多远，但永远不会向前看。这就是人们称 GPT 或其他 LLM 为 *单向* 变换器的原因。从右边的脑部到左边的脑部永远不会传递信息。这使得生成式变换器易于训练和运行，但这对它们处理信息的方式产生了巨大的影响。
- en: Downward (“dumbward”)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 向下（“向下”）
- en: 'The minibrains get their answers in a layer only from minibrains in the same
    layer before those get their answers for this layer. This means that any “chain
    of reasoning” in layer *i* can only be *i* reasoning steps deep, if we count the
    thinking the minibrain does in every layer as one reasoning step. But there’s
    no way for a minibrain to provide an insight gleaned at a later layer to a minibrain
    at a lower level for further processing. No way, that is, except one: while the
    LLM is generating text, the result of the very highest layer—the token—is produced,
    and it forms the very basis for the first layer of the next minibrain. This thinking
    aloud is the only way the model can let information flow from higher layers to
    lower layers‒it churns it around in its head, so to say. Reminiscent of the saying,
    “How could I know what I’m thinking before I’ve heard what I’m saying,” this principle
    forms the basis of chain-of-thought prompting (see [Chapter 8](ch08.html#ch08_01_conversational_agency_1728429579285372)).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 小型大脑只从同一层中的其他小型大脑那里获取答案，在它们为这一层提供答案之前。这意味着层 *i* 中的任何“推理链”只能有 *i* 个推理步骤深，如果我们把每个层中小型大脑的思考算作一个推理步骤的话。但是，小型大脑没有方法将后来层中获得的洞察提供给较低层的小型大脑以进行进一步处理。没有方法，也就是说，除了一个：当大型语言模型生成文本时，最高层——标记的——结果被产生，它构成了下一个小型大脑第一层的坚实基础。这种自言自语是模型唯一能让信息从高层流向低层的方式——它在其脑海中反复思考，可以说。这与俗语“在我听到我说的话之前，我怎么知道我在想什么”有异曲同工之妙，这一原则构成了思维链提示（见第8章）的基础。
- en: 'Let’s look at an example. How many words does the paragraph directly above
    contain? If you’re anything like me, you’ll not actually bother to count, and
    you’ll expect the authors to just tell you. Very well, we will: it’s 173\. But
    for the sake of argument, you could have looked up and counted them for yourself,
    right?'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个例子。上面的段落包含多少个单词？如果你像我一样，你实际上不会费心去数，你期望作者直接告诉你。很好，我们会：它是173个。但为了辩论，你也可以自己查找并数一数，对吧？
- en: We asked ChatGPT this question by feeding it this chapter up to and including
    the question “How many words does the paragraph directly above contain?” It answered,
    `The paragraph directly above contains 348 words.` Not only is it off, it’s terribly,
    hopelessly off. Far too many words for that paragraph, but far too few for the
    whole text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过给ChatGPT提供这一章的内容，包括问题“上面的段落包含多少个单词？”来询问这个问题。它回答道，`上面的段落包含348个单词。` 不仅不准确，而且错误得令人绝望。那个段落有太多单词，但整个文本又太少。
- en: But of course, we’re demanding something incredibly hard from the LLM here.
    Humans would do better.^([15](ch02.html#id394)) They can read through the text
    again and maintain an inner counter. That doesn’t work for the LLM because it
    only reads over the text once and can’t look back. So while the minibrains are
    processing the paragraph for the one and only time, they don’t know that the critical
    feature they should isolate is word count, because that request appears below
    the chapter’s text. They’re busy considering semantic implications, tone and style,
    and a myriad of surface features, and they’re not giving their full attention
    to the one thing that will turn out to matter.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然，我们对LLM的要求非常苛刻。人类会做得更好。[15](ch02.html#id394) 他们可以再次阅读文本并保持一个内心的计数器。这对LLM不起作用，因为它只阅读一次文本，无法回顾。所以当小型大脑在处理段落时，它们不知道它们应该隔离的关键特征是单词数，因为那个请求出现在章节文本下方。它们正忙于考虑语义含义、语气和风格以及无数表面特征，并且没有将全部注意力集中在最终会变得重要的事情上。
- en: That’s why order is critical for prompt engineering—it can easily make the difference
    between a prompt that works and one that fails. Indeed, when I asked the word
    count question at the beginning instead…well, ChatGPT still didn’t get the answer
    right because counting is hard for LLMs. But at least it came much closer, claiming
    173\. In [Chapter 6](ch06.html#ch06a_assembling_the_prompt_1728442733857948),
    we’ll return to that theme of the ordering of the different parts of your prompt.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，顺序对于提示工程至关重要——它可以轻易地决定一个提示是否有效或失败。确实，当我一开始就问单词数问题时……嗯，ChatGPT仍然没有给出正确的答案，因为对LLM来说计数很困难。但至少它接近了，声称有173个。在第6章中，我们将回到你提示的不同部分的排序这个主题。
- en: Tip
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you want to know whether a capability is realistic for an LLM to handle,
    ask yourself this question:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道一个能力是否适合大型语言模型处理，问问自己这个问题：
- en: Could a human expert who knows all the relevant general knowledge by heart complete
    the prompt in a single go without backtracking, editing, or note-taking?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够记住所有相关通用知识的人类专家能否一次性完成提示，而不需要回溯、编辑或做笔记？
- en: Conclusion
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: We discussed four central facts in this chapter. First, LLMs are document completion
    engines. Second, they mimic the documents they have seen during training. Third,
    LLMs produce one token at a time, with no option to pause or edit previous tokens.
    And finally, LLMs read through the text once, from beginning to end. Let’s see
    how these facts translate into a general prompt engineering paradigm in the next
    chapter.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中讨论了四个核心事实。首先，LLM是文档补全引擎。其次，它们模仿它们在训练期间看到的文档。第三，LLM一次产生一个标记，没有暂停或编辑先前标记的选项。最后，LLM从头到尾读取文本。让我们看看这些事实如何转化为下一章中的一般提示工程范式。
- en: ^([1](ch02.html#id333-marker)) The model can’t google *directly*, at least,
    but it can be connected to systems that can google. We will discuss this form
    of tool use in [Chapter 8](ch08.html#ch08_01_conversational_agency_1728429579285372).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch02.html#id333-marker)) 模型不能直接谷歌，至少目前是这样，但它可以连接到可以谷歌的系统。我们将在[第8章](ch08.html#ch08_01_conversational_agency_1728429579285372)中讨论这种工具使用的形式。
- en: ^([2](ch02.html#id334-marker)) In the next chapter, we’ll introduce some ways
    in which raw LLMs are aligned or improved in post training and how these can add
    the ability to express doubt. However, this is not a native capacity of the basic
    LLM structure, which is the focus of this chapter.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch02.html#id334-marker)) 在下一章中，我们将介绍一些原始LLM在训练后如何对齐或改进以及这些改进如何增加表达怀疑的能力。然而，这并不是基本LLM结构的原生能力，这是本章的重点。
- en: ^([3](ch02.html#id335-marker)) It’s true that the model can predict some parts
    with high certainty and some with low certainty. For example, it will be much
    more certain in predicting the next word of “John F. Kennedy was killed in the
    year,” than it would be certain in predicting the next word of “Zacharias B. Fulltrodd
    was killed in the year.” It reads a lot about the former death, while the second
    one, which is made up, could have taken place in any year. However, that uncertainty
    does not correlate with an expression of uncertainty or doubt in the training
    set—the model will fully buy into the assumption that there is a text that starts
    talking about Zacharias B. Fulltrodd’s death. It has no reason to believe that
    this text is any more unreliable in relation to Zacharias’s death than the typical
    JFK-related text it came across in its training set is in relation to JFK’s death.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch02.html#id335-marker)) 的确，模型可以以高概率预测某些部分，以低概率预测其他部分。例如，在预测“John F. Kennedy在一年中被杀”的下一个词时，它将比预测“Zacharias
    B. Fulltrodd在一年中被杀”的下一个词更有把握。它对前者的死亡了解很多，而后者是虚构的，可能发生在任何一年。然而，这种不确定性并不与训练集中表达的不确定性或怀疑相关——模型将完全接受存在一个关于Zacharias
    B. Fulltrodd死亡的文本的假设。它没有理由相信这个文本在Zacharias的死亡方面比它在训练集中遇到的典型JFK相关文本在JFK的死亡方面更不可靠。
- en: ^([4](ch02.html#id338-marker)) Although the closest human analog to what’s going
    on is probably the psychological phenomenon of confabulation, rather than hallucination.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch02.html#id338-marker)) 尽管最接近人类的现象可能是虚构的心理现象，而不是幻觉。
- en: ^([5](ch02.html#id340-marker)) You can check this by making a second query to
    the LLM. See [Chapter 7](ch07.html#ch07_taming_the_model_1728407187651669).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch02.html#id340-marker)) 你可以通过对LLM进行第二次查询来检查这一点。参见[第7章](ch07.html#ch07_taming_the_model_1728407187651669)。
- en: ^([6](ch02.html#id367-marker)) This is because English is the most frequently
    used language in most training datasets, and tokenizers are normally optimized
    to have a good compression rate on the training set.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch02.html#id367-marker)) 这是因为在大多数训练数据集中，英语是最常用的语言，分词器通常被优化以在训练集上具有良好的压缩率。
- en: ^([7](ch02.html#id370-marker)) This is true at least as long as you keep the
    temperature parameter to 0\. We’ll discuss temperature > 0 in the next section.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch02.html#id370-marker)) 只要你将温度参数保持在0，这一点就是正确的。我们将在下一节讨论温度>0的情况。
- en: ^([8](ch02.html#id371-marker)) At least, it’s equivalent to a completely new
    pass. It’s not literally a completely new pass from a computational perspective.
    For example, the prompt will typically be processed only once to save work.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch02.html#id371-marker)) 至少，它相当于一个全新的遍历。从计算的角度来看，这并不是字面上的全新遍历。例如，提示通常只处理一次以节省工作量。
- en: ^([9](ch02.html#id372-marker)) There’s interesting research going on to offer
    more flexibility in taking more time when needed, so maybe that will change.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch02.html#id372-marker)) 正在进行有趣的研究，以在需要时提供更多灵活性，因此这可能会改变。
- en: ^([10](ch02.html#id375-marker)) I maintain that a sufficiently careful reading
    of *The Silmarillion* would reveal that it’s boredom, in fact, that’s the real
    gift Ilúvatar’s Younger Children should treasure above all others.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch02.html#id375-marker)) 我认为，如果足够仔细地阅读《精灵宝钻》，会发现无聊实际上才是伊露维塔的年轻后代应该比其他任何东西都珍视的真正礼物。
- en: ^([11](ch02.html#id379-marker)) But it’s not completely deterministic, because
    of random rounding errors. Computed probabilities can (depending on the model)
    vary by several percentage points on reruns, so what the most likely token is
    can change.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch02.html#id379-marker)) 但这并非完全确定，因为随机舍入误差的存在。计算出的概率（取决于模型）在重新运行时可能会变化几个百分点，因此最可能的标记可能会改变。
- en: ^([12](ch02.html#id385-marker)) We only draw four layers to illustrate the point,
    but real-world LLMs usually have tens of layers. GPT-3 has 96, and newer models
    (like GPT-4) tend to have over 100\.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch02.html#id385-marker)) 我们只画了四层来阐述这个观点，但现实世界的LLM通常有数十层。GPT-3有96层，而新的模型（如GPT-4）往往超过100层。
- en: ^([13](ch02.html#id386-marker)) See [“The Illustrated Transformer”](https://oreil.ly/UXKOt).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch02.html#id386-marker)) 请参阅[《图解Transformer》](https://oreil.ly/UXKOt)。
- en: ^([14](ch02.html#id388-marker)) This wasn’t the case in the original transformer
    architecture, but it has become the norm for text-generating LLMs.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch02.html#id388-marker)) 在原始的Transformer架构中并非如此，但已成为文本生成型LLM的规范。
- en: ^([15](ch02.html#id394-marker)) And of course, classical computer code would
    be best.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch02.html#id394-marker)) 当然，经典的计算机代码会是最好的。
