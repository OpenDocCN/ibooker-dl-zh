- en: 2 Pretrained networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 预训练网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容包括
- en: Running pretrained image-recognition models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行预训练图像识别模型
- en: An introduction to GANs and CycleGAN
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANs和CycleGAN简介
- en: Captioning models that can produce text descriptions of images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够生成图像文本描述的字幕模型
- en: Sharing models through Torch Hub
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Torch Hub分享模型
- en: We closed our first chapter promising to unveil amazing things in this chapter,
    and now it’s time to deliver. Computer vision is certainly one of the fields that
    have been most impacted by the advent of deep learning, for a variety of reasons.
    The need to classify or interpret the content of natural images existed, very
    large datasets became available, and new constructs such as convolutional layers
    were invented and could be run quickly on GPUs with unprecedented accuracy. All
    of these factors combined with the internet giants’ desire to understand pictures
    taken by millions of users with their mobile devices and managed on said giants’
    platforms. Quite the perfect storm.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一章结束时承诺在这一章中揭示令人惊奇的事物，现在是时候兑现了。计算机视觉无疑是深度学习的出现最受影响的领域之一，原因有很多。存在对自然图像进行分类或解释内容的需求，非常庞大的数据集变得可用，以及发明了新的构造，如卷积层，并且可以在GPU上以前所未有的准确性快速运行。所有这些因素与互联网巨头希望理解数百万用户使用移动设备拍摄的图片，并在这些巨头平台上管理的愿望相结合。简直是一场完美的风暴。
- en: We are going to learn how to use the work of the best researchers in the field
    by downloading and running very interesting models that have already been trained
    on open, large-scale datasets. We can think of a pretrained neural network as
    similar to a program that takes inputs and generates outputs. The behavior of
    such a program is dictated by the architecture of the neural network and by the
    examples it saw during training, in terms of desired input-output pairs, or desired
    properties that the output should satisfy. Using an off-the-shelf model can be
    a quick way to jump-start a deep learning project, since it draws on expertise
    from the researchers who designed the model, as well as the computation time that
    went into training the weights.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何使用该领域最优秀研究人员的工作，通过下载和运行已经在开放的大规模数据集上训练过的非常有趣的模型。我们可以将预训练的神经网络看作类似于一个接受输入并生成输出的程序。这样一个程序的行为由神经网络的架构和训练过程中看到的示例所决定，以期望的输入-输出对或输出应满足的期望属性。使用现成的模型可以快速启动深度学习项目，因为它利用了设计模型的研究人员的专业知识，以及用于训练权重的计算时间。
- en: 'In this chapter, we will explore three popular pretrained models: a model that
    can label an image according to its content, another that can fabricate a new
    image from a real image, and a model that can describe the content of an image
    using proper English sentences. We will learn how to load and run these pretrained
    models in PyTorch, and we will introduce PyTorch Hub, a set of tools through which
    PyTorch models like the pretrained ones we’ll discuss can be easily made available
    through a uniform interface. Along the way, we’ll discuss data sources, define
    terminology like *label*, and attend a zebra rodeo.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索三种流行的预训练模型：一种可以根据内容标记图像的模型，另一种可以从真实图像中制作新图像，以及一种可以使用正确的英语句子描述图像内容的模型。我们将学习如何在PyTorch中加载和运行这些预训练模型，并介绍PyTorch
    Hub，这是一组工具，通过这些工具，像我们将讨论的预训练模型这样的PyTorch模型可以通过统一接口轻松提供。在这个过程中，我们将讨论数据来源，定义术语如*标签*，并参加斑马竞技表演。
- en: If you’re coming to PyTorch from another deep learning framework, and you’d
    rather jump right into learning the nuts and bolts of PyTorch, you can get away
    with skipping to the next chapter. The things we’ll cover in this chapter are
    more fun than foundational and are somewhat independent of any given deep learning
    tool. That’s not to say they’re not important! But if you’ve worked with pretrained
    models in other deep learning frameworks, then you already know how powerful a
    tool they can be. And if you’re already familiar with the generative adversarial
    network (GAN) game, you don’t need us to explain it to you.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是从其他深度学习框架转到PyTorch，并且宁愿直接学习PyTorch的基础知识，您可以跳到下一章。本章涵盖的内容比基础知识更有趣，而且与任何给定的深度学习工具有一定的独立性。这并不是说它们不重要！但是，如果您在其他深度学习框架中使用过预训练模型，那么您已经知道它们可以是多么强大的工具。如果���已经熟悉生成对抗网络（GAN）游戏，那么我们不需要向您解释。
- en: We hope you keep reading, though, since this chapter hides some important skills
    under the fun. Learning how to run a pretrained model using PyTorch is a useful
    skill--full stop. It’s especially useful if the model has been trained on a large
    dataset. We will need to get accustomed to the mechanics of obtaining and running
    a neural network on real-world data, and then visualizing and evaluating its outputs,
    whether we trained it or not.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您继续阅读，因为本章隐藏了一些重要的技能。学习如何使用PyTorch运行预训练模型是一项有用的技能--毫无疑问。如果模型经过大型数据集的训练，这将尤其有用。我们需要习惯在真实世界数据上获取和运行神经网络的机制，然后可视化和评估其输出，无论我们是否对其进行了训练。
- en: 2.1 A pretrained network that recognizes the subject of an image
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 识别图像主题的预训练网络
- en: As our first foray into deep learning, we’ll run a state-of-the-art deep neural
    network that was pretrained on an object-recognition task. There are many pretrained
    networks that can be accessed through source code repositories. It is common for
    researchers to publish their source code along with their papers, and often the
    code comes with weights that were obtained by training a model on a reference
    dataset. Using one of these models could enable us to, for example, equip our
    next web service with image-recognition capabilities with very little effort.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为我们对深度学习的首次尝试，我们将运行一个在对象识别任务上预训练的最先进的深度神经网络。可以通过源代码存储库访问许多预训练网络。研究人员通常会在其论文中发布源代码，而且通常该代码附带通过在参考数据集上训练模型获得的权重。使用其中一个模型可以使我们例如，可以轻松地为我们的下一个网络服务配备图像识别功能。
- en: The pretrained network we’ll explore here was trained on a subset of the ImageNet
    dataset ([http://imagenet.stanford.edu](http://imagenet.stanford.edu)). ImageNet
    is a very large dataset of over 14 million images maintained by Stanford University.
    All of the images are labeled with a hierarchy of nouns that come from the WordNet
    dataset ([http://wordnet.princeton.edu](http://wordnet.princeton.edu)), which
    is in turn a large lexical database of the English language.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里探索的预训练网络是在ImageNet数据集的一个子集上训练的（[http://imagenet.stanford.edu](http://imagenet.stanford.edu)）。ImageNet是由斯坦福大学维护的一个非常庞大的数据集，包含超过1400万张图像。所有图像都标有来自WordNet数据集（[http://wordnet.princeton.edu](http://wordnet.princeton.edu)）的名词层次结构，WordNet是一个大型的英语词汇数据库。
- en: The ImageNet dataset, like several other public datasets, has its origin in
    academic competitions. Competitions have traditionally been some of the main playing
    fields where researchers at institutions and companies regularly challenge each
    other. Among others, the ImageNet Large Scale Visual Recognition Challenge (ILSVRC)
    has gained popularity since its inception in 2010\. This particular competition
    is based on a few tasks, which can vary each year, such as image classification
    (telling what object categories the image contains), object localization (identifying
    objects’ position in images), object detection (identifying and labeling objects
    in images), scene classification (classifying a situation in an image), and scene
    parsing (segmenting an image into regions associated with semantic categories,
    such as cow, house, cheese, hat). In particular, the image-classification task
    consists of taking an input image and producing a list of 5 labels out of 1,000
    total categories, ranked by confidence, describing the content of the image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet数据集，像其他几个公共数据集一样，起源于学术竞赛。竞赛一直是研究机构和公司研究人员经常挑战彼此的主要领域之一。自2010年创立以来，ImageNet大规模视觉识别挑战赛（ILSVRC）已经变得越来越受欢迎。这个特定的竞赛基于一些任务，每年可能会有所不同，例如图像分类（告诉图像包含哪些对象类别）、对象定位（识别图像中对象的位置）、对象检测（识别和标记图像中的对象）、场景分类（对图像中的情况进行分类）和场景解析（将图像分割成与语义类别相关的区域，如牛、房子、奶酪、帽子）。特别是，图像分类任务包括获取输入图像并生成5个标签列表，来自1000个总类别，按置信度排序，描述图像的内容。
- en: The training set for ILSVRC consists of 1.2 million images labeled with one
    of 1,000 nouns (for example, “dog”), referred to as the *class* of the image.
    In this sense, we will use the terms *label* and *class* interchangeably. We can
    take a peek at images from ImageNet in figure 2.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ILSVRC的训练集包含了120万张图像，每张图像都标有1000个名词中的一个（例如，“狗”），被称为图像的*类别*。在这个意义上，我们将使用*标签*和*类别*这两个术语来互换使用。我们可以在图2.1中看到来自ImageNet的图像。
- en: '![](../Images/CH02_F01_Stevens2_GS.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F01_Stevens2_GS.png)'
- en: Figure 2.1 A small sample of ImageNet images
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 ImageNet图像的一个小样本
- en: '![](../Images/CH02_F02_Stevens2_GS.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F02_Stevens2_GS.png)'
- en: Figure 2.2 The inference process
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 推理过程
- en: We are going to end up being able to take our own images and feed them into
    our pretrained model, as pictured in figure 2.2\. This will result in a list of
    predicted labels for that image, which we can then examine to see what the model
    thinks our image is. Some images will have predictions that are accurate, and
    others will not!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终将能够将我们自己的图像输入到我们的预训练模型中，如图2.2所示。这将导致该图像的预测标签列表，然后我们可以检查模型认为我们的图像是什么。有些图像的预测是准确的，而其他的则不是！
- en: 'The input image will first be preprocessed into an instance of the multidimensional
    array class `torch.Tensor`. It is an RGB image with height and width, so this
    tensor will have three dimensions: the three color channels, and two spatial image
    dimensions of a specific size. (We’ll get into the details of what a tensor is
    in chapter 3, but for now, think of it as being like a vector or matrix of floating-point
    numbers.) Our model will take that processed input image and pass it into the
    pretrained network to obtain scores for each class. The highest score corresponds
    to the most likely class according to the weights. Each class is then mapped one-to-one
    onto a class label. That output is contained in a `torch.Tensor` with 1,000 elements,
    each representing the score associated with that class.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像将首先被预处理为`torch.Tensor`类的实例。它是一个具有高度和宽度的RGB图像，因此这个张量将具有三个维度：三个颜色通道和特定大小的两个空间图像维度。（我们将在第3章详细介绍张量是什么，但现在，可以将其视为浮点数的向量或矩阵。）我们的模型将获取处理过的输入图像，并将其传递到预训练网络中，以获取每个类别的分数。最高分对应于权重下最可能���类别。然后，每个类别都被一对一地映射到一个类别标签。该输出包含一个具有1000个元素的`torch.Tensor`，每个元素代表与该类别相关的分数。
- en: Before we can do all that, we’ll need to get the network itself, take a peek
    under the hood to see how it’s structured, and learn about how to prepare our
    data before the model can use it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行所有这些之前，我们需要获取网络本身，看看它的结构，了解如何准备数据以便模型使用。
- en: 2.1.1 Obtaining a pretrained network for image recognition
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 获取用于图像识别的预训练网络
- en: 'As discussed, we will now equip ourselves with a network trained on ImageNet.
    To do so, we’ll take a look at the TorchVision project ([https://github.com/pytorch/vision](https://github.com/pytorch/vision)),
    which contains a few of the best-performing neural network architectures for computer
    vision, such as AlexNet ([http://mng.bz/lo6z](http://mng.bz/lo6z)), ResNet ([https://arxiv.org/pdf/
    1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)), and Inception v3 ([https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)).
    It also has easy access to datasets like ImageNet and other utilities for getting
    up to speed with computer vision applications in PyTorch. We’ll dive into some
    of these further along in the book. For now, let’s load up and run two networks:
    first AlexNet, one of the early breakthrough networks for image recognition; and
    then a residual network, ResNet for short, which won the ImageNet classification,
    detection, and localization competitions, among others, in 2015\. If you didn’t
    get PyTorch up and running in chapter 1, now is a good time to do that.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如讨论的那样，我们现在将配备一个在ImageNet上训练过的网络。为此，我们将查看TorchVision项目（[https://github.com/pytorch/vision](https://github.com/pytorch/vision)），其中包含一些最佳性能的计算机视觉神经网络架构，如AlexNet（[http://mng.bz/lo6z](http://mng.bz/lo6z)）、ResNet（[https://arxiv.org/pdf/
    1512.03385.pdf](https://arxiv.org/pdf/1512.03385.pdf)）和Inception v3（[https://arxiv.org/pdf/1512.00567.pdf](https://arxiv.org/pdf/1512.00567.pdf)）。它还可以轻松访问ImageNet等数据集，以及其他用于快速掌握PyTorch中计算机视觉应用的实用工具。我们将在本书后面深入研究其中一些。现在，让我们加载并运行两个网络：首先是AlexNet，这是早期用于图像识别的突破性网络；然后是残差网络，简称ResNet，它在2015年赢得���ImageNet分类、检测和定位比赛等多个比赛。如果你在第1章中没有安装PyTorch，现在是一个很好的时机。
- en: 'The predefined models can be found in `torchvision.models` (code/p1ch2/2 _pre_trained_networks.ipynb):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 预定义的模型可以在`torchvision.models`（code/p1ch2/2 _pre_trained_networks.ipynb）中找到：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can take a look at the actual models:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看一下实际的模型：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The capitalized names refer to Python classes that implement a number of popular
    models. They differ in their architecture--that is, in the arrangement of the
    operations occurring between the input and the output. The lowercase names are
    convenience functions that return models instantiated from those classes, sometimes
    with different parameter sets. For instance, `resnet101` returns an instance of
    `ResNet` with 101 layers, `resnet18` has 18 layers, and so on. We’ll now turn
    our attention to AlexNet.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大写的名称指的是实现一些流行模型的Python类。它们在架构上有所不同--即，在输入和输出之间发生的操作排列方式不同。小写的名称是方便函数，返回从这些类实例化的模型，有时使用不同的参数集。例如，`resnet101`返回一个具有101层的`ResNet`实例，`resnet18`有18层，依此类推。现在我们将注意力转向AlexNet。
- en: 2.1.2 AlexNet
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 AlexNet
- en: 'The AlexNet architecture won the 2012 ILSVRC by a large margin, with a top-5
    test error rate (that is, the correct label must be in the top 5 predictions)
    of 15.4%. By comparison, the second-best submission, which wasn’t based on a deep
    network, trailed at 26.2%. This was a defining moment in the history of computer
    vision: the moment when the community started to realize the potential of deep
    learning for vision tasks. That leap was followed by constant improvement, with
    more modern architectures and training methods getting top-5 error rates as low
    as 3%.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet架构以绝对优势赢得了2012年ILSVRC，其前5个测试错误率（即，正确标签必须在前5个预测中）为15.4%。相比之下，第二名提交的模型，不是基于深度网络的，错误率为26.2%。这是计算机视觉历史上的一个决定性时刻：社区开始意识到深度学习在视觉任务中的潜力。这一飞跃随后不断改进，更现代的架构和训练方法使得前5个错误率降至3%。
- en: By today’s standards, AlexNet is a rather small network, compared to state-of-the-art
    models. But in our case, it’s perfect for taking a first peek at a neural network
    that does something and learning how to run a pretrained version of it on a new
    image.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从今天的标准来看，与最先进的模型相比，AlexNet是一个相对较小的网络。但在我们的情况下，它非常适合初次了解一个做某事的神经网络，并学习如何在新图像上运行预训练版本。
- en: We can see the structure of AlexNet in figure 2.3\. Not that we have all the
    elements for understanding it now, but we can anticipate a few aspects. First,
    each block consists of a bunch of multiplications and additions, plus a sprinkle
    of other functions in the output that we’ll discover in chapter 5\. We can think
    of it as a filter--a function that takes one or more images as input and produces
    other images as output. The way it does so is determined during training, based
    on the examples it has *seen* and on the desired outputs for those.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在图2.3中看到AlexNet的结构。虽然我们现在已经具备了理解它的所有要素，但我们可以预见一些方面。首先，每个块由一堆乘法和加法组成，加上我们将在第5章中发现的输出中的其他函数。我们可以将其视为一个滤波器--一个接受一个或多个图像作为输入并产生其他图像作为输出的函数。它的工作方式是在训练过程中确定的，基于它所*看到*的示例和所需的输出。
- en: '![](../Images/CH02_F03_Stevens2_GS.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F03_Stevens2_GS.png)'
- en: Figure 2.3 The AlexNet architecture
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 AlexNet架构
- en: In figure 2.3, input images come in from the left and go through five stacks
    of filters, each producing a number of output images. After each filter, the images
    are reduced in size, as annotated. The images produced by the last stack of filters
    are laid out as a 4,096-element 1D vector and classified to produce 1,000 output
    probabilities, one for each output class.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.3中，输入图像从左侧进入，并经过五组滤波器，每组产生多个输出图像。在每个滤波器之后，图像会按照注释的方式减小尺寸。最后一组滤波器产生的图像被布置成一个4,096元素的一维向量，并进行分类以产生1,000个输出概率，每个输出类别一个。
- en: 'In order to run the AlexNet architecture on an input image, we can create an
    instance of the `AlexNet` class. This is how it’s done:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在输入图像上运行AlexNet架构，我们可以创建一个`AlexNet`类的实例。操作如下：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, `alexnet` is an object that can run the AlexNet architecture.
    It’s not essential for us to understand the details of this architecture for now.
    For the time being, `AlexNet` is just an opaque object that can be called like
    a function. By providing `alexnet` with some precisely sized input data (we’ll
    see shortly what this input data should be), we will run a *forward pass* through
    the network. That is, the input will run through the first set of neurons, whose
    outputs will be fed to the next set of neurons, all the way to the final output.
    Practically speaking, assuming we have an `input` object of the right type, we
    can run the forward pass with `output = alexnet(input)`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`alexnet`是一个可以运行AlexNet架构的对象。目前，我们不需要了解这种架构的细节。暂时来说，`AlexNet`只是一个不透明的对象，可以像函数一样调用。通过为`alexnet`提供一些精确大小的输入数据（我们很快将看到这些输入数据应该是什么），我们将通过网络进行*前向传递*。也就是说，输入将通过第一组神经元，其输出将被馈送到下一组神经元，一直到最终输出。从实际角度来看，假设我们有一个正确类型的`input`对象，我们可以使用`output
    = alexnet(input)`来运行前向传递。
- en: 'But if we did that, we would be feeding data through the whole network to produce
    ... garbage! That’s because the network is uninitialized: its weights, the numbers
    by which inputs are added and multiplied, have not been trained on anything--the
    network itself is a blank (or rather, random) slate. We’d need to either train
    it from scratch or load weights from prior training, which we’ll do now.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们这样做，我们将通过整个网络传递数据来产生...垃圾！这是因为网络未初始化：它的权重，即输入相加和相乘的数字，尚未经过任何训练--网络本身是一个空白（或者说是随机）状态。我们需要从头开始训练它，或者加载之前训练的权重，现在我们将这样做。
- en: 'To this end, let’s go back to the `models` module. We learned that the uppercase
    names correspond to classes that implement popular architectures for computer
    vision. The lowercase names, on the other hand, are functions that instantiate
    models with predefined numbers of layers and units and optionally download and
    load pretrained weights into them. Note that there’s nothing essential about using
    one of these functions: they just make it convenient to instantiate the model
    with a number of layers and units that matches how the pretrained networks were
    built.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，让我们回到`models`模块。我们了解到大写名称对应于实现用于计算机视觉的流行架构的类。另一方面，小写名称是函数，用于实例化具有预定义层数和单元数的模型，并可选择下载和加载预训练权重。请注意，使用这些函数并非必要：它们只是方便地实例化具有与预训练网络构建方式相匹配的层数和单元数的模型。
- en: 2.1.3 ResNet
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 ResNet
- en: Using the `resnet101` function, we’ll now instantiate a 101-layer convolutional
    neural network. Just to put things in perspective, before the advent of residual
    networks in 2015, achieving stable training at such depths was considered extremely
    hard. Residual networks pulled a trick that made it possible, and by doing so,
    beat several benchmarks in one sweep that year.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`resnet101`函数，我们现在将实例化一个101层的卷积神经网络。为了让事情有个对比，2015年之前，在残差网络出现之前，实现这样深度的稳定训练被认为是极其困难的。残差网络使用了一个技巧，使这成为可能，并通过这样做，在当年一举超过了几个基准。
- en: 'Let’s create an instance of the network now. We’ll pass an argument that will
    instruct the function to download the weights of `resnet101` trained on the ImageNet
    dataset, with 1.2 million images and 1,000 categories:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建网络的一个实例。我们将传递一个参数，指示函数下载在ImageNet数据集上训练的`resnet101`的权重，该数据集包含1,200,000张图像和1,000个类别：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: While we’re staring at the download progress, we can take a minute to appreciate
    that `resnet101` sports 44.5 million parameters--that’s a lot of parameters to
    optimize automatically!
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们盯着下载进度时，我们可以花一分钟来欣赏`resnet101`拥有4450万个参数--这是一个需要自动优化的大量参数！
- en: 2.1.4 Ready, set, almost run
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 准备好了，几乎可以运行了
- en: 'OK, what did we just get? Since we’re curious, we’ll take a peek at what a
    `resnet101` looks like. We can do so by printing the value of the returned model.
    This gives us a textual representation of the same kind of information we saw
    in 2.3, providing details about the structure of the network. For now, this will
    be information overload, but as we progress through the book, we’ll increase our
    ability to understand what this code is telling us:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们刚刚得到了什么？由于我们很好奇，我们将看一眼`resnet101`是什么样子。我们可以通过打印返回模型的值来做到这一点。这给了我们一个文本表示形式，提供了与我们在2.3中看到的相同类型的关于网络结构的详细信息。目前，这将是信息过载，但随着我们在书中的进展，我们将增加理解这段代码告诉我们的能力：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'What we are seeing here is `modules`, one per line. Note that they have nothing
    in common with Python modules: they are individual operations, the building blocks
    of a neural network. They are also called *layers* in other deep learning frameworks.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到的是`modules`，每行一个。请注意，它们与Python模块没有任何共同之处：它们是单独的操作，神经网络的构建模块。在其他深度学习框架中，它们也被称为*层*。
- en: 'If we scroll down, we’ll see a lot of `Bottleneck` modules repeating one after
    the other (101 of them!), containing convolutions and other modules. That’s the
    anatomy of a typical deep neural network for computer vision: a more or less sequential
    cascade of filters and nonlinear functions, ending with a layer (`fc`) producing
    scores for each of the 1,000 output classes (`out_features`).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们向下滚动，我们会看到很多`Bottleneck`模块一个接一个地重复（共101个！），包含卷积和其他模块。这就是典型的用于计算机视觉的深度神经网络的解剖学：一个或多或少顺序级联的滤波器和非线性函数，最终以一个层（`fc`）产生每个1,000个输出类别（`out_features`）的分数。
- en: 'The `resnet` variable can be called like a function, taking as input one or
    more images and producing an equal number of scores for each of the 1,000 ImageNet
    classes. Before we can do that, however, we have to preprocess the input images
    so they are the right size and so that their values (colors) sit roughly in the
    same numerical range. In order to do that, the `torchvision` module provides `transforms`,
    which allow us to quickly define pipelines of basic preprocessing functions:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`resnet`变量可以像函数一样调用，输入一个或多个图像，并为每个1,000个ImageNet类别产生相同数量的分数。然而，在这之前，我们必须对输入图像进行预处理，使其具有正确的大小，并使其值（颜色）大致处于相同的数值范围内。为了做到这一点，`torchvision`模块提供了`transforms`，允许我们快速定义基本预处理函数的流水线：'
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In this case, we defined a `preprocess` function that will scale the input
    image to 256 × 256, crop the image to 224 × 224 around the center, transform it
    to a tensor (a PyTorch multidimensional array: in this case, a 3D array with color,
    height, and width), and normalize its RGB (red, green, blue) components so that
    they have defined means and standard deviations. These need to match what was
    presented to the network during training, if we want the network to produce meaningful
    answers. We’ll go into more depth about transforms when we dive into making our
    own image-recognition models in section 7.1.3.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们定义了一个`preprocess`函数，将输入图像缩放到256×256，将图像裁剪到围绕中心的224×224，将其转换为张量（一个PyTorch多维数组：在这种情况下，一个带有颜色、高度和宽度的3D数组），并对其RGB（红色、绿色、蓝色）组件进行归一化，使其具有定义的均值和标准差。如果我们希望网络产生有意义的答案，这些值需要与训练期间呈现给网络的值匹配。当我们深入研究如何制作自己的图像识别模型时，我们将更深入地了解transforms，见第7.1.3节。
- en: 'We can now grab a picture of our favorite dog (say, bobby.jpg from the GitHub
    repo), preprocess it, and then see what ResNet thinks of it. We can start by loading
    an image from the local filesystem using Pillow ([https://pillow.readthedocs.io/en/stable](https://pillow.readthedocs.io/en/stable)),
    an image-manipulation module for Python:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以获取我们最喜欢的狗的图片（比如，GitHub仓库中的bobby.jpg），对其进行预处理，然后看看ResNet对其的看法。我们可以从本地文件系统中使用Pillow（[https://pillow.readthedocs.io/en/stable](https://pillow.readthedocs.io/en/stable)）加载图像，这是Python的图像处理模块：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If we were following along from a Jupyter Notebook, we would do the following
    to see the picture inline (it would be shown where the `<PIL.JpegImagePlugin...`
    is in the following):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们是从Jupyter Notebook中跟随进行的，我们将执行以下操作以内联查看图片（它将显示在以下内容中的`<PIL.JpegImagePlugin...`处）：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Otherwise, we can invoke the `show` method, which will pop up a window with
    a viewer, to see the image shown in figure 2.4:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，我们可以调用`show`方法，这将弹出一个带有查看器的窗口，以查看图2.4中显示的图像：
- en: '![](../Images/CH02_F04_Stevens2_GS.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F04_Stevens2_GS.png)'
- en: Figure 2.4 Bobby, our very special input image
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 Bobby，我们非常特殊的输入图像
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we can pass the image through our preprocessing pipeline:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过我们的预处理流程传递图像：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we can reshape, crop, and normalize the input tensor in a way that the
    network expects. We’ll understand more of this in the next two chapters; hold
    tight for now:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以以网络期望的方式重塑、裁剪和归一化输入张量。我们将在接下来的两章中更多地了解这一点；现在请耐心等待：
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We’re now ready to run our model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备运行我们的模型。
- en: 2.1.5 Run!
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.5 运行！
- en: 'The process of running a trained model on new data is called *inference* in
    deep learning circles. In order to do inference, we need to put the network in
    `eval` mode:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习领域，对新数据运行经过训练的模型的过程称为*推断*。为了进行推断，我们需要将网络设置为`eval`模式：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we forget to do that, some pretrained models, like *batch normalization*
    and *dropout*, will not produce meaningful answers, just because of the way they
    work internally. Now that `eval` has been set, we’re ready for inference:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们忘记这样做，一些预训练模型，如*批量归一化*和*丢弃*，将不会产生有意义的答案，这仅仅是因为它们内部的工作方式。现在`eval`已经设置好，我们准备进行推断：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A staggering set of operations involving 44.5 million parameters has just happened,
    producing a vector of 1,000 scores, one per ImageNet class. That didn’t take long,
    did it?
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一个涉及4450万参数的惊人操作集刚刚发生，产生了一个包含1,000个分数的向量，每个分数对应一个ImageNet类别。这没花多少时间，是吧？
- en: We now need to find out the label of the class that received the highest score.
    This will tell us what the model saw in the image. If the label matches how a
    human would describe the image, that’s great! It means everything is working.
    If not, then either something went wrong during training, or the image is so different
    from what the model expects that the model can’t process it properly, or there’s
    some other similar issue.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要找出获得最高分数的类别的标签。这将告诉我们模型在图像中看到了什么。如果标签与人类描述图像的方式相匹配，那太棒了！这意味着一切正常。如果不匹配，那么要么在训练过程中出了问题，要么图像与模型期望的差异太大，模型无法正确处理，或者存在其他类似问题。
- en: To see the list of predicted labels, we will load a text file listing the labels
    in the same order they were presented to the network during training, and then
    we will pick out the label at the index that produced the highest score from the
    network. Almost all models meant for image recognition have output in a form similar
    to what we’re about to work with.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看预测标签列表，我们将加载一个文本文件，列出标签的顺序与网络在训练期间呈现给网络的顺序相同，然后我们将挑选出网络产生的最高分数的索引处的标签。几乎所有用于图像识别的模型的输出形式与我们即将处理的形式类似。
- en: 'Let’s load the file containing the 1,000 labels for the ImageNet dataset classes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载包含ImageNet数据集类别的1,000个标签的文件：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At this point, we need to determine the index corresponding to the maximum
    score in the `out` tensor we obtained previously. We can do that using the `max`
    function in PyTorch, which outputs the maximum value in a tensor as well as the
    indices where that maximum value occurred:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要确定`out`张量中对应最高分数的索引。我们可以使用PyTorch中的`max`函数来做到这一点，该函数输出张量中的最大值以及发生最大值的索引：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now use the index to access the label. Here, `index` is not a plain
    Python number, but a one-element, one-dimensional tensor (specifically, `tensor([207])`),
    so we need to get the actual numerical value to use as an index into our `labels`
    list using `index[0]`. We also use `torch.nn.functional.softmax` ([http://mng.bz/BYnq](http://mng.bz/BYnq))
    to normalize our outputs to the range [0, 1], and divide by the sum. That gives
    us something roughly akin to the confidence that the model has in its prediction.
    In this case, the model is 96% certain that it knows what it’s looking at is a
    golden retriever:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用索引来访问标签。这里，`index`不是一个普通的Python数字，而是一个单元素、一维张量（具体来说，`tensor([207])`），所以我们需要获取实际的数值以用作索引进入我们的`labels`列表，使用`index[0]`。我们还使用`torch.nn.functional.softmax`
    ([http://mng.bz/BYnq](http://mng.bz/BYnq)) 来将我们的输出归一化到范围[0, 1]，并除以总和。这给了我们大致类似于模型对其预测的信心。在这种情况下，模型有96%的把握知道它正在看的是一只金毛寻回犬：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Uh oh, who’s a good boy?
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 哦哦，谁是个好孩子？
- en: 'Since the model produced scores, we can also find out what the second best,
    third best, and so on were. To do this, we can use the `sort` function, which
    sorts the values in ascending or descending order and also provides the indices
    of the sorted values in the original array:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型生成了分数，我们还可以找出第二、第三等等最好的是什么。为此，我们可以使用`sort`函数，它可以将值按升序或降序排序，并在原始数组中提供排序后值的索引：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We see that the first four are dogs (redbone is a breed; who knew?), after which
    things start to get funny. The fifth answer, “tennis ball,” is probably because
    there are enough pictures of tennis balls with dogs nearby that the model is essentially
    saying, “There’s a 0.1% chance that I’ve completely misunderstood what a tennis
    ball is.” This is a great example of the fundamental differences in how humans
    and neural networks view the world, as well as how easy it is for strange, subtle
    biases to sneak into our data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到前四个是狗（红骨是一种品种；谁知道？），之后事情开始变得有趣起来。第五个答案“网球”可能是因为有足够多的狗旁边有网球的图片，以至于模型基本上在说：“我有0.1%的机会完全误解了什么是网球。”这是人类和神经网络在看待世界的根本差异的一个很好的例子，以及奇怪、微妙的偏见如何很容易潜入我们的数据中。
- en: Time to play! We can go ahead and interrogate our network with random images
    and see what it comes up with. How successful the network will be will largely
    depend on whether the subjects were well represented in the training set. If we
    present an image containing a subject outside the training set, it’s quite possible
    that the network will come up with a wrong answer with pretty high confidence.
    It’s useful to experiment and get a feel for how a model reacts to unseen data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 玩耍的时候到了！我们可以继续用随机图像询问我们的网络���看看它会得出什么结果。网络的成功程度很大程度上取决于主题在训练集中是否得到很好的代表。如果我们呈现一个包含训练集之外主题的图像，网络很可能会以相当高的信心给出错误答案。实验和了解模型对未见数据的反应是很有用的。
- en: We’ve just run a network that won an image-classification competition in 2015\.
    It learned to recognize our dog from examples of dogs, together with a ton of
    other real-world subjects. We’ll now see how different architectures can achieve
    other kinds of tasks, starting with image generation.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚运行了一个在2015年赢得图像分类比赛的网络。它学会了从狗的例子中识别我们的狗，以及许多其他真实世界的主题。现在我们将看看不同的架构如何实现其他类型的任务，从图像生成开始。
- en: 2.2 A pretrained model that fakes it until it makes it
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 一个假装到成功的预训练模型
- en: Let’s suppose, for a moment, that we’re career criminals who want to move into
    selling forgeries of “lost” paintings by famous artists. We’re criminals, not
    painters, so as we paint our fake Rembrandts and Picassos, it quickly becomes
    apparent that they’re amateur imitations rather than the real deal. Even if we
    spend a bunch of time practicing until we get a canvas that *we* can’t tell is
    fake, trying to pass it off at the local art auction house is going to get us
    kicked out instantly. Even worse, being told “This is clearly fake; get out,”
    doesn’t help us improve! We’d have to randomly try a bunch of things, gauge which
    ones took *slightly* longer to recognize as forgeries, and emphasize those traits
    on our future attempts, which would take far too long.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设一下，我们是职业罪犯，想要开始销售著名艺术家的“失落”画作的赝品。我们是罪犯，不是画家，所以当我们绘制我们的假雷姆布兰特和毕加索时，很快就会显而易见它们是业余的模仿品而不是真品。即使我们花了很多时间练习，直到我们也无法分辨出画作是假的，试图在当地艺术拍卖行兜售也会立即被赶出去。更糟糕的是，被告知“这显然是假的；滚出去”，并不能帮助我们改进！我们将不得不随机尝试很多事情，评估哪些需要*稍微*长一点时间才能识别为赝品，并在未来的尝试中强调这些特征，这将花费太长时间。
- en: Instead, we need to find an art historian of questionable moral standing to
    inspect our work and tell us exactly what it was that tipped them off that the
    painting wasn’t legit. With that feedback, we can improve our output in clear,
    directed ways, until our sketchy scholar can no longer tell our paintings from
    the real thing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们需要找到一个道德标准有问题的艺术史学家来检查我们的作品，并告诉我们究竟是什么让他们发现这幅画不真实。有了这个反馈，我们可以以明确、有针对性的方式改进我们的作品，直到我们的可疑学者再也无法将我们的画作与真品区分开来。
- en: Soon, we’ll have our “Botticelli” in the Louvre, and their Benjamins in our
    pockets. We’ll be rich!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，我们的“波提切利”将在卢浮宫展出，他们的百元钞票将进入我们的口袋。我们会变得富有！
- en: While this scenario is a bit farcical, the underlying technology is sound and
    will likely have a profound impact on the perceived veracity of digital data in
    the years to come. The entire concept of “photographic evidence” is likely to
    become entirely suspect, given how easy it will be to automate the production
    of convincing, yet fake, images and video. The only key ingredient is data. Let’s
    see how this process works.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种情景有点荒谬，但其基础技术是可靠的，并且很可能会对未来几年数字数据的真实性产生深远影响。整个“照片证据”的概念很可能会变得完全可疑，因为制作令人信服但虚假的图像和视频将变得非常容易。唯一的关键因素是数据。让我们看看这个过程是如何运作的。
- en: 2.2.1 The GAN game
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 GAN游戏
- en: In the context of deep learning, what we’ve just described is known as *the
    GAN game*, where two networks, one acting as the painter and the other as the
    art historian, compete to outsmart each other at creating and detecting forgeries.
    GAN stands for *generative adversarial network*, where *generative* means something
    is being created (in this case, fake masterpieces), *adversarial* means the two
    networks are competing to outsmart the other, and well, *network* is pretty obvious.
    These networks are one of the most original outcomes of recent deep learning research.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的背景下，我们刚刚描述的被称为*GAN游戏*，其中两个网络，一个充当画家，另一个充当艺术史学家，竞争着互相愚弄，创造和检测伪造品。GAN代表*生成对抗网络*，其中*生成*表示正在创建某物（在本例中是假的杰作），*对抗*表示两个网络正在竞争愚弄对方，而*网络*显而易见。这些网络是最近深度学习研究的最原创的成果之一。
- en: Remember that our overarching goal is to produce synthetic examples of a class
    of images that cannot be recognized as fake. When mixed in with legitimate examples,
    a skilled examiner would have trouble determining which ones are real and which
    are our forgeries.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，我们的总体目标是生成一类图像的合成示例，这些示例无法被识别为伪造品。当与合法示例混合在一起时，一个熟练的检查员会很难确定哪些是真实的，哪些是我们的伪造品。
- en: The *generator* network takes the role of the painter in our scenario, tasked
    with producing realistic-looking images, starting from an arbitrary input. The
    *discriminator* network is the amoral art inspector, needing to tell whether a
    given image was fabricated by the generator or belongs in a set of real images.
    This two-network design is atypical for most deep learning architectures but,
    when used to implement a GAN game, can lead to incredible results.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成器*网络在我们的场景中扮演画家的角色，负责从任意输入开始生成逼真的图像。*鉴别器*网络是无情的艺术检查员，需要判断给定的图像是由生成器制作还是属于真实图像集。这种双网络设计对于大多数深度学习架构来说是非典型的，但是，当用于实现GAN游戏时，可以产生令人难以置信的结果。'
- en: '![](../Images/CH02_F05_Stevens2_GS.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F05_Stevens2_GS.png)'
- en: Figure 2.5 Concept of a GAN game
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 GAN游戏的概念
- en: Figure 2.5 shows a rough picture of what’s going on. The end goal for the generator
    is to fool the discriminator into mixing up real and fake images. The end goal
    for the discriminator is to find out when it’s being tricked, but it also helps
    inform the generator about the identifiable mistakes in the generated images.
    At the start, the generator produces confused, three-eyed monsters that look nothing
    like a Rembrandt portrait. The discriminator is easily able to distinguish the
    muddled messes from the real paintings. As training progresses, information flows
    back from the discriminator, and the generator uses it to improve. By the end
    of training, the generator is able to produce convincing fakes, and the discriminator
    no longer is able to tell which is which.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5展示了大致的情况。生成器的最终目标是欺骗鉴别器，混淆真实和虚假图像。鉴别器的最终目标是发现自己被欺骗，但它也帮助生成器找出生成图像中的可识别错误。在开始阶段，生成器产生混乱的三眼怪物，看起来一点也不像伦勃朗的肖像画。鉴别器很容易区分混乱的混乱图像和真实的绘画作品。随着训练的进行，信息从鉴别器返回，生成器利用这些信息进行改进。训练结束时，生成器能够产生令人信服的伪造品，鉴别器不再能分辨哪个是真实的。
- en: Note that “Discriminator wins” or “Generator wins” shouldn’t be taken literally--there’s
    no explicit tournament between the two. However, both networks are trained based
    on the outcome of the other network, which drives the optimization of the parameters
    of each network.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“鉴别器获胜”或“生成器获胜”不应被字面意义上解释--两者之间没有明确的比赛。然而，两个网络都是基于另一个网络的结果进行训练的，这推动了每个网络参数的优化。
- en: 'This technique has proven itself able to lead to generators that produce realistic
    images from nothing but noise and a conditioning signal, like an attribute (for
    example, for faces: young, female, glasses on) or another image. In other words,
    a well-trained generator learns a plausible model for generating images that look
    real even when examined by humans.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术已经被证明能够导致生成器从仅有噪音和一个条件信号（比如，对于人脸：年轻、女性、戴眼镜）生成逼真图像，换句话说，一个训练良好的生成器学会了一个可信的模型，即使被人类检查也看起来很真实。
- en: 2.2.2 CycleGAN
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 CycleGAN
- en: An interesting evolution of this concept is the CycleGAN. A CycleGAN can turn
    images of one domain into images of another domain (and back), without the need
    for us to explicitly provide matching pairs in the training set.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念的一个有趣演变是CycleGAN。CycleGAN可以将一个域的图像转换为另一个域的图像（反之亦然），而无需我们在训练集中明确提供匹配对。
- en: In figure 2.6, we have a CycleGAN workflow for the task of turning a photo of
    a horse into a zebra, and vice versa. Note that there are two separate generator
    networks, as well as two distinct discriminators.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.6中，我们有一个CycleGAN工作流程，用于将一匹马的照片转换为斑马，反之亦然。请注意，有两个独立的生成器网络，以及两个不同的鉴别器。
- en: '![](../Images/CH02_F06_Stevens2_GS.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F06_Stevens2_GS.png)'
- en: Figure 2.6 A CycleGAN trained to the point that it can fool both discriminator
    networks
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 一个CycleGAN训练到可以愚弄两个鉴别器网络的程度
- en: As the figure shows, the first generator learns to produce an image conforming
    to a target distribution (zebras, in this case) starting from an image belonging
    to a different distribution (horses), so that the discriminator can’t tell if
    the image produced from a horse photo is actually a genuine picture of a zebra
    or not. At the same time--and here’s where the *Cycle* prefix in the acronym comes
    in--the resulting fake zebra is sent through a different generator going the other
    way (zebra to horse, in our case), to be judged by another discriminator on the
    other side. Creating such a cycle stabilizes the training process considerably,
    which addresses one of the original issues with GANs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，第一个生成器学会生成符合目标分布（在这种情况下是斑马）的图像，从属于不同分布（马）的图像开始，以便鉴别器无法判断从马照片生成的图像是否真的是斑马的真实图片。同时--这就是缩写中*Cycle*前缀的含义--生成的假斑马被发送到另一个生成器，沿���另一条路（在我们的情况下是从斑马到马），由另一个鉴别器进行判断。创建这样一个循环显著稳定了训练过程，解决了GAN的一个最初问题。
- en: 'The fun part is that at this point, we don’t need matched horse/zebra pairs
    as ground truths (good luck getting them to match poses!). It’s enough to start
    from a collection of unrelated horse images and zebra photos for the generators
    to learn their task, going beyond a purely supervised setting. The implications
    of this model go even further than this: the generator learns how to selectively
    change the appearance of objects in the scene without supervision about what’s
    what. There’s no signal indicating that manes are manes and legs are legs, but
    they get translated to something that lines up with the anatomy of the other animal.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在这一点上，我们不需要匹配的马/斑马对作为地面真相（祝你好运，让它们匹配姿势！）。从一组不相关的马图片和斑马照片开始对生成器进行训练就足够了，超越了纯粹监督的设置。这个模型的影响甚至超出了这个范围：生成器学会了如何有选择性地改变场景中物体的外观，而不需要关于什么是什么的监督。没有信号表明鬃毛是鬃毛，腿是腿，但它们被转换成与另一种动物的解剖学相一致的东西。
- en: 2.2.3 A network that turns horses into zebras
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 将马变成斑马的网络
- en: We can play with this model right now. The CycleGAN network has been trained
    on a dataset of (unrelated) horse images and zebra images extracted from the ImageNet
    dataset. The network learns to take an image of one or more horses and turn them
    all into zebras, leaving the rest of the image as unmodified as possible. While
    humankind hasn’t held its breath over the last few thousand years for a tool that
    turn horses into zebras, this task showcases the ability of these architectures
    to model complex real-world processes with distant supervision. While they have
    their limits, there are hints that in the near future we won’t be able to tell
    real from fake in a live video feed, which opens a can of worms that we’ll duly
    close right now.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以玩这个模型。CycleGAN网络已经在从ImageNet数据集中提取的（不相关的）马图片和斑马图片数据集上进行了训练。网络学会了将一张或多张马的图片转换成斑马，尽可能保持其余部分的图像不变。虽然人类在过去几千年里并没有为将马变成斑马的工具而屏住呼吸，但这个任务展示了这些架构模拟复杂现实世界过程的能力，远程监督。虽然它们有局限性，但有迹象表明，在不久的将来，我们将无法在实时视频中区分真实和虚假，这打开了一个我们将立即关闭的潘多拉魔盒。
- en: 'Playing with a pretrained CycleGAN will give us the opportunity to take a step
    closer and look at how a network--a generator, in this case--is implemented. We’ll
    use our old friend ResNet. We’ll define a `ResNetGenerator` class offscreen. The
    code is in the first cell of the 3_cyclegan.ipynb file, but the implementation
    isn’t relevant right now, and it’s too complex to follow until we’ve gotten a
    lot more PyTorch experience. Right now, we’re focused on *what* it can do, rather
    than *how* it does it. Let’s instantiate the class with default parameters (code/p1ch2/3_cyclegan.ipynb):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 玩一个预训练的CycleGAN将给我们一个机会，更近距离地看一看网络--在这种情况下是一个生成器--是如何实现的。我们将使用我们的老朋友ResNet。我们将在屏幕外定义一个`ResNetGenerator`类。代码在3_cyclegan.ipynb文件的第一个单元格中，但实现目前并不相关，而且在我们获得更多PyTorch经验之前，它太复杂了。现在，我们专注于*它能做什么*，而不是*它是如何做到的*。让我们用默认参数实例化这个类（code/p1ch2/3_cyclegan.ipynb）：
- en: '[PRE17]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `netG` model has been created, but it contains random weights. We mentioned
    earlier that we would run a generator model that had been pretrained on the horse2zebra
    dataset, whose training set contains two sets of 1068 and 1335 images of horses
    and zebras, respectively. The dataset be found at [http://mng.bz/8pKP](http://mng.bz/8pKP).
    The weights of the model have been saved in a .pth file, which is nothing but
    a `pickle` file of the model’s tensor parameters. We can load those into `ResNetGenerator`
    using the model’s `load _state_dict` method:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`netG`模型已经创建，但它包含随机权重。我们之前提到过，我们将运行一个在horse2zebra数据集上预训练的生成器模型，该数据集的训练集包含1068张马和1335张斑马的图片。数据集可以在[http://mng.bz/8pKP](http://mng.bz/8pKP)找到。模型的权重已保存在.pth文件中，这只是模型张量参数的`pickle`文件。我们可以使用模型的`load_state_dict`方法将它们加载到`ResNetGenerator`中：'
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: At this point, `netG` has acquired all the knowledge it achieved during training.
    Note that this is fully equivalent to what happened when we loaded `resnet101`
    from `torchvision` in section 2.1.3; but the `torchvision.resnet101` function
    hid the loading from us.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`netG`已经获得了在训练过程中获得的所有知识。请注意，这与我们在第2.1.3节中从`torchvision`中加载`resnet101`时发生的情况完全相同；但`torchvision.resnet101`函数将加载过程隐藏了起来。
- en: 'Let’s put the network in `eval` mode, as we did for `resnet101`:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将网络设置为`eval`模式，就像我们为`resnet101`所做的那样：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Printing out the model as we did earlier, we can appreciate that it’s actually
    pretty condensed, considering what it does. It takes an image, recognizes one
    or more horses in it by looking at pixels, and individually modifies the values
    of those pixels so that what comes out looks like a credible zebra. We won’t recognize
    anything zebra-like in the printout (or in the source code, for that matter):
    that’s because there’s nothing zebra-like in there. The network is a scaffold--the
    juice is in the weights.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前打印模型一样，我们可以欣赏到它实际上相当简洁，考虑到它的功能。它接收一幅图像，通过查看像素识别出一匹或多匹马，并单独修改这些像素的值，使得输出看起来像一匹可信的斑马。我们在打印输出中（或者源代码中）不会认出任何类似斑马的东西：那是因为里面没有任何类似斑马的东西。这个网络是一个脚手架--关键在于权重。
- en: 'We’re ready to load a random image of a horse and see what our generator produces.
    First, we need to import `PIL` and `torchvision`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备加载一张随机的马的图像，看看我们的生成器会产生什么。首先，我们需要导入`PIL`和`torchvision`：
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we define a few input transformations to make sure data enters the network
    with the right shape and size:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一些输入转换，以确保数据以正确的形状和大小进入网络：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s open a horse file (see figure 2.7):'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开一个马文件（见图2.7）：
- en: '![](../Images/CH02_F07_Stevens2_GS.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F07_Stevens2_GS.png)'
- en: Figure 2.7 A man riding a horse. The horse is not having it.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 一个骑马的人。马似乎不太乐意。
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'OK, there’s a dude on the horse. (Not for long, judging by the picture.) Anyhow,
    let’s pass it through preprocessing and turn it into a properly shaped variable:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，有个家伙骑在马上。（看图片的话，可能不会持续太久。）不管怎样，让我们通过预处理并将其转换为正确形状的变量：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We shouldn’t worry about the details right now. The important thing is that
    we follow from a distance. At this point, `batch_t` can be sent to our model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在不必担心细节。重要的是我们要保持距离跟随。此时，`batch_t`可以被发送到我们的模型：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`batch_out` is now the output of the generator, which we can convert back to
    an image:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_out`现在是生成器的输出，我们可以将其转换回图像：'
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Oh, man. Who rides a zebra that way? The resulting image (figure 2.8) is not
    perfect, but consider that it is a bit unusual for the network to find someone
    (sort of) riding on top of a horse. It bears repeating that the learning process
    has not passed through direct supervision, where humans have delineated tens of
    thousands of horses or manually Photoshopped thousands of zebra stripes. The generator
    has learned to produce an image that would fool the discriminator into thinking
    that was a zebra, and there was nothing fishy about the image (clearly the discriminator
    has never been to a rodeo).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，天啊。谁���那样骑斑马呢？结果图像（图2.8）并不完美，但请考虑到网络发现有人（某种程度上）骑在马上有点不寻常。需要重申的是，学习过程并没有经过直接监督，人类没有勾勒出成千上万匹马或手动Photoshop成千上万条斑马条纹。生成器已经学会生成一幅图像，可以愚弄鉴别器，让它认为那是一匹斑马，而图像并没有什么奇怪之处（显然鉴别器从未去过竞技场）。
- en: '![](../Images/CH02_F08_Stevens2_GS.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F08_Stevens2_GS.png)'
- en: Figure 2.8 A man riding a zebra. The zebra is not having it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 一个骑斑马的人。斑马似乎不太乐意。
- en: Many other fun generators have been developed using adversarial training or
    other approaches. Some of them are capable of creating credible human faces of
    nonexistent individuals; others can translate sketches into real-looking pictures
    of imaginary landscapes. Generative models are also being explored for producing
    real-sounding audio, credible text, and enjoyable music. It is likely that these
    models will be the basis of future tools that support the creative process.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他有趣的生成器是使用对抗训练或其他方法开发的。其中一些能够创建出可信的不存在个体的人脸；其他一些可以将草图转换为看起来真实的虚构景观图片。生成模型也被用于产生听起来真实的音频、可信的文本和令人愉悦的音乐。这些模型很可能将成为未来支持创造过程的工具的基础。
- en: On a serious note, it’s hard to overstate the implications of this kind of work.
    Tools like the one we just downloaded are only going to become higher quality
    and more ubiquitous. Face-swapping technology, in particular, has gotten considerable
    media attention. Searching for “deep fakes” will turn up a plethora of example
    content[¹](#pgfId-1017181) (though we must note that there is a nontrivial amount
    of not-safe-for-work content labeled as such; as with everything on the internet,
    click carefully).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 说真的，这种工作的影响难以言表。像我们刚刚下载的这种工具只会变得更加高质量和更加普遍。特别是换脸技术已经引起了相当多的媒体关注。搜索“deep fakes”会出现大量示例内容[¹](#pgfId-1017181)（尽管我们必须指出有相当数量的不适宜工作场所的内容被标记为这样；就像互联网上的一切一样，要小心点击）。
- en: 'So far, we’ve had a chance to play with a model that sees into images and a
    model that generates new images. We’ll end our tour with a model that involves
    one more, fundamental ingredient: natural language.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们有机会玩弄一个能看到图像的模型和一个能生成新图像的模型。我们将以一个涉及另一个基本要素的模型结束我们的旅程：自然语言。
- en: 2.3 A pretrained network that describes scenes
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 描述场景的预训练网络
- en: 'In order to get firsthand experience with a model involving natural language,
    we will use a pretrained image-captioning model, generously provided by Ruotian
    Luo.[²](#pgfId-1019341) It is an implementation of the NeuralTalk2 model by Andrej
    Karpathy. When presented with a natural image, this kind of model generates a
    caption in English that describes the scene, as shown in figure 2.9\. The model
    is trained on a large dataset of images along with a paired sentence description:
    for example, “A Tabby cat is leaning on a wooden table, with one paw on a laser
    mouse and the other on a black laptop.” [³](#pgfId-1019370)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了亲身体验涉及自然语言的模型，我们将使用一个由Ruotian Luo慷慨提供的预训练图像字幕模型。这是Andrej Karpathy的NeuralTalk2模型的一个实现。当呈现一幅自然图像时，这种模型会生成一个用英语描述场景的字幕，如图2.9所示。该模型是在大量图像数据集上训练的，配有一对句子描述：例如，“一只虎斑猫斜靠在木桌上，一只爪子放在激光鼠标上，另一只放在黑色笔记本电脑上。”[³](#pgfId-1019370)
- en: '![](../Images/CH02_F09_Stevens2_GS.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F09_Stevens2_GS.png)'
- en: Figure 2.9 Concept of a captioning model
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 字幕模型的概念
- en: This captioning model has two connected halves. The first half of the model
    is a network that learns to generate “descriptive” numerical representations of
    the scene (Tabby cat, laser mouse, paw), which are then taken as input to the
    second half. That second half is a *recurrent neural network* that generates a
    coherent sentence by putting those numerical descriptions together. The two halves
    of the model are trained together on image-caption pairs.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字幕模型有两个连接的部分。模型的第一部分是一个网络，学习生成场景的“描述性”数值表示（Tabby猫，激光鼠标，爪子），然后将这些数值描述作为第二部分的输入。第二部分是一个*循环神经网络*，通过将这些数值描述组合在一起生成连贯的句子。模型的两个部分一起在图像-字幕对上进行训练。
- en: The second half of the model is called *recurrent* because it generates its
    outputs (individual words) in subsequent forward passes, where the input to each
    forward pass includes the outputs of the previous forward pass. This generates
    a dependency of the next word on words that were generated earlier, as we would
    expect when dealing with sentences or, in general, with sequences.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的后半部分被称为*循环*，因为它在后续的前向传递中生成其输出（单词），其中每个前向传递的输入包括前一个前向传递的输出。这使得下一个单词依赖于先前生成的单词，这是我们在处理句子或一般序列时所期望的。
- en: 2.3.1 NeuralTalk2
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 神经对话2
- en: 'The NeuralTalk2 model can be found at [https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch](https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch).
    We can place a set of images in the `data` directory and run the following script:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 神经对话2模型可以在[https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch](https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch)找到。我们可以将一组图像放在`data`目录中，并运行以下脚本：
- en: '[PRE26]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Let’s try it with our horse.jpg image. It says, “A person riding a horse on
    a beach.” Quite appropriate.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用我们的horse.jpg图像。它说：“一个人骑着马在海滩上。”非常合适。
- en: 'Now, just for fun, let’s see if our CycleGAN can also fool this NeuralTalk2
    model. Let’s add the zebra.jpg image in the data folder and rerun the model: “A
    group of zebras are standing in a field.” Well, it got the animal right, but it
    saw more than one zebra in the image. Certainly this is not a pose that the network
    has ever seen a zebra in, nor has it ever seen a rider on a zebra (with some spurious
    zebra patterns). In addition, it is very likely that zebras are depicted in groups
    in the training dataset, so there might be some bias that we could investigate.
    The captioning network hasn’t described the rider, either. Again, it’s probably
    for the same reason: the network wasn’t shown a rider on a zebra in the training
    dataset. In any case, this is an impressive feat: we generated a fake image with
    an impossible situation, and the captioning network was flexible enough to get
    the subject right.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就只是为了好玩，让我们看看我们的CycleGAN是否也能愚弄这个神经对话2模型。让我们在数据文件夹中添加zebra.jpg图像并重新运行模型：“一群斑马站在田野上。”嗯，它正确地识别了动物，但它在图像中看到了不止一只斑马。显然，这不是网络曾经见过斑马的姿势，也没有见过斑马上有骑手（带有一些虚假的斑马图案）。此外，斑马很可能是以群体形式出现在训练数据集中，因此我们可能需要调查一些偏见。字幕网络也没有描述骑手。同样，这可能是同样的原因：网络在训练数据集中没有看到骑手骑在斑马上。无论如何，这是一个令人印象深刻的壮举：我们生成了一张带有不可能情景的假图像，而字幕网络足够灵活，能够正确地捕捉主题。
- en: We’d like to stress that something like this, which would have been extremely
    hard to achieve before the advent of deep learning, can be obtained with under
    a thousand lines of code, with a general-purpose architecture that knows nothing
    about horses or zebras, and a corpus of images and their descriptions (the MS
    COCO dataset, in this case). No hardcoded criterion or grammar--everything, including
    the sentence, emerges from patterns in the data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想强调的是，像这样的东西，在深度学习出现之前是极其难以实现的，现在可以用不到一千行代码，使用一个不知道关于马或斑马的通用架构，以及一组图像和它们的描述（在这种情况下是MS
    COCO数据集）来获得。没有硬编码的标准或语法--一切，包括句子，都是从数据中的模式中产生的。
- en: The network architecture in this last case was, in a way, more complex than
    the ones we saw earlier, as it includes two networks. One is recurrent, but it
    was built out of the same building blocks, all of which are provided by PyTorch.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，网络架构在某种程度上比我们之前看到的更复杂，因为它包括两个网络。其中一个是循环的，但是它是由PyTorch提供的相同构建块构建的。
- en: At the time of this writing, models such as these exist more as applied research
    or novelty projects, rather than something that has a well-defined, concrete use.
    The results, while promising, just aren’t good enough to use ... yet. With time
    (and additional training data), we should expect this class of models to be able
    to describe the world to people with vision impairment, transcribe scenes from
    video, and perform other similar tasks.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，这样的模型更多地存在于应用研究或新颖项目中，而不是具有明确定义的具体用途。尽管结果令人鼓舞，但还不足以使用...至少现在还不足够。随着时间的推移（和额外的训练数据），我们应该期望这类模型能够向视觉受损的人描述世界，从视频中转录场景，以及执行其他类似的任务。
- en: 2.4 Torch Hub
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 Torch Hub
- en: Pretrained models have been published since the early days of deep learning,
    but until PyTorch 1.0, there was no way to ensure that users would have a uniform
    interface to get them. TorchVision was a good example of a clean interface, as
    we saw earlier in this chapter; but other authors, as we have seen for CycleGAN
    and NeuralTalk2, chose different designs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习的早期就开始发布预训练模型，但直到PyTorch 1.0，没有办法确保用户可以获得统一的接口来获取它们。TorchVision是一个良好的接口示例，正如我们在本章前面看到的那样；但其他作者，正如我们在CycleGAN和神经对话2中看到的那样，选择了不同的设计。
- en: PyTorch 1.0 saw the introduction of Torch Hub, which is a mechanism through
    which authors can publish a model on GitHub, with or without pretrained weights,
    and expose it through an interface that PyTorch understands. This makes loading
    a pretrained model from a third party as easy as loading a TorchVision model.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 1.0引入了Torch Hub，这是一个机制，通过该机制，作者可以在GitHub上发布一个模型，带有或不带有预训练权重，并通过PyTorch理解的接口公开它。这使得从第三方加载预训练模型就像加载TorchVision模型一样简单。
- en: 'All it takes for an author to publish a model through the Torch Hub mechanism
    is to place a file named hubconf.py in the root directory of the GitHub repository.
    The file has a very simple structure:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过Torch Hub机制发布模型所需的全部工作就是在GitHub存储库的根目录中放置一个名为hubconf.py的文件。该文件具有非常简单的结构：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Optional list of modules the code depends on
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 代码依赖的可选模块列表
- en: ❷ One or more functions to be exposed to users as entry points for the repository.
    These functions should initialize models according to the arguments and return
    them
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个或多个要向用户公开作为存储库入口点的函数。这些函数应根据参数初始化模型并返回它们
- en: In our quest for interesting pretrained models, we can now search for GitHub
    repositories that include hubconf.py, and we’ll know right away that we can load
    them using the torch.hub module. Let’s see how this is done in practice. To do
    that, we’ll go back to TorchVision, because it provides a clean example of how
    to interact with Torch Hub.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们寻找有趣的预训练模型的过程中，现在我们可以搜索包含hubconf.py的GitHub存储库，我们会立即知道可以使用torch.hub模块加载它们。让我们看看实际操作是如何进行的。为此，我们将回到TorchVision，因为它提供了一个清晰的示例，展示了如何与Torch
    Hub交互。
- en: 'Let’s visit [https://github.com/pytorch/vision](https://github.com/pytorch/vision)
    and notice that it contains a hubconf.py file. Great, that checks. The first thing
    to do is to look in that file to see the entry points for the repo--we’ll need
    to specify them later. In the case of TorchVision, there are two: `resnet18` and
    `resnet50`. We already know what these do: they return an 18-layer and a 50-layer
    ResNet model, respectively. We also see that the entry-point functions include
    a `pretrained` keyword argument. If `True`, the returned models will be initialized
    with weights learned from ImageNet, as we saw earlier in the chapter.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们访问[https://github.com/pytorch/vision](https://github.com/pytorch/vision)，注意其中包含一个hubconf.py文件。很好，检查通过。首先要做的事情是查看该文件，看看存储库的入口点--我们稍后需要指定它们。在TorchVision的情况下，有两个：`resnet18`和`resnet50`。我们已经知道这些是做什么的：它们分别返回一个18层和一个50层的ResNet模型。我们还看到入口点函数包括一个`pretrained`关键字参数。如果为`True`，返回的模型将使用从ImageNet学习到的权重进行初始化，就像我们在本章前面看到的那样。
- en: 'Now we know the repo, the entry points, and one interesting keyword argument.
    That’s about all we need to load the model using torch.hub, without even cloning
    the repo. That’s right, PyTorch will handle that for us:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道存储库、入口点和一个有趣的关键字参数。这就是我们加载模型所需的全部内容，使用torch.hub，甚至无需克隆存储库。没错，PyTorch会为我们处理：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Name and branch of the GitHub repo
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ GitHub存储库的名称和分支
- en: ❷ Name of the entry-point function
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 入口点函数的名称
- en: ❸ Keyword argument
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 关键字参数
- en: This manages to download a snapshot of the master branch of the pytorch/vision
    repo, along with the weights, to a local directory (defaults to .torch/hub in
    our home directory) and run the `resnet18` entry-point function, which returns
    the instantiated model. Depending on the environment, Python may complain that
    there’s a module missing, like `PIL`. Torch Hub won’t install missing dependencies,
    but it will report them to us so that we can take action.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载pytorch/vision存储库的主分支的快照，以及权重，到本地目录（默认为我们主目录中的.torch/hub），并运行`resnet18`入口点函数，返回实例化的模型。根据环境的不同，Python可能会抱怨缺少模块，比如`PIL`。Torch
    Hub不会安装缺少的依赖项，但会向我们报告，以便我们采取行动。
- en: At this point, we can invoke the returned model with proper arguments to run
    a forward pass on it, the same way we did earlier. The nice part is that now every
    model published through this mechanism will be accessible to us using the same
    modalities, well beyond vision.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们可以使用适当的参数调用返回的模型，在其上运行前向传递，就像我们之前做的那样。好处在于，现在通过这种机制发布的每个模型都将以相同的方式对我们可用，远远超出视觉领域。
- en: Note that entry points are supposed to return models; but, strictly speaking,
    they are not forced to. For instance, we could have an entry point for transforming
    inputs and another one for turning the output probabilities into a text label.
    Or we could have an entry point for just the model, and another that includes
    the model along with the pre- and postprocessing steps. By leaving these options
    open, the PyTorch developers have provided the community with just enough standardization
    and a lot of flexibility. We’ll see what patterns will emerge from this opportunity.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，入口点应该返回模型；但严格来说，它们并不一定要这样做。例如，我们可以有一个用于转换输入的入口点，另一个用于将输出概率转换为文本标签。或者我们可以有一个仅包含模型的入口点，另一个包含模型以及预处理和后处理步骤。通过保持这些选项开放，PyTorch开发人员为社区提供了足够的标准化和很大的灵活性。我们将看到从这个机会中会出现什么样的模式。
- en: Torch Hub is quite new at the time of writing, and there are only a few models
    published this way. We can get at them by Googling “github.com hubconf.py.” Hopefully
    the list will grow in the future, as more authors share their models through this
    channel.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Torch Hub还很新，只有少数模型是以这种方式发布的。我们可以通过谷歌搜索“github.com hubconf.py”来找到它们。希望在未来列表会增长，因为更多作者通过这个渠道分享他们的模型。
- en: 2.5 Conclusion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 结论
- en: We hope this was a fun chapter. We took some time to play with models created
    with PyTorch, which were optimized to carry out specific tasks. In fact, the more
    enterprising of us could already put one of these models behind a web server and
    start a business, sharing the profits with the original authors![⁴](#pgfId-1024180)
    Once we learn how these models are built, we will also be able to use the knowledge
    we gained here to download a pretrained model and quickly fine-tune it on a slightly
    different task.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这是一个有趣的章节。我们花了一些时间玩弄用PyTorch创建的模型，这些模型经过优化，可以执行特定任务。事实上，我们中更有进取心的人已经可以将其中一个模型放在Web服务器后面，并开始一项业务，与原始作者分享利润！一旦我们了解了这些模型是如何构建的，我们还��能够利用在这里获得的知识下载一个预训练模型，并快速对稍有不同的任务进行微调。
- en: We will also see how building models that deal with different problems on different
    kinds of data can be done using the same building blocks. One thing that PyTorch
    does particularly right is providing those building blocks in the form of an essential
    toolset--PyTorch is not a very large library from an API perspective, especially
    when compared with other deep learning frameworks.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将看到如何使用相同的构建块构建处理不同问题和不同类型数据的模型。PyTorch做得特别好的一件事是以基本工具集的形式提供这些构建块--从API的角度来看，PyTorch并不是一个非常庞大的库，特别是与其他深度学习框架相比。
- en: This book does not focus on going through the complete PyTorch API or reviewing
    deep learning architectures; rather, we will build hands-on knowledge of these
    building blocks. This way, you will be able to consume the excellent online documentation
    and repositories on top of a solid foundation.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不专注于完整地介绍PyTorch API或审查深度学习架构；相反，我们将建立对这些构建块���实践知识。这样，您将能够在坚实的基础上消化优秀的在线文档和存储库。
- en: 'Starting with the next chapter, we’ll embark on a journey that will enable
    us to teach our computer skills like those described in this chapter from scratch,
    using PyTorch. We’ll also learn that starting from a pretrained network and fine-tuning
    it on new data, without starting from scratch, is an effective way to solve problems
    when the data points we have are not particularly numerous. This is one further
    reason pretrained networks are an important tool for deep learning practitioners
    to have. Time to learn about the first fundamental building block: tensors.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 从下一章开始，我们将踏上一段旅程，使我们能够从头开始教授计算机技能，使用PyTorch。我们还将了解，从预训练网络开始，并在新数据上进行微调，而不是从头开始，是解决问题的有效方法，特别是当我们拥有的数据点并不是特别多时。这是预训练网络是深度学习从业者必备的重要工具的另一个原因。是时候了解第一个基本构建块了：张量。
- en: 2.6 Exercises
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 练习
- en: Feed the image of the golden retriever into the horse-to-zebra model.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将金毛猎犬的图像输入到马到斑马模型中。
- en: What do you need to do to the image to prepare it?
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你需要对图像进行哪些处理？
- en: What does the output look like?
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出是什么样的？
- en: Search GitHub for projects that provide a hubconf.py file.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在GitHub上搜索提供hubconf.py文件的项目。
- en: How many repositories are returned?
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回了多少个存储库？
- en: Find an interesting-looking project with a hubconf.py. Can you understand the
    purpose of the project from the documentation?
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找一个带有hubconf.py的看起来有趣的项目。你能从文档中理解项目的目的吗？
- en: Bookmark the project, and come back after you’ve finished this book. Can you
    understand the implementation?
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收藏这个项目，在完成本书后回来。你能理解实现吗？
- en: 2.7 Summary
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 总结
- en: A pretrained network is a model that has already been trained on a dataset.
    Such networks can typically produce useful results immediately after loading the
    network parameters.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预训练网络是已经在数据集上训练过的模型。这样的网络通常在加载网络参数后可以立即产生有用的结果。
- en: By knowing how to use a pretrained model, we can integrate a neural network
    into a project without having to design or train it.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过了解如何使用预训练模型，我们可以将神经网络集成到项目中，而无需设计或训练它。
- en: AlexNet and ResNet are two deep convolutional networks that set new benchmarks
    for image recognition in the years they were released.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AlexNet和ResNet是两个深度卷积网络，在它们发布的年份为图像识别设立了新的基准。
- en: Generative adversarial networks (GANs) have two parts--the generator and the
    discriminator--that work together to produce output indistinguishable from authentic
    items.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）有两部分--生成器和判别器--它们共同工作以产生与真实物品无法区分的输出。
- en: CycleGAN uses an architecture that supports converting back and forth between
    two different classes of images.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CycleGAN使用一种支持在两种不同类别的图像之间进行转换的架构。
- en: NeuralTalk2 uses a hybrid model architecture to consume an image and produce
    a text description of the image.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NeuralTalk2使用混合模型架构来消耗图像并生成图像的文本描述。
- en: Torch Hub is a standardized way to load models and weights from any project
    with an appropriate hubconf.py file.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torch Hub是一种标准化的方式，可以从具有适当的hubconf.py文件的任何项目中加载模型和权重。
- en: '* * *'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^(1.)A relevant example is described in the Vox article “Jordan Peele’s simulated
    Obama PSA is a double-edged warning against fake news,” by Aja Romano; [http://mng.bz/dxBz
    (warning: coarse language](http://mng.bz/dxBz%20(warning:%20coarse%20language)).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ^(1.)Vox文章“乔丹·皮尔模拟奥巴马公益广告是对假新闻的双刃警告”中描述了一个相关例子，作者是阿贾·罗曼诺；[http://mng.bz/dxBz
    (警告：粗俗语言](http://mng.bz/dxBz%20(warning:%20coarse%20language)).
- en: ^(2.)We maintain a clone of the code at [https://github.com/deep-learning-with-pytorch/ImageCaptioning
    .pytorch](https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ^(2.)我们在[https://github.com/deep-learning-with-pytorch/ImageCaptioning .pytorch](https://github.com/deep-learning-with-pytorch/ImageCaptioning.pytorch)上维护代码的克隆。
- en: ^(3.)Andrej Karpathy and Li Fei-Fei, “Deep Visual-Semantic Alignments for Generating
    Image Descriptions,” [https://cs.stanford.edu/people/karpathy/cvpr2015.pdf](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ^(3.)Andrej Karpathy和Li Fei-Fei，“用于生成图像描述的深度视觉语义对齐”，[https://cs.stanford.edu/people/karpathy/cvpr2015.pdf](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf).
- en: ^(4.)Contact the publisher for franchise opportunities!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ^(4.)联系出版商了解特许经营机会！
